<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
library(ggplot2)
library(randomForest)
library(colorspace)

library(mlr)
library(ElemStatLearn) # for spam data set
library(mboost)
library(mlbench)

@

<<size = "scriptsize", include=FALSE>>=
source("rsrc/cim1_optim.R")
@

<<setup, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{Boosting - Advanced}
\lecture{Introduction to Machine Learning}

\sloppy
\section{Gradient boosting and trees}

\begin{vbframe}{Gradient boosting and trees}

Trees are mainly used as base learners for gradient boosting in ML.
A great deal of research has been done on this combination so far, and it often provides the best results.

\begin{blocki}{Reminder: Advantages of trees}
\item No problems with categorical features.
\item No problems with outliers in feature values.
\item No problems with missing values.
\item No problems with monotone transformations of features.
\item Trees (and stumps!) can be fitted quickly, even for large $n$.
\item Trees have a simple built-in type of variable selection.
%\item Interpretation of Trees is rather easy.
\end{blocki}
Gradient boosted trees retains all of them, and strongly improves the trees' predictive power.
Furthermore, it is possible to adapt gradient boosting especially to tree learners.

\framebreak

One can write a tree as: $ b(\xb) = \sum_{t=1}^{T} c_t \I(\xb \in R_t) $,
where $R_t$ are the terminal regions and $c_t$ the corresponding means.

\lz

This special additive structure can be exploited by boosting:  %Finished here

\begin{align*}
  \fm(\xb) &= \fmd(\xb) +  \betam \bmm(\xb) \\
         &= \fmd(\xb) +  \betam \sum_{t=1}^{\Tm} \ctm \I(\xb \in \Rtm)
\end{align*}

Actually, we do not have to find $\ctm$ and $\betam$ in two separate steps
(fitting against pseudo-residuals, then line search).
Also note that the $\ctm$ will not really be loss-optimal as we used squared error loss
to fit them against the pseudo residuals.

\framebreak

What we will do instead, is:

$$ \fm(\xb) = \fmd(\xb) +  \sum_{j=1}^{\Tm} \ctmt \I(\xb \in \Rtm) $$

We now induce the structure of the tree with respect to squared error loss,
but then determine / change all $\ctmt$ individually and directly L-optimally:


\vspace{-0.2cm}

$$ \ctmt = \argmin_{c} \sum_{\xi \in \Rtm} L(\yi, \fmd(\xi) + c) $$

\vspace{-0.5cm}

\begin{center}

\includegraphics[width=0.38\textwidth]{figure_man/gbm_leaf_adjustment.pdf}

\end{center}

\framebreak

\input{algorithms/gradient_tree_boosting_algorithm.tex}

\end{vbframe}

\begin{vbframe}{Binary classification}


For $\Yspace = \{0, 1\}$, we simply have to select an appropriate loss function, so let's
use binomial loss as in logistic regression:

$$ \Lxy = - y \fx + ln(1 + \exp(\fx))$$

Then,

\vspace{-0.5cm}

\begin{align*}
\tilde{r} &=-\fp{\Lxy}{\fx} \\
&= y - \frac{\exp(\fx)}{1 + \exp(\fx)} \\
&= y - \frac{1}{1 + \exp(-\fx)} = y - s(\fx).
\end{align*}

Here, $s(\fx)$ is the logistic sigmoid function, applied to a scoring model.
So effectively the pseudo residuals are $y - \pix$.

Through $\pix = s(\fx)$ we can also estimate posterior probabilities.

\framebreak
%
% \lz
%
% As already mentioned, AdaBoost performs forward stagewise additive modelling
% under exponential loss.
%
% \lz
%
% In principle we could also have used the exponential loss for classification with gradient boosting.
% In practice there is no big difference, although binomial loss makes a bit more sense from a theoretical
% (maximum likelihood) perspective.
%
% \lz
%
% \begin{table}
% \centering
% \begin{tabular}{lll}
% &Binomial&Exponential\\
% $y \in \{-1, 1\}$&$\ln(1+\exp(-2y\fx))$&$\exp(-y\fx)$\\
% $y \in \{0, 1\}$&$-y\fx+\ln(1+\exp(\fx))$&$\exp(-(2y-1)\fx)$
% \end{tabular}
% \end{table}


\end{vbframe}

\begin{vbframe}{Multi-class problems}

We proceed as in softmax regression and model a categorical distribution, with multinomial loss.
For $\Yspace = \{1, \ldots, g\}$, we create $g$ discriminant functions $\fxk$, one for each class, each an additive model of trees.

We define the $\pi_k(\xb)$ through the softmax function:
$$ \pikx = s_k(f_1(\xb), \ldots, f_g(\xb)) = \exp(\fxk) / \sum_{j=1}^g \exp(f_j(\xb)) $$

Multinomial loss $L$:
$$ L(y, f_1(\xb), \ldots f_g(\xb)) = - \sumkg \I(y = k) \ln \pikx $$

And for the derivative the following holds:
$$-\fp{L(y_k, f_1(\xb), \ldots, f_g(\xb))}{\fxk} =  \I(y = k) - \pikx $$

\framebreak

\lz

Determining the tree structure by squared-error-loss works just like before in the 2 class problem.

\lz

In the estimation of the $c$ though, all the models depend on each other because of the definition
of $L$. Optimizing this is more difficult, so we will skip the details and just present the results.

\lz

The estimated class for $x$ is of course exactly the $k$ for which $\pikx$ is maximal.

\framebreak

\input{algorithms/gradient_boosting_for_k_classification.tex}

\framebreak

\begin{blocki}{Derivation of the algorithm:}

  \item from Friedman, J. H. - Greedy Function Approximation: A Gradient Boosting Machine (1999)

  \item In each iteration $m$ we calculate the pseudo residuals
        $$\rmi_k = \I(\yi = k) - \pi_k^{[m-1]}(\xi),$$
        where $\pi_k^{[m-1]}(\xi)$ is derived from $f^{[m-1]}(\mathbf{x})$

  \item Thus, $g$ trees are induced at each iteration $m$ to predict the corresponding current pseudo residuals for each class on the probability scale.

  \item Each of these trees has $T$ terminal nodes with corresponding regions $R_{tk}^{[m]}$.

\end{blocki}

\framebreak

\begin{itemize}

  \item The model updates $\hat{c}_{tk}^{[m]}$ corresponding to these regions are the solution to

  $$ \hat{c}_{tk}^{[m]} = \argmin_{c} \sum_{i=1}^n \sum_{k=1}^g L \left( \yi_k, f^{[m-1]}(\mathbf{x}^{(i)}) + \sum_{t=1}^T \hat{c}_{tk} \I\left(\xi \in R_{t}^{[m]}\right) \right)$$

  where $L$ is the multinomial loss function $L(y, f_1(\xb), \ldots f_g(\xb)) = - \sumkg \I(y = k) \ln \pikx$
  and $\pikx = \frac{\exp(f_k(\xb))}{\sum_j \exp(f_j(\xb))}$ as before.

  \item This has no closed form solution and additionally, the regions corresponding to the different class tress overlap, so that the solution does not reduce to a separate calculation within each region of each tree.

\end{itemize}

\framebreak

\begin{itemize}

  \item Hence, we approximate the solution with a single Newton-Raphson step, using a diagonal approximation to the Hessian.

  \item This decomposes the problem into a separate calculation for each terminal node of each tree.

  \item The result is

  $$\hat{c}_{tk}^{[m]} =
      \frac{g-1}{g}\frac{\sum_{\xi \in R_{tk}^{[m]}} \rmi_k}{\sum_{\xi \in R_{tk}^{[m]}} \left|\rmi_k\right|\left(1 - \left|\rmi_k\right|\right)}$$

  \item The update is then done by
  $$\hat{f}_k^{[m]}(x) = \hat{f}_k^{[m-1]}(x) + \sum_t \hat{c}_{tk}^{[m]} \I\left(x \in R_{tk}^{[m]}\right)$$

\end{itemize}

\end{vbframe}


\begin{vbframe}{Additional information}

By choosing a suitable loss function it is also possible to model a large number of different problem domains
\begin{itemize}
  \item Regression
  \item (Multi-class) Classification
  \item Count data
  \item Survival data
  \item Ordinal data
  \item Quantile regression
  \item Ranking problems
  \item ...
\end{itemize}

\lz

% Boosting is closely related to L1 regularization.

% \lz

Different base learners increase flexibility (see componentwise gradient boosting).
If we model only individual variables, the resulting regularized variable selection
is closely related to L1 regularization.

\framebreak

For example, using the pinball loss in boosting
$$
L(y, f(\xv)) = \left\{
\begin{array}{lc}
(1 - \alpha)(f(\xv) - y), & \text{if}\ y < f(\xv) \\
\alpha(y - f(\xv)),       & \text{if}\ y \geq f(\xv)
\end{array}
\right.
$$
models the $\alpha$-quantiles:

\begin{center}
\includegraphics[scale=0.5]{figure_man/quantile_boosting.png}
\end{center}

\framebreak

The AdaBoost fit has the structure of an additive model with \enquote{basis functions} $\bmm (x)$.

\lz

It can be shown (see Hastie et al. 2009, chapter 10) that AdaBoost corresponds to minimizing the empirical risk in each iteration $m$ using the {\em exponential} loss function:
\begin{align*}
  L(y, \fmh(\mathbf{x}))    &= \exp\left(-y\fmh(\mathbf{x})\right) \\
  \riske(\fmh)              &= \sumin L(\yi, \fmh(\xi)) \\
                            &= \sumin L(\yi, \fmdh(\xi) + \beta b(\xi))
\end{align*}


% \begin{align*}
%   \sum_{i=1}^n \exp\left(-\yi \cdot \left(\beta b\left(\xi\right)
%   + \fmdh\left(\xi\right)\right)\right),
% \end{align*}
with minimizing over $\beta$ and $b$ and where $\fmdh$ is the boosting fit in iteration $m-1$.

% \framebreak

% AdaBoost is the empirical equivalent to the forward piecewise solution of the minimization problem

% \begin{align*}
%   \text{arg} \min_{f} \E_{y|x}( \exp (- y \cdot \fx))\ .
% \end{align*}

% \lz

% Therefore, the boosting fit is an estimate of function
% \begin{align*}
%   f^*(x) = 0.5 \cdot \log \left( \frac{\text{P} (y = 1 | x)}
%   {\text{P} (y = -1 | x)}\right) \ ,
% \end{align*}
% which solves the former problem theoretically.

% \lz

% Obvious idea: generalization on other loss functions, use of alternative basis methods.

\end{vbframe}
\begin{vbframe}{Take home message}
Gradient boosting is a statistical reinterpretation of the older AdaBoost algorithm.

\lz

Base learners are added in a \enquote{greedy} fashion, so that they point in the direction of the negative gradient of the empirical risk.

\lz

Regression base learners are fitted even for classification problems.

\lz

Often the base learners are (shallow) trees, but arbitrary base learners are possible.

\lz

The method can be adjusted flexibly by changing the loss function, as long as it's differentiable.

\lz

Methods to evaluate variable importance and to do variable selection exist.

\end{vbframe}

\begin{vbframe}{Gradient Boosting Playground}
\begin{center}

\includegraphics[width=0.7\textwidth]{figure_man/gbm_playground.png}

\href{http://arogozhnikov.github.io/2016/07/05/gradient_boosting_playground.html}{\beamergotobutton{Open in browser.}}

\end{center}
\end{vbframe}

\begin{vbframe}{mlrPlayground}
\begin{center}

\includegraphics[width=\linewidth]{figure_man/mlrplayground_welcome.png}

\href{https://compstat-lmu.shinyapps.io/mlrPlayground/}{\beamergotobutton{Open in browser.}}

\end{center}
\end{vbframe}
% \begin{vbframe}{RF vs AdaBoost vs GBM}

% Recall the Spirals data from mlbench. We are using stumps again. Performance is measured with 3 times repeated 10CV.

%<<fig.height=4, eval = FALSE, include = FALSE>>=
% load("rsrc/comparing_methods.RData")

%ggplot(data = ggd2[ggd2$learner %in% c("rf", "ada", "gbm", "rf.fulltree"), ],
%  aes(x = iters, y = mmce, col = learner)) + geom_line()
%@

% \end{vbframe}

\endlecture
