<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
library(ggplot2)
library(randomForest)
library(colorspace)

library(mlr)
library(ElemStatLearn) # for spam data set
library(mboost)
library(mlbench)

@

<<size = "scriptsize", include=FALSE>>=
source("rsrc/cim1_optim.R")
@

<<setup, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{Boosting - Advanced}
\lecture{Introduction to Machine Learning}

\sloppy
\section{Gradient boosting and trees}

\begin{vbframe}{Gradient boosting and trees}

Trees are mainly used as base learners for gradient boosting in ML.
A great deal of research has been done on this combination so far, and it often provides the best results.

\begin{blocki}{Reminder: Advantages of trees}
\item No problems with categorical features.
\item No problems with outliers in feature values.
\item No problems with missing values.
\item No problems with monotone transformations of features.
\item Trees (and stumps!) can be fitted quickly, even for large $n$.
\item Trees have a simple built-in type of variable selection.
%\item Interpretation of Trees is rather easy.
\end{blocki}
Gradient boosted trees retains all of them, and strongly improves the trees' predictive power.
Furthermore, it is possible to adapt gradient boosting especially to tree learners.

\framebreak

One can write a tree as: $ b(\xb) = \sum_{t=1}^{T} c_t \I(\xb \in R_t) $,
where $R_t$ are the terminal regions and $c_t$ the corresponding means.

\lz

This special additive structure can be exploited by boosting:  %Finished here

\begin{align*}
  \fm(\xb) &= \fmd(\xb) +  \betam \bmm(\xb) \\
         &= \fmd(\xb) +  \betam \sum_{t=1}^{\Tm} \ctm \I(\xb \in \Rtm)
\end{align*}

Actually, we do not have to find $\ctm$ and $\betam$ in two separate steps
(fitting against pseudo-residuals, then line search).
Also note that the $\ctm$ will not really be loss-optimal as we used squared error loss
to fit them against the pseudo residuals.

\framebreak

What we will do instead, is:

$$ \fm(\xb) = \fmd(\xb) +  \sum_{j=1}^{\Tm} \ctmt \I(\xb \in \Rtm) $$

We now induce the structure of the tree with respect to squared error loss,
but then determine / change all $\ctmt$ individually and directly L-optimally:


\vspace{-0.2cm}

$$ \ctmt = \argmin_{c} \sum_{\xi \in \Rtm} L(\yi, \fmd(\xi) + c) $$

\vspace{-0.5cm}

\begin{center}

\includegraphics[width=0.38\textwidth]{figure_man/gbm_leaf_adjustment.pdf}

\end{center}

\framebreak

\input{algorithms/gradient_tree_boosting_algorithm.tex}

\end{vbframe}

\begin{vbframe}{Binary classification}


For $\Yspace = \{0, 1\}$, we simply have to select an appropriate loss function, so let's
use binomial loss as in logistic regression:

$$ \Lxy = - y \fx + ln(1 + \exp(\fx))$$

Then,

\vspace{-0.5cm}

\begin{align*}
\tilde{r} &=-\fp{\Lxy}{\fx} \\
&= y - \frac{\exp(\fx)}{1 + \exp(\fx)} \\
&= y - \frac{1}{1 + \exp(-\fx)} = y - s(\fx).
\end{align*}

Here, $s(\fx)$ is the logistic sigmoid function, applied to a scoring model.
So effectively the pseudo residuals are $y - \pix$.

Through $\pix = s(\fx)$ we can also estimate posterior probabilities.

\framebreak
%
% \lz
%
% As already mentioned, AdaBoost performs forward stagewise additive modelling
% under exponential loss.
%
% \lz
%
% In principle we could also have used the exponential loss for classification with gradient boosting.
% In practice there is no big difference, although binomial loss makes a bit more sense from a theoretical
% (maximum likelihood) perspective.
%
% \lz
%
% \begin{table}
% \centering
% \begin{tabular}{lll}
% &Binomial&Exponential\\
% $y \in \{-1, 1\}$&$\ln(1+\exp(-2y\fx))$&$\exp(-y\fx)$\\
% $y \in \{0, 1\}$&$-y\fx+\ln(1+\exp(\fx))$&$\exp(-(2y-1)\fx)$
% \end{tabular}
% \end{table}


\end{vbframe}

\begin{vbframe}{Multi-class problems}

We proceed as in softmax regression and model a categorical distribution, with multinomial loss.
For $\Yspace = \{1, \ldots, g\}$, we create $g$ discriminant functions $\fxk$, one for each class, each an additive model of trees.

We define the $\pi_k(\xb)$ through the softmax function:
$$ \pikx = s_k(f_1(\xb), \ldots, f_g(\xb)) = \exp(\fxk) / \sum_{j=1}^g \exp(f_j(\xb)) $$

Multinomial loss $L$:
$$ L(y, f_1(\xb), \ldots f_g(\xb)) = - \sumkg \I(y = k) \ln \pikx $$

And for the derivative the following holds:
$$-\fp{L(y_k, f_1(\xb), \ldots, f_g(\xb))}{\fxk} =  \I(y = k) - \pikx $$

\framebreak

\lz

Determining the tree structure by squared-error-loss works just like before in the 2 class problem.

\lz

In the estimation of the $c$ though, all the models depend on each other because of the definition
of $L$. Optimizing this is more difficult, so we will skip the details and just present the results.

\lz

The estimated class for $x$ is of course exactly the $k$ for which $\pikx$ is maximal.

\framebreak

\input{algorithms/gradient_boosting_for_k_classification.tex}

\framebreak

\begin{blocki}{Derivation of the algorithm:}

  \item from Friedman, J. H. - Greedy Function Approximation: A Gradient Boosting Machine (1999)

  \item In each iteration $m$ we calculate the pseudo residuals
        $$\rmi_k = \I(\yi = k) - \pi_k^{[m-1]}(\xi),$$
        where $\pi_k^{[m-1]}(\xi)$ is derived from $f^{[m-1]}(\mathbf{x})$

  \item Thus, $g$ trees are induced at each iteration $m$ to predict the corresponding current pseudo residuals for each class on the probability scale.

  \item Each of these trees has $T$ terminal nodes with corresponding regions $R_{tk}^{[m]}$.

\end{blocki}

\framebreak

\begin{itemize}

  \item The model updates $\hat{c}_{tk}^{[m]}$ corresponding to these regions are the solution to

  $$ \hat{c}_{tk}^{[m]} = \argmin_{c} \sum_{i=1}^n \sum_{k=1}^g L \left( \yi_k, f^{[m-1]}(\mathbf{x}^{(i)}) + \sum_{t=1}^T \hat{c}_{tk} \I\left(\xi \in R_{t}^{[m]}\right) \right)$$

  where $L$ is the multinomial loss function $L(y, f_1(\xb), \ldots f_g(\xb)) = - \sumkg \I(y = k) \ln \pikx$
  and $\pikx = \frac{\exp(f_k(\xb))}{\sum_j \exp(f_j(\xb))}$ as before.

  \item This has no closed form solution and additionally, the regions corresponding to the different class tress overlap, so that the solution does not reduce to a separate calculation within each region of each tree.

\end{itemize}

\framebreak

\begin{itemize}

  \item Hence, we approximate the solution with a single Newton-Raphson step, using a diagonal approximation to the Hessian.

  \item This decomposes the problem into a separate calculation for each terminal node of each tree.

  \item The result is

  $$\hat{c}_{tk}^{[m]} =
      \frac{g-1}{g}\frac{\sum_{\xi \in R_{tk}^{[m]}} \rmi_k}{\sum_{\xi \in R_{tk}^{[m]}} \left|\rmi_k\right|\left(1 - \left|\rmi_k\right|\right)}$$

  \item The update is then done by
  $$\hat{f}_k^{[m]}(x) = \hat{f}_k^{[m-1]}(x) + \sum_t \hat{c}_{tk}^{[m]} \I\left(x \in R_{tk}^{[m]}\right)$$

\end{itemize}

\end{vbframe}

\begin{vbframe}{Regularization and shrinkage}

If GB runs for a long number of iterations, it can overfit due to its aggressive loss
minimization.

\begin{blocki}{Options for regularization}
\item Limit the number of boosting iterations $M$ (\enquote{early stopping}), i.e., limit the number of additive components.
\item Limit the depth of the trees. This can also be interpreted as choosing the order of interaction.
\item Shorten the step length $\betam$ in each iteration.
\end{blocki}

The latter is achieved by multiplying $\betam$ with a small $\nu \in (0,1]$:
$$ \fm(\xb) = \fmd(\xb) + \nu \betam b(\xb, \thetam) $$

$\nu$ is called \textbf{shrinkage parameter} or \textbf{learning rate}.


\framebreak

Obviously, the optimal values for $M$ and $\nu$ strongly depend on each other:
By increasing $M$ one can use a smaller value for $\nu$ and vice versa.

\lz

In practice it is often recommended to choose $\nu$ quite small and choose $M$ by cross-validation.

\lz

It's probably best to tune all three parameters jointly based on the training data
via cross-validation or a related method.

\end{vbframe}

\begin{vbframe}{Stochastic gradient boosting}

This is a minor modification to boosting to incorporate the advantages of bagging into the method.
The idea was formulated quite early by Breiman.

\lz

Instead of fitting on all the data points, a random subsample is drawn in each iteration.

\lz

Especially for small training sets, this simple modification often leads to
significant empirical improvements.
How large the improvements are depends on data structure, size of the data set,
base learner and size of the subsamples (so this is another tuning parameter).


\end{vbframe}


%\begin{vbframe}{Variable importance}

%As for random forests, we can construct a variable importance measure to help
%interpret the model.

%\lz
%For a regression tree $b$, we can define such a measure $I^2_j(b)$ as the sum over the reduction of
%impurity ($\rightarrow$ reduction of variance) at all inner knots where the tree splits with respect to feature $x_j$.

%\framebreak

%For an additive boosting model one just takes

%$$ I^2_j = \frac{1}{M} \sum_{m=1}^M I^2_j(\bmm) $$

%%From those importance values we take the squared root and scale them so that the most important feature gets the value 100.
%To get the relative influence measures, the resulting $I^2_j, j = 1, \dots, p$ are scaled so that they sum to 1.

%\lz
%For a $g$-class problem one has $g$

%$$ \fxk = \sum_{m=1}^M b_k^{[m]}(x), \quad
%  I^2_{jk} = \frac{1}{M} \sum_{m=1}^M I^2_j(b_k^{[m]}) $$

%$$ I^2_j = \frac{1}{g} \sum_{k=1}^g I^2_{jk} $$

%\end{vbframe}


\begin{vbframe}{Example: Spam detection}

% The data set we will examine briefly in the following was collected at the Hewlett Packard laboratories
% to train a personalized spam mail detector.

% \lz

% It contains data of 4601 emails. 2788 mails were regular mails and 1813 were spam.
% There are 57 numerical predictors available measuring e.g. the frequency of the most frequent words
% and special characters as well as runlengths of words in all capitals.

% \lz

% We use the R package \pkg{gbm}, which implements the introduced version of gradient boosting.

% \framebreak

% <<gbm-spam-example, eval = FALSE, echo = TRUE>>=
% library(gbm)
% data(spam, package = "ElemStatLearn")
% spam$spam = as.numeric(spam$spam) - 1 # gbm requires target to be 0/1
% gbm(spam ~ ., data = spam,
%   distribution = "bernoulli", # classification
%   n.trees = 100, # M = 100
%   interaction.depth = 2, # max. tree depth = 2
%   shrinkage = 0.001, # nu = 0.001
% )
% @
%
% \lz

We fit a gradient boosting model for different parameter values

\begin{table}[]
\centering
\begin{tabular}{l|c}
Parameter name      & Values                         \\
\hline
distribution        & Bernoulli (for classification) \\
shrinkage $\nu$     & ${0.001, 0.01, 0.1}$           \\
number of trees $M$ & $[0,\dots,20000]$              \\
max. tree depth     & ${1,4,7,10,13,16}$
\end{tabular}
\end{table}

We observe the error on a separate test set to find the optimal parameters.


\framebreak

Misclassification rate for different hyperparameter settings (shrinkage and maximum tree depth) of gradient boosting:

<<gbm-spam-results, fig.height=4.5>>=
load("rsrc/gbm_spam_results_long.RData")

ggd = error.df
ggd$shrinkage = as.factor(ggd$shrinkage)
ggplot(data = ggd, aes(x = M, y = err, col = max.tree.depth)) +
  geom_line(alpha= .8) + facet_grid(~ shrinkage, labeller = label_both) + #coord_cartesian(ylim = c(0, 0.20)) +
  ylab("mmce (test data)") + scale_x_continuous("iterations M [1000]",
    c(1, 5, 10, 15, 20) * 1e3,
    labels = c("1", "5", "10", "15", "20")) +
  scale_y_continuous(trans = "log2")
@

% \begin{figure}
%   \includegraphics[width=10cm, height=6cm]{figure_man/gbm_spam_effects.pdf}
% \end{figure}

% \framebreak

% \begin{figure}
  % \includegraphics[width=8cm]{figure_man/gbm_spam_imp_ggplot.pdf}
  % \caption{\footnotesize Variable Importance for model with $\nu = 0.1, M = 1380$ and tree depth of $4$.}
% \end{figure}

% \framebreak
<<include=FALSE, eval=FALSE>>=
gbm.perf(mod$learner.model, method = "OOB")
@

% \begin{figure}
%  \includegraphics[width=6cm]{figure_man/gbm_spam_gbmperf.pdf}
%  \caption{\footnotesize Deviance}
% \end{figure}

% \framebreak
<<include=FALSE, eval=FALSE>>=
plot.gbm(mod$learner.model, 52)
plot.gbm(mod$learner.model, 25)
@
%
% \begin{figure}
%   \includegraphics[width=8cm, height=5cm]{figure_man/gbm_spam_partdep.pdf}
%   \caption{\footnotesize Partial Dependency Plot for 2 important features.
%   Plotted is f in dependency of one feature, if all other features are integrated over.}
% \end{figure}

\end{vbframe}

\begin{vbframe}{Additional information}

By choosing a suitable loss function it is also possible to model a large number of different problem domains
\begin{itemize}
  \item Regression
  \item (Multi-class) Classification
  \item Count data
  \item Survival data
  \item Ordinal data
  \item Quantile regression
  \item Ranking problems
  \item ...
\end{itemize}

\lz

% Boosting is closely related to L1 regularization.

% \lz

Different base learners increase flexibility (see componentwise gradient boosting).
If we model only individual variables, the resulting regularized variable selection
is closely related to L1 regularization.

\framebreak

For example, using the pinball loss in boosting
$$
L(y, f(\xv)) = \left\{
\begin{array}{lc}
(1 - \alpha)(f(\xv) - y), & \text{if}\ y < f(\xv) \\
\alpha(y - f(\xv)),       & \text{if}\ y \geq f(\xv)
\end{array}
\right.
$$
models the $\alpha$-quantiles:

\begin{center}
\includegraphics[scale=0.5]{figure_man/quantile_boosting.png}
\end{center}

\framebreak

The AdaBoost fit has the structure of an additive model with \enquote{basis functions} $\bmm (x)$.

\lz

It can be shown (see Hastie et al. 2009, chapter 10) that AdaBoost corresponds to minimizing the empirical risk in each iteration $m$ using the {\em exponential} loss function:
\begin{align*}
  L(y, \fmh(\mathbf{x}))    &= \exp\left(-y\fmh(\mathbf{x})\right) \\
  \riske(\fmh)              &= \sumin L(\yi, \fmh(\xi)) \\
                            &= \sumin L(\yi, \fmdh(\xi) + \beta b(\xi))
\end{align*}


% \begin{align*}
%   \sum_{i=1}^n \exp\left(-\yi \cdot \left(\beta b\left(\xi\right)
%   + \fmdh\left(\xi\right)\right)\right),
% \end{align*}
with minimizing over $\beta$ and $b$ and where $\fmdh$ is the boosting fit in iteration $m-1$.

% \framebreak

% AdaBoost is the empirical equivalent to the forward piecewise solution of the minimization problem

% \begin{align*}
%   \text{arg} \min_{f} \E_{y|x}( \exp (- y \cdot \fx))\ .
% \end{align*}

% \lz

% Therefore, the boosting fit is an estimate of function
% \begin{align*}
%   f^*(x) = 0.5 \cdot \log \left( \frac{\text{P} (y = 1 | x)}
%   {\text{P} (y = -1 | x)}\right) \ ,
% \end{align*}
% which solves the former problem theoretically.

% \lz

% Obvious idea: generalization on other loss functions, use of alternative basis methods.

\end{vbframe}
\begin{vbframe}{Take home message}
Gradient boosting is a statistical reinterpretation of the older AdaBoost algorithm.

\lz

Base learners are added in a \enquote{greedy} fashion, so that they point in the direction of the negative gradient of the empirical risk.

\lz

Regression base learners are fitted even for classification problems.

\lz

Often the base learners are (shallow) trees, but arbitrary base learners are possible.

\lz

The method can be adjusted flexibly by changing the loss function, as long as it's differentiable.

\lz

Methods to evaluate variable importance and to do variable selection exist.

\end{vbframe}

\begin{vbframe}{Gradient Boosting Playground}
\begin{center}

\includegraphics[width=0.7\textwidth]{figure_man/gbm_playground.png}

\href{http://arogozhnikov.github.io/2016/07/05/gradient_boosting_playground.html}{\beamergotobutton{Open in browser.}}

\end{center}
\end{vbframe}

\begin{vbframe}{mlrPlayground}
\begin{center}

\includegraphics[width=\linewidth]{figure_man/mlrplayground_welcome.png}

\href{https://compstat-lmu.shinyapps.io/mlrPlayground/}{\beamergotobutton{Open in browser.}}

\end{center}
\end{vbframe}
% \begin{vbframe}{RF vs AdaBoost vs GBM}

% Recall the Spirals data from mlbench. We are using stumps again. Performance is measured with 3 times repeated 10CV.

%<<fig.height=4, eval = FALSE, include = FALSE>>=
% load("rsrc/comparing_methods.RData")

%ggplot(data = ggd2[ggd2$learner %in% c("rf", "ada", "gbm", "rf.fulltree"), ],
%  aes(x = iters, y = mmce, col = learner)) + geom_line()
%@

% \end{vbframe}

\endlecture
