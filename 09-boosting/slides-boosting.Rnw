<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
library(ggplot2)
library(randomForest)
library(colorspace)

library(mlr)
library(ElemStatLearn) # for spam data set
library(mboost)
library(mlbench)

@

<<size = "scriptsize", include=FALSE>>=
source("rsrc/cim1_optim.R")
@

<<setup, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{Boosting}
\lecture{Introduction to Machine Learning}

\sloppy
\section{Introduction}

\begin{vbframe}{Introduction to boosting}
  \begin{itemize}
    \item
      Boosting is considered to be one of the most powerful learning ideas within the last twenty years.
    \item
      Originally designed for classification, but (especially gradient) boosting handles regression (and many others tasks) nowadays naturally.
    \item
      Homogeneous ensemble method (like bagging), but fundamentally different approach.
    \item
      {\bf Idea:} Take a weak classifier and sequentially apply it to modified versions of the training data.
    \item
      We will begin by describing an older, simpler boosting algorithm, designed for binary classification, the popular \enquote{AdaBoost}.
  \end{itemize}
\end{vbframe}

\begin{vbframe}{Boosting vs. Bagging}

% The general concept of boosting is a sequential fitting of weak learner on the error:
\begin{center}
\includegraphics[width=0.65\textwidth]{figure_man/bagging_vs_boosting.png}
\end{center}

% In bagging, the models are fitted parallel and not sequential.

\end{vbframe}

\begin{vbframe}{The boosting question}

The first boosting algorithm ever was in fact no algorithm for practical purposes but the proof in a theoretical discussion:

\lz

\enquote{Does the existence of a weak learner for a certain problem imply
the existence of a strong learner?} (Kearns 1988)

\lz

\begin{itemize}
\item \textbf{Weak learners} are defined as a prediction rule with a correct classification rate that is at least slightly better than random guessing (> 50\% accuracy).
\item We call a learner a \textbf{strong learner} \enquote{if there exists a polynomial-time algorithm that achieves low error with high confidence for all concepts in the class} (Schapire 1990)

\end{itemize}

In practice, it is typically easy to construct weak learners, but very difficult to get a strong one.

\end{vbframe}



\section{AdaBoost}

\begin{vbframe}{The boosting answer - AdaBoost}

Any weak (base) learner can be iteratively boosted to become also
a strong learner (Schapire and Freund 1990).
The proof of this ground-breaking idea generated the first boosting algorithm.

\begin{itemize}
  \item The \textbf{AdaBoost} (Adaptive Boosting) algorithm is a \textbf{boosting} method
    for binary classification by Freund and Schapire (1997).
  \item The base learner is sequentially applied to weighted training observations.
  \item After each base learner fit, currently misclassified observation receive a higher weight for
    the next iteration, so we focus more on the \enquote{harder} instances.
\end{itemize}

Leo Breiman (referring to the success of AdaBoost):

\enquote{Boosting is the best off-the-shelf classifier in the world.}

\framebreak

\begin{itemize}
  \item Assume target variable $y$ encoded as $\{-1,1\}$,
    and weak base learner (e.g. tree stumps) from a hypothesis space $\mathcal{B}$.
  \item Base learner models $\bmm$ are binary classifiers that map to $\Yspace = \{-1,+1\}$.
    We might sometimes write $b(\xb, \thetam)$, instead.
  \item Predictions from all base models $\bmm$ are combined to form:
    $$
    \fx = \sum_{m=1}^{M} \betam \bmm(\xb)
    $$
  \item Weights $\betam$ are computed by the boosting algorithm.
    Their effect is to give higher values to base learners with higher predictive accuracy.
  \item Number of iterations $M$ main tuning parameter.
  \item The discrete prediction function is $\hx = \text{sign}(\fx) \in \{-1,1\}$.
\end{itemize}

\framebreak

\begin{algorithm}[H]
  \setstretch{1.25}
  \caption*{AdaBoost}
  \begin{algorithmic}[1]
    \State Initialize observation weights: $w^{[1](i)} = \frac{1}{n} \quad \forall i \in \nset$
    \For {$m = 1 \to M$}
      \State Fit classifier to training data with weights $\wm$ and get $\bmmh$
      \State Calculate weighted in-sample misclassification rate
      $$
        \errm = \frac{\sumin \wmi \cdot \left[\yi \neq \bmmh(\xi)\right]}{\sumin \wmi}
      $$
      \State Compute: $ \betamh = \frac{1}{2} \log \left( \frac{1 - \errm}{\errm}\right)$
      \State Set: $w^{[m+1](i)} = \wmi \cdot \exp\left(\betamh \cdot
        \left[\yi \neq \bmmh(\xi)\right] \right)$
    \EndFor
    \State Output: $\fxh = \sum_{m=1}^{M} \betamh \bmmh(\xb)$
  \end{algorithmic}
\end{algorithm}

\end{vbframe}

% \begin{vbframe}{Adaboost weights}
<<eval = FALSE>>=
err = seq(0.01, 1, .01)
beta = log((1-err)/err)
qplot(err, beta, geom = "line",
 xlab = expression(err^m), ylab = expression(beta^m))
@
% \end{vbframe}

\begin{vbframe}{Adaboost illustration}

\begin{columns}
\column{4cm}

\includegraphics[width=4cm]{figure_man/adaboost_example1.png}

{\footnotesize Schapire, Boosting, 2012.}

\column{6cm}

\begin{itemize}
  \item $n = 10$ observations and $M = 3$ iterations, tree stumps as base learners
  \item The label of every observation is represented by $+$ or $-$
  \item The size of the label represents the weight of an observation in the corresponding iteration
  \item Dark grey area: base model in iteration $m$ predicts $+$
  \item Light grey area: base model in iteration $m$ predicts $-$
\end{itemize}

\end{columns}

\framebreak

\begin{columns}
\column{7cm}

\includegraphics[width=7cm]{figure_man/adaboost_example2.png}

{\footnotesize Schapire, Boosting, 2012.}

\column{3cm}

The three base models are combined into one classifier.

All observations are correctly classified.

\end{columns}

\end{vbframe}

\begin{vbframe}{AdaBoost Weights}

The base-learner weights of AdaBoost can be treated as measure how much a single base-learner was able to learn:
\begin{itemize}

  \item
    We expect a weak learner to be slightly better than random guessing: $\errm \leq 0.5$ \\
    $\rightarrow$ First base-learner $\text{err}^{[1]} = 0.3$

  \item
    The ratio $(1 - \errm) / \errm$ used to calculate the weights is therefore the proportion of correct vs. misclassified examples \\
    $\rightarrow$ First base-learner $0.7 / 0.3 \approx 2.33$

  \item
    The logarithm scales the ratio to a more intuitive weighting scheme:
    \begin{itemize}

      \item
        $0.5 \log((1 - \errm) / \errm) = 0$ $\Leftrightarrow$ base-learner $m$ is not able to learn anything $\rightarrow\ \errm = 0.5$

      \item
        $0.5 \log((1 - \errm) / \errm) > 0$ $\Leftrightarrow$ base-learner $m$ is able to learn something $\rightarrow\ \errm < 0.5$

    \end{itemize}
    $\rightarrow$ Weight of the first base-learner: $0.5 \log(2.33) \approx 0.42$

\end{itemize}

\framebreak

The next step is to update the observation weights $w_i$ for for each observation $i$.

\begin{itemize}

  \item For iteration $m + 1$, this is done by
  $$w^{[m+1](i)} = \wmi \cdot \exp \left(\betamh \cdot \left[\yi \neq \bmmh(\xi)\right] \right)$$

  \item If the base learner predicts the $i$-th observation correctly, $\exp \left(\betamh \cdot \left[\yi \neq \bmmh(\xi)\right] \right)$ reduces to $\exp(0) = 1$, which means that the weight for observation $i$ stays unchanged since $w^{[m+1](i)} = \wmi \cdot 1 = \wmi$.

  \item Considering the example, we have 7 correctly observations for which the initial observation weights $1/n = 0.1$ are kept untouched.

  \item The 3 wrongly classified observations get new observation weights

  \begin{align*}
    w^{[2](i)} &= w^{[1](i)} \cdot \exp \left(\hat \beta^{[1]} \cdot \left[\yi \neq \hat{b}^{[1]}(\xi)\right] \right) \\
               &= \frac{1}{10} \cdot \exp \left(0.42 \cdot 1 \right) \\
               &\approx 0.11
  \end{align*}

  \item The resulting new weights for the missclassified observations are now slightly greater compared to the initial weights of $0.1$.

  \item This forces the next base learner $b^{[2]}(\xi)$ to concentrate more on those.

\end{itemize}

\end{vbframe}

\begin{vbframe}{Example: Bagging vs Boosting}

Random Forest versus AdaBoost (both with stumps) on Spirals data from mlbench ($n=200$, sd$=0$).
Performance (mmce) is measured with $3 \times 10$ repeated CV.

<<comparing-methods-plot, fig.height=4.6>>=
load("rsrc/comparing_methods_result.RData")
ggplot(data = subset(result, learner %in% c("rf_stump", "ada")),
  aes(x = M, y = mmce, col = learner, group = learner)) + geom_line() + xlab("m")
@

\framebreak

\begin{itemize}

  \item We see that, weak learners are not as powerful in combination with bagging compared with boosting.

  \item This is mainly caused by the fact that bagging only performs variance reduction and that tree stumps are very stable

\end{itemize}

<<fig.align="center", echo=FALSE, fig.width=16, fig.height=6>>=
load("rsrc/stump_plots.RData")
gridExtra::grid.arrange(learnerPredPlot1, learnerPredPlot2, ncol = 2)
@

\end{vbframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Overfitting behavior}

A long-lasting discussion in the context of AdaBoost is its overfitting behavior.

% \lz

% \textbf{When a prediction rule concentrates too much on peculiarities of the specific sample of training observations, it will often perform poorly on a new data set.}

% \lz

The main instrument to avoid overfitting is the stopping iteration $M$:
\begin{itemize}
\item High values of $M$ lead to complex solutions. Overfitting?
\item Small values of $M$ lead to simple solutions. Underfitting?
\end{itemize}
Although eventually it will overfit, AdaBoost in general shows a rather slow overfitting behavior.
\end{vbframe}

% \begin{vbframe}{Software in R}
%   \begin{itemize}
%     \item Bagging: Package \pkg{mlr} is able to bag any learner with \code{makeBaggingWrapper()}.
%   \item Random Forests: Package \pkg{randomForest} with function
%     \code{randomForest()} based on CART.
%   \item AdaBoost: included in the packages \code{ada} and \code{boosting}.
% \end{itemize}
% \end{vbframe}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Gradient Boosting}

\begin{vbframe}{Gradient descent}
% \begin{vbframe}{Forward stagewise additive modeling}

% Let's recall gradient descent from numerical optimization.
Let $\risk(\theta)$ be (just for the next few slides) an arbitrary, differentiable, unconstrained objective function, which we want to minimize. The gradient $\nabla \risk(\theta)$ is the direction of the steepest ascent, $-\nabla \risk(\theta)$ of \textbf{steepest descent}.

\lz

For an intermediate solution $\theta^{[k]}$ during minimization, we can iteratively improve by updating
$$
\theta^{[k+1]} = \theta^{[k]} - \beta \nabla \risk(\theta^{[k]}) \qquad  \text{for } 0 < \beta \leq c\left(\theta^{[k]}\right)
$$
% \enquote{Walking down the hill, towards the valley.}

% $f(x_1, x_2) = -\sin(0.8 x_1) \cdot \frac{1}{2\pi} \exp\left( (x_2 x_1 + \pi / 2)^2 \right)$
<<sd-plot>>=
#modified from ../../../cim1/2017/11-Optimierung/functions.

sd_plot = function(col = terrain_hcl, theta = 40, phi = 40, xlab = "x", ylab = "y") {
  if (is.function(col)) col = col(nrow(z) * ncol(z))

  par(mfrow = c(1, 2))

  par(mar = rep(0.5, 4))
  require("colorspace")
  pmat = persp2(x, y, z, theta = theta, phi = phi, ticktype = "detailed",
      xlab = xlab, ylab = ylab, zlab = "", col = col, lwd = 0.3, border = NA)

  for (j in seq_along(p)) {
    t3d = trans3d(p[[j]][[1]], p[[j]][[2]], do.call(foo, p[[j]]), pmat)
    if (j > 1) {
      t3d2 = trans3d(p[[j - 1]][[1]], p[[j - 1]][[2]], do.call(foo, p[[j- 1]]), pmat)
      lines(c(t3d$x, t3d2$x), c(t3d$y, t3d2$y))
      points(x = t3d2$x, y = t3d2$y, pch = 16, col = heat_hcl(1))
    }
    points(x = t3d$x, y = t3d$y, pch = 16, col = heat_hcl(1))
  }
  par(mar = c(4.1, 4.1, 1.1, 1.1))
  image(x, y, z, col = col, xlab = xlab, ylab = ylab, useRaster = TRUE)
  contour(x, y, z, add = TRUE, nlevels = 15)
  for (j in seq_along(p)) {
    if (j > 1) {
      lines(c(p[[j]][1], p[[j - 1]][1]), c(p[[j]][2], p[[j - 1]][2]))
      points(p[[j - 1]][1], p[[j - 1]][2], pch = 16, col = heat_hcl(1))
    }
    points(p[[j]][1], p[[j]][2], pch = 16, col = heat_hcl(1))
  }
  invisible(NULL)
}

@
<<gradient-descent, fig.align="center", echo=FALSE, fig.width=8, fig.height=4, out.height="3cm", out.width="6cm">>=
foo = function(x, y) {
  -1 * sin(.8 * pi*x) * dnorm(-y * x, mean = pi / 2, sd = 0.8)
}

x = seq(0, 2.5, length = 50)
y = seq(-3, 1, length = 50)
z = outer(x, y, foo)
p = c(list(list(1.8, -.5)), optim0(1.8, -.5, FUN = foo, maximum = FALSE, maxit = 19))

sd_plot(phi = 35, theta = -20, xlab = "x_1", ylab = "x_2", col = viridis::viridis)
@

% {\scriptsize step size  $\beta = 1$}

% \framebreak

% $\beta$ is called \textbf{step size}, and can be set by
% \begin{itemize}
% \item fixing it to a (smallish) constant
% \item adapting it according to previous gradient values, the local Hessian, etc.
% \item line search methods, which solve $\beta^{[k]} = \argmin_{\beta} f\left(x^{[k]} - \beta \nabla f\left(x^{[k]}\right)\right)$. Only one real parameter $\beta$, i.e, \enquote{easy} to solve\dots
% \end{itemize}




\end{vbframe}


\begin{vbframe}{Forward stagewise additive modeling}

Assume a regression problem for now (as this is simpler to explain);
and assume a space of base learners $\mathcal{B}$.

\lz

% A weak learner should have the property that it delivers better predictions than by random chance (e.g. for a balanced training set a misclassification error less than 1/2).

% \lz

We want to learn an additive model:

$$
\fx = \sum_{m=1}^M \betam b(\xb, \thetam)
$$

Hence, we minimize the empirical risk:

$$
\riskef = \sum_{i=1}^n L\left(\yi,\fxi \right) =
\sum_{i=1}^n L\left(\yi, \sum_{m=1}^M \betam b\left(\xi, \thetam\right)\right)
$$


% \framebreak

% A common \textbf{loss} for \textbf{regression} is the \textbf{squared error} with
% $\Lxy = (y-\fx)^2$.

% \lz

% Apparently, $\risk$ depends on the \textbf{base learners} $b(x, \thetam)$,
% or rather their parameters $\thetam$ and weights $\betam$. Hence, we have to optimize these.

% \lz

\framebreak

Because of the additive structure, it is difficult to jointly minimize $\riskef$ w.r.t. $\left(\left(\beta^{[1]}, \theta^{[1]}\right), \ldots, \left(\beta^{[M]}, \theta^{[M]}\right)\right)$, which is a very high-dimensional parameter space.

\lz

Moreover, considering trees as base learners is worse, as we would now have to grow $M$ trees in parallel so they
  work optimally together as an ensemble.

\lz

Finally, stagewise additive modeling has nice properties, which we want to make use of, e.g. for regularization, early stopping, \dots

\framebreak

Hence, we add additive components in a greedy fashion by sequentially minimizing the risk only w.r.t. the next additive component:

$$ \min \limits_{\beta, \theta} \sum_{i=1}^n L\left(\yi, \fmdh\left(\xi\right) + \beta b\left(\xi, \theta\right)\right) $$

\lz

Doing this iteratively is called \textbf{forward stagewise additive modeling}

\input{algorithms/forward_stagewise_additive_modeling.tex}

\end{vbframe}



\begin{vbframe}{Gradient boosting}

The algorithm we just introduced isn't really an algorithm, but rather an abstract principle.
We need to find the new additive component $b\left(x, \thetam\right)$ and its
weight coefficient $\betam$ in each iteration $m$.
This can be done by gradient descent, but in function space.

\lz

Thought experiment:
Consider a completely non-parametric model $f$,
where we can arbitrarily define its predictions on every point of the training data $\xi$. So we basically specify
$f$ as a discrete, finite vector.

  $$\left(f(\xb^{(1)}), \ldots,  f(\xb^{(n)})\right)^\top $$

This implies $n$ parameters $\fxi$ (and the model would provide no generalization...).

Furthermore, we assume our loss function $L$ to be differentiable.


\framebreak

Now we want to minimize the risk of such a model with gradient descent (yes, this makes no sense,
suspend all doubts for a few seconds).

So, we calculate the gradient at a point of the parameter space, that is the derivative with respect to each component of the parameter vector.

$$
  \fp{\riske}{\fxi} = \fp{\sum_j L(y^{(j)}, f(x^{(j)}))}{\fxi} = \fp{\Lxyi}{\fxi}
$$

The gradient descent update for each vector component of $f$ is:

$$
  \fxi \leftarrow \fxi - \beta \fp{\Lxyi}{\fxi}
$$

This tells us how we could \enquote{nudge} our whole function $f$ in the direction of the data to
reduce its empirical risk.


\framebreak

Combining this with the iterative additive procedure
of \enquote{forward stagewise modelling}, we are at the spot $\fmd$ during minimization.
At this point, we calculate the direction of the negative gradient:

$$ \rmi = -\left[\fp{\Lxyi}{f(\xi)}\right]_{f=\fmd} $$

The pseudo residuals $\rmi$ match the usual residuals for the squared loss:


$$
- \fp{\Lxy}{\fx} = - \fp{0.5(y - \fx)^2}{\fx} = y - \fx
$$


\framebreak

% We find our $\betam$ by minimizing with line search:

% $$
  % \betam = \argmin_{\beta} \sumin L(\yi, \fmdh(x) + \beta b(x, \thetamh)),
% $$

% where $h(x, \thetam) = \rmm$.

% \lz

What is the point in doing all this? A model parameterized in this way is senseless,
as it is just memorizing the instances of the training data...?

\vspace*{0.3cm}


So, we restrict our additive components to $b\left(\xb, \thetam\right) \in \mathcal{B}$.

% \framebreak

The pseudo-residuals are calculated exactly as stated above,
then we fit a regression model $b(\xb, \thetam)$ to them:
$$ \thetamh = \argmin_{\theta} \sum_{i=1}^n (\rmi - b(\xi, \theta))^2 $$
So, evaluated on the training data,
our $b(\xb, \thetam)$ corresponds as closely as possible to the negative
loss function gradient and generalizes to the whole space.

\vspace*{0.3cm}

\textbf{In a nutshell}: One boosting iteration is exactly one approximated gradient step in function space,
which minimizes the empirical risk as much as possible.


\framebreak


<<gbm-illu-plot, echo=FALSE, results="asis", out.width="0.9\\textwidth", fig.width=7, fig.height=6>>=
source("rsrc/boosting_illustration_plot.R")

set.seed(123)
x = sort(runif(100, 0, 18))
y = sin(x / 2) + rnorm(length(x), 0, sd = 0.2)

plotBoostingIllustration(x, y, nboost = 8, learning_rate = 0.2, basis_fun = bTrafo, plot_frames = c(2))
@

\end{vbframe}
%% Combining this with the iterative additive procedure
%% of \enquote{forward stagewise modelling}, we are at the spot $\fmd$ during minimization.
%% At this point, we now calculate the direction of the negative gradient:
%%
%% $$ \rmi = -\left[\fp{\Lxyi}{f(\xi)}\right]_{f=\fmd} $$
%%
%% We will call these $\rmi$ \textbf{pseudo residuals}. For squared loss they match the usual residuals
%%
%%
%% $$
%% - \fp{\Lxy}{\fx} = - \fp{0.5(y - \fx)^2}{\fx} = y - \fx
%% $$
%%
%%
%% \framebreak
%%
%% % We find our $\betam$ by minimizing with line search:
%%
%% % $$
%%   % \betam = \argmin_{\beta} \sumin L(\yi, \fmdh(x) + \beta b(x, \thetamh)),
%% % $$
%%
%% % where $h(x, \thetam) = \rmm$.
%%
%% % \lz
%%
%% What is the point in doing all this? A model parameterized in this way is senseless,
%% as it is just memorizing the instances of the training data...?
%%
%% \lz
%%
%% So, we restrict our additive components to $b\left(x, \thetam\right) \in \mathcal{B}$.
%%
%% % \framebreak
%%
%% The pseudo-residuals are calculated exactly as stated above,
%% then we fit a regression model $b(\bm{x}, \thetam)$ to them:
%% $$ \thetamh = \argmin_{\theta} \sum_{i=1}^n (\rmi - b(\xi, \theta))^2 $$
%% So, evaluated on the training data,
%% our $b(x, \thetam)$ corresponds as closely as possible to the negative
%% loss function gradient and generalizes to the whole space.
%%
%% \lz
%%
%% \textbf{In a nutshell}: One boosting iteration is exactly one approximated gradient step in function space,
%% which minimizes the empirical risk as much as possible.
%%
%% \end{vbframe}

\begin{vbframe}{Gradient boosting algorithm}

\input{algorithms/gradient_boosting_general.tex}

Note that we also initialize the model in a loss-optimal manner.

\end{vbframe}

\begin{vbframe}{Gradient boosting illustration - 1}


Assume one feature $x$ and a target $y$.
<<echo=FALSE, out.width="0.4\\textwidth", fig.width=4, fig.height=3>>=
nsim = 50L

set.seed(31415)
x = seq(0, 10, length.out = nsim)
y = 4 + 3 * x + 5 * sin(x) + rnorm(nsim, 0, 2)

plot(x = x, y = y, xlab = "Feature x", ylab = "Target y")
@

\begin{enumerate}

  \item
    We start with the simplest model, the optimal constant w.r.t. the $L_2$ loss (mean of the target variable).

  \item
    To improve the model we calculate the pointwise residuals on the training data $\yi - f\left(\xi\right)$ and fit a GAM fitted on the residuals.

  \item
    The GAM fitted on the residuals is then multiplied by a learning rate of $0.2$ and added to the previous model.

  \item
    This procedure is repeated multiple times.

\end{enumerate}

\end{vbframe}


<<echo=FALSE, results="asis", out.width="0.8\\textwidth", fig.width=7, fig.height=6>>=
source("rsrc/boosting_intro_animation.R")

basisTrafo = function (x) {
  margin = 0.1 * (max(x) - min(x))
  splines::splineDesign(knots = seq(min(x) - margin, max(x) + margin, length.out = 40), x = x)
}
boosting_iters = c(1,2,3,5,10,100)

for (nboost in boosting_iters) {
  cat("\\begin{vbframe}{Gradient boosting illustration - 1}\n")
  plotLineaBoosting(x, y, nboost, 0.2, basis_fun = basisTrafo)
  if (nboost > 1)
    cat("\\addtocounter{framenumber}{-1}\n\n")
  cat("\\end{vbframe}\n\n")
}
@

\begin{vbframe}{Gradient boosting illustration - 2}

We will consider a regression problem on the following slides:

\begin{itemize}
  \item
    We draw points from a sine-function with some additional noise.

  \item
    We use squared loss for $L$.

  \item
    Our base learner are tree stumps.

  \item
    The left plot shows the additive model learnt so far with the data, the right plot shows the residuals to which we fit the next base learner.

  \item
    The plot is generated by a self-implemented version of boosting, where the step size is obtained by line search.

  %\item Quiz question: Why is the obtained optimal step size always 1?

\end{itemize}

\end{vbframe}

<<gbm-anim-prep, include = FALSE>>=
source("rsrc/gbm_anim.R")

set.seed(122)
n = 50L
x = sort(10 * runif(n))
X = data.frame(x1 = x)
y = sin(x) + rnorm(length(x), mean = 0, sd = 0.04)

z = anim(X = X, y = y, M = 100, demo = FALSE, data.all.iterations = TRUE)

plotGBMIteration = function(m) {
  return(plotModel(m - 1, X[,1L], yhat = z$traces[[m]]$yhat, r = z$traces[[m]]$r,
    rhat = z$traces[[m]]$rhat, beta = z$betas[[m]]))
}
@

<<gbm-anim-plot, echo=FALSE, results="asis">>=
for (nboost in c(1:11, 31)) {
  cat("\\begin{vbframe}{Gradient boosting illustration - 2}\n")
  plotAnimation(plotGBMIteration, nboost)
  if (nboost > 1)
    cat("\\addtocounter{framenumber}{-1}\n\n")
  cat("\\end{vbframe}\n\n")
}
@

\begin{vbframe}{Gradient boosting illustration - 2}

<<gbm-sine-plot, fig.height=4>>=
source("rsrc/gbm_sine.R")
p1 = doPlot(sd = 0.0, shrinkage = 1) + theme(legend.position = "none")
p2 = doPlot(sd = 0.1, shrinkage = 1)
grid.arrange(p1, p2, nrow = 1, widths = c(0.25, 0.3))
@
\begin{itemize}

  \item
    Iterating this very simple base learner yields a rather nice approximation of a smooth model in the end.

  \item
    Severe overfitting apparent in the noisy case. We'll discuss and solve this problem later.

\end{itemize}

\end{vbframe}

\begin{vbframe}{Gradient Boosting Visualization}
\begin{center}
\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm1.jpg}
\href{http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html}{\beamergotobutton{Open in browser.}}
\end{center}
\end{vbframe}

% \begin{vbframe}{Gradient Boosting Visualization}
% \begin{center}
% \includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm2.jpg}
% \href{http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html}{\beamergotobutton{Open in browser.}}
% \end{center}
% \addtocounter{framenumber}{-1}
% \end{vbframe}

\begin{vbframe}{Gradient Boosting Visualization}
\begin{center}
\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm3.jpg}
\href{http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html}{\beamergotobutton{Open in browser.}}
\end{center}
\addtocounter{framenumber}{-1}
\end{vbframe}

% \begin{vbframe}{Gradient Boosting Visualization}
% \begin{center}
% \includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm4.jpg}
% \href{http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html}{\beamergotobutton{Open in browser.}}
% \end{center}
% \addtocounter{framenumber}{-1}
% \end{vbframe}

\begin{vbframe}{Gradient Boosting Visualization}
\begin{center}
\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm5.jpg}
\href{http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html}{\beamergotobutton{Open in browser.}}
\end{center}
\addtocounter{framenumber}{-1}
\end{vbframe}

% \begin{vbframe}{Gradient Boosting Visualization}
% \begin{center}
% \includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm6.jpg}
% \href{http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html}{\beamergotobutton{Open in browser.}}
% \end{center}
% \addtocounter{framenumber}{-1}
% \end{vbframe}

\begin{vbframe}{Gradient Boosting Visualization}
\begin{center}
\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm7.jpg}
\href{http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html}{\beamergotobutton{Open in browser.}}
\end{center}
\addtocounter{framenumber}{-1}
\end{vbframe}

% \begin{vbframe}{Gradient Boosting Visualization}
% \begin{center}
% \includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm8.jpg}
% \href{http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html}{\beamergotobutton{Open in browser.}}
% \end{center}
% \addtocounter{framenumber}{-1}
% \end{vbframe}

% \begin{vbframe}{Gradient Boosting Visualization}
% \begin{center}
% \includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm9.jpg}
% \href{http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html}{\beamergotobutton{Open in browser.}}
% \end{center}
% \addtocounter{framenumber}{-1}
% \end{vbframe}

\begin{vbframe}{Gradient Boosting Visualization}
\begin{center}
\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm10.jpg}
\href{http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html}{\beamergotobutton{Open in browser.}}
\end{center}
\addtocounter{framenumber}{-1}
\end{vbframe}

\begin{vbframe}{Gradient boosting and trees}

Trees are mainly used as base learners for gradient boosting in ML.
A great deal of research has been done on this combination so far, and it often provides the best results.

\begin{blocki}{Reminder: Advantages of trees}
\item No problems with categorical features.
\item No problems with outliers in feature values.
\item No problems with missing values.
\item No problems with monotone transformations of features.
\item Trees (and stumps!) can be fitted quickly, even for large $n$.
\item Trees have a simple built-in type of variable selection.
%\item Interpretation of Trees is rather easy.
\end{blocki}
Gradient boosted trees retains all of them, and strongly improves the trees' predictive power.
Furthermore, it is possible to adapt gradient boosting especially to tree learners.

\framebreak

One can write a tree as: $ b(\xb) = \sum_{t=1}^{T} c_t \I(\xb \in R_t) $,
where $R_t$ are the terminal regions and $c_t$ the corresponding means.

\lz

This special additive structure can be exploited by boosting:  %Finished here

\begin{align*}
  \fm(\xb) &= \fmd(\xb) +  \betam \bmm(\xb) \\
         &= \fmd(\xb) +  \betam \sum_{t=1}^{\Tm} \ctm \I(\xb \in \Rtm)
\end{align*}

Actually, we do not have to find $\ctm$ and $\betam$ in two separate steps
(fitting against pseudo-residuals, then line search).
Also note that the $\ctm$ will not really be loss-optimal as we used squared error loss
to fit them against the pseudo residuals.

\framebreak

What we will do instead, is:

$$ \fm(\xb) = \fmd(\xb) +  \sum_{j=1}^{\Tm} \ctmt \I(\xb \in \Rtm) $$

We now induce the structure of the tree with respect to squared error loss,
but then determine / change all $\ctmt$ individually and directly L-optimally:


\vspace{-0.2cm}

$$ \ctmt = \argmin_{c} \sum_{\xi \in \Rtm} L(\yi, \fmd(\xi) + c) $$

\vspace{-0.5cm}

\begin{center}

\includegraphics[width=0.38\textwidth]{figure_man/gbm_leaf_adjustment.pdf}

\end{center}

\framebreak

\input{algorithms/gradient_tree_boosting_algorithm.tex}

\end{vbframe}

\begin{vbframe}{Binary classification}


For $\Yspace = \{0, 1\}$, we simply have to select an appropriate loss function, so let's
use binomial loss as in logistic regression:

$$ \Lxy = - y \fx + ln(1 + \exp(\fx))$$

Then,

\vspace{-0.5cm}

\begin{align*}
\tilde{r} &=-\fp{\Lxy}{\fx} \\
&= y - \frac{\exp(\fx)}{1 + \exp(\fx)} \\
&= y - \frac{1}{1 + \exp(-\fx)} = y - s(\fx).
\end{align*}

Here, $s(\fx)$ is the logistic sigmoid function, applied to a scoring model.
So effectively the pseudo residuals are $y - \pix$.

Through $\pix = s(\fx)$ we can also estimate posterior probabilities.

\framebreak
%
% \lz
%
% As already mentioned, AdaBoost performs forward stagewise additive modelling
% under exponential loss.
%
% \lz
%
% In principle we could also have used the exponential loss for classification with gradient boosting.
% In practice there is no big difference, although binomial loss makes a bit more sense from a theoretical
% (maximum likelihood) perspective.
%
% \lz
%
% \begin{table}
% \centering
% \begin{tabular}{lll}
% &Binomial&Exponential\\
% $y \in \{-1, 1\}$&$\ln(1+\exp(-2y\fx))$&$\exp(-y\fx)$\\
% $y \in \{0, 1\}$&$-y\fx+\ln(1+\exp(\fx))$&$\exp(-(2y-1)\fx)$
% \end{tabular}
% \end{table}


\end{vbframe}

\begin{vbframe}{Multi-class problems}

We proceed as in softmax regression and model a categorical distribution, with multinomial loss.
For $\Yspace = \{1, \ldots, g\}$, we create $g$ discriminant functions $\fxk$, one for each class, each an additive model of trees.

We define the $\pi_k(\xb)$ through the softmax function:
$$ \pikx = s_k(f_1(\xb), \ldots, f_g(\xb)) = \exp(\fxk) / \sum_{j=1}^g \exp(f_j(\xb)) $$

Multinomial loss $L$:
$$ L(y, f_1(\xb), \ldots f_g(\xb)) = - \sumkg \I(y = k) \ln \pikx $$

And for the derivative the following holds:
$$-\fp{L(y_k, f_1(\xb), \ldots, f_g(\xb))}{\fxk} =  \I(y = k) - \pikx $$

\framebreak

\lz

Determining the tree structure by squared-error-loss works just like before in the 2 class problem.

\lz

In the estimation of the $c$ though, all the models depend on each other because of the definition
of $L$. Optimizing this is more difficult, so we will skip the details and just present the results.

\lz

The estimated class for $x$ is of course exactly the $k$ for which $\pikx$ is maximal.

\framebreak

\input{algorithms/gradient_boosting_for_k_classification.tex}

\framebreak

\begin{blocki}{Derivation of the algorithm:}

  \item from Friedman, J. H. - Greedy Function Approximation: A Gradient Boosting Machine (1999)

  \item In each iteration $m$ we calculate the pseudo residuals
        $$\rmi_k = \I(\yi = k) - \pi_k^{[m-1]}(\xi),$$
        where $\pi_k^{[m-1]}(\xi)$ is derived from $f^{[m-1]}(\mathbf{x})$

  \item Thus, $g$ trees are induced at each iteration $m$ to predict the corresponding current pseudo residuals for each class on the probability scale.

  \item Each of these trees has $T$ terminal nodes with corresponding regions $R_{tk}^{[m]}$.

\end{blocki}

\framebreak

\begin{itemize}

  \item The model updates $\hat{c}_{tk}^{[m]}$ corresponding to these regions are the solution to

  $$ \hat{c}_{tk}^{[m]} = \argmin_{c} \sum_{i=1}^n \sum_{k=1}^g L \left( \yi_k, f^{[m-1]}(\mathbf{x}^{(i)}) + \sum_{t=1}^T \hat{c}_{tk} \I\left(\xi \in R_{t}^{[m]}\right) \right)$$

  where $L$ is the multinomial loss function $L(y, f_1(\xb), \ldots f_g(\xb)) = - \sumkg \I(y = k) \ln \pikx$
  and $\pikx = \frac{\exp(f_k(\xb))}{\sum_j \exp(f_j(\xb))}$ as before.

  \item This has no closed form solution and additionally, the regions corresponding to the different class tress overlap, so that the solution does not reduce to a separate calculation within each region of each tree.

\end{itemize}

\framebreak

\begin{itemize}

  \item Hence, we approximate the solution with a single Newton-Raphson step, using a diagonal approximation to the Hessian.

  \item This decomposes the problem into a separate calculation for each terminal node of each tree.

  \item The result is

  $$\hat{c}_{tk}^{[m]} =
      \frac{g-1}{g}\frac{\sum_{\xi \in R_{tk}^{[m]}} \rmi_k}{\sum_{\xi \in R_{tk}^{[m]}} \left|\rmi_k\right|\left(1 - \left|\rmi_k\right|\right)}$$

  \item The update is then done by
  $$\hat{f}_k^{[m]}(x) = \hat{f}_k^{[m-1]}(x) + \sum_t \hat{c}_{tk}^{[m]} \I\left(x \in R_{tk}^{[m]}\right)$$

\end{itemize}

\end{vbframe}

\begin{vbframe}{Regularization and shrinkage}

If GB runs for a long number of iterations, it can overfit due to its aggressive loss
minimization.

\begin{blocki}{Options for regularization}
\item Limit the number of boosting iterations $M$ (\enquote{early stopping}), i.e., limit the number of additive components.
\item Limit the depth of the trees. This can also be interpreted as choosing the order of interaction.
\item Shorten the step length $\betam$ in each iteration.
\end{blocki}

The latter is achieved by multiplying $\betam$ with a small $\nu \in (0,1]$:
$$ \fm(\xb) = \fmd(\xb) + \nu \betam b(\xb, \thetam) $$

$\nu$ is called \textbf{shrinkage parameter} or \textbf{learning rate}.


\framebreak

Obviously, the optimal values for $M$ and $\nu$ strongly depend on each other:
By increasing $M$ one can use a smaller value for $\nu$ and vice versa.

\lz

In practice it is often recommended to choose $\nu$ quite small and choose $M$ by cross-validation.

\lz

It's probably best to tune all three parameters jointly based on the training data
via cross-validation or a related method.

\end{vbframe}

\begin{vbframe}{Stochastic gradient boosting}

This is a minor modification to boosting to incorporate the advantages of bagging into the method.
The idea was formulated quite early by Breiman.

\lz

Instead of fitting on all the data points, a random subsample is drawn in each iteration.

\lz

Especially for small training sets, this simple modification often leads to
significant empirical improvements.
How large the improvements are depends on data structure, size of the data set,
base learner and size of the subsamples (so this is another tuning parameter).


\end{vbframe}


%\begin{vbframe}{Variable importance}

%As for random forests, we can construct a variable importance measure to help
%interpret the model.

%\lz
%For a regression tree $b$, we can define such a measure $I^2_j(b)$ as the sum over the reduction of
%impurity ($\rightarrow$ reduction of variance) at all inner knots where the tree splits with respect to feature $x_j$.

%\framebreak

%For an additive boosting model one just takes

%$$ I^2_j = \frac{1}{M} \sum_{m=1}^M I^2_j(\bmm) $$

%%From those importance values we take the squared root and scale them so that the most important feature gets the value 100.
%To get the relative influence measures, the resulting $I^2_j, j = 1, \dots, p$ are scaled so that they sum to 1.

%\lz
%For a $g$-class problem one has $g$

%$$ \fxk = \sum_{m=1}^M b_k^{[m]}(x), \quad
%  I^2_{jk} = \frac{1}{M} \sum_{m=1}^M I^2_j(b_k^{[m]}) $$

%$$ I^2_j = \frac{1}{g} \sum_{k=1}^g I^2_{jk} $$

%\end{vbframe}


\begin{vbframe}{Example: Spam detection}

% The data set we will examine briefly in the following was collected at the Hewlett Packard laboratories
% to train a personalized spam mail detector.

% \lz

% It contains data of 4601 emails. 2788 mails were regular mails and 1813 were spam.
% There are 57 numerical predictors available measuring e.g. the frequency of the most frequent words
% and special characters as well as runlengths of words in all capitals.

% \lz

% We use the R package \pkg{gbm}, which implements the introduced version of gradient boosting.

% \framebreak

% <<gbm-spam-example, eval = FALSE, echo = TRUE>>=
% library(gbm)
% data(spam, package = "ElemStatLearn")
% spam$spam = as.numeric(spam$spam) - 1 # gbm requires target to be 0/1
% gbm(spam ~ ., data = spam,
%   distribution = "bernoulli", # classification
%   n.trees = 100, # M = 100
%   interaction.depth = 2, # max. tree depth = 2
%   shrinkage = 0.001, # nu = 0.001
% )
% @
%
% \lz

We fit a gradient boosting model for different parameter values

\begin{table}[]
\centering
\begin{tabular}{l|c}
Parameter name      & Values                         \\
\hline
distribution        & Bernoulli (for classification) \\
shrinkage $\nu$     & ${0.001, 0.01, 0.1}$           \\
number of trees $M$ & $[0,\dots,20000]$              \\
max. tree depth     & ${1,4,7,10,13,16}$
\end{tabular}
\end{table}

We observe the error on a separate test set to find the optimal parameters.


\framebreak

Misclassification rate for different hyperparameter settings (shrinkage and maximum tree depth) of gradient boosting:

<<gbm-spam-results, fig.height=4.5>>=
load("rsrc/gbm_spam_results_long.RData")

ggd = error.df
ggd$shrinkage = as.factor(ggd$shrinkage)
ggplot(data = ggd, aes(x = M, y = err, col = max.tree.depth)) +
  geom_line(alpha= .8) + facet_grid(~ shrinkage, labeller = label_both) + #coord_cartesian(ylim = c(0, 0.20)) +
  ylab("mmce (test data)") + scale_x_continuous("iterations M [1000]",
    c(1, 5, 10, 15, 20) * 1e3,
    labels = c("1", "5", "10", "15", "20")) +
  scale_y_continuous(trans = "log2")
@

% \begin{figure}
%   \includegraphics[width=10cm, height=6cm]{figure_man/gbm_spam_effects.pdf}
% \end{figure}

% \framebreak

% \begin{figure}
  % \includegraphics[width=8cm]{figure_man/gbm_spam_imp_ggplot.pdf}
  % \caption{\footnotesize Variable Importance for model with $\nu = 0.1, M = 1380$ and tree depth of $4$.}
% \end{figure}

% \framebreak
<<include=FALSE, eval=FALSE>>=
gbm.perf(mod$learner.model, method = "OOB")
@

% \begin{figure}
%  \includegraphics[width=6cm]{figure_man/gbm_spam_gbmperf.pdf}
%  \caption{\footnotesize Deviance}
% \end{figure}

% \framebreak
<<include=FALSE, eval=FALSE>>=
plot.gbm(mod$learner.model, 52)
plot.gbm(mod$learner.model, 25)
@
%
% \begin{figure}
%   \includegraphics[width=8cm, height=5cm]{figure_man/gbm_spam_partdep.pdf}
%   \caption{\footnotesize Partial Dependency Plot for 2 important features.
%   Plotted is f in dependency of one feature, if all other features are integrated over.}
% \end{figure}

\end{vbframe}

\begin{vbframe}{Additional information}

By choosing a suitable loss function it is also possible to model a large number of different problem domains
\begin{itemize}
  \item Regression
  \item (Multi-class) Classification
  \item Count data
  \item Survival data
  \item Ordinal data
  \item Quantile regression
  \item Ranking problems
  \item ...
\end{itemize}

\lz

% Boosting is closely related to L1 regularization.

% \lz

Different base learners increase flexibility (see componentwise gradient boosting).
If we model only individual variables, the resulting regularized variable selection
is closely related to L1 regularization.

\framebreak

For example, using the pinball loss in boosting
$$
L(y, f(\xv)) = \left\{
\begin{array}{lc}
(1 - \alpha)(f(\xv) - y), & \text{if}\ y < f(\xv) \\
\alpha(y - f(\xv)),       & \text{if}\ y \geq f(\xv)
\end{array}
\right.
$$
models the $\alpha$-quantiles:

\begin{center}
\includegraphics[scale=0.5]{figure_man/quantile_boosting.png}
\end{center}

\framebreak

The AdaBoost fit has the structure of an additive model with \enquote{basis functions} $\bmm (x)$.

\lz

It can be shown (see Hastie et al. 2009, chapter 10) that AdaBoost corresponds to minimizing the empirical risk in each iteration $m$ using the {\em exponential} loss function:
\begin{align*}
  L(y, \fmh(\mathbf{x}))    &= \exp\left(-y\fmh(\mathbf{x})\right) \\
  \riske(\fmh)              &= \sumin L(\yi, \fmh(\xi)) \\
                            &= \sumin L(\yi, \fmdh(\xi) + \beta b(\xi))
\end{align*}


% \begin{align*}
%   \sum_{i=1}^n \exp\left(-\yi \cdot \left(\beta b\left(\xi\right)
%   + \fmdh\left(\xi\right)\right)\right),
% \end{align*}
with minimizing over $\beta$ and $b$ and where $\fmdh$ is the boosting fit in iteration $m-1$.

% \framebreak

% AdaBoost is the empirical equivalent to the forward piecewise solution of the minimization problem

% \begin{align*}
%   \text{arg} \min_{f} \E_{y|x}( \exp (- y \cdot \fx))\ .
% \end{align*}

% \lz

% Therefore, the boosting fit is an estimate of function
% \begin{align*}
%   f^*(x) = 0.5 \cdot \log \left( \frac{\text{P} (y = 1 | x)}
%   {\text{P} (y = -1 | x)}\right) \ ,
% \end{align*}
% which solves the former problem theoretically.

% \lz

% Obvious idea: generalization on other loss functions, use of alternative basis methods.

\end{vbframe}
\begin{vbframe}{Take home message}
Gradient boosting is a statistical reinterpretation of the older AdaBoost algorithm.

\lz

Base learners are added in a \enquote{greedy} fashion, so that they point in the direction of the negative gradient of the empirical risk.

\lz

Regression base learners are fitted even for classification problems.

\lz

Often the base learners are (shallow) trees, but arbitrary base learners are possible.

\lz

The method can be adjusted flexibly by changing the loss function, as long as it's differentiable.

\lz

Methods to evaluate variable importance and to do variable selection exist.

\end{vbframe}

\begin{vbframe}{Gradient Boosting Playground}
\begin{center}

\includegraphics[width=0.7\textwidth]{figure_man/gbm_playground.png}

\href{http://arogozhnikov.github.io/2016/07/05/gradient_boosting_playground.html}{\beamergotobutton{Open in browser.}}

\end{center}
\end{vbframe}


% \begin{vbframe}{RF vs AdaBoost vs GBM}

% Recall the Spirals data from mlbench. We are using stumps again. Performance is measured with 3 times repeated 10CV.

%<<fig.height=4, eval = FALSE, include = FALSE>>=
% load("rsrc/comparing_methods.RData")

%ggplot(data = ggd2[ggd2$learner %in% c("rf", "ada", "gbm", "rf.fulltree"), ],
%  aes(x = iters, y = mmce, col = learner)) + geom_line()
%@

% \end{vbframe}

\endlecture
