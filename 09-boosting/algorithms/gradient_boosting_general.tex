\begin{algorithm}[H]
  \begin{footnotesize}
  \begin{center}
  \caption{Gradient Boosting Algorithm.}
    \begin{algorithmic}[1]
      \State Initialize $\hat{f}^{[0]}(\xb) = \argmin_{\theta} \sumin L(\yi, b(\xi, \theta))$
      \For{$m = 1 \to M$}
          \State For all $i$: $\rmi = -\left[\fp{\Lxyi}{\fxi}\right]_{f=\fmdh}$
        \State Fit a regression base learner to the pseudo-residuals $\rmi$:
        \State $\thetamh = \argmin \limits_\theta \sumin (\rmi - b(\xi, \theta))^2$
        \State Line search: $\betamh = \argmin_{\beta} \sumin L(\yi, \fmd(x) + \beta b(\xb, \thetamh))$
        \State Update $\fmh(\xb) = \fmdh(\xb) + \betamh b(\xb, \thetamh)$
      \EndFor
      \State Output $\fh(\xb) = \hat{f}^{[M]}(\xb)$
    \end{algorithmic}
    \end{center}
    \end{footnotesize}
\end{algorithm}
