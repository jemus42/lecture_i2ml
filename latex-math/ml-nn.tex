% ml - NNs
\newcommand{\neurons}{z_1,\dots,z_M} % vector of neurons

% deeplearning - regularization
\newcommand{\Oreg}{\mathnormal{R}_{reg}(\theta|X,y)}				% regularized objective function
\newcommand{\Ounreg}{\mathnormal{R}_{emp}(\theta|X,y)}			% unconstrained objective function
\newcommand{\Pen}{\Omega(\theta)}							% penalty
\newcommand{\Oregweight}{\mathnormal{R}_{reg}(w|X,y)}			% regularized objective function with weight 
\newcommand{\Oweight}{\mathnormal{R}_{emp}(w|X,y)}				% unconstrained objective function with weight
\newcommand{\Oweighti}{\mathnormal{R}_{emp}(w_i|X,y)}			% unconstrained objective function with weight w_i
\newcommand{\Oweightopt}{\mathnormal{J}(w^*|X,y)}				% unconstrained objective function withoptimal  weight
\newcommand{\Oopt}{\hat{\mathnormal{J}}(\theta|X,y)}				% optimal objective function 
\newcommand{\Odropout}{\mathnormal{J}(\theta, \mu|X,y)}	           	% dropout objective function

% deeplearning - optimization
\newcommand{\riskt}{\mathcal{R}(\theta)}  			% simple risk R(\theta)
\newcommand{\Lxyti}{L(\yi, f(x^{(i)}, \theta))} 
\newcommand{\Lmomentum}{L(\yi, f(x^{(i)}, \theta + \varphi \nu))}                  % momentum risk

% deeplearning - autoencoders
\newcommand{\uauto}{L(x,g(f(x)))}			% undercomplete autoencoder objective function
\newcommand{\dauto}{L(x,g(f(\tilde{x})))}		% denoising autoencoder objective function
