%This file is a child of preamble.Rnw in the style folder
%if you want to add stuff to the preamble go there to make
%your changes available to all childs

<<setup-child, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@
% Load all R packages and set up knitr
<<setup, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{Hyperparameter Tuning}
\lecture{Introduction to Machine Learning}
\sloppy

\begin{vbframe}{Hyperparameter tuning}
Many hyperparameters (HPs) or decisions for an ML algorithm are not decided by the fitting procedure. We distinguish between: 
\begin{itemize}
\item \textbf{Model parameters} are optimized during training, by some form of loss minimization. They are an \textbf{output} of the training. 

Examples:
\begin{itemize}
  \item Coefficients $\thetab$ of a linear model $\fx = \thetab^\top\xb$
  \item Optimal splits of a tree learner
\end{itemize}
\item \textbf{Hyperparameters} must be specified before the training phase. They are an \textbf{input} of the training.

Examples:
\begin{itemize}
  \item How small a leaf may become for a tree
  \item $k$ and which distance measure to use for kNN
  \item The complexity penalty to be used (e.g. $L_1$, $L_2$ penalization) and the complexity control parameter $\lambda$ for regularization
\end{itemize}
\end{itemize}

\framebreak 

\begin{itemize}
\item HPs have to be set either by the user or by (smart) default values.
\item Our goal is to optimize these w.r.t. the estimated prediction error; this implies an independent test set, or cross-validation.
\item The same applies to preprocessing, feature construction and other model-relevant operations. In general we might be interested in optimizing an entire ML \enquote{pipeline}.
\end{itemize}

\begin{figure}
  \includegraphics[width = 0.8\textwidth]{figure_man/automl1.png}
\end{figure}

\end{vbframe}

\begin{vbframe}{Why tuning is important}
\begin{itemize}
\item Hyperparameters control the complexity of a model, i.e., how flexible the model is.
\item If a model is too flexible so that it simply \enquote{memorizes} the training data, we will face the dreaded problem of overfitting. 
\item Hence, control of capacity, i.e., proper setting of hyperparameters
 can prevent overfitting the model on the training set.
\item Many other factors like optimization control settings, distance functions, scaling, algorithmic variants in the fitting procedure can heavily influence model performance in non-trivial ways. It is extremely hard to guess the correct choices here.
\end{itemize}
\end{vbframe}

% \framebreak

  % Possible scenarios for finding default hyperparameters:

  % \begin{itemize}
  %   \item If the learner's performance is fairly insensitive to changes of a hyperparameter, we don't really have to worry as long as we remain within the range of reasonable values.
  %   \item Constant default: we can benchmark the learner across a broad range of data sets and scenarios and try to find hyperparameter values that work well in many different situations. Quite optimistic?
  %   \item Dynamic (heuristic) default: We can benchmark the learner across a broad range of data sets and scenarios and try to find an easily computable function that sets the hyperparameter in a data dependent way,
  %   e.g. using \texttt{mtry}$ = p/3$ for RF.\\
  %     How to construct or learn that heuristic function, though...?
  %   \item In some cases, can try to set hyperparameters optimally by extracting more info from the fitted model. E.g. \texttt{ntrees} for a random forest (does OOB error increase or decrease if you remove trees from the ensemble?).
  % \end{itemize}
% \end{vbframe}

\begin{vbframe}{Types of hyperparameters}
    \begin{itemize}
    \item Numerical parameters (real valued / integers)
    \begin{itemize}
      \item $mtry$ in a random forest
      \item Neighborhood size $k$ for kNN
      \item The complexity control parameter $\lambda$ for regularization 
    \end{itemize}
    \item Categorical parameters:
    \begin{itemize}
      \item Which split criterion for classification trees?
      \item Which distance measure for kNN?
      \item Which type of complexity penalization to use for reguarlized empirical risk minimization? 
    \end{itemize}
    \item Ordinal parameters:
    \begin{itemize}
      \item $\{$\texttt{low}, \texttt{medium}, \texttt{high}$\}$
    \end{itemize}
    \item Dependent parameters:
    \begin{itemize}
      \item If we use the Gaussian kernel for the SVM, what is its width?
    \end{itemize}
  \end{itemize}
\end{vbframe}

\begin{vbframe}{Components of tuning problem}
\begin{itemize}
\item The learner (possibly: several competing learners?)
\item The performance measure. Determined by the application. Not necessarily identical to the loss function that the learner tries to minimize.
  We could even be interested in multiple measures simultaneously, 
  e.g., accuracy and sparseness of our model, TPR and PPV, etc. 
\item A (resampling) procedure for estimating the predictive performance 
\item The learner's hyperparameters and their respective regions-of-interest over which we optimize
\end{itemize}
\end{vbframe}



\begin{vbframe}{Hyperparameter Tuning}
% \begin{itemize}
% \item Optimize hyperparameters for learner w.r.t. prediction error
Tuner proposes configuration, eval by resampling, tuner receives performance, iterate
% \end{itemize}
\begin{columns}[c, onlytextwidth]
\column{0.45\textwidth}
  \includegraphics[trim={0cm 0cm 0cm 0cm}, clip, width=1.2\textwidth]{figure_man/chain.jpg}
\column{0.45\textwidth}
\includegraphics[trim={1cm 0cm 1cm 0cm}, clip, width=1.2\textwidth]{figure_man/tuning_process.jpg}
\end{columns}

\end{vbframe}


\begin{vbframe}{Why is tuning so hard?}
\begin{itemize}
%\item Lots of literature exists for models, far less on efficient tuning.
\item Tuning is derivative-free (\enquote{black box problem}): It is usually impossible to compute derivatives of the objective (i.e., the resampled performance measure) that we optimize with regard to the HPs. All we can do is evaluate the performance for a given hyperparameter configuration.
\item Every evaluation requires one or multiple train and predict steps of the learner. I.e., every evaluation is very \textbf{expensive}.
\item Even worse: the answer we get from that evaluation is \textbf{not exact, but stochastic} in most settings, as we use resampling.
% \item Even worse: the function value we get from that evaluation is \textbf{likely also biased} -- it is difficult to evaluate the tested hyperparameter settings \emph{honestly}, i.e., in such a way that we neither over- nor underestimate their performance if we only have a limited amount of data available. (Remember resampling-based performance evaluation \& its problems -- this gets worse where tuning comes into play.)
\item Categorical and dependent hyperparameters aggravate our difficulties: the space of hyperparameters we optimize over has a non-metric, complicated structure.
\item For large and difficult problems parallelizing the computation seems relevant, to evaluate multiple HP configurations in parallel or to speed up the resampling-based performance evaluation
\end{itemize}
\end{vbframe}


\begin{vbframe}{Grid search}

\begin{itemize}
\item Simple technique which is still quite popular, tries all 
  HP combinations on a multi-dimensional discretized grid
\item For each hyperparameter a finite set of candidates is predefined
\item Then, we simply search all possible combinations in arbitrary order
\end{itemize}

\begin{footnotesize}
\begin{center}
Grid search over 10x10 points
\end{center}
\end{footnotesize}

<<fig.height=3.5, fig.align = "center">>=
plotTune = function(d) {
  d$TestAccuracy = mvtnorm::dmvnorm(x = d, mean = c(5,5), sigma = 40 * diag(2)) * 120 + 0.4
  pl = ggplot(data = d, aes(x = x, y = y, color = TestAccuracy))
  pl = pl + geom_point(size = d$TestAccuracy * 4)
  pl = pl + xlab("Parameter 1") + ylab("Parameter 2") + coord_fixed()
  return(pl)
}
x = y = seq(-10, 10, length.out = 10)
d = expand.grid(x = x, y = y)
pl = plotTune(d)
print(pl)
@


\framebreak

\begin{blocki}{Advantages}
\item Very easy to implement, therefore very popular
\item  All parameter types possible
\item  Parallelization is trivial
\end{blocki}

\begin{blocki}{Disadvantages}
\item  Combinatorial explosion, inefficient
\item  Searches large irrelevant areas
\item  Which values / discretization?
\end{blocki}
\end{vbframe}


\begin{vbframe}{Random search}

\begin{itemize}
\item Small variation of grid search
\item Uniformly sample from the region-of-interest
\end{itemize}


\begin{footnotesize}
\begin{center}
Random search over 100 points
\end{center}
\end{footnotesize}

<<fig.height=3.5>>=
x = runif(40, -10, 10)
y = runif(40, -10, 10)
d = data.frame(x = x, y = y)
pl = plotTune(d)
print(pl)
@

\framebreak

\begin{blocki}{Advantages}
\item Very easy to implement, therefore very popular
\item  All parameter types possible
\item  Parallelization is trivial
\item Anytime algorithm - we can always increase the budget when we are not satisfied
\item Often better than grid search, as each individual parameter has been tried with $m$ different values, when the search budget was $m$. Mitigates the problem of discretization
\end{blocki}

\begin{blocki}{Disadvantages}
\item As for grid search, many evaluations in areas with low likelihood for improvement
\end{blocki}
\end{vbframe}

\begin{vbframe}{Tuning Example}

Tuning gradient boosting with random search and 5CV on the spam data set for AUC

\begin{columns}
\begin{column}{0.35\textwidth}
\begin{tiny}
\begin{tabular}{|l|l|l|l}
Parameter          &  Type     & Min & Max \\
\hline
n.trees            &  integer  &   3   & 500 \\
shrinkage          &  numeric  &   0   &   1 \\
interaction        &  integer  &   1   &   5 \\
bag.fraction       &  numeric  & 0.2   & 0.9 \\
\end{tabular}
\end{tiny}
\end{column}
\begin{column}{0.65\textwidth}


<<fig.height=3.5>>=
tr = load2("rsrc/tune_example.RData")
df = as.data.frame(tr$opt.path)
ggd = df[, c("dob", "auc.test.mean")]
colnames(ggd) = c("iter", "auc")
ggd$auc = cummax(ggd$auc)
pl = ggplot(ggd, aes(x = iter, y = auc))
pl = pl + geom_line()
pl = pl + theme(axis.text=element_text(size=18), axis.title=element_text(size=22))
print(pl)
@

\end{column}
\end{columns}


\end{vbframe}

\begin{vbframe}{Advanced Tuning Techniques}
\begin{itemize}
\item Stochastic local search, e.g. simulated annealing
\item Genetic algorithms / CMAES
\item Iterated F-Racing
\item Model-based Optimization / Bayesian Optimization
\item Hyperband
\item $\ldots$
\end{itemize}
\end{vbframe}

\endlecture
