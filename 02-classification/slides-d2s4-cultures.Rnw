% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@
% Load all R packages and set up knitr
<<setup-r, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{9}{The Two Cultures of Statistical Modeling}
\lecture{Introduction to Machine Learning}


% \begin{vbframe}{Modeling: two cultures}
% \begin{center}
% <<fig.height=3, fig.width=12>>=
% grid.newpage()
% pushViewport(viewport(x = 0.1, y = 0.5, width = 0.14, height = 0.56, angle = 45))
% grid.rect(gp = gpar(lwd = 4))
% grid.text("x", rot = -45, gp = gpar(fontsize = 24, fontface = "bold"))
% popViewport()
% pushViewport(viewport(x = 0.5, y = 0.5, width = 0.4, height = 0.8, angle = 0))
% grid.rect(gp = gpar(lwd = 4))
% grid.text("System", gp = gpar(fontsize = 24, fontface = "bold"))
% popViewport()
% pushViewport(viewport(x = 0.9, y = 0.5, width = 0.14, height = 0.56, angle = 45))
% grid.rect(gp = gpar(lwd = 4))
% grid.text("y", rot = -45, gp = gpar(fontsize = 24, fontface = "bold"))
% popViewport()
% grid.lines(c(0.1 + sqrt(2*0.07^2), 0.3), rep(0.5, 2), arrow = arrow(length = unit(0.2, "inch"), type = "closed"),
%            gp = gpar(lwd = 4))
% grid.lines(c(0.7, 0.9 - sqrt(2*0.07^2)), rep(0.5, 2), arrow = arrow(length = unit(0.2, "inch"), type = "closed"),
%            gp = gpar(lwd = 4))
% @
% \end{center}
% System: nature, organism, chemical reaction, technical process, human behavior, \ldots
% \end{vbframe}
%
% \begin{vbframe}{two cultures: classical statistics}
% We assume that one can sufficiently describe the unknown system by the stochastic model
% $$y = f(x, \theta) + \epsilon$$
% and a \emph{parametric} function class for $f()$ is given. \\
% \lz
% \emph{Under the assumption} that the model is correct, now the usual operation of statistical inference can be built up:
% \begin{itemize}
%  \item Hypothesis tests, variance analysis, model comparison
%  \item Confidence intervals for parameters and predictions
% \end{itemize}
%
% \newpage
% Many articles in JASA, the Annals of Statistics etc. start with
% \enquote{Assume that the data are generated by the following model \ldots}.
%
% \begin{itemize}
% \item \textbf{Advantages:}
%   \begin{itemize}
%   \item Parameter can be interpreted if the model is easy enough
%   \item Good theory for model diagnostics
%   \end{itemize}
%  \item \textbf{Disadvantages (Breiman 2001):}
%    \begin{itemize}
%   \item \enquote{irrelevant theory}
%   \item \enquote{questionable scientific conclusions}
%   \end{itemize}
% \end{itemize}
% \end{vbframe}
%
% \begin{vbframe}{Two cultures: machine learning}
% \begin{center}
% <<fig.height=6, fig.width=12>>=
% grid.newpage()
% pushViewport(viewport(x = 0.1, y = 0.75, width = 0.14, height = 0.28, angle = 45))
% grid.rect(gp = gpar(lwd = 4))
% grid.text("x", rot = -45, gp = gpar(fontsize = 24, fontface = "bold"))
% popViewport()
% pushViewport(viewport(x = 0.5, y = 0.75, width = 0.4, height = 0.4, angle = 0))
% grid.rect(gp = gpar(lwd = 4))
% grid.text("System", gp = gpar(fontsize = 24, fontface = "bold"))
% popViewport()
% pushViewport(viewport(x = 0.9, y = 0.75, width = 0.14, height = 0.28, angle = 45))
% grid.rect(gp = gpar(lwd = 4))
% grid.text("y", rot = -45, gp = gpar(fontsize = 24, fontface = "bold"))
% popViewport()
% grid.lines(c(0.1 + sqrt(2*0.07^2), 0.3), rep(0.75, 2), arrow = arrow(length = unit(0.2, "inch"), type = "closed"),
%            gp = gpar(lwd = 4))
% grid.lines(c(0.7, 0.9 - sqrt(2*0.07^2)), rep(0.75, 2), arrow = arrow(length = unit(0.2, "inch"), type = "closed"),
%            gp = gpar(lwd = 4))
% grid.roundrect(0.5, 0.2, 0.7, 0.3, name = "rr", gp = gpar(lwd = 4))
% grid.text("Approximation by\nneuronal network, tree, ...", y = 0.2,
%           gp = gpar(fontsize = 24, fontface = "bold"))
% grid.curve(0.1, 0.55, 0.15, 0.2,
%    arrow = arrow(length = unit(0.2, "inch"), type = "closed"),
%    gp = gpar(lwd = 4))
% grid.curve(0.85, 0.2, 0.9, 0.55,
%    arrow = arrow(length = unit(0.2, "inch"), type = "closed"),
%    gp = gpar(lwd = 4))
% @
% \end{center}
% The system is considered as unknown, explicit modelling is not even tried.
% Essential measure of goodness is the prediction quality.
%
% \framebreak
%
% Use of methods which are able to model many systems (universal approximations), development of methods by examples of nature, intuitive behavioral or because of computational attractivity.
% \vspace{0.25cm}
% \begin{itemize}
%  \item \textbf{Advantages:}
%    \begin{itemize}
%    \item Many more model classes are available, much quicker development and implementation of new ideas.
%    \item For predictions, knowledge about the distribution of the parameters,
%    diligent task, knowledge about the error distribution is sufficient.
%    \end{itemize}
%  \item \textbf{Disadvantages:}
%    \begin{itemize}
%    \item Models are mostly hard to interpret (\enquote{black box})
%    \end{itemize}
% \end{itemize}
% \vspace{0.25cm}
% Many models which originally come from machine learning are also statistical mainstream nowadays (and have in parts been considerably improved).


\begin{vbframe}{Modeling: two cultures}
\textbf{Statistics, the Data Modeling Culture}
\begin{center}
\vspace{1cm}
  \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1cm,
      thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}]
    \node[punkt] (nature) {nature};
    \node[left=of nature] (x) {x};
    \node[right=of nature] (y) {y};
    \path[every node/.style={font=\sffamily\small}]
    (nature) edge node {} (y)
    (x) edge node  {} (nature)
    ;
  \end{tikzpicture} \\
\vspace{1cm}
\begin{itemize}
  \item In a strongly simplified world an arbitrary outcome $y$ is produced by the nature given the covariates $x$
  \item The knowledge about the natures true mechanisms range between entirely unknown and established (scientific) explanations of the mechanism
  \item Example: Outcome $y$ is the rent for apartments and covariates $x$ are size, number of bathrooms and location
\end{itemize}
\end{center}

\framebreak

\begin{itemize}
  \item Focuses on the modeling of data, which can be reduced to two targets:
  \begin{enumerate}
    \item Learn a model to predict the outcome for new covariates
    \item Get a better understanding about the relationship between covariates and outcome
  \end{enumerate}
\end{itemize}
\begin{center}
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1cm,
        thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}]
      \node[punkt] (natur) {Logistic Regression, \\Cox Model, \\GEE, \\ \ldots};
      \node[left=of natur] (x) {x};
      \node[right=of natur] (y) {y};
      \path[every node/.style={font=\sffamily\small}]
      (natur) edge node {} (y)
      (x) edge node  {} (natur)
      ;
    \end{tikzpicture}
  \end{center}
\begin{itemize}
  \item Find a stochastic model of the data-generating process:
  $$y = f(x, \text{parameters}, \text{random error})$$
\end{itemize}

\framebreak

In this \enquote{data modeling culture} a stochastic model for the data- generating process is assumed
\begin{blocki}{Typical assumptions and restrictions}
  \item Specific stochastic model that generated the data
  \item Distribution of residuals
  \item Linearity (e.g. linear predictor)
  \item Manual specification of interactions
\end{blocki}

\framebreak

\begin{blocki}{Problems}
  \item Conclusions about model, not about nature
  \item Assumptions often violated
  \item Often improper model evaluation\\
  $\Rightarrow$ can lead to irrelevant theory and questionable statistical conclusions
  \item Focus not on prediction
  \item Data models fail in areas like image and speech recognition
\end{blocki}

\framebreak

\textbf{Machine Learning, the Algorithmic Modeling Culture}
\lz
  \begin{center}
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1cm,
        thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}]
      \node[punkt] (natur) {unknown};
      \node[left=of natur] (x) {x};
      \node[right=of natur] (y) {y};
      \node[below=of natur, label=below:algorithm] (algo) { \includegraphics*[width = 3cm]{figure_man/machine.png}};
      \path[every node/.style={font=\sffamily\small}]
      (natur) edge node {} (y)
      (x) edge node  {} (natur)
      (x) edge[<->, bend right=30] node {} (algo)
      (y) edge[<->, bend left=30] node {} (algo)
      ;
    \end{tikzpicture}
  \end{center}
\lz
Find a function $\fx$ that minimizes the loss: $\Lxy$

\framebreak

\begin{itemize}
  \item In the \enquote{algorithmic modeling culture}, the true mechanism is treated as unknown
  \item It is not the target to find the true data-generating mechanism but to use an algorithm that imitates the mechanism as good as possible
  \item Modeling is reduced to a mere problem of function optimization: Given the covariates $x$, outcome $y$ and a loss function find a function $\fx$ which minimizes the loss for the prediction of the outcome
\end{itemize}
\begin{blocki}{Algorithm in Machine Learning}
  \item Boosting
  \item Support Vector Machines
  \item Artificial neural networks
  \item Random Forests
  \item Hidden Markov
  \item Bayes-Nets
  \item \ldots
\end{blocki}

\framebreak
\ \\
\lz
\structure{Summary}\\
\emph{The data modeling culture tries to find the true data-generating mechanism, the algorithmic modeling culture tries to imitate the true mechanism as good as possible.}\\
\vspace{1cm}
\structure{Rashomon Effect}\\
\emph{(Often) Many different models describe a situation equally accurate which makes it difficult to find the true mechanism in the data modeling cultures.}

\framebreak

\begin{blocki}{Dimensionality of the data}
  \item The higher the dimensionality of the data (\# covariates) the more difficult is the separation of signal and noise
  \item Common practice in data modeling: variable selection (by expert selection or data driven) and reduction of dimensionality (e.g. PCA)
  \item Common practice in algorithmic modeling: Engineering of new features (covariates) to increase predictive accuracy; algorithms robust for many covariates
\end{blocki}

\framebreak

\textbf{Prediction vs. Interpretation}\\
\vspace{1.5cm}
\begin{center}
  \begin{tikzpicture}
    \draw (0,0) -- (0,1.5) -- (9.9,0) -- (0,0);
    \draw  (0, 1.6) -- (10,1.6) -- (10, 0.1) -- (0, 1.6);
    \node at (1.5, 0.2) {Interpretability};
    \node at (8, 1.3) {Predictive accuracy};
    \node at (1.3, -0.5) {Tree};
    \node at (9, -0.5) {Random Forest};
    \node at (1.5, -1.2) {Logistic Regression};
    \node at (9, -1.2) {Neural networks};
    \node at (1.3, -1.9) {\ldots};
    \node at (9, -1.9) {\ldots};
  \end{tikzpicture}
\end{center}

\framebreak

\begin{itemize}
  \item There is a trade-off between interpretability and predictive accuracy: the models that are good in prediction are often complex and models that are easy to interpret are often bad predictors
  \item Example trees and random forests: A single decision tree is very intuitive and easy to read for non-professionals, but they are unstable and give weak predictions while a complex aggregation of decision trees (random forest) has an excellent prediction accuracy, but it is impossible to interpret the model structure
\end{itemize}

\framebreak

\begin{blocki}{Goodness of model}
  \item Data modeling culture: Goodness of fit often based on model assumptions (e.g. AIC) and calculated on training data
  \item Algorithmic modeling culture: Evaluation of predictive accuracy with an extra test set or cross validation
\end{blocki}
How good is a statistical model if the predictive accuracy is weak? Is it legit to interpret parameters and p-values?

\framebreak

Different notation for machine learning and statistics
\lz
  \begin{table}
    \begin{tabular}{rr}
      \hline
      Machine Learning & Statistics \\
      \hline
      Feature,Attribute & Covariate \\
      Minimizing loss & Maximizing likelihood \\
      Learning & Fitting/Estimation \\
      Weights & Parameter/Coefficient \\
      Bias term & Intercept \\
      Hypothesis & Model \\
      Example/Instance & Observation \\
      Supervised Learning & Regression/Classification \\
      Label & Response \\
      Data mining (good) & Data mining (bad)\\
      Log. regr. is classification & Log. regr. is regression\\
      \hline
    \end{tabular}
  \end{table}
\end{vbframe}

\endlecture
