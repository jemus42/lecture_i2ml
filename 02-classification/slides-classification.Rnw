% Introduction to Machine Learning
% Day 1

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@
% Load all R packages and set up knitr
<<setup-r, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{5}{Introduction to Classification}
\lecture{Introduction to Machine Learning}
\framebreak

% <<include=FALSE>>=
%   library(datasets)
% df <- as.data.frame(Titanic)
% titanic.raw <- NULL
% for (i in 1:4)
% {
%   titanic.raw <- cbind(titanic.raw,
%                        rep(as.character(df[,i]),df$Freq))
% }
% titanic.raw <- as.data.frame(titanic.raw)
% names(titanic.raw) <- names(df)[1:4]
% @
%
\begin{vbframe}{Classification}
% The Titanic dataset is a famous beginner's problem for binary classification (see for example \href{https://www.kaggle.com/c/titanic}{the Titanic competition on kaggle.com}).
% The goal is to classify the passengers of the Titanic into Survived $\in$ $\{Yes, No\}$ given information about the class they traveled in, the sex and the age.
%
%
%  % \column{0.5\textwidth}
% \begin{center}
% \textbf{Titanic Passengers} \\
% \vspace{0.25cm}
% <<>>=
%   kable(unique(titanic.raw)[c(1:8,12:15),], row.names = FALSE)
% @
%   \end{center}
%
%  \framebreak

 % \begin{vbframe}{Classification}
We want to assign new observations to known categories according to criteria learned from a training set.
{\centering \includegraphics[height = .7\textheight, width = .8\textwidth]{figure_man/classifier.pdf}}

% \end{vbframe}
%
%   \begin{vbframe}{Classification}

  Assume we are given a \emph{classification problem}:
  \begin{eqnarray*}
  & x \in \Xspace \quad & \text{feature vector}\\
  & y \in \Yspace = \gset \quad & \text{\emph{categorical} output variable (label)}\\
  &\D = \Dset & \text{observations of $x$ and $y$}
  \end{eqnarray*}


  Classification usually means to construct $g$ discriminant functions $f_1(x), \ldots f_g(x)$,
  so that we choose our class as
  $$h(x) = \argmax_k f_k(x)$$ for $k = 1, 2,\ldots, g$.

  \lz

  This divides the feature space into $g$ \emph{decision regions} $\{x \in \Xspace | h(x) = k\}$.
  These regions are separated by the \emph{decision boundaries} where ties occur between these
  regions.

\framebreak

<<>>=
iris_petal <- makeClassifTask(data = iris[,-(1:2)], target = "Species")
iris_sepal <- makeClassifTask(data = iris[,-(3:4)], target = "Species")
iris_sepal_bin <- makeClassifTask(data = subset(iris[,-(3:4)], Species != "setosa"),
                                  target = "Species")
# plotLearnerPrediction(makeLearner("classif.kknn"), iris_petal, cv=0, prob.alpha = FALSE, gridsize = 100)
# plotLearnerPrediction(makeLearner("classif.kknn"), iris_sepal, cv=0, prob.alpha = FALSE, gridsize = 100)
plotLearnerPrediction(makeLearner("classif.kknn", k = 25),
                      iris_sepal_bin, cv = 0, prob.alpha = FALSE, gridsize = 400) +
  scale_fill_viridis_d()
@


<<echo=FALSE, warning=FALSE, message=FALSE>>=
# n = 100
# library(kknn)
# bin.class = data.frame(x1 = runif(n=n, 0, 5),
#                         x2 = runif(n=n, -2, 4))
#
# bin.class $class = (bin.class $x1 + 0.5*bin.class $x2 + rnorm(n, sd = 0.3)) > 3
# bin.class $class = factor(bin.class $class, levels = c(TRUE,FALSE), labels = c(1,2))
# bin.class.task =  makeClassifTask('bin.class.task', bin.class , 'class')
# lrn1 = makeLearner("classif.kknn", predict.type='prob')
# plotLearnerPrediction(lrn1, bin.class.task, cv=0, prob.alpha = FALSE, gridsize = 400)

@



\end{vbframe}
\begin{vbframe}{Linear classifier}

  If these functions $f_k(x)$ can be specified as linear functions,
  we will call the classifier a \emph{linear classifier}. We can then write a
  decision boundary as $x^T\theta = 0$, which is a hyperplane separating two classes.

    \lz

  If only 2 classes exist (\textbf{binary classification}), we can simply use a single discriminant function $f(x) = f_1(x) - f_2(x)$
  (note that it would be more natural here to label the classes with \{+1, -1\} or \{0, 1\}).

  \lz
  Note that all linear classifiers can represent non-linear decision boundaries in our original input space if we include \emph{derived features} like higher order interactions, polynomials or other transformations of $x$ in the model.


  \framebreak

<<>>=
 plotLearnerPrediction(makeLearner("classif.logreg"),
                      iris_sepal_bin, cv = 0, prob.alpha = FALSE, gridsize = 400) +
  scale_fill_viridis_d()
@

\end{vbframe}
\begin{vbframe}{Classification Approaches}

  Two fundamental approaches exist to construct classifiers:\\
  The \textbf{generative approach} and the \textbf{discriminant approach}.

\lz
They tackle the classification problem from different angles:

\begin{itemize}
\item \emph{Generative} classification approaches assume a data generating process in which the distribution of the features $x$ is different for the various classes of the output $y$, and try to learn these conditional distributions:\\ \enquote{Which $y$ tends to have $x$ like these?}
\lz
\item \emph{Discriminant} approaches use \emph{empirical risk minimization} based on a suitable loss function:\\ \enquote{What is the best prediction for $y$ given these $x$?}
\end{itemize}
\end{vbframe}

\begin{vbframe}{Generative approach}

  The \emph{generative approach}
  models $\pdfxyk$, usually by making some assumptions about the structure of these distributions, and employs the Bayes theorem:
  $$\pikx = \postk = \frac{\P(x | y = k) \P(y = k)}{\P(x)} \propto \pdfxyk \pik $$
  to allow the computation of $\pikx$.

  The discriminant functions are then $\pikx$ or $\lpdfxyk + \lpik$.\\

  Prior class probabilities $\pik$ are easy to estimate from the training data.

  \lz

  Examples:
  \begin{itemize}
  \item Naive Bayes classifier
  \item Linear discriminant analysis (generative, linear)
  \item Quadratic discriminant analysis (generative, not linear)
  \end{itemize}

{\small Note: LDA and QDA have 'discriminant' in their name, but are generative models! (\dots sorry.)}

\framebreak


\lz
\lz

\textbf{Representation:} Conditional feature distributions $\pdfxyk$ and prior label probabilities $\pik$. \\
Often restricted to certain kinds of distributions (e.g. $\mathcal N(\mu,\Sigma)$) depending on the specific method, representation then via the distributions' parameters.

\lz

\textbf{Optimization:} Often analytic solutions (LDA, QDA); density estimation (Naive Bayes).

\lz

\textbf{Evaluation:} Classification loss functions. Typically: negative log posterior probability.

\end{vbframe}

\begin{vbframe}{Discriminant approach}
  The \emph{discriminant approach} tries to optimize the discriminant functions directly, usually via empirical
  risk minimization.
  $$ \fh = \argmin_{f \in \Hspace} \riske(f) = \argmin_{f \in \Hspace} \sumin \Lxyi.$$
  \lz
  Examples:
  \begin{itemize}
  \item Logistic regression (discriminant, linear)
  \item kNN classifier (discriminant, not linear)
  \end{itemize}

\lz
Representation and optimization depend on the specific learner.\\
Evaluation via classification loss functions.

\end{vbframe}
% \begin{vbframe}{Binary Classification loss}
%   We will now introduce a loss function for binary output.
%   Notice that $f(x)$ outputs a score and $\sign(\fx)$ will be the corresponding label.
%
%   \lz
%
%   Most following loss functions will depend on the so-called \emph{margin}.
%   We are using the coding $y \in \{-1, 1\}$.
%   $$
%   y\fx =  \begin{cases} > 0  \quad &\text{ if } y = \sign(\fx) \text{ (correct classification)} \\
%                         < 0 \quad &\text{ if } y \ne \sign(\fx) \text{ (misclassification)} \end{cases}
%   $$
%
%
%
%
%   \framebreak
%
%
%   \end{vbframe}
%
%
%   \begin{vbframe}{Binary classification loss - 0-1 loss}
%   \begin{itemize}
%   \item $\Lxy = [y \neq f(x)] = [\yf < 0]$
%   \item Intuitive, often what we are interested in
%   \item Not even continuous, even for linear $f$ the optimization problem is NP-hard and
%     close to intractable
%   \item It's better to optimize other loss functions!
%   \end{itemize}
%
%   <<echo=FALSE, results='hide', fig.height=3, fig.align='center'>>=
%   x = seq(-2, 2, by = 0.01); y = as.numeric(x < 0)
%   qplot(x, y, geom = "line", xlab = expression(yf(x)), ylab = expression(L(yf(x))))
%   @
%
%  %  The minimizer of $\risk(f)$ for the 0-1-loss is
%  %
%  %   \begin{eqnarray*}
%  % \fh(x) &=&    \footnotesize \begin{cases} 1 \quad \text{ if } \pix > 1/2 \\ -1 \quad \pix < 1/2  \end{cases}
%  % \end{eqnarray*}
%
%   \end{vbframe}

  %
  % \begin{vbframe}{Multiclass 0-1 loss and Bayes classifier}
  %
  % Assume $h \in \Yspace$ with $|\Yspace| = g$. We can define the 0-1-loss for multiclass:
  % $$L(y, \hx) = [y \neq \hx]$$.
  % We can in general rewrite the loss again as:
  % \begin{eqnarray*}
  %   \risk(h) & = & \E_{xy}[L(y, h)] = E_x [ E_{y|x} [ L(y, \hx) ] ] =  \\
  %            & = & E_x \sum_{k \in \Yspace} L(k, \hx) P(y = k| x = x) \\
  %            & = & E_x \sum_{k \in \Yspace} L(k, \hx) \pikx
  % \end{eqnarray*}
  %
  % NB: This works, too, (of course) for $\Yspace = \{-1, 1\}$ and a score function $f$:
  % $$
  % \risk(f) = \mathbb{E}_x [L(1, f(x)) \pix + L(-1, f(x)) (1 - \pix)].
  % $$
  %
  % \framebreak
  % We can again minimize pointwise, and for a general cost-sensitive loss $L(y, h)$ this is:
  %
  % \begin{eqnarray*}
  %   \hxh &=& \argmin_{l \in \Yspace} \sum_{k \in \Yspace} L(k, l) \pikx \\
  % \end{eqnarray*}
  %
  % For the 0-1 loss this becomes:
  %
  % $$
  % \hxh = \argmin_{k \in \Yspace} (1 - \pikx) = \argmax_{k \in \Yspace} \pikx
  % $$
  %
  % If we know $\Pxy$ perfectly (and hence $\pikx$), we have basically constructed the loss-optimal
  % classifier and we call it the \emph{Bayes classifier} and its expected loss the \emph{Bayes loss} or
  % \emph{Bayes error rate} for 0-1-loss.


  % and get the risk function


  % The minimizer of $\risk(f)$ for the 0-1-loss is

  % \begin{eqnarray*}
  % \fh(x) &=&    \footnotesize \begin{cases} 1 \quad \text{ if } \pix > 1/2 \\ -1 \quad \pix < 1/2  \end{cases}
  % \end{eqnarray*}

 % \lz


  %\end{vbframe}


  % \framebreak

  % \textbf{Square-loss:}


  % If we use instead the \emph{square-loss}

  % $$
  % \Lxy = (1-y\fx)^2,
  % $$

  % we get the risk function

  % \begin{eqnarray*}
  % \risk(f) &=& \mathbb{E}_x [(1-\fx)^2 \pix + (1+\fx)^2 (1-\pix)] \\
  % &=& \mathbb{E}_x [1 + 2\fx + \fx^2-4\fx\pix].
  % \end{eqnarray*}

  % By differentiating w. r. t. $f(x)$ we get the minimizer of $\risk(f)$ for the square loss function

  % \begin{eqnarray*}
  % \fh(x) &=& 2\pix -1.
  % \end{eqnarray*}

  % \framebreak

  % \vspace*{0.2cm}

  % The square loss function tends to penalize outliers excessively. Functions which yield high values of $(x)$ will perform poorly with the square loss function, since high values of $yf(x)$ will be penalized severely, regardless of whether the signs of $y$ and $f(x)$ match.

  % <<echo=FALSE, results='hide', fig.height= 1.8, fig.asp = 0.4>>=
  % x = seq(-2, 5, by = 0.01)
  % plot(x, (1-x)^2, type = "l", xlab = expression(yf(x)), ylab = expression(paste((1-yf(x))^2)), main = "Square loss")
  % box()
  % @

  % \lz

  % \end{vbframe}

  % \begin{vbframe}{Bin. classif. losses - Hinge loss}
  % \begin{itemize}
  % \item $\Lxy = \max\{0, 1 - \yf\}$, used in SVMs
  % \item Convex
  % \item No derivatives for $\yf = 1$, optimization becomes harder
  % \item More robust, outliers in $y$ are less problematic
  % \end{itemize}
  %
  % <<echo=FALSE, results='hide', fig.height=3, fig.align='center'>>=
  % x = seq(-2, 2, by = 0.01); y = pmax(0, 1 - x)
  % qplot(x, y, geom = "line", xlab = expression(yf(x)), ylab = expression(L(yf(x))))
  % @
  %
  % % \framebreak
  %
  % % we get the risk function
  %
  % % \begin{eqnarray*}
  % % \risk(f) &=& \mathbb{E}_x [\max\{0, 1 - \fx\} \pix + \max\{0, 1 + y\fx\} (1-\pix)].
  % % \end{eqnarray*}
  %
  % % The minimizer of $\risk(f)$ for the hinge loss function is
  %
  % % \begin{eqnarray*}
  %   % $fh(x) =  \footnotesize \begin{cases} 1 \quad \text{ if } \pix > 1/2 \\ -1 \quad \pix < 1/2  \end{cases}$
  % % \end{eqnarray*}
  %
  % \end{vbframe}

  % \begin{vbframe}{Bin. classif. losses - Cross-entropy loss}
  % \begin{itemize}
  %   \item Using the alternative label convention $y\in \{0, 1\}$
  %   \item $\Lxy = -y\ln(\pix)-(1-y)\ln(1-\pix)$
  %   \item Basically the same as the logistic loss when we act on $\pix \in [0,1]$ instead of $\fx \in \R$
  %   \item  The cross entropy loss is closely related to the Kullback-Leibler divergence, which will be introduced later in the chapter.
  %   \item Very often used in neural networks with binary output nodes for classification.
  % \end{itemize}
  %
  %
  % \end{vbframe}
  %
  %
  % \begin{vbframe}{Bin. classif. losses - Exponential loss}
  % \begin{itemize}
  %   \item $\Lxy = \exp(-y\fx)$, used in AdaBoost
  % \item Convex, differentiable, not robust
  % \item Quite similar to logistic loss
  % \end{itemize}
  %
  % <<fig.height=3>>=
  % x = seq(-2, 2, by = 0.01); y = exp(-x)
  % qplot(x, y, geom = "line", xlab = expression(yf(x)), ylab = expression(L(yf(x))))
  % @
  %
  % % we get the risk function
  %
  % % \vspace*{-.5cm}
  %
  % % \begin{eqnarray*}
  % % \risk(f) &=& \mathbb{E}_x [\exp(-\fx) \pix + \exp(f(x)) (1-\pix)]
  % % \end{eqnarray*}
  %
  % \end{vbframe}
  %
  % \begin{vbframe}{Risk minimizing functions}
  %
  % Overview of binary classification losses and the corresponding risk minimizing functions:
  %
  % \lz
  %
  % \begin{tabular}{p{2.5cm}|p{3.5cm}|p{4.5cm}}
  %   loss name & loss formula  & minimizing function \\
  %   \hline
  %   0-1 & $[y \neq \hx]$ & $\hxh = \footnotesize \begin{cases} 1 \quad \text{ if } \pix > 1/2 \\ -1 \quad \pix < 1/2  \end{cases}$ \\
  %   Hinge & $\max\{0, 1 - \yf\}$ & $\fh(x) =  \footnotesize \begin{cases} 1 \quad \text{ if } \pix > 1/2 \\ -1 \quad \pix < 1/2  \end{cases}$ \\
  %   Logistic & $\ln(1+\exp(-y\fx))$ & $\fh(x) =  \ln \biggl(\frac{\pix}{1-\pix}\biggr)$ \\
  %   Cross entropy & $-y\ln(\pix) \newline -(1-y)\ln(1-\pix)$ & \\
  %   & & \\
  %   Exponential & $\exp(-y\fx)$ &
  %
  % \end{tabular}
  %
  % \end{vbframe}

  % \section{Selected methods for regression and classification}



  \endlecture
