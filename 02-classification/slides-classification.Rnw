% Introduction to Machine Learning
% Day 1

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@
% Load all R packages and set up knitr
<<setup-r, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{5}{Introduction to Classification}
\lecture{Introduction to Machine Learning}
\framebreak

<<include=FALSE>>=
  library(datasets)
df <- as.data.frame(Titanic)
titanic.raw <- NULL
for (i in 1:4)
{
  titanic.raw <- cbind(titanic.raw,
                       rep(as.character(df[,i]),df$Freq))
}
titanic.raw <- as.data.frame(titanic.raw)
names(titanic.raw) <- names(df)[1:4]
@

\begin{vbframe}{Binary Classification: Example}
The Titanic dataset is a famous beginner's problem for binary classification (see for example \href{https://www.kaggle.com/c/titanic}{the Titanic competition on kaggle.com}).
The goal is to classify the passengers of the Titanic into Survived $\in$ $\{Yes, No\}$ given information about the class they travelled in, the sex and the age.


 % \column{0.5\textwidth}
\begin{center}
\textbf{Titanic Passengers} \\
\vspace{0.25cm}
<<>>=
  kable(unique(titanic.raw)[c(1:8,12:15),], row.names = FALSE)
@
  \end{center}

 \framebreak

 % \begin{vbframe}{Classification}
Another example where we want to assign the new observations to known categories according to criteria learned from a training set.
\includegraphics{figure_man/classifier.pdf}
\end{vbframe}

  \begin{vbframe}{Classification}

  Assume we are given a \emph{classification problem}:
  \begin{eqnarray*}
  & x \in \Xspace \quad & \text{feature vector}\\
  & y \in \Yspace = \gset \quad & \text{\emph{categorical} output variable (label)}\\
  &\D = \Dset & \text{observations of $x$ and $y$}
  \end{eqnarray*}


  Classification usually means to construct $g$ discriminant functions $f_1(x), \ldots f_g(x)$,
  so that we choose our class as
  $$h(x) = \argmax_k f_k(x)$$ for $k = 1, 2,\ldots, g$.

  \lz

  This divides the feature space into $g$ \emph{decision regions} $\{x \in \Xspace | h(x) = k\}$.
  These regions are separated by the \emph{decision boundaries} where ties occur between these
  regions.

\framebreak

<<echo=FALSE, warning=FALSE, message=FALSE>>=
set.seed(42)
n = 100
library(kknn)
bin.class = data.frame(x1 = runif(n=n, 0, 5),
                        x2 = runif(n=n, -2, 4))

bin.class $class = (bin.class $x1 + 0.5*bin.class $x2 + rnorm(n, sd = 0.3)) > 3
bin.class $class = factor(bin.class $class, levels = c(TRUE,FALSE), labels = c(1,2))
bin.class.task =  makeClassifTask('bin.class.task', bin.class , 'class')
lrn1 = makeLearner("classif.kknn", predict.type='prob')
plotLearnerPrediction(lrn1, bin.class.task, cv=0, prob.alpha = FALSE, gridsize = 400)
@

\end{vbframe}
\begin{vbframe}{Linear classifier}

  If these functions $f_k(x)$ can be specified as linear functions (possibly through a rank-preserving,
  monotone transformation), we will call the classifier a \emph{linear classifier}. We can then write a
  decision boundary as $x^T\theta = 0$, which is a hyperplane separating two classes.

  \lz
  Note that all linear classifiers can represent non-linear decision boundaries in our original input space,
  if we opt to manually construct derived features like higher order interactions or polynomial features.

  \lz

  If only 2 classes exist, we could also construct a single discriminant function $f(x) = f_1(x) - f_2(x)$
  (note that it would be more natural here to label the classes with \{+1, -1\} or \{0, 1\}).
<<>>=
  lrn1 = makeLearner("classif.logreg", predict.type='prob')
  plotLearnerPrediction(lrn1, bin.class.task, cv=0, prob.alpha = FALSE, gridsize = 300)
@

\end{vbframe}
\begin{vbframe}{Classification Approaches}

  Two fundamental approaches exist to construct classifiers:\\
  The \emph{generative approach} and the \emph{discriminant approach}.

\lz
They tackle the classification problem from different angles:\\
The generative approach assumes a data generating process, in which the $x$ have different distributions, depending on the class.
The discriminant approach attempts to minimize a loss function.

\end{vbframe}
\begin{vbframe}{Generative approach}

  The \emph{generative approach} employs the Bayes theorem:
  $$\pikx = \postk = \frac{\P(x | y = k) \P(y = k)}{\P(x)} \propto \pdfxyk \pik $$
  and models $\pdfxyk$ (usually by assuming something about the structure of this distribution)
  to allow the computation of $\pikx$.
  Discriminant function are now $\pikx$ or $\lpdfxyk + \lpik$
  \lz

  Examples:
  \begin{itemize}
  \item Naive Bayes classifier
  \item Linear discriminant analysis (generative, linear)
  \item Quadratic discriminant analysis (generative, not linear)
  \end{itemize}

Note: LDA and QDA have 'discriminant' in their name, but are generative models!
\end{vbframe}


\begin{vbframe}{Discriminant approach}
  The \emph{discriminant approach} tries to model the discriminant score function directly, often by loss
  minimization.
  $$ \fh = \argmin_{f \in \Hspace} \risk(f).$$

  \begin{eqnarray*}
 \risk(f) &=& \E_{xy}[\Lxy] = \int_X \int_Y \Lxy p(x, y) dy~dx \\
  &=& \int_X \int_Y \Lxy p(y|x)p(x) dy~dx
  \end{eqnarray*}

  \lz
  Examples:
  \begin{itemize}
  \item Logistic regression (discriminant, linear)
  \item Support vector machine (discriminant, not linear)
  \end{itemize}


  \end{vbframe}
\begin{vbframe}{Classification loss}
  We will now introduce a loss function for binary output.
  Notice that $f(x)$ outputs a score and $\sign(\fx)$ will be the corresponding label.

  \lz

  Most following loss functions will depend on the so-called \emph{margin}.
  We are using the coding $y \in \{-1, 1\}$.
  $$
  y\fx =  \begin{cases} > 0  \quad &\text{ if } y = \sign(\fx) \text{ (correct classification)} \\
                        < 0 \quad &\text{ if } y \ne \sign(\fx) \text{ (misclassification)} \end{cases}
  $$




  \framebreak


  \end{vbframe}


  \begin{vbframe}{Bin. classif. losses - 0-1 loss}
  \begin{itemize}
  \item $\Lxy = [y \neq f(x)] = [\yf < 0]$
  \item Intuitive, often what we are interested in
  \item Not even continuous, even for linear $f$ the optimization problem is NP-hard and
    close to intractable
  \item It's better to optimize other loss functions!
  \end{itemize}

  <<echo=FALSE, results='hide', fig.height=3, fig.align='center'>>=
  x = seq(-2, 2, by = 0.01); y = as.numeric(x < 0)
  qplot(x, y, geom = "line", xlab = expression(yf(x)), ylab = expression(L(yf(x))))
  @

 %  The minimizer of $\risk(f)$ for the 0-1-loss is
 %
 %   \begin{eqnarray*}
 % \fh(x) &=&    \footnotesize \begin{cases} 1 \quad \text{ if } \pix > 1/2 \\ -1 \quad \pix < 1/2  \end{cases}
 % \end{eqnarray*}

  \end{vbframe}

  %
  % \begin{vbframe}{Multiclass 0-1 loss and Bayes classifier}
  %
  % Assume $h \in \Yspace$ with $|\Yspace| = g$. We can define the 0-1-loss for multiclass:
  % $$L(y, \hx) = [y \neq \hx]$$.
  % We can in general rewrite the loss again as:
  % \begin{eqnarray*}
  %   \risk(h) & = & \E_{xy}[L(y, h)] = E_x [ E_{y|x} [ L(y, \hx) ] ] =  \\
  %            & = & E_x \sum_{k \in \Yspace} L(k, \hx) P(y = k| x = x) \\
  %            & = & E_x \sum_{k \in \Yspace} L(k, \hx) \pikx
  % \end{eqnarray*}
  %
  % NB: This works, too, (of course) for $\Yspace = \{-1, 1\}$ and a score function $f$:
  % $$
  % \risk(f) = \mathbb{E}_x [L(1, f(x)) \pix + L(-1, f(x)) (1 - \pix)].
  % $$
  %
  % \framebreak
  % We can again minimize pointwise, and for a general cost-sensitive loss $L(y, h)$ this is:
  %
  % \begin{eqnarray*}
  %   \hxh &=& \argmin_{l \in \Yspace} \sum_{k \in \Yspace} L(k, l) \pikx \\
  % \end{eqnarray*}
  %
  % For the 0-1 loss this becomes:
  %
  % $$
  % \hxh = \argmin_{k \in \Yspace} (1 - \pikx) = \argmax_{k \in \Yspace} \pikx
  % $$
  %
  % If we know $\Pxy$ perfectly (and hence $\pikx$), we have basically constructed the loss-optimal
  % classifier and we call it the \emph{Bayes classifier} and its expected loss the \emph{Bayes loss} or
  % \emph{Bayes error rate} for 0-1-loss.


  % and get the risk function


  % The minimizer of $\risk(f)$ for the 0-1-loss is

  % \begin{eqnarray*}
  % \fh(x) &=&    \footnotesize \begin{cases} 1 \quad \text{ if } \pix > 1/2 \\ -1 \quad \pix < 1/2  \end{cases}
  % \end{eqnarray*}

 % \lz


  %\end{vbframe}


  % \framebreak

  % \textbf{Square-loss:}


  % If we use instead the \emph{square-loss}

  % $$
  % \Lxy = (1-y\fx)^2,
  % $$

  % we get the risk function

  % \begin{eqnarray*}
  % \risk(f) &=& \mathbb{E}_x [(1-\fx)^2 \pix + (1+\fx)^2 (1-\pix)] \\
  % &=& \mathbb{E}_x [1 + 2\fx + \fx^2-4\fx\pix].
  % \end{eqnarray*}

  % By differentiating w. r. t. $f(x)$ we get the minimizer of $\risk(f)$ for the square loss function

  % \begin{eqnarray*}
  % \fh(x) &=& 2\pix -1.
  % \end{eqnarray*}

  % \framebreak

  % \vspace*{0.2cm}

  % The square loss function tends to penalize outliers excessively. Functions which yield high values of $(x)$ will perform poorly with the square loss function, since high values of $yf(x)$ will be penalized severely, regardless of whether the signs of $y$ and $f(x)$ match.

  % <<echo=FALSE, results='hide', fig.height= 1.8, fig.asp = 0.4>>=
  % x = seq(-2, 5, by = 0.01)
  % plot(x, (1-x)^2, type = "l", xlab = expression(yf(x)), ylab = expression(paste((1-yf(x))^2)), main = "Square loss")
  % box()
  % @

  % \lz

  % \end{vbframe}

  % \begin{vbframe}{Bin. classif. losses - Hinge loss}
  % \begin{itemize}
  % \item $\Lxy = \max\{0, 1 - \yf\}$, used in SVMs
  % \item Convex
  % \item No derivatives for $\yf = 1$, optimization becomes harder
  % \item More robust, outliers in $y$ are less problematic
  % \end{itemize}
  %
  % <<echo=FALSE, results='hide', fig.height=3, fig.align='center'>>=
  % x = seq(-2, 2, by = 0.01); y = pmax(0, 1 - x)
  % qplot(x, y, geom = "line", xlab = expression(yf(x)), ylab = expression(L(yf(x))))
  % @
  %
  % % \framebreak
  %
  % % we get the risk function
  %
  % % \begin{eqnarray*}
  % % \risk(f) &=& \mathbb{E}_x [\max\{0, 1 - \fx\} \pix + \max\{0, 1 + y\fx\} (1-\pix)].
  % % \end{eqnarray*}
  %
  % % The minimizer of $\risk(f)$ for the hinge loss function is
  %
  % % \begin{eqnarray*}
  %   % $fh(x) =  \footnotesize \begin{cases} 1 \quad \text{ if } \pix > 1/2 \\ -1 \quad \pix < 1/2  \end{cases}$
  % % \end{eqnarray*}
  %
  % \end{vbframe}

  % \begin{vbframe}{Bin. classif. losses - Cross-entropy loss}
  % \begin{itemize}
  %   \item Using the alternative label convention $y\in \{0, 1\}$
  %   \item $\Lxy = -y\ln(\pix)-(1-y)\ln(1-\pix)$
  %   \item Basically the same as the logistic loss when we act on $\pix \in [0,1]$ instead of $\fx \in \R$
  %   \item  The cross entropy loss is closely related to the Kullback-Leibler divergence, which will be introduced later in the chapter.
  %   \item Very often used in neural networks with binary output nodes for classification.
  % \end{itemize}
  %
  %
  % \end{vbframe}
  %
  %
  % \begin{vbframe}{Bin. classif. losses - Exponential loss}
  % \begin{itemize}
  %   \item $\Lxy = \exp(-y\fx)$, used in AdaBoost
  % \item Convex, differentiable, not robust
  % \item Quite similar to logistic loss
  % \end{itemize}
  %
  % <<fig.height=3>>=
  % x = seq(-2, 2, by = 0.01); y = exp(-x)
  % qplot(x, y, geom = "line", xlab = expression(yf(x)), ylab = expression(L(yf(x))))
  % @
  %
  % % we get the risk function
  %
  % % \vspace*{-.5cm}
  %
  % % \begin{eqnarray*}
  % % \risk(f) &=& \mathbb{E}_x [\exp(-\fx) \pix + \exp(f(x)) (1-\pix)]
  % % \end{eqnarray*}
  %
  % \end{vbframe}
  %
  % \begin{vbframe}{Risk minimizing functions}
  %
  % Overview of binary classification losses and the corresponding risk minimizing functions:
  %
  % \lz
  %
  % \begin{tabular}{p{2.5cm}|p{3.5cm}|p{4.5cm}}
  %   loss name & loss formula  & minimizing function \\
  %   \hline
  %   0-1 & $[y \neq \hx]$ & $\hxh = \footnotesize \begin{cases} 1 \quad \text{ if } \pix > 1/2 \\ -1 \quad \pix < 1/2  \end{cases}$ \\
  %   Hinge & $\max\{0, 1 - \yf\}$ & $\fh(x) =  \footnotesize \begin{cases} 1 \quad \text{ if } \pix > 1/2 \\ -1 \quad \pix < 1/2  \end{cases}$ \\
  %   Logistic & $\ln(1+\exp(-y\fx))$ & $\fh(x) =  \ln \biggl(\frac{\pix}{1-\pix}\biggr)$ \\
  %   Cross entropy & $-y\ln(\pix) \newline -(1-y)\ln(1-\pix)$ & \\
  %   & & \\
  %   Exponential & $\exp(-y\fx)$ &
  %
  % \end{tabular}
  %
  % \end{vbframe}

  % \section{Selected methods for regression and classification}



  \endlecture
