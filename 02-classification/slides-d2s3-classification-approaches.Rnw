% Introduction to Machine Learning
% Day 1

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@
% Load all R packages and set up knitr
<<setup-r, child="../style/setup.Rnw", include = FALSE>>=
@

<<bin-task>>=
n = 30
set.seed(1234L)
tar = factor(c(rep(1L, times = n), rep(2L, times = n)))
feat1 = c(rnorm(n, sd = 1.5), rnorm(n, mean = 2, sd = 1.5))
feat2 = c(rnorm(n, sd = 1.5), rnorm(n, mean = 2, sd = 1.5))
bin.data = data.frame(feat1, feat2, tar)
bin.tsk = makeClassifTask(data = bin.data, target = "tar")
@


\lecturechapter{7}{Approaches to Classification}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{Classification Approaches Reminder}

\begin{itemize}

\item \textbf{Discriminative models}: model $p(y|x)$ directly
% and learn boundaries between classes


\begin{itemize}
\item Logistic regression
\item Support vector machine (SVM)
\end{itemize}


\item \textbf{Generative models}: model $p(x|y)$ and $p(y)$
%- the distribution of individual classes

\begin{itemize}
\item Linear discriminant analysis (LDA)
\item Quadratic discriminant analysis (QDA)
\item Na√Øve Bayes
\end{itemize}



\end{itemize}
\end{vbframe}

\begin{vbframe}{Linear discriminant analysis (LDA)}

% Linear discriminant follows a similar idea. As before, we want to classify a categorical target $y \in \Yspace = \gset$ on basis of $x$.

% \lz

LDA follows a generative approach, each class density is modeled as a \emph{multivariate Gaussian}
with equal covariance, i. e. $\Sigma_k = \Sigma \quad \forall k$. \\
$$
\pdfxyk = \frac{1}{(2\pi)^{\frac{p}{2}}|\Sigma_k|^{\frac{1}{2}}} \exp(-\frac{1}{2}(x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k))
$$

\lz
% \framebreak

Parameters $\theta$ are estimated in a straight-forward manner by estimating
\begin{eqnarray*}
\hat{\pi}_k &=& n_k / n,\text{ where $n_k$ is the number of class $k$ observations} \\
\hat{\mu}_k &=& \sum_{i: y_i = k} \frac{x_i}{n_k} \\
\hat{\Sigma} &=& \frac{1}{n - g} \sum_{k=1}^g \sum_{i: y_i = k} (x_i - \hat{\mu}_k) (x_i - \hat{\mu}_k) ^T
\end{eqnarray*}

\framebreak

\begin{itemize}
  \item Same covariance for both classes
  \item Restrictive model assumption.
\end{itemize}

<<echo = FALSE, warning=FALSE, message=FALSE, fig.height=4.5>>=
library(ggplot2)
library(MASS)
library(car)

# fake data
classa = data.frame(mvrnorm(n = 1000, mu = c(2,2), Sigma = matrix(c(2, 0, 0, 2), ncol = 2, byrow = TRUE)))
classb = data.frame(mvrnorm(n = 1000, mu = c(6,6), Sigma = matrix(c(4, -3, -3, 4), ncol = 2, byrow = TRUE)))

# Target data frame
df = cbind(classa, rep("a", ncol(classa)))
colnames(df) = c("x1", "x2", "y")
foo = cbind(classb, rep("b", ncol(classb)))
colnames(foo) = c("x1", "x2", "y")
df = rbind(df, foo)

####### LDA
## all from scratch:
mu_a = c(mean(df[which(df$y == "a"), 1]), mean(df[which(df$y == "a"), 2]))
mu_b = c(mean(df[which(df$y == "b"), 1]), mean(df[which(df$y == "b"), 2]))

foo = matrix(c(0, 0, 0, 0), ncol = 2)
for (i in 1:nrow(df)) {
  if (df[i, "y"] == "a") {
    # i = 1
    foo = foo +  as.matrix(t(df[i, c(1, 2)] - mu_a)) %*% as.matrix(df[i, c(1, 2)] - mu_a)
  }
  else {
    foo = foo +  as.matrix(t(df[i, c(1, 2)] - mu_b)) %*% as.matrix(df[i, c(1, 2)] - mu_b)
  }
}
var_lda = 1 / (nrow(df) - 2) * foo

# create ellipsoids
a_ell = as.data.frame(ellipse(center = mu_a, shape = var_lda, radius = 2, draw = FALSE))
colnames(a_ell) = c("x1", "x2")
b_ell = as.data.frame(ellipse(center = mu_b, shape = var_lda, radius = 2, draw = FALSE))
colnames(b_ell) = c("x1", "x2")

## ggplot
plot1 = ggplot(data = classa, aes(x = X1, y = X2, alpha = 0.2)) +
  geom_point(col = "blue", show.legend = FALSE) +
  geom_point(data = classb, aes(x = X1, y = X2, alpha = 0.2), col = "red", show.legend = FALSE) +
  geom_path(data = a_ell, aes(x = x1, y = x2), colour = "blue", show.legend = FALSE, size = 2) +
  geom_path(data = b_ell, aes(x = x1, y = x2), colour = "red", show.legend = FALSE, size = 2)
plot1

@


\framebreak

For the posterior probability of class $k$ it follows:
\begin{eqnarray*}
\pikx &\propto& \pi_k \cdot \pdfxyk \\
&=& \pi_k \exp(- \frac{1}{2} x^T\Sigma^{-1}x - \frac{1}{2} \mu_k^T \Sigma^{-1} \mu_k + x^T \Sigma^{-1} \mu_k ) \\
&=& \exp(- \frac{1}{2} x^T\Sigma^{-1}x) \exp(\log \pi_k  - \frac{1}{2} \mu_k^T \Sigma^{-1} \mu_k + x^T \Sigma^{-1} \mu_k ) \\
&=& \exp(- \frac{1}{2} x^T\Sigma^{-1}x) \exp(\theta_{0k} + x^T \theta_k)
\end{eqnarray*}

by defining
$\theta_{0k} := \log \pi_k  - \frac{1}{2} \mu_k^T \Sigma^{-1} \mu_k$ and $\theta_k := \Sigma^{-1} \mu_k$.

\framebreak

Finally, the posterior probability becomes

$$
\pikx = \frac{\pi_k \cdot \pdfxyk }{\pdfx} = \frac{\exp(\theta_{0k} + x^T \theta_k)}{\sum_j \exp(\theta_{0j} + x^T \theta_j)}
$$

(the term $\exp(- \frac{1}{2} x^T\Sigma^{-1}x)$ will cancel out in numerator and denominator).

\lz

And (simplified) discriminant functions can be defined as
$$ f_k(x) =  \theta_{0k} + x^T \theta_k $$
Hence, LDA provides a Linear classifier with linear decision boundaries.

% \lz

% \framebreak

% If we compare the likelihood of two classes $k, l \in \gset$ via the log-odds, we end up in a linear function of x

% \small
% \begin{eqnarray*}
% \log \frac{\pikx}{\pi_l(x)}&=& \log\exp(\theta_{0k} + x^T \theta_k)-\log\exp(\theta_{0l} + x^T \theta_l) \\
% &=& (\theta_{0k} - \theta_{0l}) + x^T(\theta_k-\theta_l)
% \end{eqnarray*}

% \normalsize
% This equation is linear in $x$, so the decision boundary between the classes can
% only be linear. Linear discriminant analysis provides a linear classifier.



% Finally we will predict the most probable category given $x$

% $$
% \yh = h(x) = \argmax_{k \in \gset} \pikx.
% $$

\framebreak

<<echo = FALSE, warning=FALSE, message=FALSE>>=
plotLearnerPrediction("classif.lda", bin.tsk)
@

\end{vbframe}


\begin{vbframe}{Quadratic discriminant analysis (QDA)}

QDA is a direct generalization of LDA, where the class densities are now Gaussians with unequal covariances $\Sigma_k$.
$$
\pdfxyk = \frac{1}{(2\pi)^{\frac{p}{2}}|\Sigma_k|^{\frac{1}{2}}} \exp(-\frac{1}{2}(x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k))
$$

\lz

Parameters are estimated in a straight-forward manner by:\\
\begin{eqnarray*}
\hat{\pi}_j &=& \frac{n_j}{n},\text{ where $n_j$ is the number of class $j$ observations} \\
\hat{\mu}_j &=& \sum_{i: y_i = j} \frac{x_i}{n_j} \\
\hat{\Sigma_j} &=& \frac{1}{n_j - 1} \sum_{i: y_i = j} (x_i - \hat{\mu}_j) (x_i - \hat{\mu}_j)^T \\
\end{eqnarray*}

\framebreak

\begin{itemize}
  \item Covariance matrices can differ over classes.
  \item Yields better data fit but also requires estimation of more parameters.
\end{itemize}

<<echo = FALSE, warning=FALSE, message=FALSE, fig.height = 4.5>>=
########### QDA
mu_a = c(mean(df[which(df$y == "a"), 1]), mean(df[which(df$y == "a"), 2]))
mu_b = c(mean(df[which(df$y == "b"), 1]), mean(df[which(df$y == "b"), 2]))

foo_a = matrix(c(0, 0, 0, 0), ncol = 2)
for (i in 1:nrow(df[which(df$y == "a"), ])) {
    foo_a = foo_a +  as.matrix(t(df[which(df$y == "a"), ][i, c(1, 2)] - mu_a)) %*% as.matrix(df[which(df$y == "a"), ][i, c(1, 2)] - mu_a)
}
var_qda_a = 1 / (nrow(classa) - 1) * foo_a

foo_b = matrix(c(0, 0, 0, 0), ncol = 2)
for (i in 1:nrow(df[which(df$y == "b"), ])) {
  foo_b = foo_b +  as.matrix(t(df[which(df$y == "b"), ][i, c(1, 2)] - mu_b)) %*% as.matrix(df[which(df$y == "b"), ][i, c(1, 2)] - mu_b)
}
var_qda_b = (1 / (nrow(classb) - 1) ) * foo_b

# create ellipsoids
a_ell = as.data.frame(ellipse(center = mu_a, shape = var_qda_a, radius = 2, draw = FALSE))
colnames(a_ell) = c("x1", "x2")
b_ell = as.data.frame(ellipse(center = mu_b, shape = var_qda_b, radius = 2, draw = FALSE))
colnames(b_ell) = c("x1", "x2")

plot1 = ggplot(data = classa, aes(x = X1, y = X2, alpha = 0.2)) +
  geom_point(col = "blue", show.legend = FALSE) +
  geom_point(data = classb, aes(x = X1, y = X2, alpha = 0.2), col = "red", show.legend = FALSE) +
  geom_path(data = a_ell, aes(x = x1, y = x2), colour = "blue", show.legend = FALSE, size = 2) +
  geom_path(data = b_ell, aes(x = x1, y = x2), colour = "red", show.legend = FALSE, size = 2)
plot1
@

\framebreak



\begin{eqnarray*}
\pikx &\propto& \pi_k \cdot \pdfxyk \\
&=& \pi_k |\Sigma_k|^{-\frac{1}{2}}\exp(- \frac{1}{2} x^T\Sigma_k^{-1}x - \frac{1}{2} \mu_k^T \Sigma_k^{-1} \mu_k + x^T \Sigma_k^{-1} \mu_k )
\end{eqnarray*}

Taking the log of the above, we can define a discriminant function that is quadratic in $x$.

$$
\log \pi_k - \frac{1}{2} \log |\Sigma_k| - \frac{1}{2} x^T\Sigma_k^{-1}x - \frac{1}{2} \mu_k^T \Sigma_k^{-1} \mu_k + x^T \Sigma_k^{-1} \mu_k
$$


% Let's look at the log-odds now. \\

% \begin{eqnarray*}
% \log \frac{\pikx}{\pi_g(x)}&=& \log \frac{\pi_k}{\pi_g}
% - \frac{1}{2} \log \frac{|\Sigma_k|}{|\Sigma_g|}
% + x^T(\Sigma_k^{-1}\mu_k - \Sigma_g^{-1}\mu_g) \\
% &-& \frac{1}{2} x^T (\Sigma_k^{-1} - \Sigma_g^{-1})x
% - \frac{1}{2} x^T (\mu_k^T\Sigma_k^{-1}\mu_k - \mu_g^T\Sigma_g^{-1}\mu_g)
% \end{eqnarray*}

% We see that this function is quadratic in $x$, hence we obtain a quadratic decision boundary.

% \framebreak


% Finally we will predict again the most probable category given $x$

% $$
% \yh = h(x) = \argmax_{k \in \gset} \pikx.
% $$

\framebreak

<<fig.height=5>>=
plotLearnerPrediction("classif.qda", bin.tsk)
@

\framebreak

<<fig.height=5>>=
n = 30
set.seed(1234L)
tar = factor(c(rep(1L, times = n), rep(2L, times = n)))
x1 = c(rnorm(n, sd = 2), rnorm(n))
x2 = c(rnorm(n, mean = 1,  sd = 2), rnorm(n))
qda.data = data.frame(x1, x2, tar)
tsk = makeClassifTask(data = qda.data, target = "tar")
plotLearnerPrediction("classif.qda", tsk)
@

\end{vbframe}

\begin{vbframe}{Naive Bayes classifier}

Another generative technique for categorical response $y \in \gset$ is called \emph{Naive Bayes classifier}.
Here, we make the \enquote{naive} \emph{conditional independence assumption}, that the features given the category $y$ are conditionally independent of each other, i. e.,

% The technique is based on \emph{Bayes theorem}
% $$
% \pikx = \postk = \frac{\pdfxyk \cdot \pi_k}{\pdfx} = \frac{\text{likelihood } \cdot \text{ prior}}{ \text{evidence}},
% $$
$$
\pdfxyk = p((x_1, x_2, ..., x_p)|y = k)=\prod_{j=1}^p p(x_j|y = k).
$$

\lz

Putting this together we get
$$
\pikx  \propto \pik \cdot \prod_{j=1}^p p(x_j|y = k)
$$

\framebreak



\begin{itemize}
  \item Covariance matrices can differ over both classes but have to be diagonal.
  \item Assumption of uncorrelated features
  \item Good performance despite this strong assumption
\end{itemize}

<<echo = FALSE, warning=FALSE, message=FALSE, fig.height=4>>=
var_nb_a = matrix(c(2, 0, 0, 2), ncol = 2)
var_nb_b = matrix(c(4, 0, 0, 4), ncol = 2)

a_ell = as.data.frame(ellipse(center = mu_a, shape = var_nb_a, radius = 2, draw = FALSE))
colnames(a_ell) = c("x1", "x2")
b_ell = as.data.frame(ellipse(center = mu_b, shape = var_nb_b, radius = 2, draw = FALSE))
colnames(b_ell) = c("x1", "x2")

plot1 = ggplot(data = classa, aes(x = X1, y = X2, alpha = 0.2)) +
  geom_point(col = "blue", show.legend = FALSE) +
  geom_point(data = classb, aes(x = X1, y = X2, alpha = 0.2), col = "red", show.legend = FALSE) +
  geom_path(data = a_ell, aes(x = x1, y = x2), colour = "blue", show.legend = FALSE, size = 2) +
  geom_path(data = b_ell, aes(x = x1, y = x2), colour = "red", show.legend = FALSE, size = 2)
plot1
@



% The Naive Bayes classifier is then obtained by maximizing the above equation
% $$
% \yh = h(x) = \argmax_{k\in \gset}\pi_k\prod_{i=1}^p p(x_i | y = k).
% $$

\framebreak

Parameters estimation now has become simple, as we only have to estimate $\pdf(x_j | y = k)$, which is univariate (given the class k).

\lz

For numerical $x_j$, often a univariate Gaussian is assumed, and we estimate $(\mu_j, \sigma^2_j)$ in the standard manner. Note, that we now have constructed a QDA model with strictly diagonal covariance structures for each class, hence this leads to quadratic discriminant functions.

\lz

For categorical features $x_j$, we simply use a Bernoulli / categorical distribution model for $p(x_j | y = k)$ and estimate the probabilities for $(j,k)$ by simply counting of relative frequencies in the standard manner.
The resulting classifier is linear in these frequencies.

\lz
Furthermore, one can show that the Naive Bayes is a linear classifier in a particular feature space if the features are from exponential families (e. g. binomial, multinomial, normal distribution).

\framebreak

<<>>=
learner = makeLearner("classif.naiveBayes")
plotLearnerPrediction(learner, iris.task)

@





% In the general categorical case the modeled likelihood for $x_j$ with parameters $p_{kj}$ is:
% % with $x = (x_1, \ldots \x_p)$
%
% $$
% p(x_j | y = k) = \frac{(\sum x_i)!}{\prod_i x_i!} \prod_j p_{kj}^{[x_j = j]}
% $$
%
% and for the completely observed data this becomes a multinomial distribution
%
% $$
% \frac{(\sum_i x_i)!}{\prod_i x_i!} \prod_j p_{kj}^{v_{kj}},
% $$
%
% with ${v_{kj}} = \sum_{i = 1}^n [x_j^{(i)} = 1]$ the number of times $(j, k)$ occurs.


% \framebreak
%
%
% We can now prove that the decision boundaries between klasses k and l are linear:
%
% $$
% \log \frac{\pi_k(x)}{\pi_l(x)} \propto \log\frac{\pi_k}{\pi_l} + \sum_j v_{kj} \ln p_{kj} - \sum_j v_{lj} \ln p_{lj}
% $$
%
% This is a linear function in the parameter vector $v = (v_{11}, \ldots, v_{1p}, \ldots, v_{g1} \ldots v_{gp})$.
%
% \framebreak
%
% Laplace smoothing: If a given class and feature value never occur together in the training data, then the frequency-based probability estimate will be zero.
%
% \lz
%
% This is problematic because it will wipe out all information in the other probabilities when they are multiplied.
%
% \lz
%
% A simple numerical correction, especially needed for smaller sample size, is to set $p_{kj} = \epsilon > 0$ instead of $p_{kj} = 0$.


\end{vbframe}

% \begin{vbframe}{Naive Bayes as a linear classifier}

% In general, the \emph{Naive Bayes classifier} is \textbf{not} a \emph{linear} classifier.

% \lz

% However, it can be shown that the \emph{Naive Bayes classifier} is a linear classifier in a particular feature space if the features are from exponential families (e. g. binomial, multinomial, normal distribution) .

% \end{vbframe}


\begin{vbframe}{Naive bayes: application as spam filter}
\begin{itemize}
  \item In the late 90s, Naive Bayes became popular for email spam-filter programs
  \item Word counts were used as features to detect spam mails (e.g. "Viagra" often occurs in spam mail)
  \item Independence assumption: occurence of two words in mail is not correlated
  \item Seems naive ("Viagra" more likely to occur in context with "Buy now" than "flower"), but leads to less required parameters and therefore better generalization.
\end{itemize}
\end{vbframe}


\endlecture
