% Introduction to Machine Learning
% Day 1

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@
% Load all R packages and set up knitr
<<setup-r, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{6}{Introduction to Classification}
\lecture{Introduction to Machine Learning}


\framebreak
\begin{vbframe}{Classification losses}



  % For classification problems, i. e. output variable $y \in \{-1, 1\}$ the risk can be written as


  % For categorical output variable $y\in \gset$ we can use the \emph{0-1-loss}

  % $$
  % L(y, h(x)) =\I_{y \ne h(x)} =
     % \footnotesize \begin{cases} 1 \quad \text{ if } y \ne h(x) \\ 0 \quad    \text{ if } y = h(x)  \end{cases}
  % $$

  % by applying \emph{Bayes theorem}

  % \begin{eqnarray*}
  % p(x, y) = p(y|x)p(x).
  % \end{eqnarray*}

  % \framebreak

  % This can be simplified to

  % $$
  % \risk(f) = \int_X [L(1, f(x)) \pix + L(-1, f(x)) (1 - \pix) p(x)] dx.
  % $$

  % For given $x$, one can minimize the risk pointwise for any convex loss function by differentiating the last equality with respect to $f$. Thus, minimizers for all of the loss functions are easily obtained as functions of only $f(x)$ and $\pi(x)$.

  % \framebreak

  We will now introduce some loss functions, mainly for binary output.

  Notice that $f(x)$ outputs a score and $\sign(\fx)$ will be the corresponding label.

  \lz

  Most following loss functions will depend on the so-called \emph{margin}
  $$
  y\fx =  \begin{cases} > 0  \quad &\text{ if } y = \sign(\fx) \text{ (correct classification)} \\
                        < 0 \quad &\text{ if } y \ne \sign(\fx) \text{ (misclassification)} \end{cases}
  $$


  % \begin{eqnarray*}
  % \risk(f) &=& \E_{xy}[\Lxy] = \int_X \int_Y \Lxy p(x, y) dy~dx \\
  % &=& \int_X \int_Y \Lxy p(y|x)p(x) dy~dx
  % \end{eqnarray*}

  \framebreak


  \end{vbframe}


  \begin{vbframe}{Bin. classif. losses - 0-1 loss}
  \begin{itemize}
  \item $\Lxy = [y \neq f(x)] = [\yf < 0]$
  \item Intuitive, often what we are interested in
  \item Not even continuous, even for linear $f$ the optimization problem is NP-hard and
    close to intractable
  \end{itemize}

  <<echo=FALSE, results='hide', fig.height=3, fig.align='center'>>=
  x = seq(-2, 2, by = 0.01); y = as.numeric(x < 0)
  qplot(x, y, geom = "line", xlab = expression(yf(x)), ylab = expression(L(yf(x))))
  @
  \end{vbframe}

  \begin{vbframe}{Multiclass 0-1 loss and Bayes classifier}

  Assume $h \in \Yspace$ with $|\Yspace| = g$. We can define the 0-1-loss for multiclass:
  $$L(y, \hx) = [y \neq \hx]$$.
  We can in general rewrite the loss again as:
  \begin{eqnarray*}
    \risk(h) & = & \E_{xy}[L(y, h)] = E_x [ E_{y|x} [ L(y, \hx) ] ] =  \\
             & = & E_x \sum_{k \in \Yspace} L(k, \hx) P(y = k| x = x) \\
             & = & E_x \sum_{k \in \Yspace} L(k, \hx) \pikx
  \end{eqnarray*}

  NB: This works, too, (of course) for $\Yspace = \{-1, 1\}$ and a score function $f$:
  $$
  \risk(f) = \mathbb{E}_x [L(1, f(x)) \pix + L(-1, f(x)) (1 - \pix)].
  $$

  \framebreak
  We can again minimize pointwise, and for a general cost-sensitive loss $L(y, h)$ this is:

  \begin{eqnarray*}
    \hxh &=& \argmin_{l \in \Yspace} \sum_{k \in \Yspace} L(k, l) \pikx \\
  \end{eqnarray*}

  For the 0-1 loss this becomes:

  $$
  \hxh = \argmin_{k \in \Yspace} (1 - \pikx) = \argmax_{k \in \Yspace} \pikx
  $$

  If we know $\Pxy$ perfectly (and hence $\pikx$), we have basically constructed the loss-optimal
  classifier and we call it the \emph{Bayes classifier} and its expected loss the \emph{Bayes loss} or
  \emph{Bayes error rate} for 0-1-loss.


  % and get the risk function


  % The minimizer of $\risk(f)$ for the 0-1-loss is

  % \begin{eqnarray*}
  % \fh(x) &=&    \footnotesize \begin{cases} 1 \quad \text{ if } \pix > 1/2 \\ -1 \quad \pix < 1/2  \end{cases}
  % \end{eqnarray*}

  \lz


  \end{vbframe}


  % \framebreak

  % \textbf{Square-loss:}


  % If we use instead the \emph{square-loss}

  % $$
  % \Lxy = (1-y\fx)^2,
  % $$

  % we get the risk function

  % \begin{eqnarray*}
  % \risk(f) &=& \mathbb{E}_x [(1-\fx)^2 \pix + (1+\fx)^2 (1-\pix)] \\
  % &=& \mathbb{E}_x [1 + 2\fx + \fx^2-4\fx\pix].
  % \end{eqnarray*}

  % By differentiating w. r. t. $f(x)$ we get the minimizer of $\risk(f)$ for the square loss function

  % \begin{eqnarray*}
  % \fh(x) &=& 2\pix -1.
  % \end{eqnarray*}

  % \framebreak

  % \vspace*{0.2cm}

  % The square loss function tends to penalize outliers excessively. Functions which yield high values of $(x)$ will perform poorly with the square loss function, since high values of $yf(x)$ will be penalized severely, regardless of whether the signs of $y$ and $f(x)$ match.

  % <<echo=FALSE, results='hide', fig.height= 1.8, fig.asp = 0.4>>=
  % x = seq(-2, 5, by = 0.01)
  % plot(x, (1-x)^2, type = "l", xlab = expression(yf(x)), ylab = expression(paste((1-yf(x))^2)), main = "Square loss")
  % box()
  % @

  % \lz

  % \end{vbframe}

  \begin{vbframe}{Bin. classif. losses - Hinge loss}
  \begin{itemize}
  \item $\Lxy = \max\{0, 1 - \yf\}$, used in SVMs
  \item Convex
  \item No derivatives for $\yf = 1$, optimization becomes harder
  \item More robust, outliers in $y$ are less problematic
  \end{itemize}

  <<echo=FALSE, results='hide', fig.height=3, fig.align='center'>>=
  x = seq(-2, 2, by = 0.01); y = pmax(0, 1 - x)
  qplot(x, y, geom = "line", xlab = expression(yf(x)), ylab = expression(L(yf(x))))
  @

  % \framebreak

  % we get the risk function

  % \begin{eqnarray*}
  % \risk(f) &=& \mathbb{E}_x [\max\{0, 1 - \fx\} \pix + \max\{0, 1 + y\fx\} (1-\pix)].
  % \end{eqnarray*}

  % The minimizer of $\risk(f)$ for the hinge loss function is

  % \begin{eqnarray*}
    % $fh(x) =  \footnotesize \begin{cases} 1 \quad \text{ if } \pix > 1/2 \\ -1 \quad \pix < 1/2  \end{cases}$
  % \end{eqnarray*}

  \end{vbframe}

  \begin{vbframe}{Bin. classif. losses - Logistic loss}
  \begin{itemize}
    \item $\Lxy = \ln(1+\exp(-y\fx))$, used in logistic regression
    \item Also called Bernoulli loss
  \item Convex, differentiable, not robust
  \end{itemize}

  <<fig.height=3>>=
  x = seq(-2, 2, by = 0.01); y = log(1 + exp(-x))
  qplot(x, y, geom = "line", xlab = expression(yf(x)), ylab = expression(L(yf(x))))
  @
  % the minimizer of $\risk(f)$ for the logistic loss function is

  % \begin{eqnarray*}
  % \fh(x) &=&  \ln \biggl(\frac{\pix}{1-\pix}\biggr)
  % \end{eqnarray*}

  % The function is undefined when $\pix = 1$ or $\pix = 0$, but predicts a smooth curve which grows when $\pix$ increases and equals $0$ when $\pix = 0.5$


  \end{vbframe}

  \begin{vbframe}{Bin. classif. losses - Cross-entropy loss}
  \begin{itemize}
    \item Using the alternative label convention $y\in \{0, 1\}$
    \item $\Lxy = -y\ln(\pix)-(1-y)\ln(1-\pix)$
    \item Basically the same as the logistic loss when we act on $\pix \in [0,1]$ instead of $\fx \in \R$
    \item  The cross entropy loss is closely related to the Kullback-Leibler divergence, which will be introduced later in the chapter.
    \item Very often used in neural networks with binary output nodes for classification.
  \end{itemize}


  \end{vbframe}


  \begin{vbframe}{Bin. classif. losses - Exponential loss}
  \begin{itemize}
    \item $\Lxy = \exp(-y\fx)$, used in AdaBoost
  \item Convex, differentiable, not robust
  \item Quite similar to logistic loss
  \end{itemize}

  <<fig.height=3>>=
  x = seq(-2, 2, by = 0.01); y = exp(-x)
  qplot(x, y, geom = "line", xlab = expression(yf(x)), ylab = expression(L(yf(x))))
  @

  % we get the risk function

  % \vspace*{-.5cm}

  % \begin{eqnarray*}
  % \risk(f) &=& \mathbb{E}_x [\exp(-\fx) \pix + \exp(f(x)) (1-\pix)]
  % \end{eqnarray*}

  \end{vbframe}
  %
  % \begin{vbframe}{Risk minimizing functions}
  %
  % Overview of binary classification losses and the corresponding risk minimizing functions:
  %
  % \lz
  %
  % \begin{tabular}{p{2.5cm}|p{3.5cm}|p{4.5cm}}
  %   loss name & loss formula  & minimizing function \\
  %   \hline
  %   0-1 & $[y \neq \hx]$ & $\hxh = \footnotesize \begin{cases} 1 \quad \text{ if } \pix > 1/2 \\ -1 \quad \pix < 1/2  \end{cases}$ \\
  %   Hinge & $\max\{0, 1 - \yf\}$ & $\fh(x) =  \footnotesize \begin{cases} 1 \quad \text{ if } \pix > 1/2 \\ -1 \quad \pix < 1/2  \end{cases}$ \\
  %   Logistic & $\ln(1+\exp(-y\fx))$ & $\fh(x) =  \ln \biggl(\frac{\pix}{1-\pix}\biggr)$ \\
  %   Cross entropy & $-y\ln(\pix) \newline -(1-y)\ln(1-\pix)$ & \\
  %   & & \\
  %   Exponential & $\exp(-y\fx)$ &
  %
  % \end{tabular}
  %
  % \end{vbframe}

  % \section{Selected methods for regression and classification}

  \begin{vbframe}{Classification}

  Assume we are given a \emph{classification problem}:
  \begin{eqnarray*}
  & x \in \Xspace \quad & \text{feature vector}\\
  & y \in \Yspace = \gset \quad & \text{\emph{categorical} output variable (label)}\\
  &\D = \Dset & \text{observations of $x$ and $y$}
  \end{eqnarray*}


  Classification usually means to construct $g$ discriminant functions $f_1(x), \ldots f_g(x)$,
  so that we choose our class as
  $$h(x) = \argmax_k f_k(x)$$

  \lz

  This divides the feature space into $g$ \emph{decision regions} $\{x \in \Xspace | h(x) = k\}$.
  These regions are separated by the \emph{decision boundaries} where ties occur between these
  regions.

  \lz

  If these functions $f_k(x)$ can be specified as linear functions (possibly through a rank-preserving,
  monotone transformation), we will call the classifier a \emph{linear classifier}. We can then write a
  decision boundary as $x^T\theta = 0$, which is a hyperplane separating two classes.

  \lz
  Note that all linear classifiers can represent non-linear decision boundaries in our original input space,
  if we opt to manually construct derived features like higher order interactions or polynomial features.

  \lz

  If only 2 classes exist, we could also construct a single discriminant function $f(x) = f_1(x) - f_2(x)$
  (note that it would be more natural here to label the classes with \{+1, -1\} or \{0, 1\}).

  \lz

  Two fundamental approaches exist to construct classifiers:
  The \emph{generative approach} and the \emph{discriminant approach}.

  \framebreak

  The \emph{generative approach} employs the Bayes theorem:
  $$\pikx = \postk = \frac{\P(x | y = k) \P(y = k)}{\P(x)} \propto \pdfxyk \pik $$
  and models $\pdfxyk$ (usually by assuming something about the structure of this distribution)
  to allow the computation of $\pikx$.

  \lz

  Discriminant function are now $\pikx$ or $\lpdfxyk + \lpik$

  \lz
  The \emph{discriminant approach} tries to model the discriminant score function directly, often by loss
  minimization.

  \lz
  % \framebreak

  Examples:
  \begin{itemize}
  \item Linear discriminant analysis (generative, linear)
  \item Quadratic discriminant analysis (generative, not linear)
  \item Logistic regression (discriminant, linear)
  \end{itemize}


  \end{vbframe}


  \endlecture
