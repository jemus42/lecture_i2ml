% Introduction to Machine Learning
% Day 3

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@

% Load all R packages and set up knitr
<<setup, child="../style/setup.Rnw", include = FALSE>>=
requireNamespace("ROCR")
@

\lecturechapter{11}{ROC and confusion matrix}
\lecture{Fortgeschrittene Computerintensive Methoden}

\begin{vbframe}{Notation}
The following notation will be used in this chapter:

\begin{itemize}
  \item $\Xspace$: $p$-dim. input space%, usually we assume $\Xspace = \R^p$, but categorical features can occur, too
\item $\Yspace$: target space. In this chapter binary $\Yspace = \lbrace 0, 1 \rbrace$ or multi class $\Yspace = \gset$.
\item $x = \xvec \in \Xspace$ observation.
\item A binary classifier is a function $C: \Xspace \longrightarrow \Yspace = \lbrace 0,1\rbrace$.
\item A binary scoring classifier (or ranker) is a function $f$, which outputs a score (e.g. probability) for the positive class.
\item   $n$ is the total number of observations.
\item   $\text{NEG}$ and $\text{POS}$ are the number of negative and positive observations.
\item   $\text{neg}$ and $\text{pos}$ are the fraction of negative and positive observations, respectively.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Evaluation of Binary Classifiers}

\begin{itemize}
  \item Consider a binary classifier for cancer prediction
  \item Evaluating with \textit{error rate} for imbalanced labels if often bad
  \item Assume that only 0.5\,\% of 1000 patients have cancer.
  \begin{itemize}
    \item Always returning "no disease" has an error rate of 0.5\,\%,
      which sounds good
    \item However, this sends all sick patients home, which is the worst possible system
  \end{itemize}
\end{itemize}

The phenomenon described above is referred to as \textbf{accuracy paradox}: The accuracy is only reflecting the underlying class distribution.

We need to find better performance measure than accuracy for imbalanced data.

\end{vbframe}

\begin{vbframe}{Confusion Matrix}

The confusion matrix is a $2 \times 2$ contingency table of predictions
$\hat{y}$ and true labels $y$. Several evaluation metrics can be derived from
a confusion matrix:

\begin{center}
\includegraphics[width=0.8\textwidth]{figure_man/confusion_matrix.png}
\end{center}

\end{vbframe}

\begin{vbframe}{Terminology - More Confusion}

\begin{itemize}
  \item True positive rate; aka: recall, sensitivity\
  How many of the true 1s did we predict as 1?\
  1 - TPR = false negative rate = miss rate

  \item True negative rate; aka: specificity\
  How many of the true 0s did we predict as 0?\
  1 - TNR = False positive rate = fallout = false alarm rate

  \item Positive predictive value; aka precision\
  If we predict 1 how likely is it a true 1?\
  1 - PPV = false discovery rate

  \item Negative predictive value\
  If we predict 0 how likely is it a true 0?\
  1 - PPV = false omission rate
\end{itemize}

\end{vbframe}

\begin{vbframe}{Example}

\begin{center}
\includegraphics[width=0.9\textwidth]{figure_man/confusion_matrix_example.png}
\end{center}

\end{vbframe}

\begin{vbframe}{F-Measure}

It is difficult to achieve a high \textit{precision} and high \textit{recall} simultaneously.
A trade-off offers the $F_1$-measure, which is the harmonic mean of precision
$P$ and recall $R$:

$$F_1 = 2 \cfrac{P \, R}{P + R} = \cfrac{2\text{tpr}}{\text{tpr} + \tfrac{neg}{pos} \cdot \text{fpr} + 1 } $$

\end{vbframe}


\begin{vbframe}{Multiclass Confusion Matrix}
  \begin{itemize}
    \item The performance of a multiclass classifier can also be summarized in a confusion matrix.
    \item E.g., in a 3-class classification problem with labels A, B and C:
    \item[] \includegraphics[width=0.9\textwidth]{figure_man/roc-multiclass_confmatrix.png}
    \item The overall accuracy is calculated similarly to the binary confusion matrix.
    \item Example:
          $acc = \cfrac{\# \text{TP}_A + \# \text{TP}_B + \# \text{TP}_C}{n} = \cfrac{30+60+80}{300} \approx 56\%$
  \end{itemize}

  \framebreak

  \begin{itemize}
    \item[]   \includegraphics[width=0.9\textwidth]{figure_man/roc-multiclass_confmatrix.png}
    \item Other measures, e.g. tpr, need to be calculated in a one-vs-all manner.
    \item Example (true positive rate for label A):
    $$\text{tpr}_A = \cfrac{\# \text{TP}_A}{\# \text{TP}_A + \# \text{FN}_A} = \cfrac{30}{30+50+20} = 0.3$$
    \item Example (precision for label A):
    $$\text{P}_A = \cfrac{\# \text{TP}_A}{\# \text{TP}_A + \# \text{FP}_A} = \cfrac{30}{30+20+10} = 0.5$$
  \end{itemize}

\end{vbframe}

\begin{vbframe}{Receiver Operating Characteristic (ROC)}

\begin{itemize}
  \item Plot TPR vs FPR for classifiers
  \item The best classifier lies on the top-left corner
  \item Diagonal contains worst possible solutions
  \item ROC curves are insensitive to class distribution
\end{itemize}

\begin{center}
\includegraphics[width=0.45\textwidth]{figure_man/roc-space.png}
\includegraphics[width=0.45\textwidth]{figure_man/roc-plot-classifiers.png}
\end{center}

\end{vbframe}

\begin{vbframe}{ROC Analysis}
%<!-- (from Signal Detection Theory) -->
%<!--
%\vspace{-0.25cm}
%-   initially - for distinguishing noise from not %noise
%-   so it's a way of showing the performance of %Binary Classifiers
%    -   only two classes - noise vs not noise  -->
The \textbf{R}eceiver \textbf{O}perating \textbf{C}haracteristic (ROC) curve is created by plotting the \textit{tpr} vs. \textit{fpr}, i.e. the

\begin{itemize}
  \item   True positive rate, $\text{tpr} = \cfrac{\text{\#TP}}{\text{\#TP} + \text{\#FN}}$
  \item   False positive rate $\text{fpr} = 1-\text{specificity} = \cfrac{\text{\#FP}}{\text{\#FP} + \text{\#TN}}$
\end{itemize}

Properties:
\begin{itemize}
\item   ROC curves are insensitive to class distribution.
\item   If the proportion of positive to negative instances changes, the ROC curve will not change.
%\item   ROC space is 2 dimensional, i.e. $X: \text{fpr}, Y: \text{tpr}$.
\item To see why, consider a binary confusion matrix.
\end{itemize}
%<!--%
%# ROC Space

%When evaluating a binary classifier, we often use a %confusion matrix.

%-   Here, we need only *tpr* and *fpr*
%-   $\text{tpr} = \cfrac{\text{TP}}{\text{TP} + %\text{FN}}$
%-   $\text{fpr} = \cfrac{\text{FP}}{\text{FP} + %\text{TN}}$
%-   ROC space is 2 dimensional:
%    -   $X: \text{fpr}, Y: \text{tpr}$
%-->

\framebreak
Example:
\begin{table}[]
\centering
\begin{tabular}{|l|c|c|}
                \hline
               & True Positive & True Negative \\ \hline
Pred. Positive & 40            & 25            \\ \hline
Pred. Negative & 10            & 25           \\ \hline
\end{tabular}
\end{table}

Here we have we have a proportion $\text{pos}/\text{neg} = 1$.
Now we change the proportion to $\text{pos}/\text{neg} = 2$.
\begin{table}[]
\centering
\begin{tabular}{|l|c|c|}
                \hline
               & True Positive & True Negative \\ \hline
Pred. Positive & 80            & 25            \\ \hline
Pred. Negative & 20            & 25           \\ \hline
\end{tabular}
\end{table}

The true positive rate and ($\text{tpr} = 0.8$) and the false positive rate ($\text{fpr} = 0.5$) does not change.


% \begin{itemize}
%   \item Note that the class distribution is
% the relationship of the left column to the right column.
%   \item Any performance metric that uses values from both
% columns will be inherently sensitive to class skews (such as accuracy, precision, F measure)
%   \item ROC curves are based upon tpr and fpr.
%   \item Each measure uses only values in their respective columns.
%   \item Changing the class distribution will therefore not change the ROC curve.
% \end{itemize}

\end{vbframe}

\begin{vbframe}{ROC Space Baseline}

%<!-- We put a random classifier that predicts 1 with some probability, e.g. 3 random classifiers on the baseline: -->
\begin{itemize}

\item   The best classifier lies on the top-left corner.
\item   Example: 3 classifiers that lie on the baseline, i.e. a classifier that
  \begin{itemize}
    \item   always predicts 0 (0\% chance to predict 1),
    \item   predicts 1 in 80\% cases and
    \item   always predict 1 (in 100\% cases).
  \end{itemize}
\end{itemize}

\begin{center}
\includegraphics[width=0.45\textwidth]{figure_man/roc_space.png}
\end{center}

\end{vbframe}


\begin{vbframe}{ROC Space Baseline}

In practice, we can never obtain a classifier below this line. Example:

\begin{itemize}
  \item   A classifier $C_1$ below the line with $\text{fpr} = 80\%$, and $\text{tpr} = 30\%$
  \item   We can make it better than random by inverting its prediction:
    $C_2(x) = 0$, if $C_1(x) = 1$ and  $C_2(x) = 1$, if $C_1(x) = 0$.
  \item   Position of $C_2$ is then $(1 - \text{fpr}, 1 - \text{tpr}) = (20\%, 70\%)$
\end{itemize}

\begin{center}
\includegraphics[width=0.45\textwidth]{figure_man/roc-inv.png}
\end{center}

\end{vbframe}

%
% \begin{vbframe}{ROC Convex Hull}
%
% Suppose we have 5 classifiers $C_1, C_2, ..., C_5$
%
% \begin{itemize}
% \item   We calculate $\text{fpr}$ and $\text{tpr}$ for each and plot them on one plot.
% \item   Each classifier is a single point in the ROC space.
% \end{itemize}
%
% \begin{center}
% \includegraphics[width=0.5\textwidth]{figure_man/roc-plot-classifiers.png}
% \end{center}
%
% \end{vbframe}
%
%
% \begin{vbframe}{ROC Convex Hull}
% We can then try to find classifiers that achieve the best
% $\text{fpr}$ and $\text{tpr}$.
%
% \begin{itemize}
%   \item  By the \href{http://0agr.ru/wiki/index.php/Dominance}{dominance} principle, we have the following Pareto frontier (the "ROC convex hull").
%   \item   Classifiers below this hull are always suboptimal, e.g. $C_3$.
% \end{itemize}
%
% \begin{center}
% \includegraphics[width=0.45\textwidth]{figure_man/roc-convex-hull.png}
% \end{center}
% \end{vbframe}
%
%
% \begin{vbframe}{ISO Accuracy Lines}
% There is a simple relationship between accuracy and $\text{fpr}$, $\text{tpr}$.
%
% \begin{itemize}
%   \item   Let $n$ be the number of observations,
%   \item   $\text{NEG}$ and $\text{POS}$ the number of negative and positive observations,
%   \item   $\text{neg}$ and $\text{pos}$ the fraction of negative and positive observations, respectively.
% \end{itemize}
%
%   $$\text{acc} = \text{tpr} \cdot \text{pos} + \text{neg} - \text{neg} \cdot \text{fpr}$$
%
% \begin{itemize}
%   \item   $\text{acc} = \cfrac{\text{\#TP} + \text{\#TN}}{n} =
%     \cfrac{\text{\#TP}}{n} + \cfrac{\text{\#TN}}{n} =
%     \cfrac{\text{\#TP}}{\text{POS}} \cdot \cfrac{\text{POS}}{n} +
%     \cfrac{\text{NEG} - \text{\#FP}}{n} =
%     \cfrac{\text{\#TP}}{\text{POS}} \cdot \cfrac{\text{POS}}{n} +
%     \cfrac{\text{NEG}}{n} - \cfrac{\text{\#FP}}{\text{NEG}} \cdot
%     \cfrac{\text{NEG}}{n} = \text{tpr} \cdot \text{pos} +
%     \text{neg} - \text{fpr} \cdot \text{neg}$
% \end{itemize}
% \end{vbframe}
%
%
% \begin{vbframe}{ISO Accuracy Lines}
% We can rewrite this and get
% $$\text{tpr} =  \cfrac{\text{neg}}{\text{pos}}
% \cdot \text{fpr} + \cfrac{\text{acc} -
% \text{neg}}{\text{pos}} \; \Leftrightarrow  \; y = ax + b$$
%
% $$\text{with } y = \text{tpr},  \; x = \text{fpr},  \; a =
% \cfrac{\text{neg}}{\text{pos}},  \; x =
% \cfrac{\text{neg}}{\text{pos}},  \; b = \cfrac{\text{acc} -
% \text{neg}}{\text{pos}}$$
%
% Properties:
% \begin{itemize}
%   \item   The ratio $a = \cfrac{\text{neg}}{\text{pos}}$ is the slope of the line (changing this ratio yields many different slopes).
%   \item   Changing the accuracy yields many parallel lines with the same slope because $acc$ is included in the intercept $b$.
%   \item   "Higher" lines are better w.r.t. $acc$.
% \end{itemize}
% \end{vbframe}
%
%
% \begin{vbframe}{ISO Accuracy Lines}
% To calculate the corresponding accuracy, we have to find the intersection point of the accuracy line (red) the descending diagonal (blue).
%
% \begin{center}
% \includegraphics[width=\textwidth]{figure_man/iso_lines.png}
% \end{center}
% \end{vbframe}
%
% \begin{vbframe}{ISO Accuracy Lines}
% Explanation:
% \begin{itemize}
%  \item descending diagonal: $\text{tpr} = 1-\text{fpr} \; \Leftrightarrow \text{fpr} = 1-\text{tpr}$
%  \item ISO accuracy line: $\text{tpr} =  \tfrac{\text{neg}}{\text{pos}}
% \cdot \text{fpr} + \tfrac{\text{acc} -
% \text{neg}}{\text{pos}}$
% \end{itemize}
% At the intersection between these lines we have:
% \begin{align*}
%  \text{tpr} &= \tfrac{\text{neg}}{\text{pos}} \cdot (1-\text{tpr}) + \tfrac{\text{acc} -
%  \text{neg}}{\text{pos}}
%  = \tfrac{\text{neg}}{\text{pos}} - \tfrac{\text{neg}}{\text{pos}} \cdot \text{tpr} + \tfrac{\text{acc} -
%  \text{neg}}{\text{pos}} \\
%  &= \tfrac{\text{acc}}{\text{pos}} - \tfrac{\text{neg} \cdot \text{tpr}}{\text{pos}}
%  = \tfrac{\text{acc} -\text{neg} \cdot \text{tpr}}{\text{pos}}
%  \end{align*}
%   With $\text{pos} + \text{neg} = 1$ this is equivalent to:
% \begin{align*}
%  \text{pos} \cdot \text{tpr} + \text{neg} \cdot \text{tpr} &= \text{acc} \\
%  \Leftrightarrow \hspace{2cm} \text{tpr} &= \text{acc}
%  \end{align*}
%
% $\Rightarrow$ At the intersection between the descending diagonal and the ISO accuracy line, the achieved accuracy is equal to the true positive rate.
% \end{vbframe}
%
% \begin{vbframe}{ISO Accuracy Lines vs Convex Hull}
% Recall the convex hull of the ROC plot:
%
% \begin{center}
% \includegraphics[width=0.65\textwidth]{figure_man/roc-convex-hull.png}
% \end{center}
% \end{vbframe}
%
% \begin{vbframe}{ISO Accuracy Lines vs Convex Hull}
% Each line segment of the ROC convex hull is an ISO accuracy line
% for a particular class distribution (slope) and accuracy.
% All classifiers on such a line achieve the same accuracy for this distribution:
%
% \begin{itemize}
%   \item   $\text{neg} / \text{pos} > 1$
%
%   \begin{itemize}
%     \item   Distribution with more negative observations.
%     \item   The slope is steep.
%     \item   Classifier on the left is better.
%   \end{itemize}
%
%   \item   $\text{neg} / \text{pos} < 1$
%
%   \begin{itemize}
%     \item   Distribution with more positive observations.
%     \item   The slope is flatter.
%     \item   Classifier on the right is better.
%   \end{itemize}
%
% \end{itemize}
% Each classifier on the convex hull is optimal w.r.t. accuracy and for a specific distribution.
% \end{vbframe}
%
% \begin{vbframe}{Selecting the Optimal Classifier}
% \begin{enumerate}
%   \item  Compute the ratio (slope) $\text{neg} / \text{pos}$.
%   \item  Find the classifier that achieves the highest accuracy for this ratio.
%   \item  Fix the ratio and keep increasing the accuracy until the end of the hull.
% \end{enumerate}
% \end{vbframe}
%
% \begin{vbframe}{Selecting the Optimal Classifier - Example}
% \begin{center}
% \includegraphics[width=0.55\textwidth]{figure_man/roc-convex-hull-best-acc-1.png}
% \end{center}
%
% \begin{itemize}
%   \item Distribution: $neg/pos = 1/1$, best classifier: $C_2$, accuracy $\approx 81 \%$
% \end{itemize}
% \end{vbframe}
%
% \begin{vbframe}{Selecting the Optimal Classifier - Example}
% \begin{center}
% \includegraphics[width=0.55\textwidth]{figure_man/roc-convex-hull-best-acc-2.png}
% \end{center}
%
% \begin{itemize}
%   \item Distribution: $neg/pos = 1/4$, best classifier: $C_4$, accuracy $\approx 83 \%$
% \end{itemize}
% \end{vbframe}
%
% \begin{vbframe}{Selecting the Optimal Classifier - Example}
% \begin{center}
% \includegraphics[width=0.55\textwidth]{figure_man/roc-convex-hull-best-acc-3.png}
% \end{center}
%
% \begin{itemize}
%   \item Distribution: $neg/pos = 4/1$, best classifier: $C_2$, accuracy $\approx 81 \%$
%
% \end{itemize}
% \end{vbframe}
%
% \begin{vbframe}{Scoring Classifiers}
% A scoring classifier (or ranker) is an algorithm that outputs the scores (e.g. a probabilities) for each class instead of one single
% label.
%
% Do binary classification with a ranker $f$:
% \begin{itemize}
%   \item $f$ outputs a single number
%   \item Set some threshold $\theta$ to transform the ranker into a classifier,
%     e.g. as in \href{http://0agr.ru/wiki/index.php/Logistic_Regression}{logistic regression}
%   \begin{itemize}
%       \item Predict $\hat{y} = 1$ (positive class) if $f(x) > \theta$ else predict $\hat{y} = 0$
%   \end{itemize}
%   \item How to set a threshold $\theta$?
%   \begin{itemize}
%     \item Use \href{http://0agr.ru/wiki/index.php/Cross-Validation}{crossvalidation} for finding
%         the best value for $\theta$.
%     \item Draw ROC curves, producing a point in the ROC space for \textbf{each possible threshold}.
%   \end{itemize}
% \end{itemize}
% \end{vbframe}

\begin{vbframe}{ROC for Scoring Classifiers}
\textbf{Naive Method}:

Given a ranker $f$ and a dataset with $n$ training observations:
\begin{itemize}
  \item Consider all possible thresholds ($n-1$ for $n$ observations).
  \item For each threshold: Calculate $\text{fpr}$ and $\text{tpr}$, and draw this point on the ROC space.
  \item Select the best threshold using the ROC analysis (for the ratio $\text{neg} / \text{pos}$).
\end{itemize}
\textbf{Practical Method}:
\begin{itemize}
  \item Rank test observations on decreasing score.
  \item Start in $(0, 0)$, for each observation $x$ (in the decreasing order).
  \begin{itemize}
    \item If $x$ is positive, move $1/\text{pos}$ up
    \item If $x$ is negative, move $1/\text{neg}$ right
  \end{itemize}
\end{itemize}
\end{vbframe}


\begin{vbframe}{Scoring Classifiers}

\begin{itemize}
  \item Classifiers often output scores or probabilities
  \item Set some threshold $\alpha$ to transform score into a label
  \item Predict $\yh = 1$ (positive class) if $\fx > \alpha$ else predict $\yh = 0$
  \item Draw a point in the ROC space for each possible threshold
\end{itemize}

\begin{center}
\includegraphics[width=\textwidth]{figure_man/roc-curves2.png}
\end{center}

\end{vbframe}



\begin{vbframe}{ROC Curve}
Given:
\begin{itemize}
  \item 20 observations
\end{itemize}

\tiny
\begin{tabular}{l|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.2cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}}

  \hspace{-8pt} \#&  1& 2&  3&  4&  5&  6&  7&  8&  9& 10& 11& 12& 13& 14& 15& 16& 17& 18& 19& 20 \\ \hline
  \hspace{-8pt} C & P& P & N & P & P & P & N & N & P & N & P & N & P & N & N & N & P & N & P & N \\ \hline
  \hspace{-8pt} Score & .9 & .8 &  .7 & .6 & .55 & .54 & .53 & .52 & .51 & .505 & .4 & .39 & .38 & .37 & .36 & .35 & .34 & .33 & .3 &.1
\end{tabular}
\normalsize

%<!--(https://www.dropbox.com/s/65rdiv42ixe2eac/roc-lift.xlsx) -->
\begin{itemize}
  \item $C$ is the actual class of the training observations.
  \item $\text{neg} / \text{pos} = 1$, i.e. $1/\text{pos} = 1/\text{neg} = 0.1$
%<!--   [Click here for GIF](https://hsto.org/files/267/36b/ff1/26736bff158a4d82893ff85b2022cc5b.gif "giflink") (Result on next slide) -->
\end{itemize}

\begin{itemize}
  \item Rank test observations on decreasing score.
  \item Start in $(0, 0)$, for each observation $x$ (in the decreasing order).
  \begin{itemize}
    \item If $x$ is positive, move $1/\text{pos} = 0.1$ up
    \item If $x$ is negative, move $1/\text{neg} = 0.1$ right
  \end{itemize}
\end{itemize}

<<echo=FALSE, out.width="0.8\\textwidth", fig.width=6, fig.height=4, fig.align="center">>=
source("rsrc/plot_roc.R")

df_auc = data.frame(
  '#' = 1:12,
  Truth = c("Pos", "Neg")[c(1, 1, 1, 2, 1, 2, 1, 2, 2, 2, 1, 2)],
  Score = c(0.95, 0.86, 0.69, 0.65, 0.59, 0.52, 0.51, 0.39, 0.28, 0.18, 0.15, 0.06)
)
names(df_auc) = c("#", "Truth", "Score")

thresh = 0.9
tpr = mlr::measureTPR(truth = df_auc$Truth, response = ifelse(df_auc$Score > thresh, "Pos", "Neg"), positive = "Pos")
fpr = mlr::measureFPR(truth = df_auc$Truth, response = ifelse(df_auc$Score > thresh, "Pos", "Neg"), positive = "Pos", negative = "Neg")

plotROC(df_auc, thresh)
@

Set threshold $\alpha =$ \Sexpr{thresh} yields TPR \Sexpr{tpr} and FPR \Sexpr{fpr}.


\framebreak

<<echo=FALSE, out.width="0.8\\textwidth", fig.width=6, fig.height=4, fig.align="center">>=
thresh = 0.85
tpr = mlr::measureTPR(truth = df_auc$Truth, response = ifelse(df_auc$Score > thresh, "Pos", "Neg"), positive = "Pos")
fpr = mlr::measureFPR(truth = df_auc$Truth, response = ifelse(df_auc$Score > thresh, "Pos", "Neg"), positive = "Pos", negative = "Neg")

plotROC(df_auc, thresh)
@

Set threshold $\alpha =$ \Sexpr{thresh} yields TPR \Sexpr{tpr} and FPR \Sexpr{fpr}.

\framebreak

<<echo=FALSE, out.width="0.8\\textwidth", fig.width=6, fig.height=4, fig.align="center">>=
thresh = 0.66
tpr = mlr::measureTPR(truth = df_auc$Truth, response = ifelse(df_auc$Score > thresh, "Pos", "Neg"), positive = "Pos")
fpr = mlr::measureFPR(truth = df_auc$Truth, response = ifelse(df_auc$Score > thresh, "Pos", "Neg"), positive = "Pos", negative = "Neg")

plotROC(df_auc, thresh)
@

Set threshold $\alpha =$ \Sexpr{thresh} yields TPR \Sexpr{tpr} and FPR \Sexpr{fpr}.

\framebreak

<<echo=FALSE, out.width="0.8\\textwidth", fig.width=6, fig.height=4, fig.align="center">>=
thresh = 0.6
tpr = mlr::measureTPR(truth = df_auc$Truth, response = ifelse(df_auc$Score > thresh, "Pos", "Neg"), positive = "Pos")
fpr = mlr::measureFPR(truth = df_auc$Truth, response = ifelse(df_auc$Score > thresh, "Pos", "Neg"), positive = "Pos", negative = "Neg")

plotROC(df_auc, thresh)
@

Set threshold $\alpha =$ \Sexpr{thresh} yields TPR \Sexpr{tpr} and FPR \Sexpr{fpr}.


\framebreak

<<echo=FALSE, out.width="0.8\\textwidth", fig.width=6, fig.height=4, fig.align="center">>=
thresh = 0.55
tpr = mlr::measureTPR(truth = df_auc$Truth, response = ifelse(df_auc$Score > thresh, "Pos", "Neg"), positive = "Pos")
fpr = mlr::measureFPR(truth = df_auc$Truth, response = ifelse(df_auc$Score > thresh, "Pos", "Neg"), positive = "Pos", negative = "Neg")

plotROC(df_auc, thresh)
@

Set threshold $\alpha =$ \Sexpr{thresh} yields TPR \Sexpr{tpr} and FPR \Sexpr{fpr}.

\framebreak

<<echo=FALSE, out.width="0.8\\textwidth", fig.width=6, fig.height=4, fig.align="center">>=
thresh = 0.3
tpr = mlr::measureTPR(truth = df_auc$Truth, response = ifelse(df_auc$Score > thresh, "Pos", "Neg"), positive = "Pos")
fpr = mlr::measureFPR(truth = df_auc$Truth, response = ifelse(df_auc$Score > thresh, "Pos", "Neg"), positive = "Pos", negative = "Neg")

plotROC(df_auc, thresh)
@

Set threshold $\alpha =$ \Sexpr{thresh} yields TPR \Sexpr{tpr} and FPR \Sexpr{fpr}.

\framebreak

<<echo=FALSE, out.width="0.8\\textwidth", fig.width=6, fig.height=4, fig.align="center">>=
plotROC(df_auc, 0, highlight = FALSE)
@

\framebreak

\begin{center}
\includegraphics[width=0.7\textwidth]{figure_man/roc-naive-method.png}
\end{center}

See \url{https://quayau.shinyapps.io/roc_shiny/} for a live demo.

\end{vbframe}

\begin{vbframe}{AUC: Area Under ROC Curve}

<<echo=FALSE, out.width="0.4\\textwidth", fig.width=4, fig.height=4, fig.align="center">>=
plotROC(df_auc, 0, table = FALSE, auc = TRUE, highlight = FALSE)
@

\begin{itemize}
  \item AUC represents degree or measure of separability by calculating the area under the ROC curve
  \item AUC =   1: Perfect, ranks all pos. higher than all neg.
  \item AUC = 0.5: Randomly ordered
  \item AUC =   0: Perfect, but inverted labels
\end{itemize}



The area under the ROC curve (AUC $\in [0, 1]$) is
\begin{itemize}
  \item A measure for evaluating the performance of a classifier
%<!-- https://www.quora.com/How-is-statistical-significance-determined-for-ROC-curves-and-AUC-values -->
  \item Related to the Mann-Whitney-U test, which
  \begin{itemize}
    \item Estimates the probability that randomly chosen positives
      are ranked higher than randomly chosen negatives.
    \item Relation: $AUC = \cfrac{U}{POS \cdot NEG}$.
  \end{itemize}
  % \item related to the Gini coefficient $= 2 \cdot AUC - 1$ (area above diag.)
\end{itemize}

\framebreak

\textbf{Interpetation}

\begin{itemize}
  \item Interpretation: Probability that classifier ranks a
  random positive higher than a random negative observation

\begin{center}
\includegraphics[width=0.8\textwidth,page=1]{figure_man/auc_interpretation.pdf}
\end{center}

<<echo=FALSE, fig.width=6, fig.height=3, out.width="0.8\\textwidth">>=
# df_auc = data.frame(
#   truth = c(1, 1, 1, 0, 1, 0, 0),
#   scores =  c(0.9, 0.76, 0.7, 0.5, 0.45, 0.3, 0.1)
# )
df_auc = data.frame(
  truth = ifelse(df_auc$Truth == "Pos", 1, 0),
  scores =  df_auc$Score
)
df_auc_lines = merge(df_auc[df_auc$truth == 1, ], df_auc[df_auc$truth == 0, -3], by = NULL)
df_auc_lines$correct = factor(ifelse(df_auc_lines$scores.x < df_auc_lines$scores.y, "false", "correct"), levels = c("false", "correct"))

# ggplot() + geom_point(data = df_auc, aes(x = abs(truth - 1), y = scores, color = as.factor(truth)), show.legend = FALSE) +
ggplot() +
  geom_point(data = df_auc, aes(x = abs(truth - 1), y = scores), size = 3, shape = 1, stroke = 2, color = rgb(0.4, 0.4, 0.4), alpha = 0.8) +
  geom_segment(data = df_auc_lines, aes(x = abs(truth.x - 1), xend = abs(truth.y - 1), y = scores.x, yend = scores.y, color = correct),
    show.legend = FALSE) +
  ylab("Predicted Scores\nof Classifier") +
  xlab("True Label") +
  scale_x_continuous(breaks = c(0, 1), labels = c("Positive", "Negative"), limits = c(-0.3, 1.3)) +
  theme_minimal()
@

  \item AUC is the fraction of blue (correct classified) and all lines:

<<echo=FALSE, results="asis">>=
auc = mlr::measureAUC(probabilities = df_auc$scores, truth = df_auc$truth, positive = 1)
n_true = sum(df_auc_lines$correct == "correct")
cat("\\[\\mathsf{AUC} = ", n_true, "/", nrow(df_auc_lines) ," = ", round(auc, 4), "\\]")
@

\end{itemize}

\end{vbframe}








%
% \begin{vbframe}{Example Practical Method}
% Given: 20 training observations, 12 negative and 8 positive
%
% \vspace{20pt}
%
% \tiny
% \begin{tabular}{l|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}}
%
%   \hspace{-8pt} \#&  1& 2&  3&  4&  5&  6&  7&  8&  9& 10& 11& 12& 13& 14& 15& 16& 17& 18& 19& 20 \\ \hline
%   \hspace{-8pt} C & N & N & N & N & N & N & N & N & N & N & N & N & P & P & P & P & P & P & P & P \\ \hline
%   \hspace{-8pt} Score & .18 & .24 &  .32 & .33 & .4 & .53 & .58 & .59 & .6 & .7 & .75 & .85 & .52 & .72 & .73 & .79 & .82 & .88 & .9 &.92
% \end{tabular}
% \normalsize
%
% \vspace{20pt}
% $\Rightarrow$ sort by score and draw the curves:
% \vspace{20pt}
%
% \tiny
% \begin{tabular}{l|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}}
%
%   \hspace{-8pt} \#&  20& 19&  18&  12&  17&  16&  11&  15&  14& 10& 9& 8& 7& 6& 13& 5& 4& 3& 2& 1 \\ \hline
%   \hspace{-8pt} C & P & P & P & N & P & P & N & P & P & N & N & N & N & N & P & N & N & N & N & N \\ \hline
%   \hspace{-8pt} Score & .92 & .9 &  .88 & .85 & .82 & .79 & .75 & .73 & .72 & .7 & .6 & .59 & .58 & .53 & .52 & .4 & .33 & .32 & .24 &.18
% \end{tabular}
% \normalsize
% \end{vbframe}
%
%
% \begin{vbframe}{Example Practical Method}
% \begin{center}
% \includegraphics[width=0.75\textwidth]{figure_man/roc-curve-ex2.png}
% \end{center}
% \begin{itemize}
%   \item Best accuracy achieved with observation \# 18.
%   \item Setting $\theta = 0.88 \Rightarrow$ accuracy of $15/20 \; \hat{=} \; 75 \%$.
% \end{itemize}
% \end{vbframe}



% \begin{vbframe}{Explanation Mann-Whitney-U Test}
% \begin{itemize}
% \item First we plot the ranks of all the scores as a stack of horizontal bars, and color them by the labels.
% \item Stack the green bars on top of one another, and slide them horizontally as needed to get a nice even stairstep on the right edge (See: practical method example for ROC curves):
% \end{itemize}
% \begin{center}
% \includegraphics[width=0.8\textwidth]{figure_man/roc-mannwhitney3.png}
% \end{center}
%
%
% \framebreak
%
%
% \begin{center}
% \includegraphics[width=0.5\textwidth]{figure_man/roc-mannwhitney2.png}
% \end{center}
%
% \begin{itemize}
%  \item Definition of the U statistic: $U = R_1 - \cfrac{n_1(n_1 + 1)}{2}$
%  \begin{itemize}
%   \item $R_1$ is the sum of ranks of positive cases (the area of the green bars)
%   \item $n_1$ is the number of positive cases
%  \end{itemize}
%   \item The area of the green bars on the right side is equal to $\cfrac{n_1(n_1 + 1)}{2}$.
% \end{itemize}
%
% \framebreak
% \begin{center}
% \includegraphics[width=0.5\textwidth]{figure_man/roc-mannwhitney2.png}
% \end{center}
%
% \begin{itemize}
%  \item $U =$ area of the green bars on left side
%  \item area of dashed rectangle = $n_1 \cdot n_2$
%  \item $AUC$ is $U$ normalized to the unit square,
% \end{itemize}
% $$\Longrightarrow AUC = \cfrac{U}{n_1\cdot n_2}$$
% with $n_1 = \text{POS}$ and $n_2 = \text{NEG}$.
% \end{vbframe}
%
%
% \begin{vbframe}{Partial AUC}
% \begin{itemize}
%   \item Sometimes it can be useful to look at a \href{http://journals.sagepub.com/doi/pdf/10.1177/0272989X8900900307}{specific region under the ROC curve}  $\Rightarrow$ partial AUC (pAUC).
%   \item Let $0 \leq c_1 < c_2 \leq 1$ define a region.
%   \item For example, one could focus on a region with low fpr ($c_1 = 0, c_2 = 0.2$) or a region with high tpr ($c_1 = 0.8, c_2 = 1$):
% \end{itemize}

% # <<echo = FALSE, message = FALSE, warning = FALSE, fig.width = 14, fig.height = 7, out.width="0.7\\textwidth">>=
% # #library(pROC)
% # set.seed(1)
% # D.ex <- rbinom(200, size = 1, prob = .5)
% # M1 <- rnorm(200, mean = D.ex, sd = .65)
% # M2 <- rnorm(200, mean = D.ex, sd = 1.5)
% #
% # test <- data.frame(D = D.ex, D.str = c("Healthy", "Ill")[D.ex + 1],
% #                    M1 = M1, M2 = M2, stringsAsFactors = FALSE)
% #
% # rocobj <- pROC::roc(test$D, test$M1)
% # par(mfrow = c(1, 2))
% # pROC::plot.roc(rocobj, print.auc=TRUE, auc.polygon=TRUE, partial.auc=c(1, 0.8), partial.auc.focus="sp", reuse.auc=FALSE, legacy.axes = TRUE, xlab = "fpr", ylab = "tpr", xlim = c(1, 0), ylim = c(0, 1),  auc.polygon.col="red", auc.polygon.density = 20, auc.polygon.angle = 135, partial.auc.correct = FALSE
% # )
% # pROC::plot.roc(rocobj, print.auc=TRUE, auc.polygon=TRUE, partial.auc=c(1, 0.8), partial.auc.focus="se", reuse.auc=FALSE, legacy.axes = TRUE, xlab = "fpr", ylab = "tpr", xlim = c(1, 0), ylim = c(0, 1),  auc.polygon.col="red", auc.polygon.density = 20, auc.polygon.angle = 135)
% #@

%
% \framebreak
%
% \begin{itemize}
%   \item $\text{pAUC} \in [0, c_2 - c_1]$.
%   \item The partial AUC can be corrected (see \href{http://journals.sagepub.com/doi/pdf/10.1177/0272989X8900900307}{McClish}), to have values between $0$ and $1$, where $0.5$ is non discriminant and $1$ is maximal: $$\text{pAUC}_\text{corrected} = \cfrac{1+\cfrac{\text{pAUC} - \text{min}}{\text{max} - \text{min}}}{2} $$
%   \item $\text{min}$ is the
% value of the non-discriminant AUC in the region
%   \item $\text{max}$ is the maximum possible AUC in the region
% \end{itemize}
% \end{vbframe}
%

% 
% \begin{vbframe}{Multiclass AUC}
% \begin{itemize}
%   \item Consider multiclass classification, where a classifier predicts the probability $p_k$ of belonging to class $k$ for each class.
%   \item Hand and Till (2001) proposed to average the AUC of pairwise comparisons (1 vs. 1) of a multiclass classifier.
%   \begin{itemize}
%     \item estimate $AUC(i,j)$ for each pair of class $i$ and $j$
%     \item $AUC(i,j)$ is the probability that a randomly drawn member of class $i$ has a lower probability of belonging to class $j$
%       than a randomly drawn member of class $j$.
%     \item for $K$ classes, we have ${{K}\choose{2}} = \tfrac{K (K-1)}{2}$ values of $AUC(i,j)$ that are then averaged to compute the Multiclass AUC.
%   \end{itemize}
% \end{itemize}
% \end{vbframe}
% 
% \begin{vbframe}{Calibration and Discrimination}
% We consider data with a binary outcome $y$.
% \begin{itemize}
%   \item \textbf{Calibration:} When the predicted probabilities closely agree
%     with the observed outcome (for any reasonable grouping).
%   \begin{itemize}
%     \item \textbf{Calibration in the large} is a property of the \textit{full sample}.
%     It compares the observed probability in the full sample  (e.g. proportion of observations for which $y=1$)
%    % <!-- (e.g., 10% if 10 of 100 individuals have the outcome being predicted, e.g. $y=1$) -->
%     with the average predicted probability in the full sample.
%     \item \textbf{Calibration in the small} is a property of \textit{subsets} of the sample.
%     It compares the observed probability in each subset with the average
%     predicted probability in that subset.
%   \end{itemize}
%   \item \textbf{Discrimination:} Ability to perfectly separate the population into $y=0$ and $y=1$.
%     Measures of discrimination are, for example, AUC, sensitivity, specificity.
% \end{itemize}
% \end{vbframe}
% 
% \begin{vbframe}{Calibration and Discrimination}
% %<!-- http://www.uphs.upenn.edu/dgimhsr/documents/predictionrules.sp12.pdf -->
% A well calibrated  classifier can be poorly discriminating, e.g.
% 
% \begin{table}[]
% \centering
% \begin{tabular}{rrrr}
% \hline
% Obs. Nr. & truth & Pred Rule 1 & Pred Rule 2 \\
% \hline
% 1        & 1     & 1           & 0           \\
% 2        & 1     & 1           & 0           \\
% 3        & 0     & 0           & 1           \\
% 4        & 0     & 0           & 1           \\ \hline
% Avg Prob & 50\%  & 50\%        & 50\%        \\
% \hline
% \end{tabular}
% \end{table}
% 
% \begin{itemize}
%   \item Both prediction rules have identical calibration in the large (50\%), however, rule 1 is better than rule 2.
% \end{itemize}
% 
% <<eval = FALSE, echo = FALSE>>=
% truth = c(1,1,0,0,0,0)
% pred.rule.1 = c(1,1,0,0,0,0)
% pred.rule.2 = c(0,0,0,0,1,1)
% kable(data.frame(truth = truth, "pred rule 1" = pred.rule.1, "pred rule 2" = pred.rule.2))
% @
% \end{vbframe}
% 
% \begin{vbframe}{Calibration and Discrimination}
% A well discriminating classifier can have a bad calibration, e.g.
% 
% \begin{table}[]
% \centering
% \begin{tabular}{rrrr}
% \hline
% Obs. Nr. & truth & Pred Rule 1 & Pred Rule 2 \\
% \hline
% 1        & 1     & 0.9           & 0.9         \\
% 2        & 1     & 0.9           & 0.9           \\
% 3        & 0     & 0.1          & 0.7           \\
% 4        & 0     & 0.1         & 0.7           \\ \hline
% Avg Prob & 50\%  & 50\%        & 80\%        \\
% \hline
% \end{tabular}
% \end{table}
% 
% \begin{itemize}
%   \item Both prediction rules are well discriminating (e.g., setting thresholds $\theta_1 = 0.5$, $\theta_2 = 0.8$)
%   \item Prediction rule 2 is rather poorly calibrated. The proportion of observations for which $y=1$ would be estimated with $80\%$.
% \end{itemize}
% \end{vbframe}
%
% \begin{vbframe}{ROC Analysis in R}
% \begin{itemize}
%   \item \texttt{generateThreshVsPerfData} calculates one or several performance measures for a sequence of decision thresholds from 0 to 1.
%   \item It provides S3 methods for objects of class \texttt{Prediction}, \texttt{ResampleResult}
% and \texttt{BenchmarkResult} (resulting from  \texttt{predict.WrappedModel}, \texttt{resample}
% or \texttt{benchmark}).
%   \item \texttt{plotROCCurves} plots the result of \texttt{generateThreshVsPerfData} using \texttt{ggplot2}.
%   \item More infos \url{http://mlr-org.github.io/mlr-tutorial/release/html/roc_analysis/index.html}
% \end{itemize}
% \end{vbframe}
%
% \begin{vbframe}{Example 1: Single predictions}
% \scriptsize
% <<echo=TRUE, message = FALSE>>=
% set.seed(1)
% # get train and test indices
% n = getTaskSize(sonar.task)
% train.set = sample(n, size = round(2/3 * n))
% test.set = setdiff(seq_len(n), train.set)
%
% # fit and predict
% lrn = makeLearner("classif.lda", predict.type = "prob")
% mod = train(lrn, sonar.task, subset = train.set)
% pred = predict(mod, task = sonar.task, subset = test.set)
% @
% \normalsize
% \end{vbframe}
%
% \begin{vbframe}{Example 1: Single predictions}
% We calculate fpr, tpr and compute error rates:
%
% \scriptsize
% <<echo = TRUE>>=
% df = generateThreshVsPerfData(pred, measures = list(fpr, tpr, mmce))
% @
% \normalsize
% \begin{itemize}
%   \item \texttt{generateThreshVsPerfData} returns an object of class \texttt{ThreshVsPerfData},
% which contains the performance values in the \texttt{\$data} slot.
%   \item By default, \texttt{plotROCCurves} plots the performance values of the first two measures passed
% to \texttt{generateThreshVsPerfData}.
%   \item The first is shown on the x-axis, the second on the y-axis.
% \end{itemize}
% \end{vbframe}
%
% \begin{vbframe}{Example 1: Single predictions}
% \scriptsize
% <<echo = TRUE, fig.align="center", fig.width = 5, fig.height = 5, out.width="0.55\\textwidth">>=
% df = generateThreshVsPerfData(pred, measures = list(fpr, tpr, mmce))
% plotROCCurves(df)
% @
% \normalsize
%
% \framebreak
%
% The corresponding area under curve auc can be calculated by
%
% \scriptsize
% <<echo = TRUE>>=
% performance(pred, auc)
% @
%
% \normalsize
% \texttt{plotROCCurves} always requires a pair of performance measures that are plotted against
% each other.
%
% \framebreak
%
% If you want to plot individual measures vs. the decision threshold, use
%
% \scriptsize
% <<echo = TRUE, fig.align="center", fig.height = 4, fig.width = 8, out.width="0.9\\textwidth">>=
% plotThreshVsPerf(df)
% @
% \normalsize
% \end{vbframe}
%
%
% \begin{vbframe}{Example 2: Benchmark Experiment}
% \scriptsize
% <<>>=
% options(width = 200)
% @
% <<echo = TRUE>>=
% lrn1 = makeLearner("classif.randomForest", predict.type = "prob")
% lrn2 = makeLearner("classif.rpart", predict.type = "prob")
%
% cv5 = makeResampleDesc("CV", iters = 5)
%
% bmr = benchmark(learners = list(lrn1, lrn2), tasks = sonar.task,
%   resampling = cv5, measures = list(auc, mmce), show.info = FALSE)
% bmr
% @
% \normalsize
%
% Calling \texttt{generateThreshVsPerfData} and \texttt{plotROCCurves} on the \texttt{BenchmarkResult}
% produces a plot with ROC curves for all learners in the experiment.
%
% \framebreak
%

% \scriptsize
% <<echo = TRUE, fig.align="center", fig.height = 4, fig.width = 8, out.width="\\textwidth">>=
% df = generateThreshVsPerfData(bmr, measures = list(fpr, tpr, mmce))
% plotROCCurves(df)
% @
% \framebreak
%
% \scriptsize
% <<echo = TRUE, fig.align="center", fig.height = 4, fig.width = 8, out.width="\\textwidth">>=
% plotThreshVsPerf(df)
% @
% \end{vbframe}
\endlecture
