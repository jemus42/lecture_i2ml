% Introduction to Machine Learning
% Day 3

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@

% Load all R packages and set up knitr
<<setup, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{Evaluation: Resampling and Cross-Validation}
\lecture{Introduction to Machine Learning}


\begin{vbframe}{Resampling}

\begin{itemize}
  \item Aim: Assess the performance of learning algorithm.
  \item Uses the data more efficiently then simple train-test.
  \item Repeatedly split in train and test, then average results.
  \item The usual trick is to make training sets quite larger (to keep the pessimistic bias small),
  and to handle the variance introduced by smaller test sets through many repetitions and averaging
  of results.
\end{itemize}

\begin{center}
\includegraphics[width=5.5cm]{figure_man/ml_abstraction-crop.pdf}
\end{center}

\end{vbframe}

\begin{vbframe}{Cross-Validation}

\begin{itemize}
  \item Split the data into $k$ roughly equally-sized partitions.
  \item Use each part once as test set and join the $k-1$ others for training
  \item Obtain $k$ test errors and average.
\end{itemize}

\lz

Example: 3-fold cross-validation:

\begin{center}
\includegraphics[width=8cm]{figure_man/crossvalidation.png}
\end{center}
\end{vbframe}

\begin{vbframe}{Cross-Validation - Stratification}

Stratification tries to keep the distribution of the target class (or any specific categorical feature of interest) in each fold.

\lz

Example of stratified 3-fold Cross-Validation:

\lz

<<echo=FALSE, fig.height=3.5>>=
layout(cbind(rep(1, 3), 2:4, matrix(5:16, ncol = 4, byrow = TRUE)))
par(mar = c(2,2,4,2))

plot(as.factor(c(rep(1, 3), 2)), axes = FALSE, ylim = c(0, 5), col = c("#E69F00","#56B4E9"), main = "Class Distribution")
box()

par(mar = c(1,1,1,1))
red.ind = c(1, 6, 11)
for (i in 1:3) {
    plot.new()
    text(0.5, 0.5, cex = 1.2, paste("Iteration", i))
}
for (i in 1:12) {
  if (i %% 4 == 0) {
    plot.new()
    # text(0.5, 0.5, cex = 2, bquote(paste("=> ",widehat(GE)[D[test]^.(i/4)])))
  } else {
    plot(as.factor(c(rep(1, 3), 2)),
      axes = FALSE, ylim = c(0, 5), col = c("#E69F00","#56B4E9"))
    if (i %in% red.ind) {
      rect(par("usr")[1], par("usr")[3], par("usr")[2], par("usr")[4], col = "#E69F0055")
      box(col = "#E69F00")
      plot(as.factor(c(rep(1, 3), 2)),
        axes = FALSE, ylim = c(0, 5), col = c("#E69F00","#56B4E9"), add = TRUE)
    } else {
      rect(par("usr")[1], par("usr")[3], par("usr")[2], par("usr")[4], col = "#56B4E955")
      box()
      plot(as.factor(c(rep(1, 3), 2)),
        axes = FALSE, ylim = c(0, 5), col = c("#E69F00","#56B4E9"), add = TRUE)
    }
  }
}
@
\end{vbframe}


\begin{vbframe}{Cross-Validation - Comments}

\begin{itemize}
  \item 5 or 10 folds are common, they use 80\% and 90\% of data in training
  \item $k = n$ is known as leave-one-out (LOO) cross-validation
  \item Estimates of the generalization error tend to be somewhat pessimistically biased
    (because the size of the training sets is $ n- (n/k) < n$), bias increases as $k$ gets smaller.
  \item The performance estimates for each fold are not independent, because
  of the structured overlap of the training sets.
  Hence, the variance of the estimator increases again for very large $k$ (close to LOO),
  when training sets nearly completely overlap.
  \item LOO is nearly unbiased, but has high variance.
  \item Repeated $k$-fold CV (multiple random partitions)
  can improve error estimation for small sample size.
\end{itemize}
\end{vbframe}

% \begin{vbframe}{The failure of leave-one-out}
% \begin{itemize}
% \item The \code{iris} data set has 50 instances of each class (\code{setosa}, \code{versicolor}, \code{virginica}).
% \item A majority classifier should have 33.3\% accuracy.
% \item But majority classification with LOO is unstable and will lead to a generalization error of 1 (0\% accuracy) in each fold.
% \item For demonstration let's assume \code{iris} only consists of 3 instances:
% \end{itemize}
% <<fig.height=3, echo = FALSE>>=
% par(mar = c(0, 0, 0, 0))
% plot(1, type = "n", xlim = c(-2, 10), ylim = c(0, 4), axes = FALSE)
% rect(seq(0, 4, by = 2), 0.1, seq(2, 6, by = 2), 1, col = c("#56B4E944","#56B4E944","#E69F0044"))
% rect(seq(0, 4, by = 2), 1.1, seq(2, 6, by = 2), 2, col = c("#56B4E944","#E69F0044","#56B4E944"))
% rect(seq(0, 4, by = 2), 2.1, seq(2, 6, by = 2), 3, col = c("#E69F0044", "#56B4E944","#56B4E944"))

% text(c(3, 7.3, 9.5), 3.55,
%   c("Training set (blue), test set (pink)", "Predicted label", "GE"),
%   cex = 1.3)

% text(seq(1, 5, by = 2), 2.55,
%   c("Setosa", "Versicolor", "Virginica"), cex = 1.3)
% text(seq(1, 5, by = 2), 1.55,
%   c("Setosa", "Versicolor", "Virginica"), cex = 1.3)
% text(seq(1, 5, by = 2), 0.55,
%   c("Setosa", "Versicolor", "Virginica"), cex = 1.3)
% text(7.3, 2.75, "Versicolor/", cex = 1.3)
% text(7.3, 2.45, "Virginica", cex = 1.3)
% text(7.3, 1.75, "Setosa/", cex = 1.3)
% text(7.3, 1.45, "Virginica", cex = 1.3)
% text(7.3, 0.75, "Setosa/", cex = 1.3)
% text(7.3, 0.45, "Versicolor", cex = 1.3)

% text(rep(-1, 3), c(0,1,2) + 0.55, paste("Iteration", 3:1),
%   cex = 1.3)

% for (i in 1:3) {
%   text(9.5, 2 - (i - 1) + 0.55,
%     bquote(widehat(GE)[D[test]^.(i)]), cex = 1.3)
%   text(10.2, 2 - (i - 1) + 0.55, c("= 1"), cex = 1.3)
% }
% @
% \end{vbframe}

\begin{vbframe}{Bootstrap}

The basic idea is to randomly draw $B$ training sets of size $n$ with
replacement from the original training set $\Dtrain$:
% \begin{eqnarray*}
% \Dtrain^1 &=& \{z^1_1, \ldots, z^1_n\}\\
% \vdots& \\
% \Dtrain^B &=& \{z^B_1, \ldots, z^B_n\}
% \end{eqnarray*}

\begin{center}
\begin{tikzpicture}[scale=1]
% style
\tikzstyle{rboule} = [circle,scale=0.7,ball color=red]
\tikzstyle{gboule} = [circle,scale=0.7,ball color=green]
\tikzstyle{bboule} = [circle,scale=0.7,ball color=blue]
\tikzstyle{nboule} = [circle,scale=0.7,ball color=black]
\tikzstyle{sample} = [->,thin]

% title initial sample
\path (3.5,3.75) node[anchor=east] {$\Dtrain$};

% labels
\path (3.5,3)   node[anchor=east] {$\Dtrain^1$};
\path (3.5,2.5) node[anchor=east] {$\Dtrain^2$};
\path (3.5,1.5) node[anchor=east] {$\Dtrain^B$};

\path (3.5,2) node[anchor=east] {$\vdots$};
\path[draw,dashed] (3.75,2.0) -- (4.5,2.0);

% initial sample
\path ( 3.75,3.75) node[rboule] (j01) {};
\path ( 4.00,3.75) node[gboule] (j02) {};
\path ( 4.25,3.75) node[bboule] (j03) {};
\path ( 4.5,3.75) node[nboule] (j20) {};

% bootstrap 1
\path ( 3.75, 3.0) node[rboule] {};
\path ( 4.00, 3.0) node[rboule] {};
\path ( 4.25, 3.0) node[bboule] {};
\path ( 4.5, 3.0) node[nboule] (b1) {};

% bootstrap 2
\path ( 3.75, 2.5) node[gboule] {};
\path ( 4.00, 2.5) node[bboule] {};
\path ( 4.25, 2.5) node[gboule] {};
\path ( 4.5, 2.5) node[rboule] (b2) {};

% bootstrap N
\path (3.75,1.5) node[gboule] {};
\path (4,1.5) node[rboule] {};
\path (4.25,1.5) node[nboule] {};
\path (4.5,1.5) node[nboule] (bN) {};

% arrows
\path[sample] (j20.east) edge [out=0, in=60] (b1.east);
\path[sample] (j20.east) edge [out=0, in=60] (b2.east);
\path[sample] (j20.east) edge [out=0, in=60] (bN.east);
\end{tikzpicture}
\end{center}

We define the test set in terms of out-of-bootstrap observations
$\Dtest^b = \Dtrain \setminus \Dtrain^b$.

\framebreak

\begin{itemize}
  \item Typically, $B$ is between $30$ and $200$.
  \item The variance of the bootstrap estimator tends to be smaller than the
  variance of $k$-fold CV, as training sets are independently drawn, discontinuities are smoothed out.
  \item The more iterations, the smaller the variance of the estimator.
  \item Tends to be pessimistically biased
  (because training sets contain only about $63.2 \%$ unique the observations).
  \item Bootstrapping framework might allow the use of formal inference methods
  (e.g. to detect significant performance differences between methods).
  \item Extensions exist for very small data sets, that also use the training error for
    estimation: B632 and B632+.
\end{itemize}

\end{vbframe}


\begin{vbframe}{Subsampling}

\begin{itemize}
  \item Repeated hold-out with averaging, a.k.a. monte-carlo CV
  \item Similar to bootstrap, but draws without replacement, similar comments hold
  \item Typical choices for splitting: $4/5$ or $9/10$ for training
  \item The smaller the subsampling rate, the larger the pessimistic bias.
  \item The more subsampling iterations, the smaller the variance.
\end{itemize}

\end{vbframe}

\begin{vbframe}{Bias-Variance Analysis for Subsampling}

  \begin{itemize}
    \item Let's reconsider our hold-out experiment on the spiral data from the train-test unit (maybe re-read it again)
    \item Again, we use split-rates $s \in \{0.05, 0.1, ..., 0.95\}$ for training with $|\Dtrain| = s \cdot 500$.
    \item But now we compare 50 subsampling experiments with $50 \cdot 50$ hold-out experiments per split.
    \item Every subsampling experiment is the result of averaging 50 hold-out experiments, so each performance estimate is much more reliable (but also more expensive) than one computed by a hold-out experiment.
  \end{itemize}

\framebreak

<<>>=
# rsrc data from rsrc/holdout-biasvar.R
load("rsrc/holdout-biasvar.RData")
@

<<echo=FALSE, fig.height=4>>=
library(plyr)
library(gridExtra)

ggd1 = melt(res)
colnames(ggd1) = c("split", "rep", "ssiter", "mmce")
ggd1$split = as.factor(ggd1$split)
ggd1$mse = (ggd1$mmce -  realperf)^2
ggd1$type = "holdout"
ggd1$ssiter = NULL
mse1 = ddply(ggd1, "split", summarize, mse = mean(mse))
mse1$type = "holdout"

ggd2 = ddply(ggd1, c("split", "rep"), summarize, mmce = mean(mmce))
ggd2$mse = (ggd2$mmce -  realperf)^2
ggd2$type = "subsampling"
mse2 = ddply(ggd2, "split", summarize, mse = mean(mse))
mse2$type = "subsampling"

ggd = rbind(ggd1, ggd2)
gmse = rbind(mse1, mse2)

ggd$type = as.factor(ggd$type)
pl1 = ggplot(ggd, aes(x = split, y = mmce, col = type))
pl1 = pl1 + geom_boxplot()
pl1 = pl1 + geom_hline(yintercept = realperf)
pl1 = pl1 + theme(axis.text.x = element_text(angle = 45))
print(pl1)

gmse$split = as.numeric(as.character(gmse$split))
gmse$type = as.factor(gmse$type)
@

\begin{itemize}
  \item Both experiments are compared to the "real" mmce (black line).
  \item Subsampling has the same pessimistic bias for small split rates, but much less variance overall.
  \item This allows to use much smaller test sets with good results.
\end{itemize}

\framebreak

<<echo=FALSE, fig.height=4>>=
pl2 = ggplot(gmse, aes(x = split, y = mse, col = type))
pl2 = pl2 + geom_line()
pl2 = pl2 + scale_y_log10()
pl2 = pl2 + scale_x_continuous(breaks = gmse$split)
pl2 = pl2 + theme(axis.text.x = element_text(angle = 45))
pl2 = pl2 + ylab("MSE of the mmce")
print(pl2)
@

\begin{itemize}
  \item The MSE is overall better for subsampling compared to hold-out
  \item The optimal split rate now is a higher $s \approx 0.9$
  \item We see an increase in variance at the end because the training sets become more overlapping and not independent
\end{itemize}
\end{vbframe}

\begin{vbframe}{Resampling discussion}

\begin{itemize}
  \item In ML we fit, at the end, a model on all our given data.
  \item Problem: We need to know how well this model performs in the future.
  But no data is left to reliably do this.
  \item In order to approximate this, we do the next best thing. We estimate how well
  the learner works when it sees nearly $n$  points from the same data distribution.
  \item Holdout, CV, resampling estimate exactly this number. The "pessimistic bias" refers to
  when use much less data in fitting than $n$. Then we "hurt" our learner unfairly.
  \item Strictly speaking, resampling only produces one number, the performance estimator.
  It does NOT produce models, paramaters, etc. These are intermediate results and discarded.
  \item The model and parameters are obtained when we fit the learner finally on the complete data.
  \item This is a bit weird and complicated, but we have to live with this.
\end{itemize}

\framebreak

\begin{itemize}
  \item 5CV or 10CV have become standard
  \item Do not use Hold-Out, CV with few iterations, or subsampling with a low subsampling rate for small samples, since this can cause the estimator to be extremely biased, with large variance.
  \item For small data situation with less than 500 or 200 observations, use LOO or probably better repeated CV
  \item A $\D$ with $|\D| = 100.000$ can have small sample size properties if one class has only 100 observations \ldots
  \item For some models, computationally fast calculations or approximations for the LOO exist
  \item Modern results seem to indicate that subsampling has somewhat better properties than
    bootstrapping. The repeated observations can can cause problems in training algorithms,
    especially in nested setups where the \enquote{training} set is split up again.
\end{itemize}
\end{vbframe}

\endlecture
