<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@
<<setup, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{Evaluation: Train and Test Error}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{Training Error}

The \emph{training error} (also called apparent error or resubstitution error)
is estimated by the averaging error over the same data set we fitted on:

\lz

\includegraphics[width=\textwidth, height=6cm,page=4]{figure_man/train_error.png}

\vspace{-0.8cm}

\end{vbframe}

\begin{vbframe}{Example: Polynomial Regression}
  
Assume an (unknown) sinusoidal function that $0.5 + 0.4 \cdot \sin (2 \pi x) + \epsilon$ that we sample from with some measurement error $\epsilon$.

<<echo=FALSE, out.width="0.75\\textwidth", fig.width = 8, fig.height = 4.5, fig.align="center">>=
library(ggplot2)
source("rsrc/plot_train_test.R")

.h = function(x) 0.5 + 0.4 * sin(2 * pi * x)
h = function(x) .h(x) + rnorm(length(x), mean = 0, sd = 0.05)

set.seed(1234)
x.all = seq(0, 1, length = 26L)
ind = seq(1, length(x.all), by = 2)
mydf = data.frame(x = x.all, y = h(x.all))


ggTrainTestPlot(data = mydf, truth.fun = .h, truth.min = 0, truth.max = 1, test.plot = FALSE,
  test.ind = ind)[["plot"]] + ylim(0, 1)
@

We try to approximate it with a $d$th-degree polynomial
\[ \fxt = \theta_0 + \theta_1 x + \cdots + \theta_d x^d = \sum_{j = 0}^{d} \theta_j x^j\text{.} \]

\framebreak

Models of different \textit{complexity}, i.e., of different orders of the polynomial
are fitted. How should we choose $d$?

<<echo=FALSE, fig.width = 9, fig.height = 4, fig.align="center">>=
out = ggTrainTestPlot(data = mydf, truth.fun = .h, truth.min = 0, truth.max = 1, test.plot = FALSE,
  test.ind = ind, degree = c(1, 3, 9))
out[["plot"]] + ylim(0, 1)
@

\begin{itemize}
\item d=1: \Sexpr{sprintf("%.03f", out$train.test$degree1[1])}: Clear underfitting
\item d=3: \Sexpr{sprintf("%.03f", out$train.test$degree3[1])}: Pretty OK?
\item d=9: \Sexpr{sprintf("%.03f", out$train.test$degree9[1])}: Clear overfitting
\end{itemize}

Simply using the training error seems to be a bad idea.

\end{vbframe}

\begin{vbframe}{Training Error Problems}
\begin{itemize}
  \item The training error is usually a very unreliable and overly optimistic estimator of future performance. 
    Modelling the training data is not of interest, but modelling the general structure in it. We do not want to overfit to noise
    or peculiarities.
  \item The training error of 1-NN is always zero, as each observation is its own NN during test time 
    (assuming we do not have repeated measurements with conflicting labels).
  \item Extend any ML training in the following way:
    After normal fitting, we also store the training data.
    During prediction, we first check whether $x$ is already stored in this set. If so, we replicate its label.
    The train error of such an (unreasonable) procedure will be 0.
  \item There are so called interpolators - interpolating splines, interpolating Kriging - whose predictions can always perfectly match the 
    regression targets, they are not necessarily good as they will interpolate the noise, too
\end{itemize}
\end{vbframe}

\begin{vbframe}{Training Error Problems}
\begin{itemize}
  \item Goodness-of-fit measures like (classical) $R^2$, likelihood, AIC, BIC, deviance are all based on the training error.
  \item For models of severely restricted capacity, and given enough data, the training error might provide reliable information. E.g. consider a linear model with p = 5, with 10 6 training points. But: What happens if we have less data or as p increases? Not possible to determine when training error becomes unreliable.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Test Error and Hold-Out Splitting}
The fundamental idea behind test error estimation (and everything
that will follow) is quite simple: 
\lz 
To measure performance, letâ€™s
simulate how our model will be applied on new, unseen data
So, to evaluate a given model do exactly that, predict only on data
not used during training and measure performance there.
\lz
That implies that for a given set D , we have to preserve some data
for testing that we cannot use for training, 
\end{vbframe}
    
\begin{vbframe}{Test Error and Hold-Out Splitting}
\begin{itemize}
  \item Straightforward idea: To measure performance, let's simulate how our model will be applied on new, unseen data
  \item Split data into 2 parts, e.g. 2/3 for training, 1/3 for testing
  \item Evaluate on data not used for model building, no way to \enquote{cheat}
\end{itemize}

\includegraphics[height=5.5cm]{figure_man/test_error.png}

\framebreak

Let's consider some clean test data for our sinusoidal example:
\lz
<<echo=FALSE, fig.width="0.9\\textwidth", fig.height=4>>=
ggTrainTestPlot(data = mydf, truth.fun = .h, truth.min = 0, truth.max = 1, test.plot = TRUE,
  test.ind = ind)[["plot"]] + ylim(0, 1)
@

\framebreak

\lz
<<echo=FALSE, fig.width="0.9*\\textwidth", fig.height = 4, fig.align="center">>=
ggTrainTestPlot(data = mydf, truth.fun = .h, truth.min = 0, truth.max = 1, test.plot = TRUE,
  test.ind = ind, degree = c(1, 3, 9))[["plot"]] + ylim(0, 1)
@

\begin{itemize}
\item d=1: \Sexpr{sprintf("%.03f", out$train.test$degree1[2])}: Clear underfitting
\item d=3: \Sexpr{sprintf("%.03f", out$train.test$degree3[2])}: Pretty OK?
\item d=9: \Sexpr{sprintf("%.03f", out$train.test$degree9[2])}: Clear overfitting
\end{itemize}

\framebreak


<<echo=FALSE, out.width="0.9\\textwidth", fig.height=4>>=
degrees = 1:9

errors = ggTrainTestPlot(data = mydf, truth.fun = .h, truth.min = 0, truth.max = 1, test.plot = TRUE,
  test.ind = ind, degree = degrees)[["train.test"]]

par(mar = c(4, 4, 1, 1))
#par(mar = c(4, 4, 0, 0) + 0.1)
plot(1, type = "n", xlim = c(1, 10), ylim = c(0, 0.1),
  ylab = "MSE", xlab = "degree of polynomial")
lines(degrees, sapply(errors, function(x) x["train"]), type = "b")
lines(degrees, sapply(errors, function(x) x["test"]), type = "b", col = "gray")

legend("topright", c("training error", "test error"), lty = 1L, col = c("black", "gray"))
text(3.75, 0.05, "Underfitting,\n\nHigh Bias,\nLow Variance", bg = "white")
arrows(4.75, 0.05, 2.75, 0.05, code = 2L, lty = 2L, length = 0.1)

text(6.5, 0.05, "Overfitting,\n\nLow Bias,\nHigh Variance", bg = "white")
arrows(7.5, 0.05, 5.5, 0.05, code = 1, lty = 2, length = 0.1)
@

We can also plot error measures for all polynomial degrees. We see the common monotonous decrease in training error, 
if we increase model complexity (we can adapt better to data with more flexibility) and we see the common U-shape of the test error.
First we underfit, then we over-fit, sweet-spot is in the middle. Numerically best for $d=3$.

\end{vbframe}


\begin{vbframe}{Training vs. test error}
  \vspace{-0.25cm}
  \begin{blocki}{The training error}
  \vspace{-0.25cm}
    \item is an over-optimistic (biased) estimator as the performance is measured on the same data the learned model was trained for
    \item decreases with smaller training set size as it is easier for the model to learn the underlying structure in the training set perfectly
    \item decreases with increasing model complexity as the model is able to learn more complex structures
  \end{blocki}
  \vspace{-0.25cm}
  \begin{blocki}{The test error}
  \vspace{-0.25cm}
  \item will typically decrease when the training set increases as the model generalizes better with more data (more data to learn)
  \item will have higher variance with decreasing test set size
  \item will have higher variance with increasing model complexity
  \end{blocki}
\end{vbframe}


\begin{vbframe}{Bias-Variance of holdout}
  \begin{itemize}
    \item If the size of our initial, complete data set $\D$ is limited,
      single train-test splits can be problematic.
    \item The smaller our single test set is, the higher the variance
      of our estimated performance error (e.g., if we test on one observation, in the extreme case).
      But note that by just making the test set smaller, we do not introduce any bias,
      as we simply average losses on i.i.d. observations from $\Pxy$.
    \item The smaller training set becomes, the more pessimistic bias we introduce into the model.
      Note that if $|D| = n$, our aim is to estimate the performance of a model fitted
      on n observations (as this is what we will do in the end). If we fit on less data during
      evaluation, our model will learn less, and perform worse. Very small training sets will also
      increase variance a bit.
  \end{itemize}
\end{vbframe}


% \begin{vbframe}{Bias-Variance of holdout}
%
%   Experiment:
%   \begin{itemize}
%     \item Data: simulate spiral data (sd = 0.1) from the \texttt{mlbench} package.
%     \item Learner: CART (\texttt{classif.rpart} from \texttt{mlr}).
%     \item Goal: estimate real performance of a model with $|\Dtrain| = 500$.
%     \item Get the "true" estimator by repeatedly sampling 500 observations from the simulator,
%       fit the learner, then evaluate on a really large number of observation.
%     \item Analyse different types of holdout and subsampling (= repeated holdout), with different split rates:
%     \begin{itemize}
%     \item Sample $\D$ with $|\D| = 500$ and use split-rate $s \in \{0.05, 0.1, ..., 0.95\}$ for training with $|\Dtrain| = s \cdot 500$.
%     \item Estimate performance on $\Dtest$ with $|\Dtest| = 500 \cdot (1 - s)$.
%     \item Repeat the samping of $\D$ 50 times and the splitting with $s$ 50 times ($\Rightarrow$ 2500 experiments for each split-rate).
%     \end{itemize}
%   \end{itemize}
%
% \framebreak
%
% Visualize the perfomance estimator - and the MSE of the estimator - in relation to the true error rate.

<<eval = FALSE>>=
# rsrc data from rsrc/holdout-biasvar.R
load("rsrc/holdout-biasvar.RData")
@

<<eval = FALSE, echo = FALSE, fig.height = 5>>=
ggd1 = melt(res)
colnames(ggd1) = c("split", "rep", "ssiter", "mmce")
ggd1$split = as.factor(ggd1$split)
ggd1$mse = (ggd1$mmce -  realperf)^2
ggd1$type = "holdout"
ggd1$ssiter = NULL
mse1 = ddply(ggd1, "split", summarize, mse = mean(mse))
mse1$type = "holdout"

ggd2 = ddply(ggd1, c("split", "rep"), summarize, mmce = mean(mmce))
ggd2$mse = (ggd2$mmce -  realperf)^2
ggd2$type = "subsampling"
mse2 = ddply(ggd2, "split", summarize, mse = mean(mse))
mse2$type = "subsampling"

ggd = rbind(ggd1, ggd2)
gmse = rbind(mse1, mse2)

ggd$type = as.factor(ggd$type)
pl1 = ggplot(ggd, aes(x = split, y = mmce, col = type))
pl1 = pl1 + geom_boxplot()
pl1 = pl1 + geom_hline(yintercept = realperf)
#pl1 = pl1 + theme(axis.text.x = element_text(angle = 45))

gmse$split = as.numeric(as.character(gmse$split))
gmse$type = as.factor(gmse$type)

pl2 = ggplot(gmse, aes(x = split, y = mse, col = type))
pl2 = pl2 + geom_line()
pl2 = pl2 + scale_y_log10()
pl2 = pl2 + scale_x_continuous(breaks = gmse$split)

grid.arrange(pl1 + theme_minimal(), pl2 + theme_minimal(), layout_matrix = rbind(1,1,2))
@


% \framebreak
%
%   \begin{itemize}
%     \item The training error decreases with smaller training set size as it is easier for the model to learn the underlying structure in smaller training sets perfectly.
%     \item The test error (its bias) decreases with increasing training set size as the model generalizes better with more data, however, the variance increases as the test set size decreases at the same time.
%     \item The variance of the test error should decrease if we repeat the hold-out more often. %(here 10 vs. 100 repetitions):
%
% <<echo = FALSE, cache = TRUE, eval = FALSE, out.width="0.85\\textwidth", fig.height=3>>=
% res = rbind(cbind(res.rpart, repetitions = 100), cbind(res.rpart.small, repetitions = 20))
% res$repetitions = as.factor(res$repetitions)
%
% p1 = ggplot(data = subset(res, measure == "1"), aes(x = percentage, y = mmce)) +
%   geom_errorbar(aes(ymin = mmce - sd, ymax = mmce + sd, colour = repetitions), width = 0.025, position = position_dodge(width = 0.01)) +
%   geom_line(aes(colour = repetitions), position = position_dodge(width = 0.01)) +
%   geom_point(aes(colour = repetitions), position = position_dodge(width = 0.01)) +
%   ylab("Test error") +
%   xlab("Training set percentage") +
%   theme_minimal()
% p1
% @
%
% \end{itemize}
%\framebreak


%\end{vbframe}
%
% \begin{vbframe}{Bias vs. Variance}
% Both, training error and test error are estimators and suffer, as all statistical estimators, from the bias-variance issue:
%
% % \begin{figure}
% %     \centering
% %     \includegraphics[width=6cm]{bias_variance_target}
% % \end{figure}
%
% <<fig.height=5, fig.width=5, out.width="0.5\\textwidth">>=
% n = 10
% set.seed(14)
% na_np = data.frame(x1 = rnorm(10, mean = -2, sd = 1),
%   x2 = rnorm(10, mean = 2, sd = 1),
%   Acc = "Not Accurate",
%   Pre = "Not Precise")
% set.seed(2)
% a_np = data.frame(x1 = rnorm(10, mean = 0, sd = 1),
%   x2 = rnorm(10, mean = 0, sd = 1),
%   Acc = "Accurate",
%   Pre = "Not Precise")
% set.seed(3)
% a_p = data.frame(x1 = rnorm(10, mean = 0, sd = .35),
%   x2 = rnorm(10, mean = 0, sd = .35),
%   Acc = "Accurate",
%   Pre = "Precise")
% set.seed(12)
% na_p = data.frame(x1 = rnorm(10, mean = 2, sd = .35),
%   x2 = rnorm(10, mean = 2, sd = .35),
%   Acc = "Not Accurate",
%   Pre = "Precise")
%
% plot_dat = rbind(na_np, a_p, a_np, na_p)
%
% ggplot(plot_dat, aes(x = x1, y = x2)) +
%   facet_grid(Acc ~ Pre) +
%   xlim(c(-5, 5)) + ylim(c(-5, 5)) +
%   xlab("") + ylab("") +
%   theme_bw() +
%   theme(axis.ticks = element_blank(),
%     axis.text.y = element_blank(),
%     axis.text.x = element_blank(),
%     panel.grid.major = element_blank(),
%     panel.grid.minor = element_blank(),
%     #panel.border = theme_blank(),
%     panel.background = element_blank())  +
%   annotate("path",
%     x = .25*cos(seq(0,2*pi,length.out = 100)),
%     y = .25*sin(seq(0,2*pi,length.out = 100)),
%     col = rgb(.2, .2, .2, .5)) +
%   annotate("path",
%     x = 1*cos(seq(0,2*pi,length.out = 100)),
%     y = 1*sin(seq(0,2*pi,length.out = 100)),
%     col = rgb(.2, .2, .2, .5)) +
%   annotate("path",
%     x = 2*cos(seq(0,2*pi,length.out = 100)),
%     y = 2*sin(seq(0,2*pi,length.out = 100)),
%     col = rgb(.2, .2, .2, .5)) +
%   annotate("path",
%     x = 3*cos(seq(0,2*pi,length.out = 100)),
%     y = 3*sin(seq(0,2*pi,length.out = 100)),
%     col = rgb(.2, .2, .2, .5)) +
%   annotate("path",
%     x = 4*cos(seq(0,2*pi,length.out = 100)),
%     y = 4*sin(seq(0,2*pi,length.out = 100)),
%     col = rgb(.2, .2, .2, .5)) +
%   geom_point()
% @
%
% \end{vbframe}



% \framebreak


% \begin{itemize}
% \item A learner that can learn more complex concepts (higher order polynomial) has, in general, a lower bias (but higher variance).
% \item The training error consistently decreases with model complexity.%, typically dropping to zero if we increase the model complexity.
% \item A model with zero training error is \emph{overfitting} to the training data and will typically generalize poorly.
% \end{itemize}

% \end{vbframe}

\endlecture

