<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@
<<setup, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{Evaluation: Train and Test Error}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{Training Error}

The \emph{training error} (also called apparent error or resubstitution error)
is estimated by the averaging error over the same data set we fitted on:

\lz

% FIGURE SOURCE: https://drive.google.com/open?id=1Fvan5-o14-b3aK0jfo4maWvXLD1g04nk5_yLaEyEPqA
\includegraphics[width=\textwidth, height=6cm,page=4]{figure_man/train_error.png}

\vspace{-0.8cm}

\end{vbframe}

\begin{vbframe}{Example: Polynomial Regression}

Assume an (unknown) sinusoidal function that $0.5 + 0.4 \cdot \sin (2 \pi x) + \epsilon$ that we sample from with some measurement error $\epsilon$.

<<echo=FALSE, out.width="0.75\\textwidth", fig.width = 8, fig.height = 4.5, fig.align="center">>=
library(ggplot2)
source("rsrc/plot_train_test.R")

.h = function(x) 0.5 + 0.4 * sin(2 * pi * x)
h = function(x) .h(x) + rnorm(length(x), mean = 0, sd = 0.05)

x.all = seq(0, 1, length = 26L)
ind = seq(1, length(x.all), by = 2)
mydf = data.frame(x = x.all, y = h(x.all))


ggTrainTestPlot(data = mydf, truth.fun = .h, truth.min = 0, truth.max = 1, test.plot = FALSE,
  test.ind = ind)[["plot"]] + ylim(0, 1)
@

We try to approximate it with a $d$th-degree polynomial
\[ \fxt = \theta_0 + \theta_1 x + \cdots + \theta_d x^d = \sum_{j = 0}^{d} \theta_j x^j\text{.} \]

\framebreak

Models of different \textit{complexity}, i.e., of different orders of the polynomial
are fitted. How should we choose $d$?

<<echo=FALSE, fig.width = 9, fig.height = 4, fig.align="center">>=
out = ggTrainTestPlot(data = mydf, truth.fun = .h, truth.min = 0, truth.max = 1, test.plot = FALSE,
  test.ind = ind, degree = c(1, 3, 9))
out[["plot"]] + ylim(0, 1)
@

\begin{itemize}
\item d=1: \Sexpr{sprintf("%.03f", out$train.test$degree1[1])}: Clear underfitting
\item d=3: \Sexpr{sprintf("%.03f", out$train.test$degree3[1])}: Pretty OK?
\item d=9: \Sexpr{sprintf("%.03f", out$train.test$degree9[1])}: Clear overfitting
\end{itemize}

Simply using the training error seems to be a bad idea.

\end{vbframe}

\begin{vbframe}{Training Error Problems}
\begin{itemize}
  \item The training error is usually a very unreliable and overly optimistic estimator of future performance.
    Modelling the training data is not of interest, but modelling the general structure in it. We do not want to overfit to noise
    or peculiarities.
  \item The training error of 1-NN is always zero, as each observation is its own NN during test time
    (assuming we do not have repeated measurements with conflicting labels).
  \item Extend any ML training in the following way:
    After normal fitting, we also store the training data.
    During prediction, we first check whether $x$ is already stored in this set. If so, we replicate its label.
    The train error of such an (unreasonable) procedure will be 0.
  \item There are so called interpolators - interpolating splines, interpolating Gaussian processes - whose predictions can always perfectly match the
    regression targets, they are not necessarily good as they will interpolate the noise, too
\end{itemize}
\end{vbframe}

\begin{vbframe}{Training Error Problems}
\begin{itemize}
  \item Goodness-of-fit measures like (classical) $R^2$, likelihood, AIC, BIC, deviance are all based on the training error.
  \item For models of severely restricted capacity, and given enough data, the training error might provide reliable information. E.g. consider a linear model with $p = 5$ features, with $10^6$ training points. But: What happens if we have less data or as $p$ increases? Not possible to determine when training error becomes unreliable.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Test Error and Hold-Out Splitting}
\begin{itemize}
\item The fundamental idea behind test error estimation (and everything that will follow) is quite simple
\item To measure performance, letâ€™s simulate how our model will be applied on new, unseen data
\item So, to evaluate a given model do exactly that, predict only on data not used during training and measure performance there
\item That implies that for a given set D , we have to preserve some data for testing that we cannot use for training
\end{itemize}
\end{vbframe}

\begin{vbframe}{Test Error and Hold-Out Splitting}
\begin{itemize}
  \item Split data into 2 parts, e.g. a common setup is 2/3 for training, 1/3 for testing
  \item Evaluate on data not used for model building, no way to \enquote{cheat}
\end{itemize}

% FIGURE SOURCE: https://drive.google.com/open?id=1NI798MXdADWpUuHo9Mzqz-7nJmKh6o4yoj3edalcSEo
\includegraphics[height=5.5cm]{figure_man/test_error.png}

\framebreak

Let's consider some clean test data for our sinusoidal example:
\lz
<<echo=FALSE, fig.width="0.9\\textwidth", fig.height=4>>=
ggTrainTestPlot(data = mydf, truth.fun = .h, truth.min = 0, truth.max = 1, test.plot = TRUE,
  test.ind = ind)[["plot"]] + ylim(0, 1)
@

\framebreak

\lz
<<echo=FALSE, fig.width="0.9*\\textwidth", fig.height = 4, fig.align="center">>=
ggTrainTestPlot(data = mydf, truth.fun = .h, truth.min = 0, truth.max = 1, test.plot = TRUE,
  test.ind = ind, degree = c(1, 3, 9))[["plot"]] + ylim(0, 1)
@

\begin{itemize}
\item d=1: \Sexpr{sprintf("%.03f", out$train.test$degree1[2])}: Clear underfitting
\item d=3: \Sexpr{sprintf("%.03f", out$train.test$degree3[2])}: Pretty OK?
\item d=9: \Sexpr{sprintf("%.03f", out$train.test$degree9[2])}: Clear overfitting
\end{itemize}

\framebreak


<<echo=FALSE, out.width="0.9\\textwidth", fig.height=4>>=
degrees = 1:9

errors = ggTrainTestPlot(data = mydf, truth.fun = .h, truth.min = 0, truth.max = 1, test.plot = TRUE,
  test.ind = ind, degree = degrees)[["train.test"]]

par(mar = c(4, 4, 1, 1))
#par(mar = c(4, 4, 0, 0) + 0.1)
plot(1, type = "n", xlim = c(1, 10), ylim = c(0, 0.1),
  ylab = "MSE", xlab = "degree of polynomial")
lines(degrees, sapply(errors, function(x) x["train"]), type = "b")
lines(degrees, sapply(errors, function(x) x["test"]), type = "b", col = "gray")

legend("topright", c("training error", "test error"), lty = 1L, col = c("black", "gray"))
text(3.75, 0.05, "Underfitting,\n\nHigh Bias,\nLow Variance", bg = "white")
arrows(4.75, 0.05, 2.75, 0.05, code = 2L, lty = 2L, length = 0.1)

text(6.5, 0.05, "Overfitting,\n\nLow Bias,\nHigh Variance", bg = "white")
arrows(7.5, 0.05, 5.5, 0.05, code = 1, lty = 2, length = 0.1)
@

We can also plot error measures for all polynomial degrees. We see the common monotonous decrease in training error,
if we increase model complexity (we can adapt better to data with more flexibility) and we see the common U-shape of the test error.
First we underfit, then we over-fit, sweet-spot is in the middle. Numerically best for $d=3$.

\end{vbframe}

\begin{vbframe}{Problems of test error}
\begin{itemize}
  \item In general, the test is a good way to estimate future performance, \textbf{given} that the test data is
    i.i.d. compared to the data we will see when we apply the model.
  \item The estimator on the will suffer from high variance and be less reliable if the test set is too small.
    If the test set contains less than a few hundred observations, you should worry.
  \item Small sample size problems can come in different shapes and forms in ML. Maybe your test set (for binary classification) is large,
    but one of the two classes is small.
  \item Try out different train-test splits in your own experiments and study error measurement fluctuation.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Problems of test error}
Let's produce repeated 2/3 training, 1/3 testing splits on two ML tasks: a) iris (n = 150) b) sonar (n = 208).
So we have about 50 (iris) and ca 70 (sonar) observations in our respective test sets.
The plots below show the strong stochastic fluctuation of test errors (50 repeats).

<<echo=FALSE, fig.width = "\\textwidth", fig.height = 3, fig.align="center">>=
my_ss = function(task) {
  lrn = makeLearner("classif.naiveBayes")
  r1 = subsample(lrn, task, iters = 50, show.info = FALSE)
  return(r1$measures.test[,2])
}
err1 = my_ss(iris.task)
err2 = my_ss(sonar.task)
d = rbind(
  data.frame(task = "iris", mce = err1),
  data.frame(task = "sonar", mce = err2)
)
pl = ggplot(d, aes(x = mce)) + geom_histogram()
pl = pl + facet_wrap(vars(task), scales = "free")
print(pl)
@
\end{vbframe}


\begin{vbframe}{Problems of test error}
A major point of confusion:
\begin{itemize}
\item In ML we are in a weird situation. We are usually given one data set. At the end of our model selection and evaluation process
we will likely fit one model on exactly that complete data set. As training error evaluation does not work,
we have now nothing left to evaluate exactly that model.
\item Holdout splitting (and the soon following resampling) are tools just to estimate that future performance, to put that
next to our final model. All of the models produced during that phase of evaluation are basically intermediate results.
\item Keep that already in mind now, it will help to avoid confusion when we move on to cross-validation and nested cross-validation.
\end{itemize}
\end{vbframe}


\begin{vbframe}{Training vs. test error}
  \vspace{-0.25cm}
  \begin{blocki}{The training error}
  \vspace{-0.25cm}
    \item is an over-optimistic (biased) estimator as the performance is measured on the same data the learned model was trained for
    \item decreases with smaller training set size as it is easier for the model to learn the underlying structure in the training set perfectly
    \item decreases with increasing model complexity as the model is able to learn more complex structures
  \end{blocki}
  \vspace{-0.25cm}
  \begin{blocki}{The test error}
  \vspace{-0.25cm}
  \item will typically decrease when the training set increases as the model generalizes better with more data (more data to learn)
  \item will have higher variance with decreasing test set size
  \item will have higher variance with increasing model complexity
  \end{blocki}
\end{vbframe}


% \framebreak
%
% Visualize the perfomance estimator - and the MSE of the estimator - in relation to the true error rate.

\begin{vbframe}{Bias-Variance of Hold-Out}
\begin{itemize}
\item If the size of our initial, complete data set $\D$ is limited,
  single train-test splits can be problematic.
\item The smaller our single test set is, the higher the variance
  of our estimated performance error (e.g., if we test on one observation, in the extreme case).
  But note that by just making the test set smaller, we do not introduce any bias,
  as we simply average losses on i.i.d. observations from $\Pxy$.
\item The smaller our training set becomes, the more pessimistic bias we introduce into the model.
  Note that if $|D| = n$, our aim is to estimate the performance of a model fitted
  on $n$ observations (as this is what we will do in the end). If we fit on less data during
  evaluation, our model will learn less, and perform worse. Very small training sets will also
  increase variance a bit.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Bias-Variance of Hold-Out - Experiment}
\begin{itemize}
\item Data: simulate spiral data (sd = 0.1) from \texttt{mlbench}
\item Learner: CART (\texttt{classif.rpart} from \texttt{mlr})
\item Goal: estimate real performance of a model with $|\Dtrain| = 500$
\item Get the "true" estimator by repeatedly sampling 500 observations from the simulator, fit the learner, then evaluate on $10^5$ observations - obviously cannot be done in practice
\item Sample $\D$ with $|\D| = 500$ and analyze different split-rate $s \in \{0.05, 0.1, ..., 0.95\}$ for training with $|\Dtrain| = s \cdot 500$
\item Estimate performance on $\Dtest$ with $|\Dtest| = 500 \cdot (1 - s)$
\item Repeat the experiment for each split rate 50 times
\end{itemize}

\framebreak

<<out.width="0.8\\textwidth", out.height="5cm">>=
load("rsrc/holdout-biasvar.RData")
ggd1 = reshape2::melt(res)
colnames(ggd1) = c("split", "rep", "ssiter", "mmce")
ggd1$split = as.factor(ggd1$split)
ggd1$mse = (ggd1$mmce -  realperf)^2
ggd1$type = "holdout"
ggd1$ssiter = NULL
mse1 = plyr::ddply(ggd1, "split", plyr::summarize, mse = mean(mse))
mse1$type = "holdout"

ggd2 = plyr::ddply(ggd1, c("split", "rep"), plyr::summarize, mmce = mean(mmce))
ggd2$mse = (ggd2$mmce -  realperf)^2
ggd2$type = "subsampling"
mse2 = plyr::ddply(ggd2, "split", plyr::summarize, mse = mean(mse))
mse2$type = "subsampling"

ggd = rbind(ggd1, ggd2)
gmse = rbind(mse1, mse2)
ggd$type = as.factor(ggd$type)
gmse$split = as.numeric(as.character(gmse$split))
gmse$type = as.factor(gmse$type)

pl1 = ggplot(ggd[ggd$type == "holdout", ], aes(x = split, y = mmce, col = type))
pl1 = pl1 + geom_boxplot()
pl1 = pl1 + geom_hline(yintercept = realperf)
pl1 = pl1 + theme(axis.text.x = element_text(angle = 45))
print(pl1)
@

\begin{itemize}
\item We clearly see the pessimistic bias for small training sets -- we cannot learn well here with much less data compared to n=500
\item We see the increased variance, when test sets become smaller
\end{itemize}

\framebreak

\begin{itemize}
  \item We now plot the mean quadratic error between the true performance (line in 1st plot) and the hold-out values in each boxplot
  \item The split rate with the lowest MSE value produces the best estimator, which is pretty much 2/3 data for training
  \item NB: This is a single experiment and not a scientific study, but this rule-of-thump is also validated in larger studies
\end{itemize}

<<out.width="0.7\\textwidth", out.height="4cm">>=
pl2 = ggplot(gmse[gmse$type == "holdout", ], aes(x = split, y = mse, col = type))
pl2 = pl2 + geom_line()
pl2 = pl2 + scale_y_log10()
pl2 = pl2 + scale_x_continuous(breaks = gmse$split)
pl2 = pl2 + theme(axis.text.x = element_text(angle = 45))
print(pl2)
@

\end{vbframe}
\endlecture
