% Introduction to Machine Learning
% Day 3

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@

% Load all R packages and set up knitr
<<setup, child="../style/setup.Rnw", include = FALSE>>=
@


\lecturechapter{11}{Resampling}
\lecture{Fortgeschrittene Computerintensive Methoden}

\begin{vbframe}{Resampling}

\begin{itemize}
  \item We therefore need to construct a better performance estimator through \emph{resampling},
    that uses the data more efficiently.
  \item All resampling variants operate similar: The data set is split repeatedly into
    training and tests sets, and we later aggregate (e.g. average) the results.
  \item The usual trick is to make training sets quite larger (to keep the pessimistic bias small),
    and to handle the variance introduced by smaller test sets through many repetitions and averaging
    of results.
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=5cm]{figure_man/ml_abstraction-crop}
\end{figure}
\end{vbframe}

\begin{vbframe}{Expected Test Error}

Let $a$ be a learning algorithm that is applied to training data $\D$ of size $n$ which produces a prediction model $\fh_{\D}$ and suppose that the training data consists of observations which were randomly drawn from $\Pxy$.

\lz

The \emph{expected generalization error} $EGE_n$ of the learning algorithm $a$ is the expectation of the (conditional) generalization error with respect to all possible training sets from $\Pxy$, of size $n$:

\begin{eqnarray*}
\EGEn(a) = \EDn(\GED) =  \EDn( L(y, \fh_{\D}(x)) | \D)
%&=& \frac{1}{B} \sum_{b = 1}^{B} \widehat{GE}(\fh_{\Dtrain^b}, \Dtest^b)\\
%&=& \frac{1}{B} \sum_{b = 1}^{B} \frac{1}{|\Dtest^b|} \sum_{\xyi \in \Dtest^b} L(\yi, \fh_{\Dtrain^b}(\xi))
\end{eqnarray*}

Note that the expectation $\EDn$ averages over the randomness in the data $\D$ of size $n$
that is used to produce $\fh_{\D}$.
%averages over everything that is random, including the randomness in the data $\D$ that produced $\fh_{\D}$.

\lz

$\Rightarrow$ Estimating the \emph{expected generalization error} is our goal!

\framebreak

Resampling methods are based on repeatedly splitting the observed data set into training and test sets and fitting a learning algorithm on training sets of equal size $n$.

\lz

Consider $B$ sets of training and test data generated by a resampling method (denoted by $\Dtrain^b$ and $\Dtest^b$, $b=1,\dots,B$, respectively), the \emph{expected generalization error} can then be estimated by

\begin{eqnarray*}
\widehat{EGE_n}(a) &=&
\frac{1}{B} \sum_{b = 1}^{B} \GEh{\Dtest^b} (\fh_{\Dtrain^b})\\
&=& \frac{1}{B} \sum_{b = 1}^{B} \frac{1}{|\Dtest^b|} \sum_{\xy \in \Dtest^b} L(y, \fh_{\Dtrain^b}(x))
\end{eqnarray*}

Note that the expected generalization error will depend on the cardinality of the generated training sets.

% \framebreak
%
% \begin{algorithm}[H]
%   \begin{algorithmic}[1]\setstretch{1.2}
%   \State {\bf Input: }A data set $\D$ with $n$ observations, the number
%     of subsets $B$ to compute and an outer loss $L$ to measure the performance.
%   \State{\bf Output: }Summary of the validation statistics.
%   \State Generate $B$ subsets of $\D$ named $\Dtrain^{(1)}$ to $\Dtrain^{(B)}$
%   \State Set $S \leftarrow \emptyset$
%   \For {$b = 1 \to B$}
%     \State $\Dtest^{(b)} \leftarrow \D \setminus \Dtrain^{(b)}$
%     \State $\hat{f} \leftarrow $ Fitmodel($\Dtrain^{(b)}$)
%     \State $s_i \leftarrow \sum\limits_{(x, y) \in \Dtest^{(b)}}
%       L(y, \fh_{\Dtrain}(x))$
%     \State $S \leftarrow S \cup \{s_i\}$
%   \EndFor
%   \State Summarize $S$, e.g. $\mathrm{mean}(S)$
%   \caption{\footnotesize Generic resampling}
%   \label{alg:genresamp}
%   \end{algorithmic}
% \end{algorithm}
\end{vbframe}
%
% \begin{vbframe}{How to split up your data?}
% \begin{block}{Data-rich situations:}
%
% <<data-rich,fig.width=5.5,fig.height=1, echo = FALSE>>=
% par(mar = c(0, 0, 0, 0))
% plot(1, type = "n", xlim = c(1, 10), ylim = c(0, 1),
%   axes = FALSE, xlab = "", ylab = "")
% rect(0, 0, 5, 0.5, col = 2L, border = 2L)
% text(2.5, 0.5, "Train", pos = 3L)
% rect(5, 0, 7.5, 0.5, col = 3L, border = 3L)
% text(6.25, 0.5, "Validation", pos = 3L)
% rect(7.5, 0, 10, 0.5, col = 4L, border = 4L)
% text(8.75, 0.5, "Test", pos = 3L)
% @
%
% \lz
% The best approach is to randomly divide the data set into three parts:
% \begin{itemize}
%   \item A training set, that is used to fit the model.
%   \item A validation set, to estimate prediction error for model selection.
%   \item And a test set, to assess the generalization error
%   of the final chosen model.
% \end{itemize}
%
% \end{block}
%
% \framebreak
%
% \begin{block}{But what do we do when the available data set is small?}
%   In those cases, which we will find us in most of the time, it is
%   usually inefficient to split the data into three parts.
%   We can now follow two approaches:
%
%   \lz
%
%   \begin{itemize}
%     \item \textbf{In-sample-estimates:} As the name suggests, these methods
%     use the training set, to approximate the validation step
%     analytically. Some well known methods amongst others are AIC, BIC
%     or VC-dimension.
%     \item \textbf{Resampling:} By effectively re-using samples performance can
%     still be estimated on unseen data without sacrifizing
%     cardinality of training an test sets.
% \end{itemize}
% \end{block}
%
% \end{vbframe}

\begin{vbframe}{$K$-fold cross-validation}

\begin{itemize}
  \item Split the data into $k$ roughly equally-sized partitions.
  \item Use each of the $k$ partitions once as a test set and the remaining $k - 1$ training sets to fit the model.
  \item Estimate the generalization error of all $k$ models using the respective test sets.
  \item Combine (average) the $k$ estimates of the generalization error.
\end{itemize}

Example of 3-fold Cross-Validation:

<<fig.height=3, echo = FALSE>>=
# par(mar = c(0, 0, 0, 0))
# plot(1, type = "n", xlim = c(0, 10), ylim = c(0, 1), axes = FALSE)
# rect(seq(0, 8, by = 2), 0, seq(2, 10, by = 2), 1)
# text(seq(1, 9, by = 2), 0.5, col = c(rep("red", 2), "green", rep("red", 2)),
#      c(rep("Train", 2), "Test", rep("Train", 2)))
par(mar = c(0, 0, 0, 0))
plot(1, type = "n", xlim = c(-2, 10), ylim = c(0, 3), axes = FALSE) #, main = "Example of 3-fold Cross Validation")
rect(seq(0, 4, by = 2), 0.1, seq(2, 6, by = 2), 1, col = c("#56B4E944","#56B4E944","#E69F0044"))
rect(seq(0, 4, by = 2), 1.1, seq(2, 6, by = 2), 2, col = c("#56B4E944","#E69F0044","#56B4E944"))
rect(seq(0, 4, by = 2), 2.1, seq(2, 6, by = 2), 3, col = c("#E69F0044", "#56B4E944","#56B4E944"))

text(seq(1, 5, by = 2), 2.55, col = c("#E69F00", "#56B4E9","#56B4E9"),
  rev(c("Train", "Train", "Test")))
text(seq(1, 5, by = 2), 1.55, col = c("#56B4E9","#E69F00","#56B4E9"),
  c("Train", "Test", "Train"))
text(seq(1, 5, by = 2), 0.55, col = c("#56B4E9","#56B4E9","#E69F00"),
  c("Train", "Train", "Test"))
text(rep(-1, 3), c(0,1,2) + 0.55, paste("Iteration", 3:1))
for (i in 1:3) #text(8, 2 - (i - 1) + 0.55, bquote(paste("=> ",widehat(Err)(widehat(f)[D[train]^.(i)], D[test]^.(i)))), cex = 1.3)
  text(8, 2 - (i - 1) + 0.55, bquote(paste("=> ",widehat(GE)[D[test]^.(i)])), cex = 1.3)
@

\framebreak

Stratification tries to keep the distribution of the target class (or any specific categorical feature of interest) in each fold.

\lz

Example of stratified 3-fold Cross-Validation:

<<fig.height=4, echo = FALSE>>=
layout(cbind(rep(1, 3), 2:4, matrix(5:16, ncol = 4, byrow = TRUE)))
par(mar = c(2,2,4,2))

plot(as.factor(c(rep(1, 3), 2)), axes = FALSE, ylim = c(0, 5), col = c("#E69F00","#56B4E9"), main = "Class Distribution")
box()

par(mar = c(1,1,1,1))
red.ind = c(1, 6, 11)
for (i in 1:3) {
    plot.new()
    text(0.5, 0.5, cex = 1.2, paste("Iteration", i))
}
for (i in 1:12) {
  if (i %% 4 == 0) {
    plot.new()
    text(0.5, 0.5, cex = 2,
      bquote(paste("=> ",widehat(GE)[D[test]^.(i/4)])))
  } else {
    plot(as.factor(c(rep(1, 3), 2)),
      axes = FALSE, ylim = c(0, 5), col = c("#E69F00","#56B4E9"))
    if (i %in% red.ind) {
      rect(par("usr")[1], par("usr")[3], par("usr")[2], par("usr")[4], col = "#E69F0055")
      box(col = "#E69F00")
      plot(as.factor(c(rep(1, 3), 2)),
        axes = FALSE, ylim = c(0, 5), col = c("#E69F00","#56B4E9"), add = TRUE)
    } else {
      rect(par("usr")[1], par("usr")[3], par("usr")[2], par("usr")[4], col = "#56B4E955")
      box()
      plot(as.factor(c(rep(1, 3), 2)),
        axes = FALSE, ylim = c(0, 5), col = c("#E69F00","#56B4E9"), add = TRUE)
    }
  }
}
@

\framebreak

\begin{blocki}{Some comments on cross-validation:}
  \item Common choices for $k$ are 5 or 10 ($k = n$ is known as leave-one-out (LOO)
cross-validation or jackknife).
  \item Estimates of the generalization error tend to somewhat pessimistically biased
    (because the size of the training sets is $ n- (n/k) < n$), bias increases as $k$ gets smaller.
  \item The performance estimates for each fold are NOT independent, because
    of the structured overlap of the training sets.
  \item Hence, variance of the estimator of increases again for very large $k$ (close to LOO),
    when training sets nearly completely overlap.
  \item LOO is nearly unbiased, but has high variance.
  \item Repeated $k$-fold CV (multiple random partitions)
    can improve the performance of error estimation for small sample size.
\end{blocki}
\end{vbframe}

\begin{vbframe}{The failure of leave-one-out}
\begin{itemize}
\item The \code{iris} data set has 50 instances of each class (\code{setosa}, \code{versicolor}, \code{virginica}).
\item A majority classifier should have 33.3\% accuracy.
\item But majority classification with LOO is unstable and will lead to a generalization error of 1 (0\% accuracy) in each fold.
\item For demonstration let's assume \code{iris} only consists of 3 instances:
\end{itemize}
<<fig.height=3, echo = FALSE>>=
par(mar = c(0, 0, 0, 0))
plot(1, type = "n", xlim = c(-2, 10), ylim = c(0, 4), axes = FALSE)
rect(seq(0, 4, by = 2), 0.1, seq(2, 6, by = 2), 1, col = c("#56B4E944","#56B4E944","#E69F0044"))
rect(seq(0, 4, by = 2), 1.1, seq(2, 6, by = 2), 2, col = c("#56B4E944","#E69F0044","#56B4E944"))
rect(seq(0, 4, by = 2), 2.1, seq(2, 6, by = 2), 3, col = c("#E69F0044", "#56B4E944","#56B4E944"))

text(c(3, 7.3, 9.5), 3.55,
  c("Training set (blue), test set (pink)", "Predicted label", "GE"),
  cex = 1.3)

text(seq(1, 5, by = 2), 2.55,
  c("Setosa", "Versicolor", "Virginica"), cex = 1.3)
text(seq(1, 5, by = 2), 1.55,
  c("Setosa", "Versicolor", "Virginica"), cex = 1.3)
text(seq(1, 5, by = 2), 0.55,
  c("Setosa", "Versicolor", "Virginica"), cex = 1.3)
text(7.3, 2.75, "Versicolor/", cex = 1.3)
text(7.3, 2.45, "Virginica", cex = 1.3)
text(7.3, 1.75, "Setosa/", cex = 1.3)
text(7.3, 1.45, "Virginica", cex = 1.3)
text(7.3, 0.75, "Setosa/", cex = 1.3)
text(7.3, 0.45, "Versicolor", cex = 1.3)

text(rep(-1, 3), c(0,1,2) + 0.55, paste("Iteration", 3:1),
  cex = 1.3)

for (i in 1:3) {
  text(9.5, 2 - (i - 1) + 0.55,
    bquote(widehat(GE)[D[test]^.(i)]), cex = 1.3)
  text(10.2, 2 - (i - 1) + 0.55, c("= 1"), cex = 1.3)
}
@
\end{vbframe}

\begin{vbframe}{Bootstrap}

The basic idea is to randomly draw $B$ training sets of size $n$ with
replacement from the original training set $\Dtrain$:
% \begin{eqnarray*}
% \Dtrain^1 &=& \{z^1_1, \ldots, z^1_n\}\\
% \vdots& \\
% \Dtrain^B &=& \{z^B_1, \ldots, z^B_n\}
% \end{eqnarray*}

\begin{center}
\begin{tikzpicture}[scale=1]
% style
\tikzstyle{rboule} = [circle,scale=0.7,ball color=red]
\tikzstyle{gboule} = [circle,scale=0.7,ball color=green]
\tikzstyle{bboule} = [circle,scale=0.7,ball color=blue]
\tikzstyle{nboule} = [circle,scale=0.7,ball color=black]
\tikzstyle{sample} = [->,thin]

% title initial sample
\path (3.5,3.75) node[anchor=east] {$\Dtrain$};

% labels
\path (3.5,3)   node[anchor=east] {$\Dtrain^1$};
\path (3.5,2.5) node[anchor=east] {$\Dtrain^2$};
\path (3.5,1.5) node[anchor=east] {$\Dtrain^B$};

\path (3.5,2) node[anchor=east] {$\vdots$};
\path[draw,dashed] (3.75,2.0) -- (4.5,2.0);

% initial sample
\path ( 3.75,3.75) node[rboule] (j01) {};
\path ( 4.00,3.75) node[gboule] (j02) {};
\path ( 4.25,3.75) node[bboule] (j03) {};
\path ( 4.5,3.75) node[nboule] (j20) {};

% bootstrap 1
\path ( 3.75, 3.0) node[rboule] {};
\path ( 4.00, 3.0) node[rboule] {};
\path ( 4.25, 3.0) node[bboule] {};
\path ( 4.5, 3.0) node[nboule] (b1) {};

% bootstrap 2
\path ( 3.75, 2.5) node[gboule] {};
\path ( 4.00, 2.5) node[bboule] {};
\path ( 4.25, 2.5) node[gboule] {};
\path ( 4.5, 2.5) node[rboule] (b2) {};

% bootstrap N
\path (3.75,1.5) node[gboule] {};
\path (4,1.5) node[rboule] {};
\path (4.25,1.5) node[nboule] {};
\path (4.5,1.5) node[nboule] (bN) {};

% arrows
\path[sample] (j20.east) edge [out=0, in=60] (b1.east);
\path[sample] (j20.east) edge [out=0, in=60] (b2.east);
\path[sample] (j20.east) edge [out=0, in=60] (bN.east);
\end{tikzpicture}
\end{center}

We define the test set in terms of out-of-bootstrap observations
$\Dtest^b = \Dtrain \setminus \Dtrain^b$.

\framebreak

\begin{itemize}
  \item Typically, $B$ is between $50$ and $200$.
  \item The variance of the bootstrap estimator tends to be smaller than the
  variance of $k$-fold CV, as training sets are independently drawn, discontinuities are smoothed out.
  \item The more iterations, the smaller the variance of the estimator.
  \item Tends to be pessimistically biased
  (because training sets contain only about $63.2 \%$ unique the observations).
  \item Bootstrapping framework might allow the use of formal inference methods
  (e.g. to detect significant performance differences between methods).
  \item The OOB error of a random forest is based on the same idea.
  \item Extensions exist for very small data sets, that also use the training error for
    estimation: B632 and B632+.
\end{itemize}

\end{vbframe}

\begin{vbframe}{Subsampling}

Subsampling is basically a repeated holdout and is also is very similar to bootstrapping:

\begin{itemize}
\item Sampling is done \textit{without} replacement, i.e., in each iteration $\eta < n$ observations without replacement are drawn for the training set.
\item Typical choices for the subsampling rate $\frac{\eta}{n}$ are: $4/5$ or $9/10$.
\item The smaller the subsampling rate, the larger the bias.
\item The more sampling iterations, the smaller the variance.
\end{itemize}

\end{vbframe}

\begin{vbframe}{More comments}
\begin{itemize}
\item Do not use Hold-Out, CV with few iterations, or subsampling with a low subsampling rate for small samples, since this can cause the estimator to be extremely biased, with large variance.
\item For some models, computationally extremely fast calculations or approximations for the LOO exist.
\item Recommendation: Use the LOO-CV for efficient model selection.
  Other cross-validation methods are superior most of the time.
\item Picking the number $B$ of samples in bootstrapping and subsampling is not that simple (to be time efficient).
  A reasonable setting has to be data and task dependent.
\item 5CV or 10CV have become standard, 10-times repeated 10CV for small sample sizes, but THINK about whether
  that makes sense in your application.
\item A $\D$ with $|\D| = 100.000$ can have small sample size properties if one class has only 100 observations \ldots
\item Modern results seem to indicate that subsampling has somewhat better properties than
  bootstrapping. The repeated observations can can cause problems in training algorithms,
  especially in nested setups where the \enquote{training} set is split up again.

\end{itemize}
\end{vbframe}

%
% \begin{vbframe}{Resampling in \texttt{mlr}}
%
% Returns aggregated values, predictions and some useful extra
% information.
%
% <<echo = TRUE, cache = TRUE>>=
% rdesc = makeResampleDesc("CV", iters = 3)
% r = resample("regr.rpart", bh.task, resampling = rdesc)
% # for the lazy
% r = crossval("regr.rpart", bh.task, iters = 3)
% print(r)
% @
%
% \framebreak
%
% <<echo = TRUE, cache = TRUE>>=
% print(r$aggr)
% print(head(r$measures.test))
% print(head(as.data.frame(r$pred)))
% @
%
% \framebreak
%
% Can be instantiated to ensure a fair comparison of multiple learners.
%
% <<echo = TRUE, cache = TRUE>>=
% rin = makeResampleInstance(rdesc, bh.task)
% str(rin$train.inds)
% model1 = resample("regr.rpart", bh.task, resampling = rin)
% model2 = resample("regr.randomForest", bh.task, resampling = rin)
% @
%
% \end{vbframe}

\begin{vbframe}{Benchmarking in \texttt{mlr}}

\begin{itemize}
\item In a benchmark experiment different learning methods are applied to one or several data sets with the aim to compare and rank the algorithms with respect to one or more performance measures.
\item It is important that the train and test sets are synchronized, i.e. all learning methods see the same data splits so that they are better comparable.
\end{itemize}

We will first create a list of tasks and a list of learners:

<<echo = TRUE>>=
data("BostonHousing", "mtcars", "swiss", package = c("mlbench", "datasets"))
tasks = list(
  makeRegrTask(data = BostonHousing, target = "medv"),
  makeRegrTask(data = swiss, target = "Fertility"),
  makeRegrTask(data = mtcars, target = "mpg")
)
learners = list(
  makeLearner("regr.rpart"),
  makeLearner("regr.randomForest"),
  makeLearner("regr.lm")
)
@


\framebreak

The \texttt{benchmark} function from \texttt{mlr} allows you to compare all tasks and learners w.r.t. one or more measures based on the resampling method specified in the \texttt{resamplings} argument:

<<echo = -1, cache = TRUE>>=
set.seed(1)
(bmr = benchmark(learners, tasks, resamplings = cv10, measures = mlr::mse))
@

\end{vbframe}

\begin{vbframe}{Benchmarking in \texttt{mlr}: Access Data}

<<echo = TRUE, cache = TRUE>>=
head(getBMRAggrPerformances(bmr, as.df = TRUE), 3)
head(getBMRPerformances(bmr, as.df = TRUE), 3)
head(getBMRPredictions(bmr, as.df = TRUE), 3)
@

\end{vbframe}

\begin{vbframe}{Visualizing Performances}

Inspect the distribution of the performance measures using box plots:

<<echo = TRUE, cache = TRUE, out.width="0.8\\textwidth", fig.height=5>>=
plotBMRBoxplots(bmr, measure = mlr::mse)
@

\framebreak

The plot is a \texttt{ggplot2} object that can be changed (names, colors, etc.):

<<echo = TRUE, cache = TRUE, out.width="0.8\\textwidth", fig.height=5>>=
plotBMRBoxplots(bmr, measure = mlr::mse, style = "violin") +
  aes(color = learner.id)
@

\framebreak

Visualize the aggregated performance values as dot plot:

<<echo = TRUE, cache = TRUE, out.width="0.8\\textwidth", fig.height=5>>=
plotBMRSummary(bmr)
@

\framebreak

Plot the rankings of the aggregated performance values:

<<echo = TRUE, cache = TRUE, out.width="0.8\\textwidth", fig.height=5>>=
plotBMRSummary(bmr, trafo = "rank")
@

\end{vbframe}


\begin{vbframe}{Learning Curves}

\begin{itemize}
\item The \textit{Learning Curve} compares the performance of a model on training and test data over a varying number of training instances.
\item It allows us to see how fast an inducer can learn the given relationship in the data.
\item Learning should usually be fast in the beginning and saturate after data becomes larger an larger
% \item We should generally see performance improve as the number of training points increases When we separate training and testing sets and graph them individually
% \item We can get an idea of how well the model can generalize to new data
\item It visualizes when a model has learned as much as it can when
\begin{itemize}
\item the performance on the training and test set reach a plateau.
\item there is a consistent gap between training and test error.
\end{itemize}
\end{itemize}

An ideal learning curve looks like:

<<echo = FALSE>>=
load("rsrc/learning_curve.RData")
@

<<out.width="0.8\\textwidth", fig.height=6>>=
opt = res.rpart
opt$mmce = opt$mmce - mean(opt$mmce) + 0.08

p = ggplot(data = opt, aes(x = percentage, y = mmce))
p + geom_hline((aes(yintercept = 0.08))) +
  geom_text(aes(0.1, 0.08, label = "desired error", vjust = 1, hjust = 0)) +
  geom_errorbar(aes(ymin = mmce - sd, ymax = mmce + sd, colour = measure), width = 0.025) +
  geom_line(aes(colour = measure)) +
  geom_point(aes(colour = measure)) +
  ylim(0, 0.15) +
  ylab(expression(widehat(GE))) +
  xlab("Training set percentage") +
  scale_color_discrete(labels = c("1" = expression(D[test]), "2" = expression(D[train]))) +
  theme_minimal()
@

\framebreak

In general, there are two reasons for a bad looking learning curve:

\begin{enumerate}
\item High bias in model / underfitting
\begin{itemize}
\item training and test errors converge and are high.
\item model can't learn the underlying relationship and has high systematic errors, no matter how big the training set is.
\item poor fit, which also translates into high test error.
\end{itemize}

<<echo = FALSE, cache = TRUE, eval = TRUE, out.width="0.8\\textwidth", fig.height=4>>=
p = ggplot(data = res.rpart, aes(x = percentage, y = mmce))
p + geom_hline((aes(yintercept = 0.08))) +
  geom_text(aes(0.1, 0.08, label = "desired error", vjust = 1, hjust = 0)) +
  geom_errorbar(aes(ymin = mmce - sd, ymax = mmce + sd, colour = measure), width = 0.025) +
  geom_line(aes(colour = measure)) +
  geom_point(aes(colour = measure)) +
  ylim(0, 0.2) +
  ylab(expression(widehat(GE))) +
  xlab("Training set percentage") +
  scale_color_discrete(labels = c("1" = expression(D[test]), "2" = expression(D[train]))) +
  theme_minimal()
@

\framebreak

\item High variance in model / Overfitting
\begin{itemize}
\item large gap between training and test errors.
\item model requires more training data to improve.
\item model has a poor fit and does not generalize well.
%\item Can simplify the model with fewer or less complex features
\end{itemize}

<<echo = FALSE, cache = TRUE, eval = TRUE, out.width="0.95\\textwidth", fig.height=4>>=
p = ggplot(data = res.ranger, aes(x = percentage, y = mmce))
p + geom_hline((aes(yintercept = 0.08))) +
  geom_text(aes(0.1, 0.08, label = "desired error", vjust = 1, hjust = 0)) +
  geom_errorbar(aes(ymin = mmce - sd, ymax = mmce + sd, colour = measure), width = 0.025) +
  geom_line(aes(colour = measure)) +
  geom_point(aes(colour = measure)) +
  ylim(0, 0.2) +
  ylab(expression(widehat(GE))) +
  xlab("Training set percentage") +
  scale_color_discrete(labels = c("1" = expression(D[test]), "2" = expression(D[train]))) +
  theme_minimal()
@
\end{enumerate}
\end{vbframe}

\endlecture
