<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@
<<setup, child="../style/setup.Rnw", include = FALSE>>=
requireNamespace("ROCR")
@
\lecturechapter{Evaluation: ROC Analysis 1}
\lecture{Introduction to Machine Learning}
%<!-- (from Signal Detection Theory) -->
%<!--
%\vspace{-0.25cm}
%-   initially - for distinguishing noise from not %noise
%-   so it's a way of showing the performance of %Binary Classifiers
%    -   only two classes - noise vs not noise  -->
% The \textbf{R}eceiver \textbf{O}perating \textbf{C}haracteristic (ROC) curve is created by plotting the \textit{TPR} vs. \textit{FPR}, i.e. the

\begin{vbframe}{Imbalanced Binary Labels}
\begin{itemize}
  \item Consider a binary classifier for diagnosing a serious medical condition
  \item Here, label distribution is often \textbf{imbalanced}, i.e, not many people have the disease
  \item Evaluating with error rate for imbalanced labels is often inappropriate
  \item Assume that only 0.5\,\% of 1000 patients have the disease
  \item Always returning "no disease" has an error rate of 0.5\,\%, which sounds good
  \item However, this sends all sick patients home, which is the worst possible system, even classifying everyone as "sick" might be better, depending on what happens next
  \item This problem is sometimes known as the \textbf{accuracy paradox}
\end{itemize}
\end{vbframe}

\begin{vbframe}{Binary Classifiers and Costs}
\begin{itemize}
  \item Another point of view is imbalanced costs
  \item In our example, classifying a sick patient as healthy, should incur a much higher loss then classifying a healthy patient as sick
  \item The costs depend a lot on what happens next: We can likely assume that our system is some type of screening filter,
    often the next step after labeling someone as "sick" might be a more invasive, expensive, but more reliable test for the disease
  \item Erroneously subjecting someone to that second step is not good (psychologically, economically, or because the second test might introduce
    medical risks), but sending someone home to get worse or die seems much worse
  \item Such a situation not only arises under label imbalance, but also when labels are maybe balanced but costs differ
  \item We could see this as imbalanced costs of misclassification, rather than imbalanced labels; both situations are tightly connected
\end{itemize}
\end{vbframe}

\begin{vbframe}{Binary Classifiers and Costs}
\begin{itemize}
  \item Problem is: If we could specify costs precisely, we could evaluate against them, we might even optimize our model for them
  \item This important subfield of ML is called \textbf{cost-sensitive learning}, which we will not cover in this lecture unit
  \item Unfortunately, users often have a notoriously hard time to come up with precise cost numbers in imbalanced scenarios
  \item Evaluating "from different perspectives", with multiple metrics, often helps, especially to get a first impression
    of the quality of the system
\end{itemize}
\end{vbframe}

\begin{frame}{ROC Analysis}
ROC Analysis -- which stands for "receiver operating characteristics -- is a subfield of ML which studies the evaluation of binary prediction systems.

\lz

Quoting Wikipdia:

\lz

"The ROC curve was first developed by electrical engineers and radar engineers during World War II for detecting enemy objects in battlefields and was soon introduced to psychology to account for perceptual detection of stimuli. ROC analysis since then has been used in medicine, radiology, biometrics, forecasting of natural hazards, meteorology, model performance assessment, and other areas for many decades and is increasingly used in machine learning and data mining research"
\end{frame}


\begin{vbframe}{Confusion Matrix and ROC Metrics}
\begin{itemize}
  \item From now on, we call one class "positive", one "negative" and their respective sizes
    $n_+$ and $n_-$
  \item The positive class is the more important, often smaller one
  \item We represent all predictions in a confusion matrix and count correct and incorrect class assignments
  \item False Positive means: We assigned "positive", but were wrong
\end{itemize}
% FIGURE SOURCE: No source
\includegraphics[width=0.8\textwidth, height=4cm]{figure_man/roc-confmatrix1.png}
\end{vbframe}


\begin{vbframe}{Confusion Matrix and ROC Metrics}
We now normalize in rows and cols of the confusion matrix to derive several informative metrics in imbalanced
or cost-sensitive situations:
% FIGURE SOURCE: No source
\includegraphics[width=0.8\textwidth, height=4cm]{figure_man/roc-confmatrix2.png}
\begin{itemize}
  \item \textbf{TPR}: How many of the true 1s did we predict as 1?
  \item \textbf{TNR}: How many of the true 0s did we predict as 0?
  \item \textbf{PPV}: If we predict 1 how likely is it a true 1?
  \item \textbf{NPV}: If we predict 0 how likely is it a true 0?
\end{itemize}
\end{vbframe}


\begin{vbframe}{Example}

\begin{center}
  % FIGURE SOURCE: No source
  \includegraphics[width=\textwidth, height=7cm]{figure_man/roc-confmatrix-example.png}
\end{center}

\end{vbframe}

\begin{vbframe}{More metrics and alternative terminology}

Unfortunately, for many concepts in ROC, 2-3 different terms exist.

\begin{center}
% FIGURE SOURCE: https://en.wikipedia.org/wiki/F1_score#Diagnostic_testing
\includegraphics[width=0.95\textwidth]{figure_man/roc-confmatrix-allterms.png}
\end{center}
\href{https://en.wikipedia.org/wiki/F1_score#Diagnostic_testing}{\beamergotobutton{Clickable version/picture source}} $\phantom{blablabla}$
\href{https://upload.wikimedia.org/wikipedia/commons/0/0e/DiagnosticTesting_Diagram.svg}{\beamergotobutton{Interactive diagram}}
\end{vbframe}


\begin{vbframe}{F1-Measure}

\begin{itemize}
  \item It is difficult to achieve a high \textit{Positive Predictive Value} and high \textit{True Positive Rate} simultaneously.
  \item A classifier that predicts more "positives" will tend to be more sensitive (higher TPR), but it will also tend to give more false positives (lower TNR, lower PPV).
  \item A classifier that predicts more "negatives" will tend to be more precise (higher PPV), but it will also produce more false negatives (lower TPR).
\end{itemize}
\lz
A measure that balances the two conflicting goals is the $F_1$-measure, which is the harmonic mean of PPV and TPR:
$$F_1 = 2 \cfrac{PPV \cdot TPR}{PPV + TPR}$$

Note that this measure still doesnâ€™t account for the number of true
negatives.
\end{vbframe}

\begin{vbframe}{F1-Measure}
\begin{itemize}
  \item A model with $TPR=0$ (no one predicted as positive from the positive class) or $PPV=0$ (no real positives among the predicted) has an F1 of 0
  \item Predicting always "neg" has $F_1 = 0$
  \item Predicting always "pos" has $F_1 = 2 PPV / (PPV + 1) = 2 n_+ / (n_+ + n)$, which will be rather small, if the size of the positive class $n_+$ is small.
\end{itemize}

Tabulated F1-Score for different TPR (rows) and PPV (cols) combinations. We see that the
harmonic means tends more towards the lower of the 2 combined values.
<<echo=FALSE, fig.width=6, fig.height=3, out.width="0.8\\textwidth">>=
oo = options()
options(digits = 2)
tpr = ppv = seq(0, 1, by = 0.2)
f1 = function(tpr, ppv) 2*tpr*ppv / (tpr + ppv)
g = expand.grid(tpr, ppv)
f1_tab = outer(tpr, ppv, f1)
f1_tab[1,1] = 0
colnames(f1_tab) = sprintf("%.1f", tpr)
rownames(f1_tab) = sprintf("%.1f", ppv)
print(f1_tab)
options(oo)
@
\end{vbframe}

\endlecture

