<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@
<<setup, child="../style/setup.Rnw", include = FALSE>>=
requireNamespace("ROCR")
@
\lecturechapter{Evaluation: ROC Analysis 1}
\lecture{Introduction to Machine Learning}
%<!-- (from Signal Detection Theory) -->
%<!--
%\vspace{-0.25cm}
%-   initially - for distinguishing noise from not %noise
%-   so it's a way of showing the performance of %Binary Classifiers
%    -   only two classes - noise vs not noise  -->
% The \textbf{R}eceiver \textbf{O}perating \textbf{C}haracteristic (ROC) curve is created by plotting the \textit{TPR} vs. \textit{FPR}, i.e. the

\begin{vbframe}{Imbalanced Binary Labels}
\begin{itemize}
  \item Consider a binary classifier for diagnosing a serious medical condition
  \item Here, label distribution is often \textbf{imbalanced}, i.e, not many people have the disease
  \item Evaluating with error rate for imbalanced labels is often inappropriate
  \item Assume that only 0.5\,\% of 1000 patients have the disease
  \item Always returning "no disease" has an error rate of 0.5\,\%, which sounds good
  \item However, this sends all sick patients home, which is the worst possible system, even classifying everyone as "sick" might be better, depending on what happens next
  \item This problem is sometimes known as the \textbf{accuracy paradox}
\end{itemize}
\end{vbframe}

\begin{vbframe}{Binary Classifiers and Costs}
\begin{itemize}
  \item Another point of view is imbalanced costs
  \item In our example, classifying a sick patient as healthy, should incur a much higher loss then classifying a healthy patient as sick
  \item The costs depend a lot on what happens next: We can likely assume that our system is some type of screening filter,
    often the next step after labeling someone as "sick" might be a more invasive, expensive, but more reliable test for the disease
  \item Erroneously subjecting someone to that second step is not good (psychologically, economically, or because the second test might introduce
    medical risks), but sending someone home to get worse or die seems much worse
  \item Such a situation not only arises under label imbalance, but also when labels are maybe balanced but costs differ
  \item We could see this as imbalanced costs of misclassification, rather than imbalanced labels; both situations are tightly connected
\end{itemize}
\end{vbframe}

\begin{vbframe}{Binary Classifiers and Costs}
\begin{itemize}
  \item Problem is: If we could specify costs precisely, we could evaluate against them, we might even optimize our model for them
  \item This important subfield of ML is called \textbf{cost-sensitive learning}, which we will not cover in this lecture unit
  \item Unfortunately, users often have a notoriously hard time to come up with precise cost numbers in imbalanced scenarios
  \item Evaluating "from different perspectives", with multiple metrics, often helps, especially to get a first impression
    of the quality of the system
\end{itemize}
\end{vbframe}

\begin{frame}{ROC Analysis}
ROC Analysis -- which stands for "receiver operating characteristics -- is a subfield of ML which studies the evaluation of binary prediction systems.

\lz

Quoting Wikipdia:

\lz

"The ROC curve was first developed by electrical engineers and radar engineers during World War II for detecting enemy objects in battlefields and was soon introduced to psychology to account for perceptual detection of stimuli. ROC analysis since then has been used in medicine, radiology, biometrics, forecasting of natural hazards, meteorology, model performance assessment, and other areas for many decades and is increasingly used in machine learning and data mining research"
\end{frame}


\begin{vbframe}{Confusion Matrix and ROC Metrics}
\begin{itemize}
\item
  We call one class \enquote{positive} (\(+\)) and the other
  \enquote{negative} (\(-\)).
\item
  Their respective class sizes are denoted by \(n_+\) and \(n_-\).
\item
  The positive class is the more important, often smaller one.
\item
  We represent all predictions in a confusion matrix and count correct
  and incorrect class assignments
\item
  \textbf{False Positive}: We assigned \enquote{positive}, but were
  wrong.
\end{itemize}
\begin{center}
\small
\begin{tabular}{cc|cc}
    & & \multicolumn{2}{c}{\bfseries True Class $\mathbf{y}$} \\
    & & $+$ & $-$ \\
    \hline
    \multirow{2}{*}{\parbox{2cm}{ \centering \bfseries Pred. $\yh$}} & $+$ & \bfseries True Positive (TP) & \textbf{False Positive (FP)} \\
    & $-$ & \textbf{False Negative (FN)} & \bfseries True Negative (TN)\\
\end{tabular}
\end{center}

% FIGURE SOURCE: No source
%\includegraphics[width=0.8\textwidth, height=4cm]{figure_man/roc-confmatrix1.png}

\end{vbframe}


\begin{vbframe}{Confusion Matrix and ROC Metrics}
By normalizing the rows and columns of the confusion matrix, we can
derive several metrics that help in assessing the performance in
imbalanced or cost-sensitive settings (the best possible value for those metrics is 1):

% FIGURE SOURCE: No source
%\includegraphics[width=0.8\textwidth, height=4cm]{figure_man/roc-confmatrix2.png}

\begin{itemize}
\item
  \textbf{TPR}: How many of the true \(+\) did we predict as \(+\)?
\item
  \textbf{TNR}: How many of the true \(-\) did we predict as \(-\)?
\item
  \textbf{PPV}: If we predict \(+\) how likely is it a true \(+\)?
\item
  \textbf{NPV}: If we predict \(-\) how likely is it a true \(-\)?
\end{itemize}

\begin{center}
\scriptsize
\begin{tabular}{cc|cc|c}
    & & \multicolumn{2}{c}{\bfseries True Class $\mathbf{y}$} & \\
    & & $+$ & $-$ & \\
    \hline
    \multirow{4}{*}{\parbox{0.5cm}{ \centering \bfseries Pred. $\yh$}} & \multirow{2}{*}{$+$} & \multirow{2}{*}{\bfseries True Positive (TP)} & \multirow{2}{*}{\textbf{False Positive (FP)}} & Positive Predictive\\
    & & & & Value  (PPV) = $\frac{\text{TP}}{\text{TP} + \text{FP}}$\\
    & \multirow{2}{*}{$-$} & \multirow{2}{*}{\textbf{False Negative (FN)}} & \multirow{2}{*}{\bfseries True Negative (TN)} & Negative Predictive\\
    & &  & & Value (NPV) = $\frac{\text{TN}}{\text{FN} + \text{TN}}$\\
    \hline
    & & True Positive Rate  & True Negative Rate  & Accuracy  \\
    & & (TPR) = $\frac{\text{TP}}{\text{TP} + \text{FN}}$ & (TNR) = $\frac{\text{TN}}{\text{FP} + \text{TN}}$ & = $\frac{\text{TP}+ \text{TN}}{\text{TOTAL}}$
\end{tabular}
\end{center}
\end{vbframe}


\begin{vbframe}{Example}

Suppose 2030 observations with 30 positives, 2000 negatives, and a
confusion matrix of a classifier with 91 \% accuracy:

\begin{center}

\scriptsize
\begin{tabular}{cc|cc|c}
    & & \multicolumn{2}{c}{\bfseries True Class $\mathbf{y}$} & \\
    & & $+$ & $-$ & \\
    \hline
    \multirow{4}{*}{\parbox{0.5cm}{ \centering \bfseries Pred. $\yh$}} & \multirow{2}{*}{$+$} & \bfseries True Positive (TP) & \textbf{False Positive (FP)} & \multirow{2}{*}{PPV = $\frac{20}{200} = 0.1$}\\
    & & TP = $20$ & FP = $180$ & \\
    & \multirow{2}{*}{$-$} & \textbf{False Negative (FN)} & \bfseries True Negative (TN) & \multirow{2}{*}{NPV = $\frac{1820}{1830} = 0.995$}\\
    & & FN = $10$ & TN = $1820$ & \\
    \hline
    & & TPR = $\frac{20}{30} \approx 0.67$ & TNR = $\frac{1820}{2000} = 0.91$ & Acc.= $\frac{1840}{2030} \approx 0.91$
\end{tabular}

  % FIGURE SOURCE: No source
  %\includegraphics[width=\textwidth, height=7cm]{figure_man/roc-confmatrix-example.png}

\end{center}

\begin{itemize}
\item An accuracy of 0.91 seems fine.
\item However, a PPV of 0.1 is really bad.
\end{itemize}

\end{vbframe}

\begin{vbframe}{More metrics and alternative terminology}

Unfortunately, for many concepts in ROC, 2-3 different terms exist.

\begin{center}
% FIGURE SOURCE: https://en.wikipedia.org/wiki/F1_score#Diagnostic_testing
\includegraphics[width=0.95\textwidth]{figure_man/roc-confmatrix-allterms.png}
\end{center}
\href{https://en.wikipedia.org/wiki/F1_score#Diagnostic_testing}{\beamergotobutton{Clickable version/picture source}} $\phantom{blablabla}$
\href{https://upload.wikimedia.org/wikipedia/commons/0/0e/DiagnosticTesting_Diagram.svg}{\beamergotobutton{Interactive diagram}}
\end{vbframe}


\begin{vbframe}{F1-Measure}

\begin{itemize}
  \item It is difficult to achieve a high \textit{Positive Predictive Value} and high \textit{True Positive Rate} simultaneously.
  \item A classifier that predicts more "positives" will tend to be more sensitive (higher TPR), but it will also tend to give more false positives (lower TNR, lower PPV).
  \item A classifier that predicts more "negatives" will tend to be more precise (higher PPV), but it will also produce more false negatives (lower TPR).
\end{itemize}
\lz
A measure that balances the two conflicting goals is the $F_1$-measure, which is the harmonic mean of PPV and TPR:
$$F_1 = 2 \cfrac{PPV \cdot TPR}{PPV + TPR}$$

Note that this measure still doesnâ€™t account for the number of true
negatives.
\end{vbframe}

\begin{vbframe}{F1-Measure}
\begin{itemize}
  \item A model with $TPR=0$ (no one predicted as positive from the positive class) or $PPV=0$ (no real positives among the predicted) has an F1 of 0
  \item Predicting always "neg" has $F_1 = 0$
  \item Predicting always "pos" has $F_1 = 2 PPV / (PPV + 1) = 2 n_+ / (n_+ + n)$, which will be rather small, if the size of the positive class $n_+$ is small.
\end{itemize}

Tabulated F1-Score for different TPR (rows) and PPV (cols) combinations. We see that the
harmonic means tends more towards the lower of the 2 combined values.
<<echo=FALSE, fig.width=6, fig.height=3, out.width="0.8\\textwidth">>=
oo = options()
options(digits = 2)
tpr = ppv = seq(0, 1, by = 0.2)
f1 = function(tpr, ppv) 2*tpr*ppv / (tpr + ppv)
g = expand.grid(tpr, ppv)
f1_tab = outer(tpr, ppv, f1)
f1_tab[1,1] = 0
colnames(f1_tab) = sprintf("%.1f", tpr)
rownames(f1_tab) = sprintf("%.1f", ppv)
print(f1_tab)
options(oo)
@
\end{vbframe}

\endlecture

