<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@
<<setup, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{Evaluation: Simple Metrics for Regression and Classification}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{Regression: MSE}

The \textbf{Mean Squared Error} compares the mean of the squared distances between the target variable $y$ and the predicted target $\yh$.

\[
MSE = \frac{1}{n} \sumin (\yi - \yih)^2 \in [0;\infty]
\]

Single observations with a large prediction error heavily influence the \textbf{MSE}, as they enter quadratically.

<<echo=FALSE, out.width="0.7\\textwidth", fig.width = 7, fig.height = 2.5>>=
source("rsrc/plot_loss.R")

set.seed(31415)

x = 1:5
y = 2 + 0.5 * x + rnorm(length(x), 0, 1.5)
data = data.frame(x = x, y = y)
model = lm(y ~ x)

plotModQuadraticLoss(data = data, model = model, pt_idx = c(1,4))
@

We could also sum the errors up (SSE), or take the root (RMSE) to bring the measurement back to the original scale of the outcome. 

\end{vbframe}

\begin{vbframe}{Regression: MAE}

A more robust (but not neccessarily better) way to compute a performance measure is the \textbf{Mean Absolute Error}:

\[
MAE = \frac{1}{n} \sumin |\yi - \yih| \in [0;\infty]
\]

Less influenced by large errors and maybe more intuitive than the MSE. 

<<echo=FALSE, out.width="0.7\\textwidth", fig.width = 7, fig.height = 3>>=
plotModAbsoluteLoss(data, model = model, pt_idx = c(1,4))
@

Instead of averaging we might also consider the median for even more robustness.

\end{vbframe}

\begin{vbframe}{Regression: $R^2$}

Another well known measure from statistics is $R^2$. 

\[ 
R^2 = 1 - \frac{\sumin (\yi - \yih)^2}{\sumin (\yi - \bar{y})^2} = 1 - \frac{SSE_{LinMod}}{SSE_{Intercept}} 
\]

\begin{itemize}
\item Usually introduced as \textit{fraction of variance explained} by the model
\item Much simpler explanation: It compares the SSE of a constant model (baseline) with a more complex model (LM), on some data, usually the same as used for model fitting
\item $R^2=1$ implies: all residuals are 0, we predict perfectly, $R^2=0$ implies we predict as badly as a naked constant
\item If measured on the training data, $R^2 \in [0;1]$, as the LM must be at least as good as the constant, and both SSEs are non-negative
\item On other data it could even be negative, as there is no guarantee that the LM generalizes better than a constant (overfitting possible)
\end{itemize}
\end{vbframe}

\begin{vbframe}{Generalized $R^2$ for ML}
A simple generalization of $R^2$ for ML seems to be:

\[ 
1 - \frac{Loss_{ComplexModel}}{Loss_{SimplerModel}}
\]

\begin{itemize}
\item This introduces a general measure of comparison between a simpler baseline, and a more complex model considered as an alternative
\item This works for arbitrary measures (not only SSE), for arbitrary models, on any data set of interest 
\item E.g. model vs constant, LM vs. non-linear model, tree vs. forest, model without some features vs. model with them included
\item In ML we would rather use that metric on a holdout-test set, there is no reason not to do that
\item I do not see this being used or known very often, and my terminology (generalized $R^2$) is non-standard
\end{itemize}
\end{vbframe}

\begin{vbframe}{Labels: Accuracy / MCE}

The misclassification error rate (MCE) simply counts the number of incorrect predictions and presents them as a rate, accuracy
is defined in a similar fashion for correct classifications 

\[ 
MCE = \frac{1}{n} \sumin [\yi \neq \yih] \in [0;1]
\]
\[ 
ACC = \frac{1}{n} \sumin [\yi = \yih] \in [0;1]
\]

\begin{itemize}
\item If the data set is small this can be quite a brittle measure
\item The MCE says nothing about how good or skewed predicted probabilities are
\item Errors on all classes are weighed equally, that is often inappropriate
\end{itemize}
\end{vbframe}

\begin{vbframe}{Labels: Confusion Matrix}

Much better than simply reducing prediction errors to a simple number we can tabulate them 
in a confusion matrix, tabulating true classes in rows and predicted classes in columns.
We can nicely see class sizes (predicted and true) and where errors occur.

\lz

<<echo=FALSE>>=
set.seed(31415)
r = crossval("classif.naiveBayes", iris.task, show.info = FALSE)
m = calculateConfusionMatrix(r$pred, sums = TRUE)
print(m)
@
\end{vbframe}

\begin{vbframe}{Labels: Costs}
We can also assign different costs to different errors via a cost matrix.
\[
  Costs = \meanin C[\yi, \yih]
\]
\vspace{-0.8cm}
<<echo=FALSE>>=
set.seed(31415)
r = crossval("classif.naiveBayes", iris.task, show.info = FALSE)
m = calculateConfusionMatrix(r$pred)
catf("Confusion matrix")
cc = m$result[-4,-4]
print(cc)
cc[] = c(0,2,1,1,0,1,1,5,0)
catf("Cost matrix C")
print(cc)
@
\begin{itemize}
  \item Here, we penalize errors on class \textit{versicolor} more heavily
  \item $Costs = (3 \cdot 5 + 4 \cdot 1) / 150 $
\end{itemize}
\end{vbframe}

\begin{vbframe}{Probabilities: Brier Score}
Measures squared distances of probabilities from the true class labels:
\[
BS1 = \meanin \left( \pixih - \yi \right)^2
\]

<<echo=FALSE, fig.width="0.8\\textwidth", fig.height=2.5>>=
phat = seq(0, 1, by = 0.02)
d = rbind(
  data.frame(phat = phat, BS = (1 - phat)^2, true.label = "1"),
  data.frame(phat = phat, BS = (0 - phat)^2, true.label = "0")
)
pl = ggplot(data = d, aes(x = phat, y = BS, col = true.label)) 
pl = pl + geom_line() + xlab(expression(hat(pi)(x)))
pl = pl + geom_vline(xintercept=0.5)  
pl = pl + annotate(geom = "text", label = "right", x=0.1, y=0.1, col = pal_2[2])
pl = pl + annotate(geom = "text", label = "wrong", x=0.1, y=1.0, col = pal_2[1])
pl = pl + annotate(geom = "text", label = "wrong", x=0.9, y=1.0, col = pal_2[2])
pl = pl + annotate(geom = "text", label = "right", x=0.9, y=0.1, col = pal_2[1])
print(pl)
@
\begin{itemize}
  \item Usual definition for binary case, $\yi$ must be coded as 0 and 1.
  \item Fancy name for MSE on probabilities  
\end{itemize}

\framebreak

\[
BS2 = \meanin \sum_{k=1}^g \left( \pikxih - o_k^{(i)} \right)
\]
\begin{itemize}
  \item Original one by Brier, works also for multiple classes 
  \item $ o_k^{(i)} = [ \yih = k ] $ is a 0-1-one-hot coding for labels
  \item For the binary case, BS2 is twice as large as BS1, because in BS2 we sum the squared 
    difference for each observation regarding class 0 AND class 1, not only the true class. 
\end{itemize}


\end{vbframe}

\begin{vbframe}{Probabilities: Log-Loss}
Logistic regression loss function, a.ka. Bernoulli or binomial loss, $\yi$ coded as 0 and 1.
\[
LL = \meanin \left( - \yi \log(\pixih) - (1-\yi) \log(1-\pixih) \right)
\]
<<echo=FALSE, fig.width="0.8\\textwidth", fig.height=2.5>>=
library(grid)
phat = seq(0.01, 0.99, by = 0.02)
d = rbind(
  data.frame(phat = phat, LL = -log(phat), true.label = "1"),
  data.frame(phat = phat, LL = -log(1-phat), true.label = "0")
)
pl = ggplot(data = d, aes(x = phat, y = LL, col = true.label)) 
pl = pl + geom_line() + xlab(expression(hat(pi)(x)))
pl = pl + geom_vline(xintercept=0.5) 
pl = pl + annotate(geom = "text", label = "right", x=0.1, y=0.1, col = pal_2[2])
pl = pl + annotate(geom = "text", label = "wrong", x=0.1, y=2.0, col = pal_2[1])
pl = pl + annotate(geom = "text", label = "wrong", x=0.9, y=2.0, col = pal_2[2])
pl = pl + annotate(geom = "text", label = "right", x=0.9, y=0.1, col = pal_2[1])
print(pl)
@
\begin{itemize}
  \item Optimal value is 0, \enquote{confidently wrong} is penalized heavily
  \item Multiclass version: $ LL = \meanin \sum_{k=1}^g o_k^{(i)} \log(\pikxih) $ 
\end{itemize}
\end{vbframe}


\endlecture

