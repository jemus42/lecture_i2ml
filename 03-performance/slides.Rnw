%This file is a child of preamble.Rnw in the style folder
%if you want to add stuff to the preamble go there to make
%your changes available to all childs

<<setup-child, include = FALSE, results='hide'>>=
library(knitr)
library(mlr)
library(ggplot2)
library(data.table)
library(plyr)
library(BBmisc)
library(gridExtra)
set_parent("../style/preamble.Rnw")
@

\lecturechapter{6}{Performance Estimation and Resampling}
\lecture{Fortgeschrittene Computerintensive Methoden}

\begin{vbframe}{Introduction}
In predictive modeling, performance estimation can have different goals:

\begin{itemize}
  \item \textbf{Performance estimation of a model:}
    Estimate \emph{generalization} error of a model on new (unseen) data, drawn from the same data generating process.
  \item \textbf{Performance estimation of an algorithm:}
    Estimate \emph{generalization} error of a learning algorithm, trained on a data set
    of a certain size, on new (unseen) data, all drawn from the same data generating process.
  \item \textbf{Model selection:}
    Select the best model from a set of potential candidate models (e.g., different model classes, different
    hyperparameter settings, different feature sets)
  \item \textbf{Learning curves}
    How does the generalization error scale when an algorithm is trained on training sets of different sizes?
\end{itemize}

Obviously, all goals are quite related, i.e., reliably estimate the performance is the foundation for all of them.

\end{vbframe}

% \begin{vbframe}{Performance measures}
% There is a great variety of performance measures available for all types of learning problems.
% Some of the most important ones are:
% \begin{table}
% \footnotesize
% \begin{tabular}{p{4cm}p{7cm}}
% \hline
% Name & Defintion \\
% \hline
% \multicolumn{2}{l}{\textbf{Classifcation}}\\
% Mean classification error & $mean(response != truth)$\\
% Accuracy & $mean(response == truth)$\\
% Aread under the curve & Integral that results from compution false positive and false negative rate for many different thresholds\\
% \hline
% \multicolumn{2}{l}{\textbf{Regression}}\\
% Mean of squared errors & $mean((response - truth)^2)$\\
% Mean of absolute errors & $mean(abs(response - truth))$\\
% \hline
% \multicolumn{2}{l}{\textbf{Clustering}}\\
% Dunn index & Ratio of the smallest distance between observations not in the same cluster to the largest intra-cluster distance \\
% \hline
% \multicolumn{2}{l}{\textbf{Survival}}\\
% Concordance index & Probability of concordance between predicted and observed outcomes\\
% \hline
% \end{tabular}
% \end{table}
% \end{vbframe}

\begin{vbframe}{Generalization Error}
The \emph{generalization error} is defined as the error of fixed model $\fhD$:

$$\GED = \E( L(y, \fhD(x)) | \D),$$

where

\begin{itemize}
\item $\fhD$ is a given, fixed model, fitted on a specific data set $\D$
\item $\D$ is a data set (of size n), drawn i.i.d. from the joint $\Pxy$
\item $L$ is an \emph{outer} loss, depending on the application, that measures the performance of $\fhD$
\item $\GED$ is basically the risk $\risk(\fhD)$ of $\fhD$
\item $\GED$ is conditional on the \emph{model} $\fhD$ and hence also on $\D$
\end{itemize}


\end{vbframe}


\begin{vbframe}{Inner vs. Outer Loss}

We distinguish between

%\lz
\begin{itemize}
\item the inner loss which is optimized during model fitting to obtain $\fh$.
\item the outer loss which will be used to assess the model afterwards.
\end{itemize}

%\lz
Usually, it is desired that inner and outer loss match, however, this is not always possible,
as the outer loss is often numerically hard(er) to handle during optimization and we might
opt to approximate it.

\lz Examples:
In logistic regression we minimize the binomial loss, and in SVMs the hinge loss.
But when evaluating the models we might be more interested in classification error,  or AUC, or partial AUC as more appropriate
outer loss functions.

\lz
The outer loss is also known as \emph{performance measure}.
\end{vbframe}

\begin{vbframe}{Training Error}

The \emph{training error} (also called {\it apparent error} or {\it resubstitution error}) is estimated by the average loss over the training set $\Dtrain$:
  $$\GEh{\Dtrain}(\fhDtrain) = \frac{1}{|\Dtrain|} \sum_{\xy \in \Dtrain} L(y, \fhDtrain(x))$$


\begin{itemize}
\item The training error is usually a very unreliable and overly optimistic estimator of future performance.
\item Goodness-of-fit measures like $R^2$, likelihood, AIC, BIC, deviance are all based on the training error.


\framebreak

Example 1: Recall our polynomial regression: The training error provided no help
if figuring out the optimal model complexity $d$.

<<echo=FALSE, out.width="0.9\\textwidth", fig.height=2.7>>=
.h = function(x) 0.5 + 0.4 * sin(2 * pi * x)
h = function(x) .h(x) + rnorm(length(x), mean = 0, sd = 0.05)

set.seed(1234)
x.all = seq(0, 1, length = 21L)
ind = seq(1, length(x.all), by = 2)

x = x.all[ind]
y = h(x)
x.test = x.all[-ind]
y.test = h(x.all[-ind])

line.palette = c("#E69F00AA", "#56B4E9AA", "#CC79A7AA")
baseplot = function() {
  par(mar = c(2, 2, 1, 1))
  plot(.h, lty = 2L, xlim = c(0, 1), ylim = c(-0.1, 1), ylab = "", xlab = "")
  points(x, y, pch = 19L)
  points(x.test, y.test)
  legend(x = "bottomleft", legend = c("true relationship f(x)", "training set", "test set"),
    col = "black", lty = c(2L, NA, NA), pch = c(NA, 19L, 21L))
}

p1 = lm(y ~ poly(x, 1, raw = TRUE))
p5 = lm(y ~ poly(x, 5, raw = TRUE))
p9 = lm(y ~ poly(x, 9, raw = TRUE))
mods = list(p1, p5, p9)
x.plot = seq(0, 1, length = 500L)
baseplot()
for (i in seq_along(mods)) {
  lines(x.plot, predict(mods[[i]], newdata = data.frame(x = x.plot)),
    col = line.palette[i], lwd = 2L)
}
legend("topright", paste(sprintf("d = %s", c(1, 5, 9)), c("(underfit)", "(good)", "(overfit)")),
  col = line.palette, lwd = 2L)
@

<<echo=FALSE, out.width="0.9\\textwidth", fig.height=2.7>>=
d = lapply(1:10, function(i) {
  mod = lm(y ~ poly(x, degree = i, raw = TRUE))
  list(
    train = mean((y - predict(mod, data.frame(x = x)))^2),
    test = mean((y.test - predict(mod, data.frame(x = x.test)))^2)
  )
})
par(mar = c(4, 4, 1, 1))
#par(mar = c(4, 4, 0, 0) + 0.1)
plot(1, type = "n", xlim = c(1, 10), ylim = c(0, 0.05),
  ylab = "MSE", xlab = "degree of polynomial")
lines(1:10, sapply(d, function(x) x$train), type = "b")
lines(1:10, sapply(d, function(x) x$test), type = "b", col = "gray")

legend("topright", c("training error", "test error"), lty = 1L, col = c("black", "gray"))
text(3.75, 0.02, "High Bias,\nLow Variance", bg = "white")
arrows(4.75, 0.02, 2.75, 0.02, code = 2L, lty = 2L, length = 0.1)

text(8, 0.02, "Low Bias,\nHigh Variance", bg = "white")
arrows(9, 0.02, 7, 0.02, code = 1, lty = 2, length = 0.1)
@

\framebreak

\item Example 2: Assume any ML model, now extend the training algorithm in the following way:
  On top of normal fitting, we also store $\Dtrain$.
  During prediction, we first check whether $x$ is already stored in this set. If so, we replicate its label.
  The training error of such an (unreasonable) procedure will always be zero.
\item Example 3: The training error of 1nn is always zero.
\item Example 4: The training error of an interpolating spline in regression is always zero.

\item For models of severely restricted capacity, and given enough data, the training error might provide
  reliable information. E.g. consider a linear model in 5d, with 10.000 training points.
  But: What happens if we have less data? And $p$ becomes larger? Can you precisely define
  where the training error becomes unreliable?
\end{itemize}

\end{vbframe}

\begin{vbframe}{Test error and hold-out splitting}
To reliably assess a model, we need to define

\begin{itemize}
\item how to simulate the scenario of "new unseen data" and
\item how to estimate the generalization error.
\end{itemize}

\begin{blocki}{Hold-out splitting and evaluation}
  \item The fundamental idea behind test error estimation (and everything that will follow)
    is quite simple: To measure performance, let's simulate how our model will be applied on new, unseen data
  \item So, to evaluate a given model do exactly that, predict only on data not used during training and measure performance there.
  \item That implies that for a given set $\D$, we have to preserve
    some data for testing that we cannot use for training, hence we need to define a training set $\Dtrain$ and a
    and a test set $\Dtest$, usually by randomly partitioning the original $\D$, with a given split rate.
\end{blocki}


\framebreak

The \emph{test error} (also called \emph{generalization error}) is %the expected prediction error on unseen data for a given training set $\Dtrain$ and can be
  estimated by the average loss over the independent test set $\Dtest$:
  $$\GEh{\Dtest}(\fhDtrain) = \frac{1}{|\Dtest|} \sum_{\xy \in \Dtest} L(y, \fhDtrain(x))$$

 \lz

  If we fix a training set $\Dtrain$ and a model $\fhDtrain$, we can estimate $\GEh{\Dtest}(\fhDtrain)$ through
 test sets of various sizes. As all $(x,y)$ of the test set will be i.i.d. (from $\Pxy$),
 and as $|\Dtest| = n_{test}$ will usually not be too small, we can apply the central limit theorem to
 approximate its distribution, calculate approximate confidence intervals, and perform
 sample size considerations.


\end{vbframe}

\begin{vbframe}{Training vs. test error}
  \vspace{-0.25cm}
  \begin{blocki}{The training error}
  \vspace{-0.25cm}
    \item is an over-optimistic (biased) estimator as the performance is measured on the same data the learned prediction function $\fhDtrain(x)$ was trained for.
    \item decreases with smaller training set size as it is easier for the model to learn the underlying structure in the training set perfectly.
    \item decreases with increasing model complexity as the model is able to learn more complex structures.
  \end{blocki}
  \vspace{-0.25cm}
  \begin{blocki}{The test error}
  \vspace{-0.25cm}
  \item will typically decrease when the training set increases as the model generalizes better with more data (more data to learn).
  \item will have higher variance with decreasing test set size.
  \item will have higher variance with increasing model complexity.
  \end{blocki}
\end{vbframe}

\begin{vbframe}{Bias-Variance of holdout}

  \begin{itemize}
    \item If the size of our initial, complete data set $\D$ is limited,
      single train-test splits can be problematic.
    \item The smaller our single test set is, the higher the variance
      of our estimated performance error (e.g., if we test on one observation, in the extreme case).
      But note that by just making the test set smaller, we do not introduce any bias,
      as we simply average losses on i.i.d. observations from $\Pxy$.
    \item The smaller training set becomes, the more pessimistic bias we introduce into the model.
      Note that if $|D| = n$, our aim is to estimate the performance of a model fitted
      on n observations (as this is what we will do in the end). If we fit on less data during
      evaluation, our model will learn less, and perform worse. Very small training sets will also
      increase variance a bit.
  \end{itemize}

  \framebreak

  Experiment:
  \begin{itemize}
    \item Data: simulate spiral data (sd = 0.1) from the \texttt{mlbench} package.
    \item Learner: CART (\texttt{classif.rpart} from \texttt{mlr}).
    \item Goal: estimate real performance of a model with $|\Dtrain| = 500$.
    \item Get the "true" estimator by repeatedly sampling 500 observations from the simulator,
      fit the learner, then evaluate on a really large number of observation.
    \item Analyse different types of holdout and subsampling (= repeated holdout), with different split rates:
    \begin{itemize}
    \item Sample $\D$ with $|\D| = 500$ and use split-rate $s \in \{0.05, 0.1, ..., 0.95\}$ for training with $|\Dtrain| = s \cdot 500$.
    \item Estimate performance on $\Dtest$ with $|\Dtest| = 500 \cdot (1 - s)$.
    \item Repeat the samping of $\D$ 50 times and the splitting with $s$ 50 times ($\Rightarrow$ 2500 experiments for each split-rate).
    \end{itemize}
  \end{itemize}

\framebreak

Visualize the perfomance estimator - and the MSE of the estimator - in relation to the true error rate.

<<>>=
load("rsrc/holdout-biasvar.RData")
@

<<echo = FALSE, fig.height = 5>>=
ggd1 = melt(res)
colnames(ggd1) = c("split", "rep", "ssiter", "mmce")
ggd1$split = as.factor(ggd1$split)
ggd1$mse = (ggd1$mmce -  realperf)^2
ggd1$type = "holdout"
ggd1$ssiter = NULL
mse1 = ddply(ggd1, "split", summarize, mse = mean(mse))
mse1$type = "holdout"

ggd2 = ddply(ggd1, c("split", "rep"), summarize, mmce = mean(mmce))
ggd2$mse = (ggd2$mmce -  realperf)^2
ggd2$type = "subsampling"
mse2 = ddply(ggd2, "split", summarize, mse = mean(mse))
mse2$type = "subsampling"

ggd = rbind(ggd1, ggd2)
gmse = rbind(mse1, mse2)

ggd$type = as.factor(ggd$type)
pl1 = ggplot(ggd, aes(x = split, y = mmce, col = type))
pl1 = pl1 + geom_boxplot()
pl1 = pl1 + geom_hline(yintercept = realperf)
#pl1 = pl1 + theme(axis.text.x = element_text(angle = 45))

gmse$split = as.numeric(as.character(gmse$split))
gmse$type = as.factor(gmse$type)

pl2 = ggplot(gmse, aes(x = split, y = mse, col = type))
pl2 = pl2 + geom_line()
pl2 = pl2 + scale_y_log10()
pl2 = pl2 + scale_x_continuous(breaks = gmse$split)

grid.arrange(pl1 + theme_minimal(), pl2 + theme_minimal(), layout_matrix = rbind(1,1,2))
@


% \framebreak
%
%   \begin{itemize}
%     \item The training error decreases with smaller training set size as it is easier for the model to learn the underlying structure in smaller training sets perfectly.
%     \item The test error (its bias) decreases with increasing training set size as the model generalizes better with more data, however, the variance increases as the test set size decreases at the same time.
%     \item The variance of the test error should decrease if we repeat the hold-out more often. %(here 10 vs. 100 repetitions):
%
% <<echo = FALSE, cache = TRUE, eval = FALSE, out.width="0.85\\textwidth", fig.height=3>>=
% res = rbind(cbind(res.rpart, repetitions = 100), cbind(res.rpart.small, repetitions = 20))
% res$repetitions = as.factor(res$repetitions)
%
% p1 = ggplot(data = subset(res, measure == "1"), aes(x = percentage, y = mmce)) +
%   geom_errorbar(aes(ymin = mmce - sd, ymax = mmce + sd, colour = repetitions), width = 0.025, position = position_dodge(width = 0.01)) +
%   geom_line(aes(colour = repetitions), position = position_dodge(width = 0.01)) +
%   geom_point(aes(colour = repetitions), position = position_dodge(width = 0.01)) +
%   ylab("Test error") +
%   xlab("Training set percentage") +
%   theme_minimal()
% p1
% @
%
% \end{itemize}
\framebreak


\end{vbframe}
%
% \begin{vbframe}{Bias vs. Variance}
% Both, training error and test error are estimators and suffer, as all statistical estimators, from the bias-variance issue:
%
% % \begin{figure}
% %     \centering
% %     \includegraphics[width=6cm]{bias_variance_target}
% % \end{figure}
%
% <<fig.height=5, fig.width=5, out.width="0.5\\textwidth">>=
% n = 10
% set.seed(14)
% na_np = data.frame(x1 = rnorm(10, mean = -2, sd = 1),
%   x2 = rnorm(10, mean = 2, sd = 1),
%   Acc = "Not Accurate",
%   Pre = "Not Precise")
% set.seed(2)
% a_np = data.frame(x1 = rnorm(10, mean = 0, sd = 1),
%   x2 = rnorm(10, mean = 0, sd = 1),
%   Acc = "Accurate",
%   Pre = "Not Precise")
% set.seed(3)
% a_p = data.frame(x1 = rnorm(10, mean = 0, sd = .35),
%   x2 = rnorm(10, mean = 0, sd = .35),
%   Acc = "Accurate",
%   Pre = "Precise")
% set.seed(12)
% na_p = data.frame(x1 = rnorm(10, mean = 2, sd = .35),
%   x2 = rnorm(10, mean = 2, sd = .35),
%   Acc = "Not Accurate",
%   Pre = "Precise")
%
% plot_dat = rbind(na_np, a_p, a_np, na_p)
%
% ggplot(plot_dat, aes(x = x1, y = x2)) +
%   facet_grid(Acc ~ Pre) +
%   xlim(c(-5, 5)) + ylim(c(-5, 5)) +
%   xlab("") + ylab("") +
%   theme_bw() +
%   theme(axis.ticks = element_blank(),
%     axis.text.y = element_blank(),
%     axis.text.x = element_blank(),
%     panel.grid.major = element_blank(),
%     panel.grid.minor = element_blank(),
%     #panel.border = theme_blank(),
%     panel.background = element_blank())  +
%   annotate("path",
%     x = .25*cos(seq(0,2*pi,length.out = 100)),
%     y = .25*sin(seq(0,2*pi,length.out = 100)),
%     col = rgb(.2, .2, .2, .5)) +
%   annotate("path",
%     x = 1*cos(seq(0,2*pi,length.out = 100)),
%     y = 1*sin(seq(0,2*pi,length.out = 100)),
%     col = rgb(.2, .2, .2, .5)) +
%   annotate("path",
%     x = 2*cos(seq(0,2*pi,length.out = 100)),
%     y = 2*sin(seq(0,2*pi,length.out = 100)),
%     col = rgb(.2, .2, .2, .5)) +
%   annotate("path",
%     x = 3*cos(seq(0,2*pi,length.out = 100)),
%     y = 3*sin(seq(0,2*pi,length.out = 100)),
%     col = rgb(.2, .2, .2, .5)) +
%   annotate("path",
%     x = 4*cos(seq(0,2*pi,length.out = 100)),
%     y = 4*sin(seq(0,2*pi,length.out = 100)),
%     col = rgb(.2, .2, .2, .5)) +
%   geom_point()
% @
%
% \end{vbframe}

% \begin{vbframe}{Example: Polynomial Regression}
% Assume that $y$ can be approximated by a $d^{th}$-order polynomial
% $$
% y = f(x) = \beta_0 + \beta_1 x + \ldots + \beta_d x^d = \sum_{j = 0}^{d} \beta_j
% x^j\text{.}
% $$

% \begin{itemize}
%   \item $\beta_j$ are the coefficients and $d$ is called the degree (or order).
%   \item For fixed $d$, the form of the function $\fx$ is determined
%   by the values of the coefficients $\beta_j$.
%   \item  Therefore, the model function has the form $f(x)$,
%   and the task is to find the $\beta$ that best fits the data.
% \end{itemize}

% \framebreak

% Consider the following true relationship $f(x)$ with the corresponding training and test set:


% \framebreak

% Models of different {\it complexity}, i.e., of different polynomial order $d$ are fitted to the training set:
% %How should we choose the polynomial order $d$?

% <<echo=FALSE, out.width="0.9\\textwidth", fig.height=5>>=
% @

% \framebreak

% The performance of the models on the training data $\Dtrain$ can be measured by calculating the \emph{training error} according to the mean squared error (MSE):
% $$MSE = \frac{1}{n} \sum_{i = 1}^{n}(\yi - \fh_{\Dtrain}(\xi))^2\text{.}$$
%
% <<echo=FALSE>>=
% mse = function(y, p) {
%   if (class(p) == "lm")
%     p = predict(p)
%   loss = (y - p)^2
%   sprintf("%.3f", mean(loss))
% }
%
% p1.tr.mse = mse(y, p1)
% p5.tr.mse = mse(y, p5)
% p9.tr.mse = mse(y, p9)
% @
%
% \lz
%
% \begin{center}
% Our three models produce the following training errors: \\
% d = 1: \Sexpr{p1.tr.mse}, \qquad d = 5: \Sexpr{p5.tr.mse}, \qquad d = 9: \Sexpr{p9.tr.mse}
% \end{center}
%
% \lz
%
% Apparantly this does not work to select the correct model $\Rightarrow$ use
% {\em independent} test data to evaluate the prediction error
% (\enquote{test error})
%
% \framebreak

% Choosing model complexity is always a trade-off between bias and variance with respect to model performance and in general depends on the application itself (e.g, data, choice of loss-function).

% \lz

% \begin{itemize}
%   \item The polynomial of order $d = 9$ was almost optimal on $\Dtrain$ but lacked in performing good on new data points $\Dtest$ (leads to overfitting as the model is too complex / flexible).
%   \item The linear model with $d = 1$ was not able to capture the non-linear relationship, even on $\Dtrain$ (leads to underfitting as the model is not complex enough).
% \end{itemize}

% \lz
% %We can see that the training error should not be used to evaluate model performance.

% To illustrate the bias-variance trade-off here, we model polynomials from degree 1 to 9 and look at the training and test error.

% \framebreak

% \begin{itemize}
% \item A learner that can learn more complex concepts (higher order polynomial) has, in general, a lower bias (but higher variance).
% \item The training error consistently decreases with model complexity.%, typically dropping to zero if we increase the model complexity.
% \item A model with zero training error is \emph{overfitting} to the training data and will typically generalize poorly.
% \end{itemize}

% \end{vbframe}




\begin{vbframe}{Resampling}

\begin{itemize}
  \item We therefore need to construct a better performance estimator through \emph{resampling},
    that uses the data more efficiently.
  \item All resampling variants operate similar: The data set is split repeatedly into
    training and tests sets, and we later aggregate (e.g. average) the results.
  \item The usual trick is to make training sets quite larger (to keep the pessimistic bias small),
    and to handle the variance introduced by smaller test sets through many repetitions and averaging
    of results.
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=5cm]{figure_man/ml_abstraction-crop}
\end{figure}
\end{vbframe}

\begin{vbframe}{Expected Test Error}

Let $a$ be a learning algorithm that is applied to training data $\D$ of size $n$ which produces a prediction model $\fh_{\D}$ and suppose that the training data consists of observations which were randomly drawn from $\Pxy$.

\lz

The \emph{expected generalization error} $EGE_n$ of the learning algorithm $a$ is the expectation of the (conditional) generalization error with respect to all possible training sets from $\Pxy$, of size $n$:

\begin{eqnarray*}
\EGEn(a) = \EDn(\GED) =  \EDn( L(y, \fh_{\D}(x)) | \D)
%&=& \frac{1}{B} \sum_{b = 1}^{B} \widehat{GE}(\fh_{\Dtrain^b}, \Dtest^b)\\
%&=& \frac{1}{B} \sum_{b = 1}^{B} \frac{1}{|\Dtest^b|} \sum_{\xyi \in \Dtest^b} L(\yi, \fh_{\Dtrain^b}(\xi))
\end{eqnarray*}

Note that the expectation $\EDn$ averages over the randomness in the data $\D$ of size $n$
that is used to produce $\fh_{\D}$.
%averages over everything that is random, including the randomness in the data $\D$ that produced $\fh_{\D}$.

\lz

$\Rightarrow$ Estimating the \emph{expected generalization error} is our goal!

\framebreak

Resampling methods are based on repeatedly splitting the observed data set into training and test sets and fitting a learning algorithm on training sets of equal size $n$.

\lz

Consider $B$ sets of training and test data generated by a resampling method (denoted by $\Dtrain^b$ and $\Dtest^b$, $b=1,\dots,B$, respectively), the \emph{expected generalization error} can then be estimated by

\begin{eqnarray*}
\widehat{EGE_n}(a) &=&
\frac{1}{B} \sum_{b = 1}^{B} \GEh{\Dtest^b} (\fh_{\Dtrain^b})\\
&=& \frac{1}{B} \sum_{b = 1}^{B} \frac{1}{|\Dtest^b|} \sum_{\xy \in \Dtest^b} L(y, \fh_{\Dtrain^b}(x))
\end{eqnarray*}

Note that the expected generalization error will depend on the cardinality of the generated training sets.

% \framebreak
%
% \begin{algorithm}[H]
%   \begin{algorithmic}[1]\setstretch{1.2}
%   \State {\bf Input: }A data set $\D$ with $n$ observations, the number
%     of subsets $B$ to compute and an outer loss $L$ to measure the performance.
%   \State{\bf Output: }Summary of the validation statistics.
%   \State Generate $B$ subsets of $\D$ named $\Dtrain^{(1)}$ to $\Dtrain^{(B)}$
%   \State Set $S \leftarrow \emptyset$
%   \For {$b = 1 \to B$}
%     \State $\Dtest^{(b)} \leftarrow \D \setminus \Dtrain^{(b)}$
%     \State $\hat{f} \leftarrow $ Fitmodel($\Dtrain^{(b)}$)
%     \State $s_i \leftarrow \sum\limits_{(x, y) \in \Dtest^{(b)}}
%       L(y, \fh_{\Dtrain}(x))$
%     \State $S \leftarrow S \cup \{s_i\}$
%   \EndFor
%   \State Summarize $S$, e.g. $\mathrm{mean}(S)$
%   \caption{\footnotesize Generic resampling}
%   \label{alg:genresamp}
%   \end{algorithmic}
% \end{algorithm}
\end{vbframe}
%
% \begin{vbframe}{How to split up your data?}
% \begin{block}{Data-rich situations:}
%
% <<data-rich,fig.width=5.5,fig.height=1, echo = FALSE>>=
% par(mar = c(0, 0, 0, 0))
% plot(1, type = "n", xlim = c(1, 10), ylim = c(0, 1),
%   axes = FALSE, xlab = "", ylab = "")
% rect(0, 0, 5, 0.5, col = 2L, border = 2L)
% text(2.5, 0.5, "Train", pos = 3L)
% rect(5, 0, 7.5, 0.5, col = 3L, border = 3L)
% text(6.25, 0.5, "Validation", pos = 3L)
% rect(7.5, 0, 10, 0.5, col = 4L, border = 4L)
% text(8.75, 0.5, "Test", pos = 3L)
% @
%
% \lz
% The best approach is to randomly divide the data set into three parts:
% \begin{itemize}
%   \item A training set, that is used to fit the model.
%   \item A validation set, to estimate prediction error for model selection.
%   \item And a test set, to assess the generalization error
%   of the final chosen model.
% \end{itemize}
%
% \end{block}
%
% \framebreak
%
% \begin{block}{But what do we do when the available data set is small?}
%   In those cases, which we will find us in most of the time, it is
%   usually inefficient to split the data into three parts.
%   We can now follow two approaches:
%
%   \lz
%
%   \begin{itemize}
%     \item \textbf{In-sample-estimates:} As the name suggests, these methods
%     use the training set, to approximate the validation step
%     analytically. Some well known methods amongst others are AIC, BIC
%     or VC-dimension.
%     \item \textbf{Resampling:} By effectively re-using samples performance can
%     still be estimated on unseen data without sacrifizing
%     cardinality of training an test sets.
% \end{itemize}
% \end{block}
%
% \end{vbframe}

\begin{vbframe}{$K$-fold cross-validation}

\begin{itemize}
  \item Split the data into $k$ roughly equally-sized partitions.
  \item Use each of the $k$ partitions once as a test set and the remaining $k - 1$ training sets to fit the model.
  \item Estimate the generalization error of all $k$ models using the respective test sets.
  \item Combine (average) the $k$ estimates of the generalization error.
\end{itemize}

Example of 3-fold Cross-Validation:

<<fig.height=3, echo = FALSE>>=
# par(mar = c(0, 0, 0, 0))
# plot(1, type = "n", xlim = c(0, 10), ylim = c(0, 1), axes = FALSE)
# rect(seq(0, 8, by = 2), 0, seq(2, 10, by = 2), 1)
# text(seq(1, 9, by = 2), 0.5, col = c(rep("red", 2), "green", rep("red", 2)),
#      c(rep("Train", 2), "Test", rep("Train", 2)))
par(mar = c(0, 0, 0, 0))
plot(1, type = "n", xlim = c(-2, 10), ylim = c(0, 3), axes = FALSE) #, main = "Example of 3-fold Cross Validation")
rect(seq(0, 4, by = 2), 0.1, seq(2, 6, by = 2), 1, col = c("#56B4E944","#56B4E944","#E69F0044"))
rect(seq(0, 4, by = 2), 1.1, seq(2, 6, by = 2), 2, col = c("#56B4E944","#E69F0044","#56B4E944"))
rect(seq(0, 4, by = 2), 2.1, seq(2, 6, by = 2), 3, col = c("#E69F0044", "#56B4E944","#56B4E944"))

text(seq(1, 5, by = 2), 2.55, col = c("#E69F00", "#56B4E9","#56B4E9"),
  rev(c("Train", "Train", "Test")))
text(seq(1, 5, by = 2), 1.55, col = c("#56B4E9","#E69F00","#56B4E9"),
  c("Train", "Test", "Train"))
text(seq(1, 5, by = 2), 0.55, col = c("#56B4E9","#56B4E9","#E69F00"),
  c("Train", "Train", "Test"))
text(rep(-1, 3), c(0,1,2) + 0.55, paste("Iteration", 3:1))
for (i in 1:3) #text(8, 2 - (i - 1) + 0.55, bquote(paste("=> ",widehat(Err)(widehat(f)[D[train]^.(i)], D[test]^.(i)))), cex = 1.3)
  text(8, 2 - (i - 1) + 0.55, bquote(paste("=> ",widehat(GE)[D[test]^.(i)])), cex = 1.3)
@

\framebreak

Stratification tries to keep the distribution of the target class (or any specific categorical feature of interest) in each fold.

\lz

Example of stratified 3-fold Cross-Validation:

<<fig.height=4, echo = FALSE>>=
layout(cbind(rep(1, 3), 2:4, matrix(5:16, ncol = 4, byrow = TRUE)))
par(mar = c(2,2,4,2))

plot(as.factor(c(rep(1, 3), 2)), axes = FALSE, ylim = c(0, 5), col = c("#E69F00","#56B4E9"), main = "Class Distribution")
box()

par(mar = c(1,1,1,1))
red.ind = c(1, 6, 11)
for (i in 1:3) {
    plot.new()
    text(0.5, 0.5, cex = 1.2, paste("Iteration", i))
}
for (i in 1:12) {
  if (i %% 4 == 0) {
    plot.new()
    text(0.5, 0.5, cex = 2,
      bquote(paste("=> ",widehat(GE)[D[test]^.(i/4)])))
  } else {
    plot(as.factor(c(rep(1, 3), 2)),
      axes = FALSE, ylim = c(0, 5), col = c("#E69F00","#56B4E9"))
    if (i %in% red.ind) {
      rect(par("usr")[1], par("usr")[3], par("usr")[2], par("usr")[4], col = "#E69F0055")
      box(col = "#E69F00")
      plot(as.factor(c(rep(1, 3), 2)),
        axes = FALSE, ylim = c(0, 5), col = c("#E69F00","#56B4E9"), add = TRUE)
    } else {
      rect(par("usr")[1], par("usr")[3], par("usr")[2], par("usr")[4], col = "#56B4E955")
      box()
      plot(as.factor(c(rep(1, 3), 2)),
        axes = FALSE, ylim = c(0, 5), col = c("#E69F00","#56B4E9"), add = TRUE)
    }
  }
}
@

\framebreak

\begin{blocki}{Some comments on cross-validation:}
  \item Common choices for $k$ are 5 or 10 ($k = n$ is known as leave-one-out (LOO)
cross-validation or jackknife).
  \item Estimates of the generalization error tend to somewhat pessimistically biased
    (because the size of the training sets is $ n- (n/k) < n$), bias increases as $k$ gets smaller.
  \item The performance estimates for each fold are NOT independent, because
    of the structured overlap of the training sets.
  \item Hence, variance of the estimator of increases again for very large $k$ (close to LOO),
    when training sets nearly completely overlap.
  \item LOO is nearly unbiased, but has high variance.
  \item Repeated $k$-fold CV (multiple random partitions)
    can improve the performance of error estimation for small sample size.
\end{blocki}
\end{vbframe}

\begin{vbframe}{The failure of leave-one-out}
\begin{itemize}
\item The \code{iris} data set has 50 instances of each class (\code{setosa}, \code{versicolor}, \code{virginica}).
\item A majority classifier should have 33.3\% accuracy.
\item But majority classification with LOO is unstable and will lead to a generalization error of 1 (0\% accuracy) in each fold.
\item For demonstration let's assume \code{iris} only consists of 3 instances:
\end{itemize}
<<fig.height=3, echo = FALSE>>=
par(mar = c(0, 0, 0, 0))
plot(1, type = "n", xlim = c(-2, 10), ylim = c(0, 4), axes = FALSE)
rect(seq(0, 4, by = 2), 0.1, seq(2, 6, by = 2), 1, col = c("#56B4E944","#56B4E944","#E69F0044"))
rect(seq(0, 4, by = 2), 1.1, seq(2, 6, by = 2), 2, col = c("#56B4E944","#E69F0044","#56B4E944"))
rect(seq(0, 4, by = 2), 2.1, seq(2, 6, by = 2), 3, col = c("#E69F0044", "#56B4E944","#56B4E944"))

text(c(3, 7.3, 9.5), 3.55,
  c("Training set (blue), test set (pink)", "Predicted label", "GE"),
  cex = 1.3)

text(seq(1, 5, by = 2), 2.55,
  c("Setosa", "Versicolor", "Virginica"), cex = 1.3)
text(seq(1, 5, by = 2), 1.55,
  c("Setosa", "Versicolor", "Virginica"), cex = 1.3)
text(seq(1, 5, by = 2), 0.55,
  c("Setosa", "Versicolor", "Virginica"), cex = 1.3)
text(7.3, 2.75, "Versicolor/", cex = 1.3)
text(7.3, 2.45, "Virginica", cex = 1.3)
text(7.3, 1.75, "Setosa/", cex = 1.3)
text(7.3, 1.45, "Virginica", cex = 1.3)
text(7.3, 0.75, "Setosa/", cex = 1.3)
text(7.3, 0.45, "Versicolor", cex = 1.3)

text(rep(-1, 3), c(0,1,2) + 0.55, paste("Iteration", 3:1),
  cex = 1.3)

for (i in 1:3) {
  text(9.5, 2 - (i - 1) + 0.55,
    bquote(widehat(GE)[D[test]^.(i)]), cex = 1.3)
  text(10.2, 2 - (i - 1) + 0.55, c("= 1"), cex = 1.3)
}
@
\end{vbframe}

\begin{vbframe}{Bootstrap}

The basic idea is to randomly draw $B$ training sets of size $n$ with
replacement from the original training set $\Dtrain$:
% \begin{eqnarray*}
% \Dtrain^1 &=& \{z^1_1, \ldots, z^1_n\}\\
% \vdots& \\
% \Dtrain^B &=& \{z^B_1, \ldots, z^B_n\}
% \end{eqnarray*}

\begin{center}
\begin{tikzpicture}[scale=1]
% style
\tikzstyle{rboule} = [circle,scale=0.7,ball color=red]
\tikzstyle{gboule} = [circle,scale=0.7,ball color=green]
\tikzstyle{bboule} = [circle,scale=0.7,ball color=blue]
\tikzstyle{nboule} = [circle,scale=0.7,ball color=black]
\tikzstyle{sample} = [->,thin]

% title initial sample
\path (3.5,3.75) node[anchor=east] {$\Dtrain$};

% labels
\path (3.5,3)   node[anchor=east] {$\Dtrain^1$};
\path (3.5,2.5) node[anchor=east] {$\Dtrain^2$};
\path (3.5,1.5) node[anchor=east] {$\Dtrain^B$};

\path (3.5,2) node[anchor=east] {$\vdots$};
\path[draw,dashed] (3.75,2.0) -- (4.5,2.0);

% initial sample
\path ( 3.75,3.75) node[rboule] (j01) {};
\path ( 4.00,3.75) node[gboule] (j02) {};
\path ( 4.25,3.75) node[bboule] (j03) {};
\path ( 4.5,3.75) node[nboule] (j20) {};

% bootstrap 1
\path ( 3.75, 3.0) node[rboule] {};
\path ( 4.00, 3.0) node[rboule] {};
\path ( 4.25, 3.0) node[bboule] {};
\path ( 4.5, 3.0) node[nboule] (b1) {};

% bootstrap 2
\path ( 3.75, 2.5) node[gboule] {};
\path ( 4.00, 2.5) node[bboule] {};
\path ( 4.25, 2.5) node[gboule] {};
\path ( 4.5, 2.5) node[rboule] (b2) {};

% bootstrap N
\path (3.75,1.5) node[gboule] {};
\path (4,1.5) node[rboule] {};
\path (4.25,1.5) node[nboule] {};
\path (4.5,1.5) node[nboule] (bN) {};

% arrows
\path[sample] (j20.east) edge [out=0, in=60] (b1.east);
\path[sample] (j20.east) edge [out=0, in=60] (b2.east);
\path[sample] (j20.east) edge [out=0, in=60] (bN.east);
\end{tikzpicture}
\end{center}

We define the test set in terms of out-of-bootstrap observations
$\Dtest^b = \Dtrain \setminus \Dtrain^b$.

\framebreak

\begin{itemize}
  \item Typically, $B$ is between $50$ and $200$.
  \item The variance of the bootstrap estimator tends to be smaller than the
  variance of $k$-fold CV, as training sets are independently drawn, discontinuities are smoothed out.
  \item The more iterations, the smaller the variance of the estimator.
  \item Tends to be pessimistically biased
  (because training sets contain only about $63.2 \%$ unique the observations).
  \item Bootstrapping framework might allow the use of formal inference methods
  (e.g. to detect significant performance differences between methods).
  \item The OOB error of a random forest is based on the same idea.
  \item Extensions exist for very small data sets, that also use the training error for
    estimation: B632 and B632+.
\end{itemize}

\end{vbframe}

\begin{vbframe}{Subsampling}

Subsampling is basically a repeated holdout and is also is very similar to bootstrapping:

\begin{itemize}
\item Sampling is done \textit{without} replacement, i.e., in each iteration $\eta < n$ observations without replacement are drawn for the training set.
\item Typical choices for the subsampling rate $\frac{\eta}{n}$ are: $4/5$ or $9/10$.
\item The smaller the subsampling rate, the larger the bias.
\item The more sampling iterations, the smaller the variance.
\end{itemize}

\end{vbframe}

\begin{vbframe}{More comments}
\begin{itemize}
\item Do not use Hold-Out, CV with few iterations, or subsampling with a low subsampling rate for small samples, since this can cause the estimator to be extremely biased, with large variance.
\item For some models, computationally extremely fast calculations or approximations for the LOO exist.
\item Recommendation: Use the LOO-CV for efficient model selection.
  Other cross-validation methods are superior most of the time.
\item Picking the number $B$ of samples in bootstrapping and subsampling is not that simple (to be time efficient).
  A reasonable setting has to be data and task dependent.
\item 5CV or 10CV have become standard, 10-times repeated 10CV for small sample sizes, but THINK about whether
  that makes sense in your application.
\item A $\D$ with $|\D| = 100.000$ can have small sample size properties if one class has only 100 observations \ldots
\item Modern results seem to indicate that subsampling has somewhat better properties than
  bootstrapping. The repeated observations can can cause problems in training algorithms,
  especially in nested setups where the \enquote{training} set is split up again.

\end{itemize}
\end{vbframe}

%
% \begin{vbframe}{Resampling in \texttt{mlr}}
%
% Returns aggregated values, predictions and some useful extra
% information.
%
% <<echo = TRUE, cache = TRUE>>=
% rdesc = makeResampleDesc("CV", iters = 3)
% r = resample("regr.rpart", bh.task, resampling = rdesc)
% # for the lazy
% r = crossval("regr.rpart", bh.task, iters = 3)
% print(r)
% @
%
% \framebreak
%
% <<echo = TRUE, cache = TRUE>>=
% print(r$aggr)
% print(head(r$measures.test))
% print(head(as.data.frame(r$pred)))
% @
%
% \framebreak
%
% Can be instantiated to ensure a fair comparison of multiple learners.
%
% <<echo = TRUE, cache = TRUE>>=
% rin = makeResampleInstance(rdesc, bh.task)
% str(rin$train.inds)
% model1 = resample("regr.rpart", bh.task, resampling = rin)
% model2 = resample("regr.randomForest", bh.task, resampling = rin)
% @
%
% \end{vbframe}

\begin{vbframe}{Benchmarking in \texttt{mlr}}

\begin{itemize}
\item In a benchmark experiment different learning methods are applied to one or several data sets with the aim to compare and rank the algorithms with respect to one or more performance measures.
\item It is important that the train and test sets are synchronized, i.e. all learning methods see the same data splits so that they are better comparable.
\end{itemize}

We will first create a list of tasks and a list of learners:

<<echo = TRUE>>=
data("BostonHousing", "mtcars", "swiss", package = c("mlbench", "datasets"))
tasks = list(
  makeRegrTask(data = BostonHousing, target = "medv"),
  makeRegrTask(data = swiss, target = "Fertility"),
  makeRegrTask(data = mtcars, target = "mpg")
)
learners = list(
  makeLearner("regr.rpart"),
  makeLearner("regr.randomForest"),
  makeLearner("regr.lm")
)
@


\framebreak

The \texttt{benchmark} function from \texttt{mlr} allows you to compare all tasks and learners w.r.t. one or more measures based on the resampling method specified in the \texttt{resamplings} argument:

<<echo = -1, cache = TRUE>>=
set.seed(1)
(bmr = benchmark(learners, tasks, resamplings = cv10, measures = mlr::mse))
@

\end{vbframe}

\begin{vbframe}{Benchmarking in \texttt{mlr}: Access Data}

<<echo = TRUE, cache = TRUE>>=
head(getBMRAggrPerformances(bmr, as.df = TRUE), 3)
head(getBMRPerformances(bmr, as.df = TRUE), 3)
head(getBMRPredictions(bmr, as.df = TRUE), 3)
@

\end{vbframe}

\begin{vbframe}{Visualizing Performances}

Inspect the distribution of the performance measures using box plots:

<<echo = TRUE, cache = TRUE, out.width="0.8\\textwidth", fig.height=5>>=
plotBMRBoxplots(bmr, measure = mlr::mse)
@

\framebreak

The plot is a \texttt{ggplot2} object that can be changed (names, colors, etc.):

<<echo = TRUE, cache = TRUE, out.width="0.8\\textwidth", fig.height=5>>=
plotBMRBoxplots(bmr, measure = mlr::mse, style = "violin") +
  aes(color = learner.id)
@

\framebreak

Visualize the aggregated performance values as dot plot:

<<echo = TRUE, cache = TRUE, out.width="0.8\\textwidth", fig.height=5>>=
plotBMRSummary(bmr)
@

\framebreak

Plot the rankings of the aggregated performance values:

<<echo = TRUE, cache = TRUE, out.width="0.8\\textwidth", fig.height=5>>=
plotBMRSummary(bmr, trafo = "rank")
@

\end{vbframe}


\begin{vbframe}{Learning Curves}

\begin{itemize}
\item The \textit{Learning Curve} compares the performance of a model on training and test data over a varying number of training instances.
\item It allows us to see how fast an inducer can learn the given relationship in the data.
\item Learning should usually be fast in the beginning and saturate after data becomes larger an larger
% \item We should generally see performance improve as the number of training points increases When we separate training and testing sets and graph them individually
% \item We can get an idea of how well the model can generalize to new data
\item It visualizes when a model has learned as much as it can when
\begin{itemize}
\item the performance on the training and test set reach a plateau.
\item there is a consistent gap between training and test error.
\end{itemize}
\end{itemize}

An ideal learning curve looks like:

<<echo = FALSE>>=
load("rsrc/learning_curve.RData")
@

<<out.width="0.8\\textwidth", fig.height=6>>=
opt = res.rpart
opt$mmce = opt$mmce - mean(opt$mmce) + 0.08

p = ggplot(data = opt, aes(x = percentage, y = mmce))
p + geom_hline((aes(yintercept = 0.08))) +
  geom_text(aes(0.1, 0.08, label = "desired error", vjust = 1, hjust = 0)) +
  geom_errorbar(aes(ymin = mmce - sd, ymax = mmce + sd, colour = measure), width = 0.025) +
  geom_line(aes(colour = measure)) +
  geom_point(aes(colour = measure)) +
  ylim(0, 0.15) +
  ylab(expression(widehat(GE))) +
  xlab("Training set percentage") +
  scale_color_discrete(labels = c("1" = expression(D[test]), "2" = expression(D[train]))) +
  theme_minimal()
@

\framebreak

In general, there are two reasons for a bad looking learning curve:

\begin{enumerate}
\item High bias in model / underfitting
\begin{itemize}
\item training and test errors converge and are high.
\item model can't learn the underlying relationship and has high systematic errors, no matter how big the training set is.
\item poor fit, which also translates into high test error.
\end{itemize}

<<echo = FALSE, cache = TRUE, eval = TRUE, out.width="0.8\\textwidth", fig.height=4>>=
p = ggplot(data = res.rpart, aes(x = percentage, y = mmce))
p + geom_hline((aes(yintercept = 0.08))) +
  geom_text(aes(0.1, 0.08, label = "desired error", vjust = 1, hjust = 0)) +
  geom_errorbar(aes(ymin = mmce - sd, ymax = mmce + sd, colour = measure), width = 0.025) +
  geom_line(aes(colour = measure)) +
  geom_point(aes(colour = measure)) +
  ylim(0, 0.2) +
  ylab(expression(widehat(GE))) +
  xlab("Training set percentage") +
  scale_color_discrete(labels = c("1" = expression(D[test]), "2" = expression(D[train]))) +
  theme_minimal()
@

\framebreak

\item High variance in model / Overfitting
\begin{itemize}
\item large gap between training and test errors.
\item model requires more training data to improve.
\item model has a poor fit and does not generalize well.
%\item Can simplify the model with fewer or less complex features
\end{itemize}

<<echo = FALSE, cache = TRUE, eval = TRUE, out.width="0.95\\textwidth", fig.height=4>>=
p = ggplot(data = res.ranger, aes(x = percentage, y = mmce))
p + geom_hline((aes(yintercept = 0.08))) +
  geom_text(aes(0.1, 0.08, label = "desired error", vjust = 1, hjust = 0)) +
  geom_errorbar(aes(ymin = mmce - sd, ymax = mmce + sd, colour = measure), width = 0.025) +
  geom_line(aes(colour = measure)) +
  geom_point(aes(colour = measure)) +
  ylim(0, 0.2) +
  ylab(expression(widehat(GE))) +
  xlab("Training set percentage") +
  scale_color_discrete(labels = c("1" = expression(D[test]), "2" = expression(D[train]))) +
  theme_minimal()
@
\end{enumerate}
\end{vbframe}

\begin{vbframe}{Nested resampling}
In model selection, we are interested in selecting the best model from a set of potential candidate models (e.g., different model classes, different hyperparameter settings, different feature sets).


\begin{blocki}{Problem}
    \item We cannot evaluate our finally selected inducer on the same resampling splits that we have used to
      perform model selection for it, e.g., to tune its hyperparameters.
    \item By repeatedly evaluating the inducer on the same test set, or the same CV splits, information
      about the test can enter the algorithm.
    \item Danger of overfitting to the resampling splits / overtuning!
    \item The final performance estimate will be optimistically biased.
    \item One could also see this as a problem similar to multiple testing.

\end{blocki}

\framebreak

\begin{blocki}{Instructive and problematic example}
    \item Assume a binary classif. problem with equal apriori class sizes.
    \item Assume an inducer $a(\D, \lambda)$, with hyperparameter $\lambda$.
    \item $a$ shall be a (non-sensical) feature-independent classifier, where $\lambda$ has no effect.
      $a$ predicts random labels with equal probability.
    \item Of course, a's generalization error is 50\%.
    \item A cross-validation of $a$ (with any fixed $\lambda$) will easily show this
      (given that the partitioned data set for CV is not too small).
    \item Now lets \enquote{tune} $a$, by trying out 100 different $\lambda$ values.
    \item We repeat the experiment 50 times and average results.
\end{blocki}

<<fig.height=3.5, echo = FALSE>>=
ggd = load2("rsrc/overtuning-example.RData", "res2.mean")
ggd = melt(ggd, measure.vars = colnames(res2), value.name = "tuneperf");
colnames(ggd)[1:2] = c("data.size", "iter")
ggd = sortByCol(ggd, c("data.size", "iter"))
ggd$data.size = as.factor(ggd$data.size)


pl = ggplot(ggd, aes(x = iter, y = tuneperf, col = data.size))
pl =  pl + geom_line()
print(pl)
@

\begin{itemize}
  \item Plotted is the best \enquote{tuning error} after $k$ tuning iterations
    \item We have performed the experiment for different sizes of learning data
      that where cross-validated.
    \item Experiment was simulated with mlr's \enquote{classif.featureless} learner.
    \item Quiz: How to mathematically calculate the shape of the curves?
\end{itemize}

\begin{blocki}{Simple solution}
\item Again, simply simulate what happens in model application.
\item All parts of the model building (including model selection, preprocessing) should be embedded
  in the resampling, i.e., repeated for every pair of training/test data.
\item For steps that themselves require resampling (e.g. hyperparameter tuning) this results
  in two nested resampling loops, i.e. a resampling strategy for both tuning and outer evaluation.
\item Simplest form is a 3-way split into a training, optimization and test set.
  Inducers are trained on the training set, evaluated on the optimization set.
  After the final model is selected, we fit on joint training+optimization set and evaluate
  a final time on the test set. Note that we touch the test set only once, and have no way of \enquote{cheating}.
\end{blocki}

  \framebreak

Example: Outer loop with 3-fold CV and inner loop with 4-fold CV

\begin{center}
  \includegraphics[width=9.5cm]{figure_man/Nested_Resampling.png}
\end{center}

  \framebreak

  \begin{itemize}
    \item Regression task \pkg{mlbench.friedman1}.
    \item We jointly tune over a random forest and a neural net, with parameters \enquote{mtry}
      and \enquote{size} (with mlr's ModelMultiplexer).
    \item Random train and validation set with 100 + 100 observations.
    \item 50 iterations of random search tuning with holdout-splitting where we
      train on the training set and test on the validation set.
    \item After we are done, we store the best obtained configuration, and the obtained performance estimator
      on the validation set (biased).
    \item We sample another, small independent test set, with 100 observations, and a large one with 50K.
      We train the optimal model on the train set and test on both new test sets
      (that's the nested performance estimator and also the \enquote{real} performance estimator, which we usually do not have).
    \item This procedure was repeated 100 times.
  \end{itemize}

  \framebreak

<<fig.height=3, echo = FALSE>>=
ggd = load2("rsrc/nested-resample-example.RData")
pl = ggplot(ggd, aes(y = value, x = type)) + geom_boxplot()
pl = pl + scale_x_discrete(labels = c("wrong, biased resampling", "nested resampling", "'real' test error"))
pl = pl + ylab("MSE") + xlab("") + ylim(c(0,30))
pl = pl + theme(axis.text.x = element_text(angle = 0))
print(pl)
@
\begin{itemize}
  \item In reality we would train on the joint 100+100 train-valid-set observations.
    We don't do this here for a fair comparison with the naive strategy.
  \item Note the downward bias of \enquote{naive} tuning where model selected was not embedded in an
    outer resampling loop. Nested resampling get's it right.
\end{itemize}

\end{vbframe}

% \begin{vbframe}{Nested resampling with mlr}

% <<size="footnotesize">>=
% ## Tuning in inner resampling loop
% ps = makeParamSet(
%   makeDiscreteParam("C", values = 2^(-2:2)),
%   makeDiscreteParam("sigma", values = 2^(-2:2))
% )
% ctrl = makeTuneControlGrid()
% inner = makeResampleDesc("Subsample", iters = 2L)
% lrn = makeTuneWrapper("classif.ksvm", resampling = inner,
%   par.set = ps, control = ctrl, show.info = FALSE)
% ## Outer resampling loop
% outer = makeResampleDesc("CV", iters = 3L)
% @

%   \framebreak

% <<size="footnotesize">>=
% r = resample(lrn, iris.task, resampling = outer,
%   extract = getTuneResult, show.info = FALSE); print(r)

% print(r$extract[[1L]])
% @
% \end{vbframe}


\endlecture







\lecturechapter{8}{ROC}
\lecture{Fortgeschrittene Computerintensive Methoden}


\begin{vbframe}{Notation}
The following notation will be used in this chapter:

\begin{itemize}
  \item $\Xspace$ $p$-dim. input space%, usually we assume $\Xspace = \R^p$, but categorical features can occur, too
\item $\Yspace$ target space. In this chapter binary $\Yspace = \lbrace 0, 1 \rbrace$ or multi class $\Yspace = \gset$.
\item $x = \xvec \in \Xspace$ observation.
\item A binary classifier is a function $C: \Xspace \longrightarrow \Yspace = \lbrace 0,1\rbrace$.
\item A binary scoring classifier (or ranker) is a function $f$, which outputs a score (e.g. probability) for the positive class.
\item   $n$ is the total number of observations.
\item   $\text{NEG}$ and $\text{POS}$ are the number of negative and positive observations.
\item   $\text{neg}$ and $\text{pos}$ are the fraction of negative and positive observations, respectively.
\end{itemize}
\end{vbframe}



\begin{vbframe}{Evaluation of Binary Classifiers}

\begin{itemize}

\item  Consider a binary classifier $C(x)$, e.g. for cancer prediction (with true label $y$).
    \begin{itemize}
      \item   for $C(x) = 1 = \hat{y}$, we predict cancer
      \item   for $C(x) = 0 = \hat{y}$, we predict no cancer
    \end{itemize}
\item  One possible evaluation measure is the \textit{misclassification error} or \textit{error rate} (i.e. the proportion of patients for which $\hat{y} \neq y$).
\item  Example: If 10 out of 1000 patients are misclassified, the \textit{error rate} would be 1\%.
\item  In general, lower \textit{error rates} are better.

\end{itemize}

\framebreak

\begin{itemize}
\item   Using the \textit{error rate} for imbalanced true labels is not suggested.
\item   Example: Assume that only 0.5\% of 1000 patients have cancer.
  \begin{itemize}
    \item   Always returning $C(x) = 0 = \hat{y}$ gives an \textit{error rate} of 0.5\%, which sounds good.
    \item   However, we would never predict cancer, which is bad.
  \end{itemize}
\end{itemize}

$\Rightarrow$ We also need different evaluation metrics and should not only trust the \textit{error rate}.

\end{vbframe}


\begin{vbframe}{Confusion Matrix}

The confusion matrix is a $2 \times 2$ contingency table of predictions $\hat{y}$ and true labels $y$.
Several evaluation metrics can be derived from a confusion matrix:

\begin{center}
\includegraphics[width=0.9\textwidth]{figure_man/roc-confusion_matrix.png}
\end{center}
%
% \framebreak
%
% Terminology:
%
% \begin{itemize}
%
% \item   \textbf{True positive (TP):} \newline
%     We predicted "1" and the true class is "1".
%
% \item  \textbf{True negative (TN):} \newline
%     We predicted "0" and the true class is "0".
%
% \item  \textbf{False positive (FP):} \newline
%     We predicted "1" and the true class is "0" (type I error).
%
% \item  \textbf{False negative (FN):} \newline
%     We predicted "0" and the true class is "1" (type II error).
%
% \item  \textbf{Positive (pos):} \newline
%     Fraction of true class labels with "1". %<!-- \text{POS} = \sum_i I(y_i = 1)$ and $\text{NEG} = \sum_i I(y_i = 0)$-->
%
% \item   \textbf{Negative (neg):} \newline
%     Fraction of true class labels with "0".
%
% \end{itemize}
% %<!-- (see also [Statistical Tests of Significance \& Type I and Type II Errors](http://0agr.ru/wiki/index.php/Statistical_Tests_of_Significance#Type_I_and_Type_II_Errors "wikilink2")) -->
%
%
% \framebreak
%
%
% The following measures can be obtained from the confusion matrix:
%
% \begin{itemize}
% \item   True positive rate (also known as sensitivity or recall)
%   \begin{itemize}
%     \item   Fraction of positive observations correctly classified
%     \item   $\text{tpr} = \cfrac{\text{\#TP}}{\text{\#TP} + \text{\#FN}}$
%   \end{itemize}
% \item   False positive rate (also known as fall-out)
%   \begin{itemize}
%     \item   Fraction of negative observations incorrectly classified
%     \item   $\text{fpr} = \cfrac{\text{\#FP}}{\text{\#FP} + \text{\#TN}} = \text{1 - Specificity}$
%   \end{itemize}
% \item   Accuracy
% \item   Misclassification error (or error rate)
% \item   Positive predictive value (or precision) %<!--, $P = \cfrac{\text{TP}}{\text{TP} + \text{FP}}$ -->
% \item   Negative predictive value
% \item   ...
% \end{itemize}
% %<!-- -   Support - fraction of positively classified observations
% %    -   $\text{sup} = \cfrac{\text{TP} + \text{FP}}{N} =
% %        \cfrac{\text{predicted pos}}{\text{total}}$ -->
%
%

% \end{vbframe}
%
% \begin{vbframe}{Accuracy and Misclassification Error}
%
% In practice, these are the most widely used metrics
% \begin{itemize}
%   \item   Accuracy: $\text{acc} = \cfrac{\text{\#TP} + \text{\#TN}}{N}$
%
%   \begin{itemize}
%     \item fraction of correctly classified observations
%   \end{itemize}
%
%   \item   Error rate: $\text{error} = \cfrac{\text{\#FN} + \text{\#FP}}{N} = 1 - \text{acc}$
%
%   \begin{itemize}
%     \item Fraction of misclassified observations
%   \end{itemize}
%
% \end{itemize}
% \end{vbframe}


%
%
% \begin{vbframe}{Precision (P) or Positive Predictive Value (PPV)}
%   $$P = \cfrac{\text{\#TP}}{\text{\# predicted positives}} = \cfrac{\text{\#TP}}{\text{\#TP} + \text{\#FP}}$$
%
% \begin{center}
%   or
% \end{center}
% \vspace{-0.75cm}
%
%   $$P = \cfrac{pos \cdot tpr}{pos \cdot tpr + neg \cdot fpr} = \cfrac{tpr}{tpr + \tfrac{neg}{pos} \cdot fpr}$$
%
% \begin{itemize}
% \item   Interpretation: For all observations that we predicted $\hat{y} = 1$, what fraction actually has $y = 1$?
% \item   Higher \textit{precision} is better.
% \end{itemize}
% \end{vbframe}
%
%
% \begin{vbframe}{Recall (R)}
%
% $$R = \cfrac{\text{\#TP}}{\text{\# actual positives}} = \cfrac{\text{\#TP}}{\text{\#TP + \#FN}} = \text{tpr}$$
%
% \begin{itemize}
% \item   Interpretation: For all observations that actually have $y = 1$, what fraction did we correctly detect as $\hat{y} = 1$?
% \item   Higher \textit{recall} is better.
% \item   For a classifier that always returns zero (i.e. $\hat{y} = 0$), the \textit{recall} would be zero.
% \end{itemize}
%
\end{vbframe}


%<!--
%-   Out of all the people that do actually have %cancer, how much had been identified?
%-   We don't fail to spot many people that actually %have cancer.
% -->

%<!-- The [F Measure](http://0agr.ru/wiki/index.php%/F_Measure#F_Measure "wikilink3") is a combination %of [Precision and
%Recall](http://0agr.ru/wiki/index.php/Precision_and_%Recall "wikilink4") -->

\begin{vbframe}{F-Measure}
%<!--
%$P$ and $R$ don't make sense in the isolation from each other

%-   higher level of $\rho$ may be obtained by lowering $\pi$ and
%    vice versa

%Suppose we have a ranking classifier that produces some score (e.g. a probability) for $\mathbf{x}$:

%-   we decide whether to classify it $\hat{y} = 1$ or $\hat{y} = 0$ based on some threshold parameter $\tau$.
%-   varying $\tau$, will lead to different values for precision and recall.
%-   improving recall yields a worse precision and vice versa.
%-   combine $P$ and $R$ into one measure (also see [ROC Analysis])

%$F_{\beta} = \cfrac{(\beta^2 + 1) P \, R}{\beta^2 \, %P + R}$

%-   $\beta$ is the tradeoff between $P$ and $R$
%-   if $\beta$ is close to 0, then we give more %importance to $P$
%    -   $F_0 = P$
%-   if $\beta$ is closer to $+ \infty$, we give more %importance to
%    $R$

%When $\beta = 1$ we have $F_1$ score: -->

It is difficult to achieve a high \textit{precision} and high \textit{recall} simultaneously.
A trade-off offers the $F_1$-measure, which is the harmonic mean of precision $P$ and recall $R$:

$$F_1 = 2 \cfrac{P \cdot R}{P + R} = \cfrac{2\text{tpr}}{\text{tpr} + \tfrac{neg}{pos} \cdot \text{fpr} + 1 } $$

\end{vbframe}

% \begin{vbframe}{Example}
%
% \begin{center}
% \includegraphics[width=0.7\textwidth]{figure_man/roc-confusion_matrix_example.png}
% \end{center}
%
%
% \end{vbframe}

\begin{vbframe}{Multiclass Confusion Matrix}
  \begin{itemize}
    \item The performance of a multiclass classifier can also be summarized in a confusion matrix.
    \item E.g., in a 3-class classification problem with labels A, B and C:
    \item[] \includegraphics[width=0.9\textwidth]{figure_man/roc-multiclass_confmatrix.png}
    \item The overall accuracy is calculated similarly to the binary confusion matrix.
    \item Example:
          $acc = \cfrac{\# \text{TP}_A + \# \text{TP}_B + \# \text{TP}_C}{n} = \cfrac{30+60+80}{300} \approx 56\%$
  \end{itemize}

  \framebreak

  \begin{itemize}
    \item[]   \includegraphics[width=0.9\textwidth]{figure_man/roc-multiclass_confmatrix.png}
    \item Other measures, e.g. tpr, need to be calculated in a one-vs-all manner.
    \item Example (true positive rate for label A):
    $$\text{tpr}_A = \cfrac{\# \text{TP}_A}{\# \text{TP}_A + \# \text{FN}_A} = \cfrac{30}{30+50+20} = 0.3$$
    \item Example (precision for label A):
    $$\text{P}_A = \cfrac{\# \text{TP}_A}{\# \text{TP}_A + \# \text{FP}_A} = \cfrac{30}{30+20+10} = 0.5$$
  \end{itemize}

\end{vbframe}
%
%    <<echo = FALSE, message = FALSE, warning = FALSE>>=
%   library(mlr)
%   set.seed(1)
%   lrn = makeLearner("classif.rpart")
%   iris2 = rbind(iris, iris)
%   iris2$Species = c(rep("A", 100), rep("B", 100), rep("C", 100))
%   iris2.task = makeClassifTask(data = iris2, target = "Species")
%   mod = train(lrn, iris2.task)
%   pred = predict(mod, newdata = iris2)
%   pred$data$response = c(rep("A", 30), rep("B", 50), rep("C", 20),
%                          rep("A", 20), rep("B", 60), rep("C", 20),
%                          rep("A", 10), rep("B", 10), rep("C", 80))
%   cm = calculateConfusionMatrix(pred)
%   t(cm$result)
%   @


%<!--
%# Visual Analysis

%Visual ways of evaluating the performance of a classifier

%-   [ROC Analysis](http://0agr.ru/wiki/index.php/ROC_Analysis "wikilink5") - True Positive Rate vs
    %False Positive Rate
%-   [Cumulative Gain Charts](http://0agr.ru/wiki/index.php/Cumulative_Gain_Chart "wikilink6") - True
  %  Positive Rate vs Predicted Positive Rate


%# Not Binary Classifiers

%When we have multi-class classifiers we can use:

%-   Contingency Table
%    -   just show misclassified observations side-by-side
%-   [Cost Matrix](http://0agr.ru/wiki/index.php/Cost_Matrix "wikilink8")
%    -   we define the cost for each misclassification
%    -   and calculate the total cost
%-   some measures can be extended to multiclass classifiers:
%    -   see Evaluation of Multiclass Classifiers
%-->

\begin{vbframe}{Binary Cost-sensitive Classification}
\begin{itemize}
  \item Regular classification:
  \begin{itemize}
    \item All misclassification errors equally severe.
    \item Aim: minimize the misclassification rate.
  \end{itemize}
  \item Cost-sensitive classification:
    \begin{itemize}
    \item Costs caused by different kinds of errors are not assumed to be equal.
    \item Aim: minimize expected costs.
  \end{itemize}
\end{itemize}

\framebreak

Cost matrix:
\begin{table}[]
\centering
\begin{tabular}{|l|c|c|}
\hline
                   & Actual positive & Actual negative \\ \hline
Predicted positive & $c_{11}$              & $c_{10}$              \\
Predicted negative & $c_{01}$              & $c_{00}$              \\ \hline
\end{tabular}
\end{table}
\begin{itemize}
\item \textit{Reasonableness} conditions:
     $$c_{01} > c_{11} \hspace{5pt} \text{ and } \hspace{5pt} c_{10} > c_{00}$$
\item Given the cost matrix, an example should be classified into the class that has the minimum
expected cost.
 \item Let $\pix  := \P(y = 1|x)$ and $1 - \pix  = \P(y = 0|x)$.
 \item $\P(y|x)$ is the probability estimation classifying an instance $x$ into class $y$.
% \item There are classification methods that can accomodate misclassification costs directly (e.g. \href{http://www.rdocumentation.org/UKljZ/packages/rpart/functions/rpart.html}{rpart}).
% \item Alternatively, choose thresholds to turn posterior probabilities into class labels, such that the costs are minimized.
\end{itemize}

\framebreak
Choose thresholds to turn posterior probabilities into class labels, such that the costs are minimized:
\begin{itemize}
  \item Optimal prediction for class 1
  \item[]$\Leftrightarrow$  Expected cost of predicting 1 $\leq$ expected cost of predicting 0
  \item[]$\Leftrightarrow$  $(1-\pix )c_{10} + \pix c_{11} \leq (1-\pix ) c_{00} + \pix c_{01}$
  \item If this inequality is an equality, then predicting either class is optimal
  \item[] $\Rightarrow$ Choose optimal threshold $\pix ^*$ such that
  $$\pix ^* = \cfrac{c_{10} - c_{00}}{c_{10}- c_{00} + c_{01} - c_{11}}$$
  \item The \textit{Reasonableness} conditions ensure, that $\pix ^*$ is well defined.
\end{itemize}

\end{vbframe}

\begin{vbframe}{ROC Analysis}
%<!-- (from Signal Detection Theory) -->
%<!--
%\vspace{-0.25cm}
%-   initially - for distinguishing noise from not %noise
%-   so it's a way of showing the performance of %Binary Classifiers
%    -   only two classes - noise vs not noise  -->
The \textbf{R}eceiver \textbf{O}perating \textbf{C}haracteristic (ROC) curve is created by plotting the \textit{tpr} vs. \textit{fpr}, i.e. the

\begin{itemize}
  \item   True positive rate, $\text{tpr} = \cfrac{\text{\#TP}}{\text{\#TP} + \text{\#FN}}$
  \item   False positive rate $\text{fpr} = 1-\text{specificity} = \cfrac{\text{\#FP}}{\text{\#FP} + \text{\#TN}}$
\end{itemize}

Properties:
\begin{itemize}
\item   ROC curves are insensitive to class distribution.
\item   If the proportion of positive to negative instances changes, the ROC curve will not change.
%\item   ROC space is 2 dimensional, i.e. $X: \text{fpr}, Y: \text{tpr}$.
\item To see why, consider a binary confusion matrix:
\end{itemize}
%<!--%
%# ROC Space

%When evaluating a binary classifier, we often use a %confusion matrix.

%-   Here, we need only *tpr* and *fpr*
%-   $\text{tpr} = \cfrac{\text{TP}}{\text{TP} + %\text{FN}}$
%-   $\text{fpr} = \cfrac{\text{FP}}{\text{FP} + %\text{TN}}$
%-   ROC space is 2 dimensional:
%    -   $X: \text{fpr}, Y: \text{tpr}$
%-->

\framebreak
Example:
\begin{table}[]
\centering
\begin{tabular}{|l|c|c|}
                \hline
               & True Positive & True Negative \\ \hline
Pred. Positive & 40            & 25            \\ \hline
Pred. Negative & 10            & 25           \\ \hline
\end{tabular}
\end{table}

Here we have we have a proportion $\text{pos}/\text{neg} = 1$.
Now we change the proportion to $\text{pos}/\text{neg} = 2$.
\begin{table}[]
\centering
\begin{tabular}{|l|c|c|}
                \hline
               & True Positive & True Negative \\ \hline
Pred. Positive & 80            & 25            \\ \hline
Pred. Negative & 20            & 25           \\ \hline
\end{tabular}
\end{table}

The true positive rate and ($\text{tpr} = 0.8$) and the false positive rate ($\text{fpr} = 0.5$) does not change.


% \begin{itemize}
%   \item Note that the class distribution is
% the relationship of the left column to the right column.
%   \item Any performance metric that uses values from both
% columns will be inherently sensitive to class skews (such as accuracy, precision, F measure)
%   \item ROC curves are based upon tpr and fpr.
%   \item Each measure uses only values in their respective columns.
%   \item Changing the class distribution will therefore not change the ROC curve.
% \end{itemize}

\end{vbframe}

\begin{vbframe}{ROC Space Baseline}

%<!-- We put a random classifier that predicts 1 with some probability, e.g. 3 random classifiers on the baseline: -->
\begin{itemize}

\item   The best classifier lies on the top-left corner.
\item   Example: 3 classifiers that lie on the baseline, i.e. a classifier that
  \begin{itemize}
    \item   always predicts 0 (0\% chance to predict 1),
    \item   predicts 1 in 80\% cases and
    \item   always predict 1 (in 100\% cases).
  \end{itemize}
\end{itemize}

\begin{center}
\includegraphics[width=0.45\textwidth]{figure_man/roc_space.png}
\end{center}

\end{vbframe}


\begin{vbframe}{ROC Space Baseline}

In practice, we can never obtain a classifier below this line. Example:

\begin{itemize}
  \item   A classifier $C_1$ below the line with $\text{fpr} = 80\%$, and $\text{tpr} = 30\%$
  \item   We can make it better than random by inverting its prediction:
    $C_2(x)$: if $C_1(x) = 1$, return 0; if $C_1(x) = 0$, return 1
  \item   Position of $C_2$ is then $(1 - \text{fpr}, 1 - \text{tpr}) = (20\%, 70\%)$
\end{itemize}

\begin{center}
\includegraphics[width=0.45\textwidth]{figure_man/roc-inv.png}
\end{center}

\end{vbframe}


\begin{vbframe}{ROC Convex Hull}

Suppose we have 5 classifiers $C_1, C_2, ..., C_5$

\begin{itemize}
\item   We calculate $\text{fpr}$ and $\text{tpr}$ for each and plot them on one plot.
\item   Each classifier is a single point in the ROC space.
\end{itemize}

\begin{center}
\includegraphics[width=0.5\textwidth]{figure_man/roc-plot-classifiers.png}
\end{center}

\end{vbframe}


\begin{vbframe}{ROC Convex Hull}
We can then try to find classifiers that achieve the best
$\text{fpr}$ and $\text{tpr}$.

\begin{itemize}
  \item  By the \href{http://0agr.ru/wiki/index.php/Dominance}{dominance} principle, we have the following Pareto frontier (the "ROC convex hull").
  \item   Classifiers below this hull are always suboptimal, e.g. $C_3$.
\end{itemize}

\begin{center}
\includegraphics[width=0.45\textwidth]{figure_man/roc-convex-hull.png}
\end{center}
\end{vbframe}


\begin{vbframe}{ISO Accuracy Lines}
There is a simple relationship between accuracy and $\text{fpr}$, $\text{tpr}$.

\begin{itemize}
  \item   Let $n$ be the number of observations,
  \item   $\text{NEG}$ and $\text{POS}$ the number of negative and positive observations,
  \item   $\text{neg}$ and $\text{pos}$ the fraction of negative and positive observations, respectively.
\end{itemize}

  $$\text{acc} = \text{tpr} \cdot \text{pos} + \text{neg} - \text{neg} \cdot \text{fpr}$$

\begin{itemize}
  \item   $\text{acc} = \cfrac{\text{\#TP} + \text{\#TN}}{n} =
    \cfrac{\text{\#TP}}{n} + \cfrac{\text{\#TN}}{n} =
    \cfrac{\text{\#TP}}{\text{POS}} \cdot \cfrac{\text{POS}}{n} +
    \cfrac{\text{NEG} - \text{\#FP}}{n} =
    \cfrac{\text{\#TP}}{\text{POS}} \cdot \cfrac{\text{POS}}{n} +
    \cfrac{\text{NEG}}{n} - \cfrac{\text{\#FP}}{\text{NEG}} \cdot
    \cfrac{\text{NEG}}{n} = \text{tpr} \cdot \text{pos} +
    \text{neg} - \text{fpr} \cdot \text{neg}$
\end{itemize}
\end{vbframe}


\begin{vbframe}{ISO Accuracy Lines}
We can rewrite this and get
$$\text{tpr} =  \cfrac{\text{neg}}{\text{pos}}
\cdot \text{fpr} + \cfrac{\text{acc} -
\text{neg}}{\text{pos}} \; \Leftrightarrow  \; y = ax + b$$

$$\text{with } y = \text{tpr},  \; x = \text{fpr},  \; a =
\cfrac{\text{neg}}{\text{pos}},  \; x =
\cfrac{\text{neg}}{\text{pos}},  \; b = \cfrac{\text{acc} -
\text{neg}}{\text{pos}}$$

Properties:
\begin{itemize}
  \item   The ratio $a = \cfrac{\text{neg}}{\text{pos}}$ is the slope of the line (changing this ratio yields many different slopes).
  \item   Changing the accuracy yields many parallel lines with the same slope because $acc$ is included in the intercept $b$.
  \item   "Higher" lines are better w.r.t. $acc$.
\end{itemize}
\end{vbframe}


\begin{vbframe}{ISO Accuracy Lines}
To calculate the corresponding accuracy, we have to find the intersection point of the accuracy line (red) the descending diagonal (blue).

\begin{center}
\includegraphics[width=\textwidth]{figure_man/iso_lines.png}
\end{center}
\end{vbframe}

\begin{vbframe}{ISO Accuracy Lines}
Explanation:
\begin{itemize}
 \item descending diagonal: $\text{tpr} = 1-\text{fpr} \; \Leftrightarrow \text{fpr} = 1-\text{tpr}$
 \item ISO accuracy line: $\text{tpr} =  \tfrac{\text{neg}}{\text{pos}}
\cdot \text{fpr} + \tfrac{\text{acc} -
\text{neg}}{\text{pos}}$
\end{itemize}
At the intersection between these lines we have:
\begin{align*}
 \text{tpr} &= \tfrac{\text{neg}}{\text{pos}} \cdot (1-\text{tpr}) + \tfrac{\text{acc} -
 \text{neg}}{\text{pos}}
 = \tfrac{\text{neg}}{\text{pos}} - \tfrac{\text{neg}}{\text{pos}} \cdot \text{tpr} + \tfrac{\text{acc} -
 \text{neg}}{\text{pos}} \\
 &= \tfrac{\text{acc}}{\text{pos}} - \tfrac{\text{neg} \cdot \text{tpr}}{\text{pos}}
 = \tfrac{\text{acc} -\text{neg} \cdot \text{tpr}}{\text{pos}}
 \end{align*}
  With $\text{pos} + \text{neg} = 1$ this is equivalent to:
\begin{align*}
 \text{pos} \cdot \text{tpr} + \text{neg} \cdot \text{tpr} &= \text{acc} \\
 \Leftrightarrow \hspace{2cm} \text{tpr} &= \text{acc}
 \end{align*}

$\Rightarrow$ At the intersection between the descending diagonal and the ISO accuracy line, the achieved accuracy is equal to the true positive rate.
\end{vbframe}

\begin{vbframe}{ISO Accuracy Lines vs Convex Hull}
Recall the convex hull of the ROC plot:

\begin{center}
\includegraphics[width=0.65\textwidth]{figure_man/roc-convex-hull.png}
\end{center}
\end{vbframe}

\begin{vbframe}{ISO Accuracy Lines vs Convex Hull}
Each line segment of the ROC convex hull is an ISO accuracy line
for a particular class distribution (slope) and accuracy.
All classifiers on such a line achieve the same accuracy for this distribution:

\begin{itemize}
  \item   $\text{neg} / \text{pos} > 1$

  \begin{itemize}
    \item   Distribution with more negative observations.
    \item   The slope is steep.
    \item   Classifier on the left is better.
  \end{itemize}

  \item   $\text{neg} / \text{pos} < 1$

  \begin{itemize}
    \item   Distribution with more positive observations.
    \item   The slope is flatter.
    \item   Classifier on the right is better.
  \end{itemize}

\end{itemize}
Each classifier on the convex hull is optimal w.r.t. accuracy and for a specific distribution.
\end{vbframe}

\begin{vbframe}{Selecting the Optimal Classifier}
\begin{enumerate}
  \item  Compute the ratio (slope) $\text{neg} / \text{pos}$.
  \item  Find the classifier that achieves the highest accuracy for this ratio.
  \item  Fix the ratio and keep increasing the accuracy until the end of the hull.
\end{enumerate}
\end{vbframe}

\begin{vbframe}{Selecting the Optimal Classifier - Example}
\begin{center}
\includegraphics[width=0.55\textwidth]{figure_man/roc-convex-hull-best-acc-1.png}
\end{center}

\begin{itemize}
  \item Distribution: $neg/pos = 1/1$, best classifier: $C_2$, accuracy $\approx 81 \%$
\end{itemize}
\end{vbframe}

\begin{vbframe}{Selecting the Optimal Classifier - Example}
\begin{center}
\includegraphics[width=0.55\textwidth]{figure_man/roc-convex-hull-best-acc-2.png}
\end{center}

\begin{itemize}
  \item Distribution: $neg/pos = 1/4$, best classifier: $C_4$, accuracy $\approx 83 \%$
\end{itemize}
\end{vbframe}

\begin{vbframe}{Selecting the Optimal Classifier - Example}
\begin{center}
\includegraphics[width=0.55\textwidth]{figure_man/roc-convex-hull-best-acc-3.png}
\end{center}

\begin{itemize}
  \item Distribution: $neg/pos = 4/1$, best classifier: $C_2$, accuracy $\approx 81 \%$

\end{itemize}
\end{vbframe}

\begin{vbframe}{Scoring Classifiers}
A scoring classifier (or ranker) is an algorithm that outputs the scores (e.g. a probabilities) for each class instead of one single
label.

Do binary classification with a ranker $f$:
\begin{itemize}
  \item $f$ outputs a single number
  \item Set some threshold $\theta$ to transform the ranker into a classifier,
    e.g. as in \href{http://0agr.ru/wiki/index.php/Logistic_Regression}{logistic regression}
  \begin{itemize}
      \item Predict $\hat{y} = 1$ (positive class) if $f(x) > \theta$ else predict $\hat{y} = 0$
  \end{itemize}
  \item How to set a threshold $\theta$?
  \begin{itemize}
    \item Use \href{http://0agr.ru/wiki/index.php/Cross-Validation}{crossvalidation} for finding
        the best value for $\theta$.
    \item Draw ROC curves, producing a point in the ROC space for \textbf{each possible threshold}.
  \end{itemize}
\end{itemize}
\end{vbframe}

\begin{vbframe}{ROC for Scoring Classifiers}
\textbf{Naive Method}:

Given a ranker $f$ and a dataset with $n$ training observations:
\begin{itemize}
  \item Consider all possible thresholds ($n-1$ for $n$ observations).
  \item For each threshold: Calculate $\text{fpr}$ and $\text{tpr}$, and draw this point on the ROC space.
  \item Select the best threshold using the ROC analysis (for the ratio $\text{neg} / \text{pos}$).
\end{itemize}
\textbf{Practical Method}:
\begin{itemize}
  \item Rank test observations on decreasing score.
  \item Start in $(0, 0)$, for each observation $x$ (in the decreasing order).
  \begin{itemize}
    \item If $x$ is positive, move $1/\text{pos}$ up
    \item If $x$ is negative, move $1/\text{neg}$ right
  \end{itemize}
\end{itemize}
\end{vbframe}

\begin{vbframe}{Example Naive Method}
\centering
\includegraphics[width=0.7\textwidth]{figure_man/roc-naive-method.png}

See \url{https://quayau.shinyapps.io/roc_shiny/} for a live demo.
\end{vbframe}

\begin{vbframe}{Example Practical Method}
Given:
\begin{itemize}
  \item 20 observations
\end{itemize}

\tiny
\begin{tabular}{l|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.2cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}}

  \hspace{-8pt} \#&  1& 2&  3&  4&  5&  6&  7&  8&  9& 10& 11& 12& 13& 14& 15& 16& 17& 18& 19& 20 \\ \hline
  \hspace{-8pt} C & P& P & N & P & P & P & N & N & P & N & P & N & P & N & N & N & P & N & P & N \\ \hline
  \hspace{-8pt} Score & .9 & .8 &  .7 & .6 & .55 & .54 & .53 & .52 & .51 & .505 & .4 & .39 & .38 & .37 & .36 & .35 & .34 & .33 & .3 &.1
\end{tabular}
\normalsize

%<!--(https://www.dropbox.com/s/65rdiv42ixe2eac/roc-lift.xlsx) -->
\begin{itemize}
  \item $C$ is the actual class of the training observations.
  \item $\text{neg} / \text{pos} = 1$, i.e. $1/\text{pos} = 1/\text{neg} = 0.1$
%<!--   [Click here for GIF](https://hsto.org/files/267/36b/ff1/26736bff158a4d82893ff85b2022cc5b.gif "giflink") (Result on next slide) -->
\end{itemize}

\begin{itemize}
  \item Rank test observations on decreasing score.
  \item Start in $(0, 0)$, for each observation $x$ (in the decreasing order).
  \begin{itemize}
    \item If $x$ is positive, move $1/\text{pos} = 0.1$ up
    \item If $x$ is negative, move $1/\text{neg} = 0.1$ right
  \end{itemize}
\end{itemize}

\framebreak

\begin{center}
\includegraphics[page=1]{figure_man/gif.pdf}

 $x$ is positive, move $1/\text{pos} = 0.1$ up.
\end{center}

\framebreak

\begin{center}
\includegraphics[page=2]{figure_man/gif.pdf}

 $x$ is positive, move $1/\text{pos} = 0.1$ up.
\end{center}

\framebreak

\begin{center}
\includegraphics[page=3]{figure_man/gif.pdf}

 $x$ is negative, move $1/\text{neg} = 0.1$ right.
\end{center}

\framebreak

\begin{center}
\includegraphics[page=4]{figure_man/gif.pdf}

 $x$ is positive, move $1/\text{pos} = 0.1$ up.
\end{center}

\framebreak

\begin{center}
\includegraphics[page=5]{figure_man/gif.pdf}

 $x$ is positive, move $1/\text{pos} = 0.1$ up.
\end{center}

\framebreak

\begin{center}
\includegraphics[page=6]{figure_man/gif.pdf}

 $x$ is positive, move $1/\text{pos} = 0.1$ up.
\end{center}

\framebreak

\begin{center}
\includegraphics[page=7]{figure_man/gif.pdf}

 $x$ is negative, move $1/\text{neg} = 0.1$ right.
\end{center}

\framebreak

\begin{center}
\includegraphics[page=20]{figure_man/gif.pdf}
\end{center}

\framebreak

\begin{center}
\includegraphics[width=0.5\textwidth]{figure_man/roc-curve-ex1.png}
\end{center}

\begin{itemize}
  \item Score of the best (6th) classifier is used as the threshold $\theta$.
  \item Predict positive class for $\theta \geqslant 0.54$ ($\Rightarrow$ accuracy = 0.7).
\end{itemize}
\end{vbframe}

%
% \begin{vbframe}{Example Practical Method}
% Given: 20 training observations, 12 negative and 8 positive
%
% \vspace{20pt}
%
% \tiny
% \begin{tabular}{l|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}}
%
%   \hspace{-8pt} \#&  1& 2&  3&  4&  5&  6&  7&  8&  9& 10& 11& 12& 13& 14& 15& 16& 17& 18& 19& 20 \\ \hline
%   \hspace{-8pt} C & N & N & N & N & N & N & N & N & N & N & N & N & P & P & P & P & P & P & P & P \\ \hline
%   \hspace{-8pt} Score & .18 & .24 &  .32 & .33 & .4 & .53 & .58 & .59 & .6 & .7 & .75 & .85 & .52 & .72 & .73 & .79 & .82 & .88 & .9 &.92
% \end{tabular}
% \normalsize
%
% \vspace{20pt}
% $\Rightarrow$ sort by score and draw the curves:
% \vspace{20pt}
%
% \tiny
% \begin{tabular}{l|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}|p{0.1cm}}
%
%   \hspace{-8pt} \#&  20& 19&  18&  12&  17&  16&  11&  15&  14& 10& 9& 8& 7& 6& 13& 5& 4& 3& 2& 1 \\ \hline
%   \hspace{-8pt} C & P & P & P & N & P & P & N & P & P & N & N & N & N & N & P & N & N & N & N & N \\ \hline
%   \hspace{-8pt} Score & .92 & .9 &  .88 & .85 & .82 & .79 & .75 & .73 & .72 & .7 & .6 & .59 & .58 & .53 & .52 & .4 & .33 & .32 & .24 &.18
% \end{tabular}
% \normalsize
% \end{vbframe}
%
%
% \begin{vbframe}{Example Practical Method}
% \begin{center}
% \includegraphics[width=0.75\textwidth]{figure_man/roc-curve-ex2.png}
% \end{center}
% \begin{itemize}
%   \item Best accuracy achieved with observation \# 18.
%   \item Setting $\theta = 0.88 \Rightarrow$ accuracy of $15/20 \; \hat{=} \; 75 \%$.
% \end{itemize}
% \end{vbframe}

\begin{vbframe}{Other ROC Curve Examples}
%<!-- (http://www.cs.bris.ac.uk/~flach/ICML04tutorial/ROCtutorialPartI.pdf) -->
\begin{center}
\includegraphics[width=0.6\textwidth]{figure_man/roc-curves.png}
\end{center}
\end{vbframe}

\begin{vbframe}{AUC: Area Under ROC Curve}
The area under the ROC curve (AUC $\in [0, 1]$) is
\begin{itemize}
  \item A measure for evaluating the performance of a classifier:
  \begin{itemize}
    \item AUC =   1: Perfect classifier, for which all positives are ranked higher than all negatives
    \item AUC = 0.5: Randomly ordered
    \item AUC =   0: All negatives are ranked higher than all positives
    \item Interpretation of AUC: Probability that a classifier $C$ ranks a randomly drawn
    positive observation "$+$" higher than a randomly drawn negative observation "$-$".
  \end{itemize}

%<!-- https://www.quora.com/How-is-statistical-significance-determined-for-ROC-curves-and-AUC-values -->

  \item Related to the Mann-Whitney-U test, which
  \begin{itemize}
    \item Estimates the probability that randomly chosen positives
      are ranked higher than randomly chosen negatives.
    \item Relation: $AUC = \cfrac{U}{POS \cdot NEG}$.
  \end{itemize}
  % \item related to the Gini coefficient $= 2 \cdot AUC - 1$ (area above diag.)
\end{itemize}
\end{vbframe}

\begin{vbframe}{AUC: Area Under ROC Curve}
%<!--
%-   Measure for evaluating the performance of a classifier
%-   it's the area under the ROC Curve, total area is 100%
%-->

\begin{center}
\includegraphics[width=0.75\textwidth]{figure_man/roc-auc-ex1.png}
\end{center}
\end{vbframe}

\begin{vbframe}{Explanation Mann-Whitney-U Test}
\begin{itemize}
\item First we plot the ranks of all the scores as a stack of horizontal bars, and color them by the labels.
\item Stack the green bars on top of one another, and slide them horizontally as needed to get a nice even stairstep on the right edge (See: practical method example for ROC curves):
\end{itemize}
\begin{center}
\includegraphics[width=0.8\textwidth]{figure_man/roc-mannwhitney3.png}
\end{center}


\framebreak


\begin{center}
\includegraphics[width=0.5\textwidth]{figure_man/roc-mannwhitney2.png}
\end{center}

\begin{itemize}
 \item Definition of the U statistic: $U = R_1 - \cfrac{n_1(n_1 + 1)}{2}$
 \begin{itemize}
  \item $R_1$ is the sum of ranks of positive cases (the area of the green bars)
  \item $n_1$ is the number of positive cases
 \end{itemize}
  \item The area of the green bars on the right side is equal to $\cfrac{n_1(n_1 + 1)}{2}$.
\end{itemize}

\framebreak
\begin{center}
\includegraphics[width=0.5\textwidth]{figure_man/roc-mannwhitney2.png}
\end{center}

\begin{itemize}
 \item $U =$ area of the green bars on left side
 \item area of dashed rectangle = $n_1 \cdot n_2$
 \item $AUC$ is $U$ normalized to the unit square,
\end{itemize}
$$\Longrightarrow AUC = \cfrac{U}{n_1\cdot n_2}$$
with $n_1 = \text{POS}$ and $n_2 = \text{NEG}$.
\end{vbframe}


\begin{vbframe}{Partial AUC}
\begin{itemize}
  \item Sometimes it can be useful to look at a \href{http://journals.sagepub.com/doi/pdf/10.1177/0272989X8900900307}{specific region under the ROC curve}  $\Rightarrow$ partial AUC (pAUC).
  \item Let $0 \leq c_1 < c_2 \leq 1$ define a region.
  \item For example, one could focus on a region with low fpr ($c_1 = 0, c_2 = 0.2$) or a region with high tpr ($c_1 = 0.8, c_2 = 1$):
\end{itemize}

<<echo = FALSE, message = FALSE, warning = FALSE, fig.width = 14, fig.height = 7, out.width="0.7\\textwidth">>=
#library(pROC)
set.seed(1)
D.ex <- rbinom(200, size = 1, prob = .5)
M1 <- rnorm(200, mean = D.ex, sd = .65)
M2 <- rnorm(200, mean = D.ex, sd = 1.5)

test <- data.frame(D = D.ex, D.str = c("Healthy", "Ill")[D.ex + 1],
                   M1 = M1, M2 = M2, stringsAsFactors = FALSE)

rocobj <- pROC::roc(test$D, test$M1)
par(mfrow = c(1, 2))
pROC::plot.roc(rocobj, print.auc=TRUE, auc.polygon=TRUE, partial.auc=c(1, 0.8), partial.auc.focus="sp", reuse.auc=FALSE, legacy.axes = TRUE, xlab = "fpr", ylab = "tpr", xlim = c(1, 0), ylim = c(0, 1),  auc.polygon.col="red", auc.polygon.density = 20, auc.polygon.angle = 135, partial.auc.correct = FALSE
)
pROC::plot.roc(rocobj, print.auc=TRUE, auc.polygon=TRUE, partial.auc=c(1, 0.8), partial.auc.focus="se", reuse.auc=FALSE, legacy.axes = TRUE, xlab = "fpr", ylab = "tpr", xlim = c(1, 0), ylim = c(0, 1),  auc.polygon.col="red", auc.polygon.density = 20, auc.polygon.angle = 135)
@

\framebreak

\begin{itemize}
  \item $\text{pAUC} \in [0, c_2 - c_1]$.
  \item The partial AUC can be corrected (see \href{http://journals.sagepub.com/doi/pdf/10.1177/0272989X8900900307}{McClish}), to have values between $0$ and $1$, where $0.5$ is non discriminant and $1$ is maximal: $$\text{pAUC}_\text{corrected} = \cfrac{1+\cfrac{\text{pAUC} - \text{min}}{\text{max} - \text{min}}}{2} $$
  \item $\text{min}$ is the
value of the non-discriminant AUC in the region
  \item $\text{max}$ is the maximum possible AUC in the region
\end{itemize}
\end{vbframe}



\begin{vbframe}{Multiclass AUC}
\begin{itemize}
  \item Consider multiclass classification, where a classifier predicts the probability $p_k$ of belonging to class $k$ for each class.
  \item Hand and Till (2001) proposed to average the AUC of pairwise comparisons (1 vs. 1) of a multiclass classifier.
  \begin{itemize}
    \item estimate $AUC(i,j)$ for each pair of class $i$ and $j$
    \item $AUC(i,j)$ is the probability that a randomly drawn member of class $i$ has a lower probability of belonging to class $j$
      than a randomly drawn member of class $j$.
    \item for $K$ classes, we have ${{K}\choose{2}} = \tfrac{K (K-1)}{2}$ values of $AUC(i,j)$ that are then averaged to compute the Multiclass AUC.
  \end{itemize}
\end{itemize}
\end{vbframe}

\begin{vbframe}{Calibration and Discrimination}
We consider data with a binary outcome $y$.
\begin{itemize}
  \item \textbf{Calibration:} When the predicted probabilities closely agree
    with the observed outcome (for any reasonable grouping).
  \begin{itemize}
    \item \textbf{Calibration in the large} is a property of the \textit{full sample}.
    It compares the observed probability in the full sample  (e.g. proportion of observations for which $y=1$)
   % <!-- (e.g., 10% if 10 of 100 individuals have the outcome being predicted, e.g. $y=1$) -->
    with the average predicted probability in the full sample.
    \item \textbf{Calibration in the small} is a property of \textit{subsets} of the sample.
    It compares the observed probability in each subset with the average
    predicted probability in that subset.
  \end{itemize}
  \item \textbf{Discrimination:} Ability to perfectly separate the population into $y=0$ and $y=1$.
    Measures of discrimination are, for example, AUC, sensitivity, specificity.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Calibration and Discrimination}
%<!-- http://www.uphs.upenn.edu/dgimhsr/documents/predictionrules.sp12.pdf -->
A well calibrated  classifier can be poorly discriminating, e.g.

\begin{table}[]
\centering
\begin{tabular}{rrrr}
\hline
Obs. Nr. & truth & Pred Rule 1 & Pred Rule 2 \\
\hline
1        & 1     & 1           & 0           \\
2        & 1     & 1           & 0           \\
3        & 0     & 0           & 1           \\
4        & 0     & 0           & 1           \\ \hline
Avg Prob & 50\%  & 50\%        & 50\%        \\
\hline
\end{tabular}
\end{table}

\begin{itemize}
  \item Both prediction rules have identical calibration in the large (50\%), however, rule 1 is better than rule 2.
\end{itemize}

<<eval = FALSE, echo = FALSE>>=
truth = c(1,1,0,0,0,0)
pred.rule.1 = c(1,1,0,0,0,0)
pred.rule.2 = c(0,0,0,0,1,1)
library(knitr)
kable(data.frame(truth = truth, "pred rule 1" = pred.rule.1, "pred rule 2" = pred.rule.2))
@
\end{vbframe}

\begin{vbframe}{Calibration and Discrimination}
A well discriminating classifier can have a bad calibration, e.g.

\begin{table}[]
\centering
\begin{tabular}{rrrr}
\hline
Obs. Nr. & truth & Pred Rule 1 & Pred Rule 2 \\
\hline
1        & 1     & 0.9           & 0.9         \\
2        & 1     & 0.9           & 0.9           \\
3        & 0     & 0.1          & 0.7           \\
4        & 0     & 0.1         & 0.7           \\ \hline
Avg Prob & 50\%  & 50\%        & 80\%        \\
\hline
\end{tabular}
\end{table}

\begin{itemize}
  \item Both prediction rules are well discriminating (e.g., setting thresholds $\theta_1 = 0.5$, $\theta_2 = 0.8$)
  \item Prediction rule 2 is rather poorly calibrated. The proportion of observations for which $y=1$ would be estimated with $80\%$.
\end{itemize}
\end{vbframe}

\begin{vbframe}{ROC Analysis in R}
\begin{itemize}
  \item \texttt{generateThreshVsPerfData} calculates one or several performance measures for a sequence of decision thresholds from 0 to 1.
  \item It provides S3 methods for objects of class \texttt{Prediction}, \texttt{ResampleResult}
and \texttt{BenchmarkResult} (resulting from  \texttt{predict.WrappedModel}, \texttt{resample}
or \texttt{benchmark}).
  \item \texttt{plotROCCurves} plots the result of \texttt{generateThreshVsPerfData} using \texttt{ggplot2}.
  \item More infos \url{http://mlr-org.github.io/mlr-tutorial/release/html/roc_analysis/index.html}
\end{itemize}
\end{vbframe}

\begin{vbframe}{Example 1: Single predictions}
\scriptsize
<<echo=TRUE, message = FALSE>>=
library(mlr)
set.seed(1)
# get train and test indices
n = getTaskSize(sonar.task)
train.set = sample(n, size = round(2/3 * n))
test.set = setdiff(seq_len(n), train.set)

# fit and predict
lrn = makeLearner("classif.lda", predict.type = "prob")
mod = train(lrn, sonar.task, subset = train.set)
pred = predict(mod, task = sonar.task, subset = test.set)
@
\normalsize
\end{vbframe}

\begin{vbframe}{Example 1: Single predictions}
We calculate fpr, tpr and compute error rates:

\scriptsize
<<echo = TRUE>>=
df = generateThreshVsPerfData(pred, measures = list(fpr, tpr, mmce))
@
\normalsize
\begin{itemize}
  \item \texttt{generateThreshVsPerfData} returns an object of class \texttt{ThreshVsPerfData},
which contains the performance values in the \texttt{\$data} slot.
  \item By default, \texttt{plotROCCurves} plots the performance values of the first two measures passed
to \texttt{generateThreshVsPerfData}.
  \item The first is shown on the x-axis, the second on the y-axis.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Example 1: Single predictions}
\scriptsize
<<echo = TRUE, fig.align="center", fig.width = 5, fig.height = 5, out.width="0.55\\textwidth">>=
df = generateThreshVsPerfData(pred, measures = list(fpr, tpr, mmce))
plotROCCurves(df)
@
\normalsize

\framebreak

The corresponding area under curve auc can be calculated by

\scriptsize
<<echo = TRUE>>=
performance(pred, auc)
@

\normalsize
\texttt{plotROCCurves} always requires a pair of performance measures that are plotted against
each other.

\framebreak

If you want to plot individual measures vs. the decision threshold, use

\scriptsize
<<echo = TRUE, fig.align="center", fig.height = 4, fig.width = 8, out.width="0.9\\textwidth">>=
plotThreshVsPerf(df)
@
\normalsize
\end{vbframe}


\begin{vbframe}{Example 2: Benchmark Experiment}
\scriptsize
<<>>=
options(width = 200)
@
<<echo = TRUE>>=
lrn1 = makeLearner("classif.randomForest", predict.type = "prob")
lrn2 = makeLearner("classif.rpart", predict.type = "prob")

cv5 = makeResampleDesc("CV", iters = 5)

bmr = benchmark(learners = list(lrn1, lrn2), tasks = sonar.task,
  resampling = cv5, measures = list(auc, mmce), show.info = FALSE)
bmr
@
\normalsize

Calling \texttt{generateThreshVsPerfData} and \texttt{plotROCCurves} on the \texttt{BenchmarkResult}
produces a plot with ROC curves for all learners in the experiment.

\framebreak

\scriptsize
<<echo = TRUE, fig.align="center", fig.height = 4, fig.width = 8, out.width="\\textwidth">>=
df = generateThreshVsPerfData(bmr, measures = list(fpr, tpr, mmce))
plotROCCurves(df)
@
\framebreak

\scriptsize
<<echo = TRUE, fig.align="center", fig.height = 4, fig.width = 8, out.width="\\textwidth">>=
plotThreshVsPerf(df)
@
\end{vbframe}
\endlecture
