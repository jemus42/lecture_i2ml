\begin{vbframe}{(Generalized) Linear Models -- method summary}

% \maketag{SUPERVISED} 
\maketag{regression} \maketag{classification} \maketag{PARAMETRIC} 
\maketag{WHITE-BOX} \maketag[50]{Feature selection}
\medskip

\highlight{General idea} ~~ Represent target as function of linear predictor
$\thx$ (weighted sum of features)\\
$\rightarrow$ \textbf{Interpretation:} if feature $x_j$ increases by 1 unit, the output changes by $\theta_j$ units
$\thetab$

\medskip

\highlight{Hypothesis space} ~~
$\Hspace = \left\{f: \Xspace \to \R ~|~\fx = \phi(\thetab^\top \xv)\right\}$, 
with suitable transformation $\phi(\cdot)$, e.g.,

\begin{itemize}
  \item Identity $\phi(\thetab^\top \xv) = \thetab^\top \xv$ 
  ~ $\Rightarrow$ \textbf{linear regression}
  % $\rightarrow$ continous output
  \item Logistic sigmoid function $\phi(\thx) = \frac{1}{1 + \exp(- \thx)} 
  =: \pixt$
  ~ $\Rightarrow$ \textbf{(binary) logistic regression}
  \begin{itemize}
    
    \item Probability $\pixt = \P_{\thetab} (y = 1 ~|~ \xv)$
    of belonging to one of two classes
    \item Separating hyperplane via decision rule 
    (e.g., $\yh = 1 \Leftrightarrow \pix > 0.5$)
  \end{itemize}
\end{itemize}

\begin{minipage}[b]{0.32\textwidth}
  \begin{center}
    \includegraphics[width=0.8\textwidth, trim=0 40 0 0, clip]{
    figure/lm_3d.png} \\
    \tiny{Linear regression hyperplane}
  \end{center}
\end{minipage}
\begin{minipage}[b]{0.32\textwidth}
  \begin{center}
    % \includegraphics[width=0.7\textwidth, trim=0 40 0 0, clip]{
    % ../slides/supervised-classification/figure_man/logreg-2vars-surface}\\
    \includegraphics[width=0.65\textwidth, trim=30 50 0 0, clip]{
    figure/logreg_3d}\\
    \tiny{Logistic function for bivariate input and loss-minimal $\thetab$}
  \end{center}
\end{minipage}
\begin{minipage}[b]{0.32\textwidth}
  \begin{center}
    \includegraphics[width=0.5\textwidth]{figure/logreg_2d}\\
    % \includegraphics[width=0.6\textwidth]{
    % ../slides/supervised-classification/figure_man/logreg-2vars-data} \\
    \tiny{Corresponding separating hyperplane}
\end{center}
\end{minipage}

\framebreak

\highlight{Empirical risk}

\begin{itemize}
  \item \textbf{Linear regression}
  \begin{itemize}
    
    \item Typically, based on \textbf{quadratic} loss: $\risket = 
    \sumin \left(\yi - \fxit \right)^2$ ~~
    $\Rightarrow$ OLS estimation
    % \item Alternatives: e.g., \textbf{absolute} or \textbf{Huber} loss (both 
    % improving robustness)
  \end{itemize}
  \item \textbf{Logistic regression:} based on 
  \textbf{Bernoulli / log / cross-entropy} loss ~ 
  $\Rightarrow \risket = \sumin -\yi \log 
  \left(\pixii\right) - (1 - \yi) \log \left(1 - \pixii \right)$
\end{itemize}

\medskip

\highlight{Optimization} ~~ For \textbf{OLS}: analytical solution
  $\thetabh = \olsest$, for other loss functions: numerical 
  optimization 

\medskip

% \highlight{Hyperparameters} ~~ None

\medskip

\highlight{Multi-class extension of logistic regression}

\begin{itemize}
  \item Estimate \textbf{class-wise} scoring functions:
  $\Rightarrow \pi: \Xspace \rightarrow \unitint^g, ~
  \pix = (\pikx[1], \dots, \pikx[g]), ~\sumkg \pikx = 1$
  \item Achieved through \textbf{softmax} transformation: 
  $\pikx = \exp(\thetab_k^\top \xv) \big/ \sum_{j=1}^g \exp(\thetab_j^\top 
  \xv)  $
  \item Multi-class log-loss: $\Lpixy = - \sumkg \I_{\{y = k\}} \log(\pikx)$
  \item Predict class with maximum score (or use thresholding variant)
\end{itemize}

% \highlight{Runtime behavior} ~~ $\mathcal{O}(p^2 \cdot n + p^3)$ for $n$ 
% observations and $p$ features

\end{vbframe}


% ------------------------------------------------------------------------------

\begin{frame}{(Generalized) Linear Models -- regularization}

\highlight{General idea}

\begin{itemize}
  \item Unregularized LM: risk of \textbf{overfitting} in high-dimensional 
  space with only few observations
  \item \textbf{Goal}: find compromise between model fit and generalization by 
  adding \textbf{penalty term}
  \item Regularization ubiquitous in ML, with similar techniques
\end{itemize}

%\medskip

% \highlight{Hypothesis space} ~~\\
% $\Hspace = \left\{f: \Xspace \to \R ~|~\fx = \phi(\thetab^\top \xv)\right\}$, 
% where $\phi(\cdot)$ is a transformation function.


%$\Hspace = \{ \theta_0 + \thx\ |\ (\theta_0, \thetab) \in \R^{p+1} \} $

%\medskip
\begin{columns}[c, totalwidth=\textwidth]
    \begin{column}{0.55\textwidth}
        
\highlight{Regularized empirical risk}

\begin{itemize}
  \item Empirical risk function \textbf{plus complexity penalty} 
  $J(\thetab)$, controlled by shrinkage parameter $\lambda > 0$:
  $\riskrt := \risket + \lambda \cdot J(\thetab)$
  %\item Popular regularizers
  %\begin{itemize} 
    \item \textbf{Ridge} regression: L2 penalty $J(\thetab) = \|\thetab\|_2^2 $
    \item \textbf{LASSO} regression: L1 penalty $J(\thetab) = \|\thetab\|_1 $
  %\end{itemize}
\end{itemize}

\medskip

\highlight{Optimization under regularization}
\begin{itemize}
  \item \textbf{Ridge}: analytically with 
  $\thetabh_{\text{Ridge}} = (\Xmat^\top \Xmat  + \lambda \id)^{-1} \Xmat^\top 
  \yv$
  \item \textbf{LASSO}: numerically with, e.g., (sub-)gradient descent
\end{itemize}



    \end{column}
        \begin{column}{0.45\textwidth}
          \includegraphics[width=\textwidth]{figure/l1_l2_hat.png}
    \end{column}
\end{columns}

\medskip
\highlight{Choice of regularization parameter}

\begin{itemize}
  \item Standard hyperparameter optimization problem
  \item E.g., choose $\lambda$ with minimum mean cross-validated error 
  (default in R package \texttt{glmnet})
\end{itemize}
\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{(Generalized) Linear Models -- regularization}

  \begin{columns}[T, totalwidth=\textwidth]
  \begin{column}{0.62\textwidth}

\medskip


\highlight{Ridge vs. LASSO} 

\begin{itemize}
  \item \textbf{Ridge}
  \begin{itemize} 
    \item Global shrinkage $\Rightarrow$ overall smaller but still dense $\thetab$
    \item Applicable with large number of influential features, handling 
    correlated variables by shrinking their coefficients by equal amount
  \end{itemize}
  \item \textbf{LASSO}
  \begin{itemize} 
    \item Actual variable selection by shrinking coefficients for irrelevant 
    features all the way to zero
    \item Suitable for sparse problems, ineffective with correlated 
    features (randomly selecting one)
  \end{itemize}  
  \item Neither overall better $\Rightarrow$ compromise: \textbf{elastic net}
  \begin{itemize} 
    \item Weighted combination of Ridge and LASSO
    \item Introducing additional penalization coefficient: $\riskrt = \risket 
    + \lambda_1 \cdot \|\thetab\|_1 + \lambda_2 \cdot \|\thetab\|_2^2$
  \end{itemize}  
\end{itemize}

\end{column}

\begin{column}{0.38\textwidth}
\tiny
\centering
Simulate two scenarios with 50 data sets of size $n = 100$: 

$\yv =\Xmat \boldsymbol{\beta}+ \epsilon, \quad \epsilon \sim N(0,1)$
\begin{columns}[T, totalwidth=\textwidth]
\begin{column}{0.5\textwidth}
\tiny
\begin{center}
\textbf{Ridge} performs better for: \\ 
$\boldsymbol{\beta}=(\underbrace{2,\ldots,2}_{5},\underbrace{0,\ldots,0}_{5})$\\
$ \operatorname{cor}(\Xmat_{i},\Xmat_{j})=0.8^{|i-j|}, \forall i, j$
  \end{center}
\end{column}
\begin{column}{0.5\textwidth} \tiny

\begin{center}
\textbf{Lasso} performs better for: \\
$\boldsymbol{\beta}=(2, 2, 2,\underbrace{0,\ldots,0}_{7})$ \\
$\operatorname{cor}(\Xmat_{i},\Xmat_{j})= 0, \forall i \neq j$
\end{center}
\end{column}
\end{columns}



          \includegraphics[width=\textwidth]{figure/enet_lasso_ridge_mse.png}
          
          \includegraphics[width=\textwidth]{figure/enet_tradeoff.png}
\end{column}

\end{columns}



\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{(Generalized) Linear Models -- Practical hints}

\highlight{Implementation}

\begin{itemize}
  \item \textbf{R:}
  \begin{itemize}
    \item \textbf{Unregularized:} \texttt{mlr3} learner \texttt{LearnerRegrLM}, 
    calling \texttt{stats::lm()} / \texttt{mlr3} learner 
    \texttt{LearnerClassifLogReg}, calling \texttt{stats::glm()}
    \item \textbf{Regularized:} \texttt{mlr3} learners 
    \texttt{LearnerClassifGlmnet} / 
    \texttt{LearnerRegrGlmnet}, calling \texttt{glmnet::glmnet()}
    \item For \textbf{large classification} models: \texttt{mlr3} learner     
    \texttt{LearnerClassifLiblineaR}, calling \texttt{LiblineaR::LiblineaR()}
  \end{itemize}
  \item \textbf{Python:} \texttt{LinearRegression} from package 
  \texttt{sklearn.linear\_model}, package for advanced statistical parameters 
  \texttt{statsmodels.api} 
\end{itemize}

%% WOULD DELTETE THIS!
%  \highlight{\textcolor{blue}{Check assumptions??} }\\
% Linear models are effective if the following assumptions are fulfilled:
%  \begin{itemize}
%   \item \textbf{linearity}: The expected response is a linear combination of the features.
%   \item \textbf{homoscedasticity}: The variance of residuals is equal for all features.
%   \item \textbf{independence}: All observations are independent of each other.
%   \item \textbf{normality}: Y is normally distributed for any fixed value of the features
% \end{itemize}

\end{frame}


% ------------------------------------------------------------------------------

\begin{frame}{(Generalized) Linear Models -- Pro's \& Con's}



\begin{columns}[onlytextwidth]
  \begin{column}{0.5\textwidth}
    \highlight{Advantages}
    
    \begin{procon}
      \setlength{\itemsep}{1pt}
      \setlength{\parskip}{1pt}
      \positem \textbf{Simple and fast} implementation
      \positem \textbf{Analytical} solution for quadratic loss
      \positem \textbf{Cheap} computation
      \positem Applicable for any \textbf{dataset size}, as long as number of 
      observations $\gg$ number of features
      \positem Flexibility \textbf{beyond linearity} with polynomials, 
      trigonometric transformations etc.
      \positem Intuitive \textbf{interpretability} via feature effects
      % \item fits \textbf{linearly} separable data sets very well
      \positem Statistical hypothesis \textbf{tests} for effects available
    \end{procon}
  \end{column}

  \begin{column}{0.5\textwidth}
    \highlight{Disadvantages}
    
    \begin{itemize}
      \negitem \textbf{Nonlinearity} of many real-world problems
      \negitem Further restrictive \textbf{assumptions}: linearly independent 
      features, homoskedastic residuals, normality of conditional response 
      %\textcolor{blue}{actually relevant in ML?}
      \negitem \textbf{Sensitivity} w.r.t. outliers and noisy data (especially 
      with L2 loss)
      \negitem Also a LM can \textbf{overfit} (e.g., many features and few observations) 
      \negitem Feature \textbf{interactions} must be handcrafted\\
      $\rightarrow$ practically infeasible for higher orders
      %\negitem No handling of \textbf{missing} data
    \end{itemize}
  \end{column}
\end{columns}

\vfill

\small

\conclbox{Simple, highly interpretable method, but 
with strong assumptions, practical limitations, and risk of overfitting}

\end{frame}