\begin{frame}{CART -- method summary}

\footnotesize

% \maketag{Supervised} 
\maketag{regression} \maketag{classification}
\maketag{Nonparametric} \maketag{White-box} \maketag{Feature selection}

\medskip

\begin{columns}[T, totalwidth=\textwidth]
\begin{column}{0.6\textwidth}
\highlight{General idea}
\begin{itemize}
  \item Start at root node containing all data
  \item Perform repeated \textbf{binary splits} in feature space to obtain
  \textbf{rectangular partitions} at terminal nodes $Q_1, \dots, Q_M$
  \item Splits based on reduction of node \textbf{impurity} \\
  $\rightarrow$ empirical risk minimization (\textbf{ERM})
  \item In each step:
  \begin{itemize}
    \item Find \textbf{optimal split} (feature-threshold combination) \\
    $\rightarrow$ greedy search
    \item Assign constant prediction $c_m$ to all obs. in terminal region $Q_m$\\
    $\rightarrow$ Regression: $c_m$ is average of $y$ \\
    $\rightarrow$ Classif.: $c_m$ is majority class (or class proportions)
    
  \item Stop when a pre-defined criterion is reached\\
  $\rightarrow$ See \highlight{Complexity control}
  \end{itemize}
  % \item Unless interrupted, splitting continues until each observation ends up 
  % in its own leaf node $\rightarrow$ \textbf{control complexity}
\end{itemize}

\end{column}
\begin{column}{0.45\textwidth}
\includegraphics[width=\textwidth]{../slides/trees/figure/cart_treegrow_22}
\includegraphics[width=\textwidth]{   ../slides/trees/figure/cart_splitcriteria_1} 
\end{column}
\end{columns}

\medskip

%\medskip
    \highlight{Hypothesis space} ~~
$\Hspace = \left\{ \fx: \fx = \sum\limits_{m = 1}^M c_m \I(\xv \in Q_m) 
\right\}$


% \begin{minipage}[b]{0.5\textwidth}
%   \includegraphics[width=\textwidth]{../slides/trees/figure/cart_treegrow_22} \\
%   \tiny{Classification tree for \texttt{iris} data after 3 splits}
% \end{minipage}
% \begin{minipage}[b]{0.49\textwidth}
%   \centering
%   \includegraphics[width=0.9\textwidth]{
%   ../slides/trees/figure/cart_splitcriteria_1} \\
%   \tiny{Corresponding prediction surface with axis-aligned boundaries}
% \end{minipage}%

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{CART -- method summary}

\footnotesize

\begin{columns}[T, totalwidth=\linewidth]
    \begin{column}{0.6\linewidth}
    
\highlight{Empirical risk} \\
    \begin{itemize}
\item Splitting \textbf{feature $x_j$ at split point $t$} divides a parent node $\Np$ into two child nodes:
 \begin{align*}\Nl = \{ (\xv, y) \in \Np: x_j \leq t \} \text{ and } \Nr = \{ (\xv, y) \in \Np: x_j > t \}
      \end{align*}
  %\item Calculated after each split, i.e., at each node $\Np_t$ (which is not necessarily the terminal node $Q_t$)
  
\end{itemize}
    \end{column}
    \begin{column}{0.4\linewidth}
{\centering \includegraphics[width=0.975\textwidth]{../slides/trees/figure/cart_splitcriteria_2.pdf} 
}
\end{column}
\end{columns}
\begin{itemize}
  \item Compute empirical risks in child nodes and minimize their sum to find best split (impurity reduction):
     \begin{align*}
      \argmin_{j, t} \risk(\Np, j, t) &= \argmin_{j, t} \tfrac{|\Nl|}{|\Np|} \risk(\Nl) + \tfrac{|\Nr|}{|\Np|} \risk(\Nr)
      \end{align*}
      
  \item In general, compatible with arbitrary losses -- typical choices:
  \begin{itemize}
    \footnotesize
    %\item $g$-way classification:
    %\begin{itemize}
      %\footnotesize
    %   \item $g$-way classification (\textbf{Brier score}): ~~
    %   $\risk(\Np) = \sum\limits_{(\xv,y) \in \Np} \sumkg
    %   \hat{\pik}^{(\Np)} (1 - \hat{\pik}^{(\Np)} )$
    %   % $\risk(\Np) = \sum\limits_{(\xv,y) \in \Np} \sumkg \left( \I(y = k)
    %   % - \pikx \right)^2$ 
    %   $\rightarrow$ \textbf{Gini} impurity
    %   \item $g$-way classification (\textbf{Bernoulli} loss): ~~
    %   $\risk(\Np) = \sum\limits_{(\xv,y) \in \Np} - \sumkg
    %   \hat{\pik}^{(\Np)} \log \hat{\pik}^{(\Np)}$
    %   % $\risk(\Np) = \sum\limits_{(\xv,y) \in \Np} \sumkg \I(y = k) \cdot
    %   % \log(\pikx)$ 
    %   $\rightarrow$ \textbf{entropy} impurity
    % %\end{itemize}

      \item $g$-way classification:
      \begin{tabular}{c |@{\vline}| c} 
 \textbf{Brier score} $\rightarrow$ \textbf{Gini} impurity & \textbf{Bernoulli} loss $\rightarrow$ \textbf{entropy} impurity \\ 
 \hline\\[-2ex]
 $\risk(\Np) = \sum_{(\xv,y) \in \Np} \sum_{k=1}^g
      \hat{\pik}^{(\Np)} (1 - \hat{\pik}^{(\Np)} )$ & $\risk(\Np) = -\sum_{(\xv,y) \in \Np} \sum_{k=1}^g
      \hat{\pik}^{(\Np)} \log \hat{\pik}^{(\Np)}$
\end{tabular}
      
    \item Regression (\textbf{quadratic} loss): ~~
    $\risk(\Np_t) = \sum_{(\xv,y) \in \Np} (y - c)^2$ with $c = \frac{1}{|\Np|} \sum_{(\xv,y) \in \Np} y$
  \end{itemize}
\end{itemize}

\medskip

\highlight{Optimization}

\begin{itemize}
  \item \textbf{Exhaustive} search over all split candidates, choice of 
  risk-minimal split
  \item In practice: reduce number of split candidates (e.g., using quantiles instead of all observed values)
\end{itemize}

\medskip

\highlight{Hyperparameters} ~~ \textbf{Complexity}, i.e., 
number of terminal nodes $T$ (controlled indirectly, see \highlight{Implementation}) 

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{CART -- Practical hints}

\footnotesize

\highlight{Hyperparameters and complexity control}

\begin{itemize}
  \item Unless interrupted, splitting continues until we have pure leaf nodes (costly + overfitting)
  \item Hyperparameters: Complexity controlled via tree depth, minimum number of observations per node, maximum number of leaves, minimum risk reduction per split, ...
  \item Limit tree growth / complexity via
  \begin{itemize}
    \item \textbf{Early stopping:} stop growth prematurely \\ $\rightarrow$ hard 
    to determine good stopping point before actually trying all combinations
    \item \textbf{Pruning:} grow deep trees and cut back in risk-optimal manner afterwards
  \end{itemize}
\end{itemize}

\medskip

\highlight{Bagging / boosting} ~~ 
As CART is a highly \textbf{unstable} learner, it is used as base learner in bagging (random forest) or boosting ensembles.

\medskip

\highlight{Some other decision trees}
\begin{columns}[T, totalwidth=\linewidth]
    \begin{column}{0.39\linewidth}
    \vspace{-\parsep}
        \begin{itemize}
%\item AID (Sonquist and Morgan, 1964)
\item CHAID (Kass, 1980)%\\
%$\rightarrow$ Multi-level splits
\item C4.5 (Quinlan, 1993)
\end{itemize}
    \end{column}
    \begin{column}{0.59\linewidth}
    \vspace{-\parsep}
        \begin{itemize}
\item Linear Model Trees (Potts, 2004)
\item Unbiased Recursive Partitioning (Hothorn et al., 2006)
\end{itemize}
    \end{column}
\end{columns}
   
\medskip

\highlight{Implementation}
\begin{itemize}
  \item \textbf{R:} \texttt{mlr3} learners \texttt{LearnerClassifRpart} / 
    \texttt{LearnerRegrRpart}, calling \texttt{rpart::rpart()}
  \item \textbf{Python:} \texttt{DecisionTreeClassifier} / 
  \texttt{DecisionTreeRegressor} from package \texttt{scikit-learn}
\end{itemize}

\end{frame}
% ------------------------------------------------------------------------------

\begin{frame}{CART -- Pros \& Cons}

\begin{columns}[onlytextwidth]
  \begin{column}{0.5\textwidth}
    \highlight{Advantages}
    \footnotesize
    \begin{itemize}
      \positem \textbf{Easy} to understand \& visualize (\textbf{interpretable})
      \positem Built-in \textbf{feature selection}\\
      $\rightarrow$ e.g., when features are not used for splitting
      \positem Applicable to \textbf{categorical} features \\
      $\rightarrow$ e.g., $2^m$ possible binary splits for $m$ categories\\
       $\rightarrow$ trick for regr. with L2-loss and binary classif.: categories can be sorted $\Rightarrow$ $m-1$ binary splits 
      \positem Handling of \textbf{missings} possible via surrogate splits
      \positem \textbf{Interaction} effects between features naturally included, 
      even of higher order
      \positem \textbf{Fast} computation and good scalability
      \positem High \textbf{flexibility} with custom split criteria or leaf-node 
      prediction rules
    \end{itemize}
  \end{column}
  \begin{column}{0.5\textwidth}
    \highlight{Disadvantages}
    \footnotesize
    \begin{itemize}
      \negitem Rather \textbf{poor generalization} %when used stand-alone 
      \negitem High \textbf{variance/instability}: model can change a lot when training data is minimally changed
      \negitem Can \textbf{overfit} if tree is grown too deep
      \negitem Not well-suited to model \textbf{linear} relationships
      \negitem \textbf{Bias} toward features with many unique values or categories
    \end{itemize}
  \end{column}
\end{columns}

\vfill

\small

\conclbox{Simple, good with feature selection and highly interpretable, but not 
the most performant learner}

\end{frame}

