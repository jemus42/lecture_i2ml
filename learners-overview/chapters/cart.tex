\begin{frame}{CART -- method summary}

\footnotesize

% \maketag{Supervised} 
\maketag{regression} \maketag{classification}
\maketag{Nonparametric} \maketag{White-box} \maketag{Feature selection}

\medskip

\begin{columns}[T, totalwidth=\textwidth]
\begin{column}{0.6\textwidth}
\highlight{General idea}
\begin{itemize}
  \item Start at root node containing all data
  \item Perform repeated \textbf{binary splits} in feature space to obtain
  \textbf{rectangular partitions} at terminal nodes $Q_t$
  \item Splits based on reduction of node \textbf{impurity} \\
  $\rightarrow$ empirical risk minimization (\textbf{ERM})
  \item In each step:
  \begin{itemize}
    \item Find \textbf{optimal split} (feature-threshold combination) \\
    $\rightarrow$ greedy search
    \item Assign constant prediction $c_t$ to all obs. in region $Q_t$\\
    $\rightarrow$ Regression: $c_t$ is average of $y$ \\
    $\rightarrow$ Classif.: $c_t$ is majority class (or class proportions)
    
  \item Stop when a pre-defined criterion is reached\\
  $\rightarrow$ See \highlight{Complexity control}
  \end{itemize}
  % \item Unless interrupted, splitting continues until each observation ends up 
  % in its own leaf node $\rightarrow$ \textbf{control complexity}
\end{itemize}

\medskip

%\medskip
    \highlight{Hypothesis space} ~~
$\Hspace = \left\{ \fx: \fx = \sum\limits_{t = 1}^T c_t \I(\xv \in Q_t) 
\right\}$

\end{column}
\begin{column}{0.45\textwidth}
\includegraphics[width=\textwidth]{../slides/trees/figure/cart_treegrow_22}
\includegraphics[width=\textwidth]{   ../slides/trees/figure/cart_splitcriteria_1} 
\end{column}
\end{columns}


% \begin{minipage}[b]{0.5\textwidth}
%   \includegraphics[width=\textwidth]{../slides/trees/figure/cart_treegrow_22} \\
%   \tiny{Classification tree for \texttt{iris} data after 3 splits}
% \end{minipage}
% \begin{minipage}[b]{0.49\textwidth}
%   \centering
%   \includegraphics[width=0.9\textwidth]{
%   ../slides/trees/figure/cart_splitcriteria_1} \\
%   \tiny{Corresponding prediction surface with axis-aligned boundaries}
% \end{minipage}%

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{CART -- method summary}

\footnotesize

\highlight{Empirical risk} \\

\begin{itemize}
  \item Calculated after each split, i.e., at each node $\Np_t$ (which is not necessarily the terminal node $Q_t$)
  \item In general, compatible with arbitrary losses -- typical choices:
  \begin{itemize}
    \footnotesize
    \item $g$-way classification:
    \begin{itemize}
      \footnotesize
      \item \textbf{Brier score} ~~
      $\risk(\Np_t) = \sum\limits_{(\xv,y) \in \Np_t} \sumkg
      \hat{\pik}^{(\Np_t)} \left(1 - \hat{\pik}^{(\Np_t)} \right)$
      % $\risk(\Np_t) = \sum\limits_{(\xv,y) \in \Np_t} \sumkg \left( \I(y = k)
      % - \pikx \right)^2$ 
      ~~ $\rightarrow$ \textbf{Gini} impurity
      \item \textbf{Bernoulli} loss ~~
      $\risk(\Np_t) = \sum\limits_{(\xv,y) \in \Np_t} - \sumkg
      \hat{\pik}^{(\Np_t)} \log \hat{\pik}^{(\Np_t)}$
      % $\risk(\Np_t) = \sum\limits_{(\xv,y) \in \Np_t} \sumkg \I(y = k) \cdot
      % \log(\pikx)$ 
      ~~ $\rightarrow$ \textbf{entropy} impurity
    \end{itemize}
    \item Regression: \textbf{quadratic} loss ~~
    $\risk(\Np_t) = \sum\limits_{(\xv,y) \in \Np_t} (y - c_t)^2$
  \end{itemize}
\end{itemize}

\medskip

\highlight{Optimization}

\begin{itemize}
  \item \textbf{Exhaustive} search over all split candidates, choice of 
  risk-minimal split
  \item In practice: reduce number of split candidates (e.g., using quantiles instead of all observed values)
\end{itemize}

\medskip

\highlight{Hyperparameters} ~~ \textbf{Complexity}, i.e., 
number of terminal nodes $T$ (controlled indirectly, see \highlight{Implementation}) 

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{CART -- Pro's \& Con's}

\begin{columns}[onlytextwidth]
  \begin{column}{0.5\textwidth}
    \highlight{Advantages}
    \footnotesize
    \begin{itemize}
      \positem \textbf{Easy} to understand \& visualize
      \positem Highly \textbf{interpretable}
      \positem Built-in \textbf{feature selection}
      \positem Applicable to \textbf{non-numerical} features
      \positem Handling of \textbf{missings} possible via surrogate splits
      \positem \textbf{Interaction} effects between features naturally included, 
      even of higher order
      \positem \textbf{Fast} computation and good scalability
      \positem High \textbf{flexibility} (custom split criteria or leaf-node 
      prediction rules)   
    \end{itemize}
  \end{column}
  \begin{column}{0.5\textwidth}
    \highlight{Disadvantages}
    \footnotesize
    \begin{itemize}
      \negitem Rather \textbf{poor generalization} when used stand-alone 
      \negitem High \textbf{variance/instability}: strong dependence on training 
      data
      \negitem Substantial risk of \textbf{overfitting}
      \negitem Not well-suited for modeling \textbf{linear} relationships
      \negitem \textbf{Bias} toward features with many categories
    \end{itemize}
  \end{column}
\end{columns}

\vfill

\small

\conclbox{Simple, good with feature selection and highly interpretable, but not 
the most performant learner}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{CART -- Practical hints}

\footnotesize

\highlight{Complexity control}

\begin{itemize}
  \item Unless interrupted, splitting continues until we have pure
  leaf nodes (costly + overfitting)
  \item Limit tree growth via
  \begin{itemize}
    \item \textbf{Early stopping:} stop growth prematurely \\ $\rightarrow$ hard 
    to determine good stopping point before actually trying all combinations
    \item \textbf{Pruning:} grow to large size and cut back in risk-optimal 
    manner
  \end{itemize}
\end{itemize}

\medskip

\highlight{Bagging / boosting} ~~ 
As CART are highly \textbf{unstable} predictors on their own, they are typically 
used as base learners in bagging (random forest) or boosting ensembles.

\medskip

\highlight{Implementation}
\begin{itemize}
  \item \textbf{R:} \texttt{mlr3} learners \texttt{LearnerClassifRpart} / 
    \texttt{LearnerRegrRpart}, calling \texttt{rpart::rpart()}
  \item \textbf{Python:} \texttt{DecisionTreeClassifier} / 
  \texttt{DecisionTreeRegressor} from package \texttt{scikit-learn}
  \item Complexity controlled via tree depth, minimum number of observations 
  per node, maximum number of leaves, minimum risk reduction per split, ...
\end{itemize}

\end{frame}
