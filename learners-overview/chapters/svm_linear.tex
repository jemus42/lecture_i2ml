\begin{vbframe}{Linear SVM -- method summary}

% \maketag{SUPERVISED} 
\maketag{CLASSIFICATION} \maketag[50]{REGRESSION} \maketag{PARAMETRIC} 
\maketag{WHITE-BOX} 
\medskip

\highlight{General idea}

\begin{itemize}
  \item Find linear decision boundary (\textbf{separating hyperplane}) that 
  best discriminates classes
  \begin{itemize}
    \item \textbf{Hard-margin} SVM: maximize distance (\textbf{margin} 
    $\gamma$ > 0) to closest points (\textbf{support vectors, SVs}) on each 
    side of decision boundary
    \item \textbf{Soft-margin} SVM: relax separation to allow for margin 
    violations $\Rightarrow$ maximize margin while minimizing violations
  \end{itemize}
\end{itemize}

\begin{minipage}{0.7\textwidth}

\begin{itemize}
  \item 3 types of training points
  \begin{itemize}
    \item \textbf{non-SVs} with no impact on decision boundary
    \item \textbf{SVs} located exactly on decision boundary
    \item \textbf{margin violators}
  \end{itemize}
  \item \textbf{Interpretable} weighted sum of basis functions with positive 
  coefficients for support vectors
  \item Extension to \textbf{regression} is possible but requires modifications 
  \\ $\Rightarrow$ here: only classification case
\end{itemize}

\medskip

\highlight{Hypothesis space} ~~
\textcolor{blue}{
$
% \Hspace = \left\{f: \Xspace \to \R ~|~\fx = \thetab^\top \xv + \theta_0
% \right\} 
\left \{ \fx = \sumin \beta_i \yi \langle \xi, \xv \rangle  + \theta_0 ~|~
\theta_0, \beta_i \in \R ~ \forall i \right \}$
}
\end{minipage}
\begin{minipage}{0.25\textwidth}
  \includegraphics[width=1.3\textwidth]{figure/svm_wording.png} \\
  \tiny{Soft-margin SVM with margin violations}
\end{minipage}

\framebreak

% \medskip
% \footnotesize
% \begin{minipage}{0.6\textwidth}
%   \centering
%   \includegraphics[width=0.9\textwidth]{
%   ../slides/linear-svm/figure/linear_classif_1.png}  \\
%   \tiny{Hard-margin SVM: margin is maximized by boundary on the right}
% \end{minipage}
% \hfill
% \begin{minipage}{0.3\textwidth}
%   \centering
%     %https://docs.google.com/presentation/d/1g7q1hbTNmQeuRWQIM8SF9l6iKWmJyuhyhm3s9QjA0jM/edit?usp=sharing
%   \includegraphics[width=1.1\textwidth]{figure/svm_wording.png} \\
%   \tiny{Soft-margin SVM with margin violations}
% \end{minipage}

\medskip

\highlight{Dual problem} ~~ %lecture_cim2\2020\08-linear-svm\slides-3-soft-margin-svm.Rnw
\textcolor{blue}{Motivation: \dots}

\begin{eqnarray*}
    & \max\limits_{\alphav \in \R^n} & \dualobj \\
    & \text{s.t. } & 0 \le \alpha_i \le C ~~ \forall i \in \nset ~~ (C = \infty
    \text{ for hard-margin SVM)}, \\
    & \quad & \sum_{i=1}^n \alpha_i \yi = 0
\end{eqnarray*}

\medskip

\highlight{Empirical risk} ~~ Soft-margin SVM also interpretable as 
\textbf{L2-regularized ERM}: 

\begin{minipage}[b]{0.58\textwidth}
  $$ \frac{1}{2} \|\thetab\|_2^2 + C \sumin \Lxyi$$ 
  with  
  \begin{itemize}
    \item $\|\thetab\| = 1 / \gamma$,\\
    \item $C > 0$: penalization for missclassified data points
    \item $\Lyf = \max(1-yf, 0)$: \textbf{hinge} loss \\
    $\Rightarrow$ other loss functions applicable (e.g., \textbf{Huber} loss)
  \end{itemize}
\end{minipage}
\begin{minipage}[b]{0.4\textwidth}
  \centering
  \includegraphics[height=0.4\textwidth, keepaspectratio=true]{
  figure/plot-hinge-loss.png}
\end{minipage}

\framebreak

\highlight{Optimization}

\begin{itemize}
  \item Typically, tackling \textbf{dual} problem (though feasible 
  in corresponding primal) via \textbf{quadratic programming}
  \item Popular: \textbf{sequential minimal optimization} $\Rightarrow$ 
  iterative algorithm based on breaking down objective into bivariate quadratic 
  problems with analytical solutions
\end{itemize}
\medskip

\highlight{Hyperparameters} ~~ Cost parameter \textbf{$C$}

\hfill

\includegraphics[width=0.55\textwidth]{
  figure/linear_classif_1.png}  \\
  \tiny{Hard-margin SVM: margin is maximized by boundary on the right}
  \normalsize

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{frame}{Linear SVM -- Pro's \& Con's}

\begin{columns}[onlytextwidth]
  \begin{column}{0.5\textwidth}
    \highlight{Advantages}
    \footnotesize
    \begin{itemize}
      % \positem High \textbf{accuracy}
      \positem Often \textbf{sparse} solution (w.r.t. observations)
      \positem Robust against overfitting (\textbf{regularized}); especially in 
      high-dimensional space
      \positem \textbf{Stable} solutions, as non-SV do not influence decision 
      boundary
      %\positem \textbf{memory efficient} (only use non-SVs)
    \end{itemize}
  \end{column}

  \begin{column}{0.5\textwidth}
    \highlight{Disadvantages}
    \footnotesize
    \begin{itemize}
      \negitem \textbf{Costly} implementation; long training times
      \negitem \textbf{Limited scalability} to larger data sets 
      \textcolor{blue}{\textbf{??}}
      \negitem Confined to \textbf{linear separation}
      % \negitem Poor \textbf{interpretability}
      \negitem No handling of \textbf{missing} data
    \end{itemize}
  \end{column}
\end{columns}

\vfill

\small

\conclbox{Very accurate solution for high-dimensional data that is linearly 
separable}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Linear SVM -- Practical hints}

\footnotesize

\highlight{Preprocessing} ~~
Features must be rescaled before applying SVMs (true in general for regularized 
models).

\medskip

\highlight{Tuning}

\begin{itemize}
  \item Tuning of cost parameter $C$ advisable ~~ $\Rightarrow$ strong influence 
  on resulting separating hyperplane
  \item Frequently, tuned on log-scale grid
\end{itemize}

\medskip

\highlight{Implementation} 
\begin{itemize}
  \item \textbf{R:} \texttt{mlr3} learners \texttt{LearnerClassifSVM} /
  \texttt{LearnerRegrSVM}, calling \texttt{e1071::svm()} (interface to 
  \texttt{libSVM}), with linear kernel
  \item \textbf{Python:} \texttt{sklearn.svm.SVC} from package 
  \texttt{scikit-learn} / package \texttt{libSVM}
\end{itemize}

\end{frame}
