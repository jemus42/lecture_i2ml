% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
library(xtable)
library(MASS)
library(GGally)
library(factoextra)
library(ggrepel)
library(rafalib)
library(colorspace)
library(mlr)
library(ggplot2)
library(magrittr)
@

% Load all R packages and set up knitr
<<setup, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{PCA}
\lecture{Introduction to Machine Learning}
\sloppy


\begin{vbframe}{Dimensionality Reduction Task}
\textbf{Goal}: Describe data with fewer features (reduce number of columns). \
$\Rightarrow$ there will always be an information loss.

\begin{center}
\begin{tabular}{ | c | c | c | c | c | c |}
    \hline
      & & & & & \\ \hline
      & & & & & \\ \hline
      & & & & & \\
    \hline
  \end{tabular} $\Rightarrow$
    \begin{tabular}{ | c | c | c |}
    \hline
      & &  \\ \hline
      & &  \\ \hline
      & &  \\
    \hline
  \end{tabular}
\end{center}

Unsupervised Methods:
\begin{itemize}
  \item Principle Component Analysis (PCA).
  \item Factor Analysis (FA).
  \item Feature filter methods.
\end{itemize}
\lz

Supervised Methods:
\begin{itemize}
  \item Linear Discriminant Analysis (LDA).
  \item Feature filter methods.
\end{itemize}
\end{vbframe}

\section{Principal Component Analysis}

% \begin{vbframe}{Normalizing Data}
% A variable $X$ can be normalized by substracting its values with the mean $\bar{X}$ and dividing by the standard deviation $s_X$, e.g. $\tilde{X} = \tfrac{X - \bar{X}}{s_X}.$

% \lz

% \textbf{Example:}
% <<echo = FALSE, results='hide'>>=
% bh = t(data.frame(body.height = c("Person A" = 180, "Person B" = 172, "Person C" = 175)))
% @

% Consider the following body heights measured in different units:

% <<echo=FALSE, results='asis', size='small'>>=
% bh.m = bh/100
% bh.feet = round(bh/30.48, 4)

% options(xtable.comment = FALSE)
% tab = rbind(cbind(bh, "mean" = mean(bh), "sd" = sd(bh)),
%   cbind(bh.m, "mean" = mean(bh.m), "sd" = sd(bh.m)),
%   cbind(bh.feet, "mean" = mean(bh.feet), "sd" = sd(bh.feet)))
% row.names(tab) = c("body height (cm)", "body height (m)", "body height (feet)")

% print(xtable(tab, align = c("c|", "c", "c", "c", "|c", "c")), size="\\fontsize{9pt}{10pt}\\selectfont")
% @

% After normalizing, we always obtain the normalized body height (no matter which unit was used):

% <<echo = FALSE, results='asis'>>=
% bh.norm = (bh - mean(bh))/sd(bh)
% bh.norm = cbind(bh.norm, "mean" = round(mean(bh.norm), 0), "sd" = sd(bh.norm))
% row.names(bh.norm) = "normalized body height"
% print(xtable(bh.norm, align = c("c|", "c", "c", "c", "|c", "c")), size="\\fontsize{9pt}{10pt}\\selectfont")
% @
% \end{vbframe}

% \begin{vbframe}{Normalizing Data}
% Normalizing all variables in a data set, can have several advantages:

% \begin{itemize}
%   \item It puts all variables into \textbf{comparable} units, i.e., we make sure that all normalized variables have mean 0 and standard deviation of 1.
%   \item It can avoid numerical instabilites in several algorithms, e.g. if a variable has very low / high values.
%   \item It helps in computing meaningful \textbf{distances} between observations.
% \end{itemize}
% \end{vbframe}

% \begin{vbframe}{Normalizing Data:Distances}
% There are many ways to define the distance between two points, e.g., $Z_i = (X_i, Y_i)$ and $Z_j = (X_j, Y_j)$:

% \vspace{0.5cm}

% <<echo=FALSE, fig.align="center", fig.width=6, fig.height=4, out.width="0.6\\textwidth">>=

% Colors = colorspace::rainbow_hcl(3)
% cbbPalette = c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

% getCurrentAspect = function() {
%    uy <- diff(grconvertY(1:2,"user","inches"))
%    ux <- diff(grconvertX(1:2,"user","inches"))
%    uy/ux
% }

% par(mar = c(4,4,0,0))
% plot(x = c(1L,5L), y = c(1L,4L), ylim = c(0,5), xlim = c(0,6), pch = 19,
%   xlab = "Variable X (Dimension 1)", ylab = "Variable Y (Dimension 2)")
% lines(x = c(1L,5L), y = c(1L,4L))
% text(x = c(1L,5L), y = c(1L,4L), c(expression(Z[i]), expression(Z[j])), adj = c(1.5, 0))
% lines(x = c(1L, 5L, 5L, 5L), y = c(1L, 1L, 1L, 4L), col = Colors[1])
% legend("topleft", lty = 1, legend = c("manhattan", "euclidean"), col = c(Colors[1],1))

% text(x = 5, y = 1, expression(d(Z[i],Z[j])~"= |5-1| + |4-1| = 7"), adj = c(1,1), col = Colors[1])

% asp = getCurrentAspect()
% text(x = 3, y = 2.5, expression(d(Z[i],Z[j])~"="~sqrt((5-1)^2 + (4-1)^2)~"= 5"),
%   adj = c(0.5,0), col = 1, srt = 180/pi*atan(3/4*asp))
% @

% \vspace{0.5cm}
% \begin{itemize}
%   \item\small manhattan: sum up the absolute distances in each dimension.
%   \item\small euclidean: remember Pythagoras theorem from school?
% \end{itemize}
% \end{vbframe}

% \begin{vbframe}{Normalizing Data:Distances}
% It is often a good idea to \textbf{normalize} the data before computing distances, especially when the scale of variables is different, e.g. the euclidean distance between the point $Z_1$ and $Z_2$:

% <<echo=FALSE, fig.align="center", fig.height=4, fig.width=9>>=
% par(mar = c(3.5,3.5,1,1), mfrow = c(1,2), mgp = c(2.5,1,0))

% dat = data.frame(shoe.size = c(46, 40, 44), height = c(180, 172, 175))
% plot(x = dat$shoe.size, y = dat$height, xlab = "shoe size", ylab = "body height (in cm)",
%   pch = 19, xlim = range(dat$shoe.size)*c(0.95, 1.05), ylim = range(dat$height)*c(0.98, 1.02))
% lines(x = dat$shoe.size[-3], y = dat$height[-3])
% asp = getCurrentAspect()
% text(x = mean(dat$shoe.size[-3]), y = mean(dat$height[-3]),
%   bquote(paste(d(Z[1],Z[2])~"="~sqrt((46-40)^2 + (180-172)^2)~" = ", .(sqrt((46-40)^2 + (180-172)^2)))),
%   adj = c(0.5,-0.25), col = 1, srt = 180/pi*atan(diff(dat$height[-3])/diff(dat$shoe.size[-3])*asp))
% text(x = dat$shoe.size, y = dat$height, c(expression(Z[2]), expression(Z[1]), expression(Z[3])), adj = c(-1, 0.5))

% dat$height = dat$height/100
% plot(x = dat$shoe.size, y = dat$height, xlab = "shoe size", ylab = "body height (in m)",
%   pch = 19, xlim = range(dat$shoe.size)*c(0.95, 1.05), ylim = range(dat$height)*c(0.98, 1.02))
% lines(x = dat$shoe.size[-3], y = dat$height[-3])
% asp = getCurrentAspect()
% text(x = mean(dat$shoe.size[-3]), y = mean(dat$height[-3]),
%   bquote(paste(d(Z[1],Z[2])~"="~sqrt((46-40)^2 + (1.80-1.72)^2)~" = ", .(sqrt((46-40)^2 + (1.80-1.72)^2)))),
%   adj = c(0.5,-0.25), col = 1, srt = 180/pi*atan(diff(dat$height[-3])/diff(dat$shoe.size[-3])*asp))
% text(x = dat$shoe.size, y = dat$height, c(expression(Z[2]), expression(Z[1]), expression(Z[3])), adj = c(-1, 0.5))
% @

% \vspace{0.5cm}

% On the right plot, the distance is dominated by \code{shoe size}.
% \end{vbframe}

% \begin{vbframe}{Normalizing Data: Distances}
% \small
% The normalized variable $\tilde{X}_{\code{shoe.size}}$ is computed by
% \[\tilde{X}_{\code{shoe.size}} = \tfrac{X_{\code{shoe.size}}-\bar{X}_{\code{shoe.size}}}{s_{X_{\code{shoe.size}}}}.\]
% Distances based on normalized data are better comparable and \textbf{robust} in terms of linear transformations (e.g., conversion of physical units).


% \vspace{0.5cm}

% <<echo=FALSE, fig.align="center", fig.height=4, fig.width=6, out.width="0.6\\textwidth">>=
% par(mar = c(3.5,3.5,0.1,0.1), mgp = c(2.5,1,0))

% #dat = data.frame(shoe.size = c(45, 40, 42), height = 1000*c(85, 70, 72))
% dat = as.data.frame(scale(dat))
% plot(x = dat$shoe.size, y = dat$height, xlab = "normalized shoe size", ylab = "normalized body height",
%   pch = 19, xlim = range(dat$shoe.size)*c(1.1, 1.2), ylim = range(dat$height)*c(1.1, 1.1))
% lines(x = dat$shoe.size[-3], y = dat$height[-3])
% asp = getCurrentAspect()
% text(x = mean(dat$shoe.size[-3]), y = mean(dat$height[-3]),
%   bquote(paste(d(Z[1],Z[2])~" = ", .(sqrt((dat$shoe.size[1]-dat$shoe.size[2])^2 + (dat$height[1]-dat$height[2])^2)))),
%   adj = c(0.5,0), col = 1, srt = 180/pi*atan(diff(dat$height[-3])/diff(dat$shoe.size[-3])*asp))
% text(x = dat$shoe.size, y = dat$height, c(expression(Z[2]), expression(Z[1]), expression(Z[3])), adj = c(-1, 0.5))
% @
% \end{vbframe}

% \begin{vbframe}{Normalizing: Covariance vs. Correlation}
% The \textbf{variance} of a normalized variable is always 1, its mean is always 0.

% \vspace{0.5cm}

% The \textbf{covariance} of two normalized variables $\tilde{X} = \tfrac{X - \bar{X}}{s_X}$ and $\tilde{Y} = \tfrac{Y - \bar{Y}}{s_Y}$ is the same as the \textbf{correlation} of the non-normalized variables $X$ and $Y$.

% \vspace{0.5cm}

% One can proof this with the help of
% $$s_{\tilde{X}\tilde{Y}}=\tfrac{1}{n-1}\sum_{i=1}^{n}{(\tilde{x}_i-\bar{\tilde{x}})(\tilde{y}_i-\bar{\tilde{y}})} = \hdots =  \tfrac{1}{n-1}\sum_{i=1}^{n}{\tfrac{(x_i-\bar{x})}{s_{X}}\tfrac{(y_i-\bar{y})}{s_{Y}}} = r_{XY}.$$

% <<include=FALSE, echo=FALSE, message=FALSE>>=
% library("knitr")
% set.seed(1)
% options(scipen = 1, digits = 4, width=70)
% Colors = colorspace::rainbow_hcl(3)
% cbbPalette = c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
% @
% \end{vbframe}

\begin{vbframe}{PCA Intuition}
\textbf{Motivational example I}:

\begin{itemize}
  \item Variable $x_1$ explains most of the variation.
  \item Variable $x_2$ has a lower variance than $x_1$.
  \item If we disregard $x_2$ and project the points into the 1-dimensional space of $x_1$, we do not lose much information w.r.t. variability.
\end{itemize}

\begin{center}
  \includegraphics[width = \textwidth]{figure_man/pca1.png}
\end{center}
\end{vbframe}

\begin{vbframe}{PCA Intuition}
\textbf{Motivational example II}:

\begin{itemize}
\item $x_1$ and $x_2$ are correlated and have similar variances.
\item Find a new orthogonal axes (e.g. PC1 and PC2), where PC1 explains most of the variation.
\item Rotate the points and consider PC1 and PC2 as new coordinate system (situation as in the previous example).
\item We can now project points onto PC1 and disregard PC2 (hopefully without losing much information).
\end{itemize}

\begin{center}
  \includegraphics[width = \textwidth]{figure_man/pca2.png}
\end{center}
\vspace{-20pt}
\end{vbframe}

\begin{vbframe}{PCA Intuition}
\textbf{General procedure}:

\begin{enumerate}
  \item Rotate the original $p$-dimensional coordinate system until the first PC that explains most of the variation is found.
  \item Fix the first PC and proceed with rotating the remaining $p-1$ coordinates until the second PC (which is orthogonal to the first PC) is found that explains most of the \textbf{remaining} variation, etc.
  \item We can reduce the dimensions by projecting the points onto the first, say $k<p$, PC.
\end{enumerate}
\end{vbframe}

\begin{vbframe}{PCA Intuition: Find first PC}
<<echo=FALSE, fig.height=6, fig.width=6, results="asis", out.width="0.6\\textwidth">>=
par(mar=c(0,0,3,0))
Colors = colorspace::rainbow_hcl(3)
# code from https://github.com/genomicsclass/labs/blob/master/highdim/PCA.Rmd
# mypar()
n = 50
col = Colors[1]
set.seed(123)

Y=t(mvrnorm(n,c(0,0), matrix(c(1,0.9,0.9,1),2,2)))

thelim = c(-3,3)
#par(xaxs='i',yaxs='i',mar=c(1,1,3,3)+0.1)
plot(Y[1,], Y[2,], xlim=thelim, ylim=thelim, axes=F ,ann=F)
#box()
arrows(c(0,thelim[1]),c(thelim[1],0),c(0,thelim[2]),c(thelim[2],0),0.05); ## draw custom axes
mtext(expression(x[1]),3,-1,at=0,las=1); ## y label
mtext(expression(x[2]),4,-0.75,at=0,las=1); ## x label

for (rotate in c(0,-0.5,-1, -3, -300, 3, 1)) {

  cat("  \n")
  cat("  \\end{vbframe}  \n  \\begin{vbframe}{PCA Intuition: Find first PC}  \n  \\addtocounter{framenumber}{-1}  \n")

  u = matrix(c(1,rotate),ncol=1)
  u = u/sqrt(sum(u^2))
  w=t(u)%*%Y

  par(mar=c(0,0,3,0))
  plot(t(Y), main=paste("Variance of projected points:",round(tcrossprod(w)/(n-1),2) ),
    xlim=thelim, ylim=thelim, axes=F, xlab = "", ylab = "")
  #box()
  arrows(c(0,thelim[1]),c(thelim[1],0),c(0,thelim[2]),c(thelim[2],0),0.05); ## draw custom axes
  mtext(expression(x[1]),3,-1,at=0,las=1); ## y label
  mtext(expression(x[2]),4,-0.75,at=0,las=1); ## x label
  #abline(h=0,lty=2)
  #abline(v=0,lty=2)
  abline(0,rotate,col=col)
  abline(0,ifelse(rotate == 0, 300, -1/rotate),col=col)
  Z = u%*%w
  for(i in seq(along=w))
    segments(Z[1,i],Z[2,i],Y[1,i],Y[2,i],lty=2)
  points(t(Z), col=col, pch=16, cex=0.8)
  legend("topleft", lty = c(1,2, NA), pch = c(NA,NA,16), legend = c("rotated coordinate system", "original coordinate system", "projected points"), col = c(col, "black", col))
}
text(3, 3, labels = "PC1", col = col, pos = 1)
text(3, -3, labels = "PC2", col = col, pos = 3)
@
\end{vbframe}

\begin{vbframe}{PCA Intuition: Reduce dimensionality}
Rotate the points and use PC1 and PC2 as new coordinate system.
\lz
Here, the PC1 axis explains most of the variance:

<<echo = FALSE, fig.height=6, fig.width=6, out.width="0.55\\textwidth">>=
a = pi/4
Yrotated = t(Y) %*% matrix(c(cos(a), -sin(a), sin(a), cos(a)), ncol = 2, byrow = T)
mypar(1,1)
plot(Yrotated,
  xlim = c(-3,3), ylim = c(-3,3), xlab = "PC1", ylab = "PC2", axes=F ,ann=F)
#box()
arrows(c(0,thelim[1]),c(thelim[1],0),c(0,thelim[2]),c(thelim[2],0),0.05, col = col, lwd = 1.5); ## draw custom axes
mtext("PC2",3,-1,at=0,las=1, col = col, cex = 0.8); ## y label
mtext("PC1",4,-1,at=0,las=1, col = col, cex = 0.8); ## x label
@
\end{vbframe}

\begin{vbframe}{PCA Intuition: Reduce dimensionality}
Dimensionality can be reduced by projecting the points onto the PC1 (and by disregarding PC2).
The hope is that we won't lose much information this way.

<<echo = FALSE, fig.height=6, fig.width=6, out.width="0.55\\textwidth">>=
mypar(1,1)
plot(Yrotated,
  xlim = c(-3,3), ylim = c(-3,3), xlab = "PC1", ylab = "PC2", axes=F ,ann=F)
#box()
arrows(c(0,thelim[1]),c(thelim[1],0),c(0,thelim[2]),c(thelim[2],0),0.05, col = col, lwd = 1.5); ## draw custom axes
mtext("PC2",3,-1,at=0,las=1, col = col, cex = 0.8); ## y label
mtext("PC1",4,-1,at=0,las=1, col = col, cex = 0.8); ## x label
#abline(h=0,lty=2, col=col)
#abline(v=0,lty=2, col=col)
u = matrix(c(1,0),ncol=1)
u = u/sqrt(sum(u^2))
wrotated = t(u)%*%t(Yrotated)
Zrotated = u%*%wrotated
for(i in seq(along=w))
  segments(Zrotated[1,i], Zrotated[2,i], t(Yrotated)[1,i], t(Yrotated)[2,i], lty=2)
points(t(Zrotated), col=col, pch=16, cex=0.8)

# plot(t(Zrotated), col=col, pch=16, cex=0.8, xlab = "PC1", ylab = "PC2", axes=F ,ann=F)
# arrows(c(0,thelim[1]),c(thelim[1],0),c(0,thelim[2]),c(thelim[2],0),0.05, col = col, lwd = 1.5); ## draw custom axes
# mtext("PC2",3,-1,at=0,las=1, col = col, cex = 0.8); ## y label
# mtext("PC1",4,-1,at=0,las=1, col = col, cex = 0.8); ## x label
@
\end{vbframe}

\begin{vbframe}{PCA Intuition: Summary}
\textbf{Idea:} Transform an original set of correlated metric variables to a new set of uncorrelated (orthogonal) metric variables, called principal components (PC), that explain the variability in the data.

\vspace{0.5cm}

  \begin{itemize}
    \item The objective is to investigate if only a few PCs account for most of the variability in the original data.
    \item If the objective is fulfilled, we can use fewer PCs to reduce the dimensionality.
    \item The PCs remove collinearity of the input variables as they are orthogonal to each other.
  \end{itemize}
\end{vbframe}

\begin{vbframe}{PCA Intuition: Final Remarks}
  \begin{itemize}
    \item PCA is used for dimensionality reduction by disregaring dimensions with lower variability.
    \item There is always an information loss, especially for other criteria.
    \item E.g., dimensionality reduction can worsen the classification accuracy when the task is to classify two groups:
  \end{itemize}
\begin{center}
  \includegraphics[width = \textwidth]{figure_man/pca3.png}
\end{center}
\end{vbframe}

\begin{vbframe}{Deriving the First PC Mathematically}
Aim: Find a new set of variables (PC scores) $\mathbf{pc}_1, \ldots, \mathbf{pc}_p$ based on the original data $\mathbf{X} = [\mathbf{x}_1, \hdots, \mathbf{x}_p] \in \R^{n \times p}$ so that

\begin{itemize}
  \item each PC score $\mathbf{pc}_1, \hdots, \mathbf{pc}_p$ is a linear combination of the original metric variables with coefficient weights (so-called \textbf{loading vectors}) $\mathbf{a}_1, \hdots, \mathbf{a}_p$, i.e.
    \[
    \mathbf{pc}_j = a_{j1}\mathbf{x}_1 + a_{j2}\mathbf{x}_2 + \ldots + a_{jp}\mathbf{x}_p = \mathbf{X} \mathbf{a}_j.
    \]

  \item the set is mutually uncorrelated: $Cov(\mathbf{pc}_j,\mathbf{pc}_k) = 0, \; \forall j \neq k.$

  \item the variances of the PC scores decrease:
    \[\lambda_1 > \lambda_2 > \ldots > \lambda_p, \;\;\;  \text{where } \lambda_k := Var(\mathbf{pc}_k). \]
\end{itemize}
\end{vbframe}

\begin{vbframe}{Deriving the First PC Mathematically}
%% <!-- PCA works on the covariance matrix $\Sigma$ of the data matrix $\mathbf{X} = [\mathbf{x}_1, \hdots, \mathbf{x}_p]$. -->
Let $\Sigma = \left(Cov(\xb_i, \xb_j)\right)_{ij}$


We look for the loading vector $\mathbf{a}_1 = (a_{11}, a_{21}, \hdots, a_{p1})^\top$ that maximizes the variance of $\mathbf{pc}_1$:
  \[
  \max_{\mathbf{a}_1} \ Var(\mathbf{pc}_1) = \max_{\mathbf{a}_1} Var(\mathbf{X} \mathbf{a}_1) = \max_{\mathbf{a}_1} \mathbf{a}_1^\top \Sigma \mathbf{a}_1
  \]
subject to the normalization constraint $\mathbf{a}_1^\top \mathbf{a}_1 = \sum_{k=1}^p a_{k1}^2 = 1$.

\lz
The constraint is required for identifiability reasons, otherwise we could maximize the variance by just increasing the values in $\mathbf{a}_1$.

\lz
Repeat this maximization step for the other PCs and additionally use the orthogonality constraint, i.e. for the second PC: $$\mathbf{a}_2^\top\mathbf{a}_1 = 0.$$
\end{vbframe}

% \begin{vbframe}{Exkurs: Hauptkomponentenanalyse (PCA)}

% \textbf{Gegeben: } $n$ Datenpunkte mit jeweils $p$ Merkmalen $^{(*)}$

% \lz

% \textbf{Ziel: } Projektion der $n$ Datenpunkte in einen $k$-dimensionalen Raum ($k < p$) mit möglichst wenig Informationsverlust

% \lz

% \textbf{Idee: }

% \begin{itemize}
% \item Finde eine lineare Abbildung $f: \R^p \to \R^k$, die jeden Datenpunkt $\bm{x}$ auf einen $k$-dimensionalen Punkt $\bm{z}$ abbildet.
% \item Verliere dabei so wenig wie möglich Information.
% \item Möglichst wenig Information geht verloren, wenn wir den Punkt $\bm{z}$ möglichst gut rekonstruieren können, d. h. wir können eine lineare Funktion $h: \R^k \to \R^p$ finden, sodass $\bm{x} \approx h(\bm{z})$.
% \end{itemize}

% \vfill

% $^{(*)}$ Wir nehmen an, die Datenpunkte sind um $0$ zentriert.

% \framebreak

% \begin{center}
%   \includegraphics[width = 0.8\textwidth]{figure_man/PCA-as-autoencoder.png}
% \end{center}

% \vspace*{-1cm}

% Die linearen Abbildungen $f, h$ beschreiben wir durch die Multiplikation mit Matrizen: $f: \bm{x}^T \mapsto \bm{x}^\top \bm{F} =: \bm{z}$ und $h: \bm{z}^\top \mapsto \bm{z}^\top \bm{H}$

% \framebreak

% \textbf{Ziel: } Minimiere Rekonstruktionsfehler zwischen Daten $\bm{X}$ und den projezierten und rekonstruierten Daten $\bm{XFH}$.

% $$
% \min_{\bm{F} \in \R^{m \times k}, \bm{H} \in \R^{k \times n}} \|\mathbf{X} - \bm{X}\bm{F}\bm{H}\|^2_F.
% $$

% Definieren wir $\bm{X\bm{F}} =: \bm{W}$, so entspricht das gerade dem Problem der Matrixapproximation. Die Lösung ist somit gegeben durch

% \begin{eqnarray*}
% \bm{W} &=& \bm{XF} = \mathbf{U}_k (\boldsymbol{\Sigma}_k)^{1/2} \\
% \bm{H} &=& (\boldsymbol{\Sigma}_k)^{1/2} \mathbf{V}_k^\top,
% \end{eqnarray*}

% wobei die Matrizen $\bm{U}_k \boldsymbol{\Sigma}_k \bm{V}_k$ der trunkierten Singulärwertzerlegung von $\bm{X}$ entsprechen.

% \framebreak

% Die Spalten von $\textbf{W}$ entsprechen den Hauptkomponenten (transformierte Variablen), die Zeilen von $\bm{H}$ sind die Hauptachsen.

% \lz

% Die Matrix $\bm{F}$, die die Funktion definiert, mithilfe derer die Punkte in den niedrigdimensionalen Raum projeziert werden, lässt sich herleiten

% \begin{eqnarray*}
% \bm{W} = \bm{XF} &=& \mathbf{U}_k (\boldsymbol{\Sigma}_k)^{1/2} \\
% \bm{U}_k \boldsymbol{\Sigma}_k \bm{V}_k \bm{F} &\approx& \mathbf{U}_k (\boldsymbol{\Sigma}_k)^{1/2} \qquad |  \bm{V}_k^{-1}\boldsymbol{\Sigma}_k^{-1}\bm{U}_k^{-1} \cdot \\
% \bm{F} &=& \bm{V}_k^T \left(\boldsymbol{\Sigma}_k\right)^{1 / 2}
% \end{eqnarray*}

% (was einer Drehung $\bm{V}_k^T$ und einer Skalierung $\left(\boldsymbol{\Sigma}_k\right)^{1 / 2}$ entspricht).

% \end{vbframe}

\begin{vbframe}{Example: The Olympic Heptathlon Data}
The \code{heptathlon} data set in the R package \pkg{HSAUR3} contains the competition results of 25 athletes in 7 disciplines for the Olympics held in Seoul in 1988.

<<message=FALSE, out.width="0.5\\textwidth", eval = 2, echo=FALSE>>=
install.packages("HSAUR3")
data(heptathlon, package = "HSAUR3")
@

\lz

\textbf{Aim}: Rank the athletes according to their overall performance in all 7 disciplines.

\lz

\textbf{Idea}: Use PCA to reduce the dimensionality (i.e., reduce the results of the 7 disciplines to one dimension) and compare the scores of the first PC with the official scores.
\end{vbframe}

\begin{vbframe}{Example: The Olympic Heptathlon Data}
Variables of the \code{heptathlon} data:

\begin{itemize}
  \item \code{hurdles}: results 100m hurdles (in seconds).
  \item \code{highjump}: results high jump (in m).
  \item \code{shot}: results shot putt (in m).
  \item \code{run200m}: results 200m race (in seconds).
  \item \code{longjump}: results long jump (in m).
  \item \code{javelin}: results javelin (in m).
  \item \code{run800m}: results 800m race (in seconds).
  \item \code{score}: total score of the official scoring system.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Example: The Olympic Heptathlon Data}
The variables \code{hurdles}, \code{run200m} and \code{run800m} are time measurements, i.e. low values are better.
For all other variables high values are better.

\lz

Results of the best and worst participant:

\vspace{0.5cm}

<<size='tiny', R.options=list(width = 200), echo = FALSE>>=
kable(heptathlon[c(1,25),], "latex", booktabs = TRUE)
@

\lz

We use negative time measurements so that higher values are better and therefore all variables have the same direction:

\vspace{0.5cm}

<<size='tiny', echo = FALSE>>=
heptathlon$hurdles = -heptathlon$hurdles #with(heptathlon, max(hurdles)-hurdles)
heptathlon$run200m = -heptathlon$run200m #with(heptathlon, max(run200m)-run200m)
heptathlon$run800m = -heptathlon$run800m #with(heptathlon, max(run800m)-run800m)
@

<<size='tiny', R.options=list(width = 200), echo = FALSE>>=
kable(heptathlon[c(1,25),], "latex", booktabs = TRUE)
@
\end{vbframe}

\begin{vbframe}{Scatter Plot Matrix}
<<fig.align="center", out.width = "0.95\\textwidth", echo = FALSE, message=FALSE>>=
#plot(heptathlon[, -8], pch = 19)
ggpairs(heptathlon[, -8]) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
@
\end{vbframe}

\begin{vbframe}{Correlation Matrix}
We can compute all pairwise correlations of the variables (without the \code{score} column):

<<echo=FALSE>>=
kable((cor.mat = cor(heptathlon[, -8])), "latex", booktabs = TRUE) %>%
  kable_styling(latex_options = c("striped", "scale_down"))
@
\end{vbframe}

\begin{vbframe}{Correlogram}
\lz
<<message=FALSE, fig.align="center", fig.height = 5, out.width = "\\textwidth", echo = FALSE>>=
#par(mfrow = c(1, 2)) #  tl.srt = 60
library(corrplot)
body(corrplot)[[57]] = substitute(oldpar <- par(mar = mar))
corrplot(cor.mat, type = "upper", addCoef.col = "black")
@
\end{vbframe}

\begin{vbframe}{Performing the PCA}
Remember: The first PC is the linear combination
  \[
  \mathbf{pc}_1 = a_{11} \mathbf{x}_1 + a_{12}\mathbf{x}_2+ \dots + a_{1p}\mathbf{x}_p
  \]
and has the largest sample variance among all other PCs.
%% <!-- - In R, PCA can be done using the functions \code{princomp()} and \code{prcomp()} (both contained in the R package \code{stats}). -->

\begin{itemize}
  % \item In R, the \code{princomp()} function carries out a PCA via an eigendecomposition of the sample covariance matrix $\Sigma$.
  \item If variables are on very different scales, PCA should be carried out on the correlation matrix (which is equivalent to the correlation matrix if normalized variables are used).
  \item As the variables of the heptathlon data are on different scales, we perform the PCA based on the correlation matrix.
  \item Alternatively, we could also perform the PCA based on the covariance matrix but on the normalized heptathlon data.
\end{itemize}

<<echo=FALSE, results="hide">>=
hept.pca = princomp(heptathlon[, -8], cor = TRUE)
@

% \lz

% The resulting object of \code{princomp()} contains at least:

% \begin{itemize}
%   \item The loadings $\mathbf{a}_{1}, \hdots, \mathbf{a}_{p}$,
%   \item The PC scores $\mathbf{pc}_{1}, \hdots, \mathbf{pc}_{p}$ and
%   \item The variance $\lambda_1, \hdots, \lambda_p$ (or standard deviation) of the PC scores.
% \end{itemize}
\end{vbframe}

\begin{vbframe}{Loadings}
The loadings $\mathbf{a}_{1}, \hdots, \mathbf{a}_{p}$ are given by

% <<eval = FALSE, echo=TRUE>>=
% hept.pca$loadings
% @

\vspace{1cm}

<<echo = FALSE>>=
kable(unclass(round(hept.pca$loadings, 2)), "latex", booktabs = TRUE)%>%
  kable_styling(latex_options = c("striped", "scale_down"))
@
\end{vbframe}

\begin{vbframe}{Loadings}
Visualize the coefficient weights (loadings) of the linear combinations of the PC scores:

\lz

<<message=FALSE, fig.align="center", fig.height = 5, out.width = "0.8\\textwidth", echo=FALSE>>=
corrplot(hept.pca$loadings, is.corr = FALSE)
@
\end{vbframe}

% \begin{vbframe}{Loadings}
% Check the normalization constraint of the first PC:
% <<size = "scriptsize", echo = TRUE>>=
% (a1 = hept.pca$loadings[, 1]) # loadings of the first PC
% a1 %*% a1 # check constraint: is sum of squares equal to 1?
% @

% Check the orthogonality constraint of the first two PCs:
% <<size = "scriptsize", echo = TRUE>>=
% (a2 = hept.pca$loadings[, 2]) # loadings of the second PC
% a1 %*% a2 # check if 1st and 2nd PC are orthogonal
% @
% \end{vbframe}

\begin{vbframe}{Loadings}
If we perform a PCA on the covariance matrix (without normalizing the data), each component mainly loads on a single variable:

\vspace{0.3cm}

<<fig.align="center", fig.height = 5, out.width = "0.8\\textwidth", echo = FALSE>>=
par(mar = c(0,0,1,0))
hept.pca.cov = princomp(heptathlon[, -8], cor = FALSE)
corrplot(hept.pca.cov$loadings, is.corr = FALSE)
@

\vspace{-0.2cm}

Reason: Variables have very different scales (e.g., time measurement of 200m and 800m run).
\end{vbframe}


\begin{vbframe}{Proportion of Explained Variance}
\begin{itemize}
  \item The total variance of the $p$ PC scores is equal the total variance of the original variables, i.e.,
    $$\textstyle\sum_{j=1}^p \lambda_j = s_1^2 +s_2^2 + \dots + s_p^2,$$
    where $\lambda_j$ is the variance of the $j$th PC and $s_j^2$ is the sample variance of variable $\mathbf{x}_j$.
  \item The proportion of explained variance of the $j$-th PC is $$\tfrac{\lambda_j}{\sum_{j=1}^p \lambda_j}.$$
  \item The first $k$ PCs account for a proportion $$\tfrac{\sum_{j=1}^k \lambda_j}{\sum_{j=1}^p \lambda_j}.$$
\end{itemize}
\end{vbframe}

\begin{vbframe}{Proportion of Explained Variance}
In the example above, the proportion of explained variance is given by

<<echo=FALSE>>=
pca_importance = function(x) {
  vars = x$sdev^2
  vars = vars/sum(vars)
  rbind("Standard deviation" = x$sdev,
    "Proportion of Variance" = vars,
    "Cumulative Proportion" = cumsum(vars)
  )
}

kable(pca_importance(hept.pca), "latex", booktabs = TRUE) %>%
  kable_styling(latex_options = c("striped", "scale_down"))
@

\textbf{Question}: How do we choose the number of PCs?
\end{vbframe}

\begin{vbframe}{Choosing the Number of PCs}
Two simple rules of thumb for choosing the number of PCs:
\begin{enumerate}
\item Retain the first $k$ components, which explain a large proportion of the total variation, e.g., 70-80\%.

\item Use a scree plot: Plot the component variances vs. the component number and look for an \textbf{elbow}.
For components after the \textbf{elbow}, the variance decreases more slowly.
\end{enumerate}
%% <!-- 3. If the correlation matrix was used for the PCA: Retain only the components with variances greater than one. -->

\lz

<<fig.align="center", fig.height = 3, fig.width = 6, out.width = "0.75\\textwidth">>=
par(mar = c(0,0,0,0))
fviz_eig(hept.pca, xlab = "principal component")
@
\end{vbframe}

\begin{vbframe}{PC Scores vs. Official Scores}
The first PC explains $63,72\%$ of the variation, the loadings of the first PC are:

\lz

<<echo = FALSE>>=
kable(t(hept.pca$loadings[,1, drop = FALSE]), "latex", booktabs = TRUE)%>%
  kable_styling(latex_options = c("striped", "scale_down"))
@

\lz

Dimensionality reduction:

\begin{itemize}
  \item Project all 8 features onto the first PC.
  \item Compare the scores of the first PC with the official scores used to rank the athletes.
\end{itemize}
\end{vbframe}

\begin{vbframe}{PC Scores vs. Official Scores}
The scores of the first PC $\mathbf{pc}_1$ have a similar ranking as the scores of the official scoring system, i.e., we can reduce the dimension to the first PC without losing much information:

\lz

<<fig.align="center", fig.width = 7, fig.height = 4.5, out.width= "0.75\\textwidth", echo = FALSE>>=
d = data.frame(official.score = heptathlon$score, pca.score = hept.pca$scores[row.names(heptathlon),1], name = row.names(heptathlon))
ggplot(d, aes(official.score, pca.score, label = name)) +
    geom_text_repel() +
    geom_point(color = 'red') + ylab("1st PC scores") + xlab("Official Scores")
# par(mar = c(4,4,1,1))
# #cor(heptathlon$score, hept.pca$scores[,1])
# plot(heptathlon$score, hept.pca$scores[,1], xlab = "Official Scores", ylab = "1st PC scores")
# text(heptathlon$score, hept.pca$scores[,1], row.names(heptathlon), pos = 4, cex = 0.5)
@
\end{vbframe}

\endlecture
