% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
library(MASS)
library(cluster)
library(pdist)
@

% Load all R packages and set up knitr
<<setup, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{Clustering}
\lecture{Introduction to Machine Learning}
\sloppy


\section{Motivation}

\begin{vbframe}{Supervised vs. Unsupervised Learning}

\textbf{Supervised Machine Learning:}
\begin{itemize}
  \item Supervised machine learning deals with \textbf{labeled} data, i.e., we have input data $\xb$ and the outcome $y$ of past events.
  \item Here, the aim is to learn relationships between $\xb$ and $y$.
\end{itemize}

\textbf{Unsupervised Machine Learning:}
\begin{itemize}
  \item Unsupervised machine learning deals with data that is \textbf{unlabeled}, i.e., there is no real output $y$.
  \item Here, the aim is to search for patterns within the inputs $\xb$.
\end{itemize}

\end{vbframe}

\begin{vbframe}{Motivation for Clustering}
Consider multivariate data with $n$ observations (e.g. customers) and $p$
features (e.g. characteristics of customers).

\vspace*{0.2cm}

\textbf{Task}: divide data into groups (clusters), such that

\begin{itemize}
  \item the observations in each cluster are as "similar" as possible (homogeneity
  within each cluster), and
  \item the clusters are as "far away" as possible from other clusters (heterogeneity
  between different clusters).
\end{itemize}

\vspace*{-0.3cm}

<<echo=FALSE, fig.height=3>>=
  df4 = getTaskData(iris.task)
  m = as.matrix(cbind(df4$Petal.Length, df4$Petal.Width), ncol = 2)
  cl = (kmeans(m,3))
  df4$cluster = factor(cl$cluster)
  centers = as.data.frame(cl$centers)
  ggplot(data = df4, aes(x = Petal.Length, y = Petal.Width, color = cluster)) +
   geom_point(size = 4) +
   geom_point(data = centers, aes(x = V1, y = V2, color = "red")) +
   geom_point(data = centers, aes(x = V1,y = V2, color = "red"), size = 90, alpha = .3) +
   theme(legend.position = "none")
@

\end{vbframe}

\begin{vbframe}{Clustering vs. Classification}
\begin{itemize}
  \item In classification, the groups are known and we try to learn what
  differentiates these groups (i.e., learn a classification function) to
  properly classify future data.
  \item In clustering, we look at data, where groups are unknown and try to find
  similar groups.
\end{itemize}

\lz

Why do we need clustering?

\lz

  \begin{itemize}
    \item Discovery: looking for new insights in the data (e.g. finding groups of customers that buy a similar product).
    \item Derive a reduced representation of the full data set.
  \end{itemize}
\end{vbframe}


\begin{vbframe}{Clustering: Customer Segmentation}
\begin{itemize}
  \item In marketing, customer segmentation is an important task to understand customer needs and to meet with customer expectations.
  \item Customer data is partitioned in terms of similarities and the characteristics of each group are summarized.
  \item Marketing strategies are designed and prioritized according to the group size.
\end{itemize}

% \lz
% Example Use Cases:

% \begin{itemize}
%   \item Personalized ads (e.g., recommend articles).
%   \item Music/Movie recommendation systems.
% \end{itemize}
% \end{vbframe}

% \begin{vbframe}{Clustering: Image Compression}
% \begin{itemize}
%   \item An image consists of pixels arranged in rows and columns.
%   \item Each pixel contains \textbf{RGB} color information, i.e., a mix of the intensity of 3 \textbf{primary colors}: \textbf{R}ed, \textbf{G}reen and \textbf{B}lue.
%   \item Each primary color takes intensity values between 0 and 255.
% \end{itemize}

% \begin{center}
% \includegraphics[width=0.6\textwidth]{figure_man/rgb-cube.png}

% \tiny Source: By Ferlixwangg \href{https://creativecommons.org/licenses/by-sa/4.0}{CC BY-SA 4.0}, from \href{https://commons.wikimedia.org/wiki/File:Rgb-cube.gif}{Wikimedia Commons}.
% \end{center}
% \end{vbframe}

% \begin{vbframe}{Clustering: Image Compression}
% An image can be compressed by reducing its color information, i.e., by replacing similar colors of each pixel with, say, $k$ distinct colors.

% \vspace{0.2cm}

% \textbf{Example}:

<<echo = FALSE, fig.height=5, fig.width=10, include = FALSE>>=
library(jpeg)

img = readJPEG("figure_man/colorful_bird.jpg") # Read the image
imgDm = dim(img)
# We re-arranged the 3D array into a dataset that has the coordinates of the pixel and the color information (R, G and B)
imgRGB = data.frame(
  x = rep(1:imgDm[2], each = imgDm[1]),
  y = rep(imgDm[1]:1, imgDm[2]),
  R = as.vector(img[,,1]),
  G = as.vector(img[,,2]),
  B = as.vector(img[,,3])
)

par(mfrow = c(1,2), mar = c(0,0,2,0))
# We now plot the pixels (pch = 15) with its RGB color info
plot(imgRGB$x, imgRGB$y, col = rgb(imgRGB[c("R", "G", "B")]), pch = 15, axes = F, main = "Original Image", xlab = "", ylab = "")
box()

k = 16
kMeans = kmeans(imgRGB[, c("R", "G", "B")], centers = k)
predRGB = kMeans$centers[kMeans$cluster,]

plot(imgRGB$x, imgRGB$y, col = rgb(predRGB), pch = 15, axes = F, main = "Image using 16 Colors", xlab = "", ylab = "")
box()
@
\end{vbframe}




\section{Hierarchical Clustering}

\begin{vbframe}{Hierarchical Clustering}
Hierarchical clustering is a recursive process that builds a hierarchy of clusters.
We distinguish between:

\lz

\begin{enumerate}
  \item Agglomerative (or bottom-up) clustering:
  \begin{itemize}
    \item Start: Each observations is an \textbf{individual cluster}.
    \item Repeat: Merge the two closest clusters.
    \item Stop when there is only one cluster left.
  \end{itemize}
  \item Divisive (or top-down) clustering:
  \begin{itemize}
    \item Start: All observations are within \textbf{one} cluster.
    \item Repeat: Divide the cluster that results in two clusters with biggest distance.
    \item Stop when each observation is an individual cluster.
  \end{itemize}
\end{enumerate}
\end{vbframe}

\begin{vbframe}{Hierarchical Clustering}
Let $\xdat$ be $n$ observations of $p$ features (dimensions), where $\xi = \xivec$.
A data set $\D$ is a ($n$ $\times$ $p$)-matrix of the form:

\lz

\begin{table}[]
\begin{tabular}{ccccc}
         & feature $1$ &  $\hdots$ & $\hdots$ & feature $p$ \\
\hline
$\xb^{(1)}$    & $x^{(1)}_1$    &  $\hdots$ & $\hdots$ & $x^{(1)}_p$    \\
$\vdots$ &  $\vdots$   &  $\vdots$ & $\vdots$ & $\vdots$    \\
$\xb^{(n)}$    & $x^{(n)}_1$    &  $\hdots$ & $\hdots$ & $x^{(n)}_p$ \\
\hline
\end{tabular}
\end{table}
\end{vbframe}

\begin{vbframe}{Hierarchical Clustering}

Hierarchichal clustering requires a definition for

\begin{itemize}
  \item distances $d(\xi, \xb^{(j)})$ between two observations $\xi$ and $\xb^{(j)}$:
  \begin{itemize}
    \item manhattan distance:
      $$d(\xi,\xb^{(j)})= ||\xi - \xb^{(j)}||_1 = \sum_{k=1}^p\left|x^{(i)}_k-x^{(j)}_k\right|$$
    \item euclidean distance:
      $$d(\xi,\xb^{(j)})= ||\xi - \xb^{(j)}||_2 = \sqrt{\sum_{k=1}^p\left(x^{(i)}_k-x^{(j)}_k\right)^2}$$
  \end{itemize}
  \item distances between two clusters (called linkage).
\end{itemize}
\end{vbframe}

\begin{vbframe}{Distances between Observations}
<<echo=FALSE, results='hide'>>=
Colors = c("#000000", "#E69F00", "#56B4E9", "#009E73",
           "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
palette(Colors)
@

<<echo=FALSE, fig.align="center", fig.width=6, fig.height=4, out.width="0.7\\textwidth">>=
getCurrentAspect = function() {
   uy <- diff(grconvertY(1:2,"user","inches"))
   ux <- diff(grconvertX(1:2,"user","inches"))
   uy/ux
}

par(mar = c(4,4,0,0))
plot(x = c(1L,5L), y = c(1L,4L), ylim = c(0,5), xlim = c(0,6), pch = 19,
  xlab = "Dimension 1", ylab = "Dimension 2")
lines(x = c(1L,5L), y = c(1L,4L))
text(x = c(1L,5L), y = c(1L,4L), c(expression(x^(1)), expression(x^(2))), adj = c(1.5, 0))
lines(x = c(1L, 5L, 5L, 5L), y = c(1L, 1L, 1L, 4L), col = 2)
legend("topleft", lty = 1, legend = c("manhattan", "euclidean"), col = c(2,1))

text(x = 5, y = 1, expression(d(x^(1),x^(2))~"= |5-1| + |4-1| = 7"), adj = c(1,1), col = 2)

asp = getCurrentAspect()
text(x = 3, y = 2.5, expression(d(x^(1),x^(2))~"="~sqrt((5-1)^2 + (4-1)^2)~"= 5"),
  adj = c(0.5,0), col = 1, srt = 180/pi*atan(3/4*asp))
@

\begin{itemize}
  \item manhattan: sum up the absolute distances in each dimension.
  \item euclidean: remember Pythagoras theorem from school?
  \item gower: can be used for mixed variables (categorical and numeric).
\end{itemize}
\end{vbframe}

% \begin{vbframe}{Gower Distance I}
% \begin{itemize}
%   \item The Gower's metric calculates the distance between observations $\xi$ and $\xb^{(y)}$ for each feature separately and based on its data type (i.e., categorical or numeric).
%   \item For a categorical feature $X_k$, the distance between the $i$-th and the $j$-th observation $X_{ik}$ and $X_{jk}$ is defined by
%     $$s_{ijk}=\begin{cases} 0\ \text{if}\ X_{ik}=X_{jk} \\ 1\ \text{if}\ X_{ik}\neq X_{jk}.\end{cases}$$
%   \item For a numerical feature $X_k$, the distance between the $i$-th and the $j$-th observation $X_{ik}$ and $X_{jk}$ is defined by $$s_{ijk}=\frac{|X_{ik}-X_{jk}|}{\max(X_k)-\min(X_k)}, \;\;\; \text{so that } 0 \leq s_{ijk} \leq 1.$$
% \end{itemize}
% \end{vbframe}

% \begin{vbframe}{Gower Distance II}
% The Gower's metric $S$ combines all individual distances of each feature by
% $$S_{ij}=\dfrac{\sum_{k=1}^P w_{k} s_{ijk}}{\sum_{k=1}^P w_{k}},$$ where

% \begin{itemize}
%   \item $P$: number of features.
%   \item $w_k$: weight for feature $k$ (typically $w_k = 1$).
%   \item $s_{ijk}$: the difference (distance) between $X_{ik}$ and $X_{jk}$, i.e. the $i$-th and $j$-th observation of feature $k$.
% \end{itemize}
% \end{vbframe}

% \begin{vbframe}{Gower Distance III - Example}
% \includegraphics[width = 0.99\textwidth]{figure_man/gower_distance.png}
% \end{vbframe}

\begin{vbframe}{Distances between Observations}
It is often a good idea to \textbf{normalize} the data before computing distances,
especially when the scale of features is different, e.g.:

<<echo=FALSE, fig.align="center", fig.height=4, fig.width=8>>=
par(mar = c(3.5,3.5,1,1), mfrow = c(1,2), mgp = c(2.5,1,0))

dat = data.frame(shoe.size = c(46, 40, 44), height = c(180, 172, 175))
plot(x = dat$shoe.size, y = dat$height, xlab = "shoe size", ylab = "body height (in cm)",
  pch = 19, xlim = range(dat$shoe.size)*c(0.95, 1.05), ylim = range(dat$height)*c(0.98, 1.02))
lines(x = dat$shoe.size[-3], y = dat$height[-3])
asp = getCurrentAspect()
text(x = mean(dat$shoe.size[-3]), y = mean(dat$height[-3]),
  bquote(paste(d(x^(1),x^(2))~"="~sqrt((46-40)^2 + (180-172)^2)~" = ", .(sqrt((46-40)^2 + (180-172)^2)))),
  adj = c(0.5,-0.25), col = 1, srt = 180/pi*atan(diff(dat$height[-3])/diff(dat$shoe.size[-3])*asp))
text(x = dat$shoe.size, y = dat$height, c(expression(x^(2)), expression(x^(1)), expression(x^(3))), adj = c(-1, 0.5))

dat$height = dat$height/100
plot(x = dat$shoe.size, y = dat$height, xlab = "shoe size", ylab = "body height (in m)",
  pch = 19, xlim = range(dat$shoe.size)*c(0.95, 1.05), ylim = range(dat$height)*c(0.98, 1.02))
lines(x = dat$shoe.size[-3], y = dat$height[-3])
asp = getCurrentAspect()
text(x = mean(dat$shoe.size[-3]), y = mean(dat$height[-3]),
  bquote(paste(d(x^(1),x^(2))~"="~sqrt((46-40)^2 + (1.80-1.72)^2)~" = ", .(sqrt((46-40)^2 + (1.80-1.72)^2)))),
  adj = c(0.5,-0.25), col = 1, srt = 180/pi*atan(diff(dat$height[-3])/diff(dat$shoe.size[-3])*asp))
text(x = dat$shoe.size, y = dat$height, c(expression(x^(2)), expression(x^(1)), expression(x^(3))), adj = c(-1, 0.5))
@

\lz

On the right plot, the distance is dominated by \code{shoe size}.
\end{vbframe}

\begin{vbframe}{Distances between Observations}
\small
One possibility to normalize feature $\xb_{\code{height}}$ is to compute
\[\tilde{\xb}_{\code{height}} = \tfrac{\xb_{\code{height}}-\text{mean}(\xb_{\code{height}})}{\text{sd}(\xb_{\code{height}})}.\]

\lz

Distances based on normalized data are better comparable and robust in terms of
linear transformations (e.g. unit conversion).

\vspace{0.2cm}

<<echo=FALSE, fig.align="center", fig.height=3.5, fig.width=6, out.width="0.7\\textwidth">>=
par(mar = c(3.5,3.5,0.1,0.1), mgp = c(2.5,1,0))

#dat = data.frame(shoe.size = c(45, 40, 42), height = 1000*c(85, 70, 72))
dat = as.data.frame(scale(dat))
plot(x = dat$shoe.size, y = dat$height, xlab = "normalized shoe size", ylab = "normalized body height",
  pch = 19, xlim = range(dat$shoe.size)*c(1.1, 1.2), ylim = range(dat$height)*c(1.1, 1.1))
lines(x = dat$shoe.size[-3], y = dat$height[-3])
asp = getCurrentAspect()
text(x = mean(dat$shoe.size[-3]), y = mean(dat$height[-3]),
  bquote(paste(d(x^(1),x^(2))~" = ", .(sqrt((dat$shoe.size[1]-dat$shoe.size[2])^2 + (dat$height[1]-dat$height[2])^2)))),
  adj = c(0.5,0), col = 1, srt = 180/pi*atan(diff(dat$height[-3])/diff(dat$shoe.size[-3])*asp))
text(x = dat$shoe.size, y = dat$height, c(expression(x^(2)), expression(x^(1)), expression(x^(3))), adj = c(-1, 0.5))
@
\end{vbframe}

\begin{vbframe}{Distances between Clusters (Linkage)}
\begin{itemize}
  \item Assume that all observations $\xdat$ belong to $k<n$ different clusters.
  \item For an arbitrary cluster $j$ we define its \textbf{index space} by
  \[C_j\coloneqq\{i\subset\{1,\ldots,k\}~|~\xi\text{ belongs to cluster }j\}\]
  Let $n_j = \left|C_j\right|$ be the size of cluster $j$.
  \item The linkage of two clusters $C_r$ and $C_s$ is a \enquote{score} describing their distance.
\end{itemize}

\lz

The most popular and simplest linkages are

\vspace*{0.2cm}

\begin{itemize}
  \item Single Linkage
  \item Complete Linkage
  \item Average Linkage
  \item Centroid Linkage
\end{itemize}

<<echo=FALSE, message=FALSE>>=
set.seed(12345)
n = 6
cl1 = mvrnorm(n = n, mu = c(6,2), Sigma = diag(2))
cl2 = mvrnorm(n = n, mu = c(1,-3), Sigma = diag(2)*2)
e1 = ellipsoidhull(cl1)
e2 = ellipsoidhull(cl2)
dat = rbind(cl1, cl2)
d = as.matrix(pdist(cl1, cl2))
@
\end{vbframe}

\begin{vbframe}{Single Linkage}
<<echo=FALSE, fig.pos = "H", fig.align="center", fig.width=6, fig.height=3, out.width="0.9\\textwidth">>=
single = which(d==min(d), arr.ind = TRUE)

par(mar=c(4,4,0,0))
plot(dat, pch = 19, xlim = c(-2,7), col = rep(c(2,3), each = n),
  xlab = "Dimension 1", ylab = "Dimension 2")
lines(predict(e1), col = 2)
lines(predict(e2), col = 3)

lines(rbind(cl1[single[1],], cl2[single[2],]))
text(e1$loc[1], e1$loc[2], label = expression(C[r]), col = 2)
text(e2$loc[1], e2$loc[2], label = expression(C[s]), col = 3)
@

Single linkage defines the distance of the \textbf{closest point pairs} from different clusters as the distance between two clusters:

\[d_{\text{single}}(C_r,C_s) = \min_{i \in C_r, \, j \in C_s} d(\xi,\xb^{(j)})\]
\end{vbframe}

\begin{vbframe}{Complete Linkage}
<<echo=FALSE, fig.pos = "H", fig.align="center", fig.width=6, fig.height=3, out.width="0.9\\textwidth">>=
complete = which(d==max(d), arr.ind = TRUE)

par(mar=c(4,4,0,0))
plot(dat, pch = 19, xlim = c(-2,7), col = rep(c(2,3), each = n),
  xlab = "Dimension 1", ylab = "Dimension 2")
lines(predict(e1), col = 2)
lines(predict(e2), col = 3)

lines(rbind(cl1[complete[1],], cl2[complete[2],]))
text(e1$loc[1], e1$loc[2], label = expression(C[r]), col = 2)
text(e2$loc[1], e2$loc[2], label = expression(C[s]), col = 3)
@

Complete linkage defines the distance of the \textbf{furthest point pairs} of different clusters as
the distance between two clusters:

\[d_{\text{complete}}(C_r,C_s) = \max_{i \in C_r, \, j \in C_s} d(\xi,\xb^{(j)})\]
\end{vbframe}

\begin{vbframe}{Average Linkage}
<<echo=FALSE, fig.pos = "H", fig.align="center", fig.width=6, fig.height=3, out.width="0.9\\textwidth">>=
par(mar=c(4,4,0,0))
plot(dat, pch = 19, xlim = c(-2,7), col = rep(c(2,3), each = n),
  xlab = "Dimension 1", ylab = "Dimension 2")
lines(predict(e1), col = 2)
lines(predict(e2), col = 3)

for(i in 1:nrow(cl2)) {
  for(j in 1:nrow(cl1)) lines(rbind(cl1[j,], cl2[i,]), col = "grey")
}

text(e1$loc[1], e1$loc[2], label = expression(C[r]), col = 2)
text(e2$loc[1], e2$loc[2], label = expression(C[s]), col = 3)
@

In average linkage, the distance between two clusters is defined as the average
distance across \textbf{all} pairs of two different clusters:

\[d_{\text{average}}(C_r,C_s) = \frac{1}{n_r n_s} \sum_{i \in C_r} \sum_{j \in C_s} d(\xi,\xb^{(j)})\]
\end{vbframe}

\begin{vbframe}{Centroid Linkage}
<<echo=FALSE, fig.pos = "H", fig.align="center", fig.width=6, fig.height=3, out.width="0.9\\textwidth">>=
par(mar=c(4,4,0,0))
plot(dat, pch = 19, xlim = c(-2,7), col = rep(c(2,3), each = n),
  xlab = "Dimension 1", ylab = "Dimension 2")
lines(predict(e1), col = 2)
lines(predict(e2), col = 3)

text(e1$loc[1], e1$loc[2], label = expression(C[r]), col = 2)#, adj = c(0, 0.5))
text(e2$loc[1], e2$loc[2], label = expression(C[s]), col = 3)#, adj = c(1, 0.5))


#points(x = c(e1$loc[1], e2$loc[1]), y = c(e1$loc[2], e2$loc[2]), pch = 1, cex = 4)
lines(x = c(e1$loc[1], e2$loc[1]), y = c(e1$loc[2], e2$loc[2]), type = "b", cex = 3.5)
@

\small
Centroid linkage defines the distance between two clusters as the distance between the two cluster centroids.
The centroid of a cluster $C_s$ with $n_s$ points is the mean value of each dimension:

\[\bar{\xb}_s = \frac{1}{n_s} \sum_{i \in C_s} \xi\]
\end{vbframe}

\begin{vbframe}{Example: Hierarchical Clustering}
Agglomerative hierarchical clustering starts with all points forming their own cluster and iteratively merges them until all points form a single cluster containing all points.

\lz

Example:

\begin{columns}
\begin{column}{0.5\textwidth}
\ \\
\vspace{1.5cm}
Step 1: $\{1\},\{2\},\{3\},\{4\},\{5\}$ \\
\end{column}
\begin{column}{0.4\textwidth}

<<echo=FALSE, out.width="\\textwidth", fig.width=3, fig.height=3>>=
par(mar = c(4,4,1,1))
simple.data = data.frame(x = c(0.2,0.3,0.4,0.85,0.85),
  y = c(0.4,0.65,0.6,0.7,0.9))

lab = 1:nrow(simple.data)
plot(simple.data, xlim = c(0, 1), ylim = c(0, 1), xlab = "Dimension 1", ylab = "Dimension 2")
text(simple.data, labels = lab, adj = c(-1,0.5))
@
\end{column}
\end{columns}
\end{vbframe}

\begin{vbframe}{Example: Hierarchical Clustering}
\addtocounter{framenumber}{-1}

Agglomerative hierarchical clustering starts with all points forming their own cluster and iteratively merges them until all points form a single cluster containing all points.

\lz

Example:

\begin{columns}
\begin{column}{0.5\textwidth}
\ \\
\vspace{1.3cm}
Step 1: $\{1\},\{2\},\{3\},\{4\},\{5\}$\\
Step 2: $\{1\},{\color{blue}\{2,3\}},\{4\},\{5\}$ \\
\end{column}
\begin{column}{0.4\textwidth}

<<echo=FALSE, out.width="\\textwidth", fig.width=3, fig.height=3>>=
par(mar = c(4,4,1,1))
simple.data = data.frame(x = c(0.2,0.3,0.4,0.85,0.85),
  y = c(0.4,0.65,0.6,0.7,0.9))

lab = 1:nrow(simple.data)
plot(simple.data, xlim = c(0, 1), ylim = c(0, 1), xlab = "Dimension 1", ylab = "Dimension 2")
text(simple.data, labels = lab, adj = c(-1,0.5))
lines(simple.data[2:3,], col = "blue")
@
\end{column}
\end{columns}
\end{vbframe}

\begin{vbframe}{Example: Hierarchical Clustering}
\addtocounter{framenumber}{-1}

Agglomerative hierarchical clustering starts with all points forming their own cluster and iteratively merges them until all points form a single cluster containing all points.

\lz

Example:

\begin{columns}
\begin{column}{0.5\textwidth}
\ \\
\vspace{1.1cm}
Step 1: $\{1\},\{2\},\{3\},\{4\},\{5\}$\\
Step 2: $\{1\},\{2,3\},\{4\},\{5\}$ \\
Step 3: $\{1\},{\color{blue}\{2,3\}},{\color{red}\{4,5\}}$\\
\end{column}
\begin{column}{0.4\textwidth}

<<echo=FALSE, out.width="\\textwidth", fig.width=3, fig.height=3>>=
par(mar = c(4,4,1,1))
simple.data = data.frame(x = c(0.2,0.3,0.4,0.85,0.85),
  y = c(0.4,0.65,0.6,0.7,0.9))

lab = 1:nrow(simple.data)
plot(simple.data, xlim = c(0, 1), ylim = c(0, 1), xlab = "Dimension 1", ylab = "Dimension 2")
text(simple.data, labels = lab, adj = c(-1,0.5))
lines(simple.data[2:3,], col = "blue")
lines(simple.data[4:5,], col = "red")
@

\end{column}
\end{columns}
\end{vbframe}

\begin{vbframe}{Example: Hierarchical Clustering}
\addtocounter{framenumber}{-1}

Agglomerative hierarchical clustering starts with all points forming their own cluster and iteratively merges them until all points form a single cluster containing all points.

\lz

Example:

\begin{columns}
\begin{column}{0.5\textwidth}
\ \\
\vspace{0.9cm}
Step 1: $\{1\},\{2\},\{3\},\{4\},\{5\}$\\
Step 2: $\{1\},\{2,3\},\{4\},\{5\}$ \\
Step 3: $\{1\},\{2,3\},\{4,5\}$\\
Step 4: ${\color{blue}\{1,2,3\}},{\color{red}\{4,5\}}$\\
\end{column}
\begin{column}{0.4\textwidth}

<<echo=FALSE, out.width="\\textwidth", fig.width=3, fig.height=3>>=
par(mar = c(4,4,1,1))
simple.data = data.frame(x = c(0.2,0.3,0.4,0.85,0.85),
  y = c(0.4,0.65,0.6,0.7,0.9))

lab = 1:nrow(simple.data)
plot(simple.data, xlim = c(0, 1), ylim = c(0, 1), xlab = "Dimension 1", ylab = "Dimension 2")
text(simple.data, labels = lab, adj = c(-1,0.5))
lines(simple.data[c(1:3, 1),], col = "blue")
lines(simple.data[4:5,], col = "red")
@
\end{column}
\end{columns}

\end{vbframe}

\begin{vbframe}{Example: Hierarchical Clustering}
\addtocounter{framenumber}{-1}

Agglomerative hierarchical clustering starts with all points forming their own cluster and iteratively merges them until all points form a single cluster containing all points.

\lz

Example:

\begin{columns}
\begin{column}{0.5\textwidth}
\ \\
\vspace{0.7cm}
Step 1: $\{1\},\{2\},\{3\},\{4\},\{5\}$\\
Step 2: $\{1\},\{2,3\},\{4\},\{5\}$ \\
Step 3: $\{1\},\{2,3\},\{4,5\}$\\
Step 4: $\{1,2,3\},\{4,5\}$\\
Step 5: ${\color{blue}\{1,2,3,4,5\}}$
\end{column}
\begin{column}{0.4\textwidth}

<<echo=FALSE, out.width="\\textwidth", fig.width=3, fig.height=3>>=
par(mar = c(4,4,1,1))
simple.data = data.frame(x = c(0.2,0.3,0.4,0.85,0.85),
  y = c(0.4,0.65,0.6,0.7,0.9))

lab = 1:nrow(simple.data)
plot(simple.data, xlim = c(0, 1), ylim = c(0, 1), xlab = "Dimension 1", ylab = "Dimension 2")
text(simple.data, labels = lab, adj = c(-1,0.5))
lines(simple.data[c(1,2,3,5,4,1),], col = "blue")
@

\end{column}
\end{columns}
\end{vbframe}

\begin{vbframe}{Dendrogram}
A dendrogram is a tree showing which clusters / observations are merged after each step.
The \enquote{height} is proportional to the distance between the two merged clusters:

\lz

<<echo = FALSE, results='hide'>>=
# compute distance matrix
euclid = dist(simple.data, method = "euclidean")
# do clustering with complete linkage
agglo = hclust(euclid, method = "complete")
agglo
@

<<echo=FALSE, out.width="\\textwidth", fig.width=8, fig.height=4>>=
par(mfrow = c(1,2), mar = c(4,4,1,1))
plot(simple.data, xlim = c(0, 1), ylim = c(0, 1), xlab = "Dimension 1", ylab = "Dimension 2")
text(simple.data, labels = lab, adj = c(-1,0.5))

par(mar = c(2,3,1,4))
plot(agglo, hang = -1, ylab = "", axes = F)
axis(2, line = 0, padj = 0.5)
title(ylab = "Height", line = 2)
abline(h = c(0, agglo$height), lty = 2, col = "gray70")
text(x = 5, y = c(0, agglo$height), labels = paste("Step ", 1:5), xpd = TRUE, col = "gray70", pos = 4)
@
\end{vbframe}

\begin{vbframe}{Example: Iris Data}
The data contains 150 leaf measurements for 3 flower species:

\lz

% <<eval = FALSE, echo = TRUE>>=
% pairs(iris[1:4], col = iris$Species)
% @

<<echo=FALSE, out.width="0.9\\textwidth", fig.align="center", fig.height = 5.5, fig.width=9>>=
pairs(iris[1:4], pch = 19, col = iris$Species, oma = c(2,2,2,12))
legend(0.95, 1, levels(iris$Species), xpd = TRUE, horiz = F, xjust = 0.5,
       fill = unique(iris$Species), title = "Species")
@
\end{vbframe}


\begin{vbframe}{R Example with Iris Data}
We now \enquote{forget} the real groups specified by the \code{Species} variable and try to find clusters based on the leaf measurements.

<<echo=FALSE, out.width="0.8\\textwidth", fig.align="center">>=
pairs(iris[1:4], pch = 19)
@
\end{vbframe}

\begin{vbframe}{Example: Iris Data}
% <<eval=FALSE, echo = TRUE>>=
% # compute distance matrix
% d.euclid = dist(iris[1:4], method = "euclidean")
% # do clustering with average linkage
% cl = hclust(d.euclid, method = "average")
% plot(cl, labels = FALSE, hang = -1) # plot dendrogram
% rect.hclust(cl, k = 3) # highlight the k = 3 groups
% @

\lz

<<echo=FALSE, fig.width=6, fig.height=4>>=
par(mar = c(1,4,1,0))
d.euclid = dist(iris[1:4], method = "euclidean")
cl = hclust(d.euclid, method = "average")
plot(cl, labels = FALSE, hang = -1)
rect.hclust(cl, k = 3)
@
\end{vbframe}

\begin{vbframe}{Example: Iris Data}
We can extract the clustering assignments by cutting the dendrogram, e.g. using $k=3$ clusters:

% <<eval = FALSE, echo=TRUE>>=
% group = cutree(cl, k = 3) # get clusters assignments for k=3
% pairs(iris[1:4], col = group) # plot clusters with different colors
% @

\lz

<<echo=FALSE, out.width="0.8\\textwidth", fig.width=8, fig.height=5, fig.align="center">>=
group = cutree(cl, k = 3)
pairs(iris[1:4], pch = 19, col = group)
@
\end{vbframe}

\begin{vbframe}{Properties: Single Linkage}
<<echo=FALSE>>=
#Colors = colorspace::rainbow_hcl(3)
set.seed(0)
x = rbind(
  scale(matrix(rnorm(2*20, sd = 0.4), ncol = 2), center = c(1,1), scale = F),
  scale(matrix(rnorm(2*30, sd = 0.4), ncol = 2), center = -c(1,1), scale = F)
  )
x = rbind(x, matrix(rep(seq(-1,1, length.out = 10), each = 2), ncol = 2, byrow = TRUE))
#x = rbind(x, matrix(runif(2*10, min(x), max(x)), ncol=2))
d = dist(x)

tree.sing = hclust(d,method = "single")
tree.comp = hclust(d,method = "complete")
tree.avg = hclust(d,method = "average")
tree.cen = hclust(d,method = "centroid")

k = 3

labs.sing = cutree(tree.sing, k = k)
labs.comp = cutree(tree.comp, k = k)
labs.avg = cutree(tree.avg, k = k)
labs.cen = cutree(tree.cen, k = k)
@

Single linkage introduces the \textbf{chaining problem}:

\begin{itemize}
  \item Only one pair of points needs to be close to merge clusters.
  \item A chain of points can expand a cluster over long distances.
  \item Points within a cluster can be too widely spread and not dense enough.
\end{itemize}

\lz

<<echo=FALSE, fig.height=4, out.width="0.8\\textwidth">>=
par(mfrow=c(1,2), mar = c(3,3,2,2))

plot(x,col=Colors[labs.sing], pch = 19, ylab = "", xlab = "", main = "single (chaining effect)")
plot(tree.sing, labels=F, hang=-1)
rect.hclust(tree.sing, k = k)#, border = Colors[1])
@
\end{vbframe}

\begin{vbframe}{Properties: Complete Linkage}
Complete linkage avoids chaining, but suffers from \textbf{crowding}:
\begin{itemize}
  \item Merging is based on the furthest distance of point pairs from different clusters.
  \item Points of two different clusters can thus be closer than points within a cluster.
  \item Clusters are dense, but too close to each other.
\end{itemize}

<<echo=FALSE, fig.height=4, out.width="0.8\\textwidth">>=
par(mfrow=c(1,2), mar = c(3,3,2,2))

plot(x,col=Colors[labs.comp], pch = 19, ylab = "", xlab = "", main = "complete (crowding effect)")
plot(tree.comp, labels=F, hang=-1)
rect.hclust(tree.comp, k = k)#, border = Colors[1])
@
\end{vbframe}

\begin{vbframe}{Properties: Average Linkage}
\begin{itemize}
  \item Average linkage is based on the average distance between clusters and tries to avoid crowding and chaining.
  \item Produces clusters that are quite dense and rather far apart.
\end{itemize}

\lz

<<echo=FALSE, fig.height=3.5, fig.width=6, out.width="0.8\\textwidth">>=
par(mfrow=c(2,3), mar = c(3,3,2,2))

plot(x,col=Colors[labs.sing], pch = 19, ylab = "", xlab = "", main = "single (chaining effect)")
plot(x,col=Colors[labs.comp], pch = 19, ylab = "", xlab = "", main = "complete (crowding effect)")
plot(x,col=Colors[labs.avg], pch = 19, ylab = "", xlab = "", main = "average")
#plot(x,col=Colors[labs.cen], pch = 19, ylab = "", xlab = "", main = "centroid")

par(mar = c(1,4,2,1))
plot(tree.sing, labels=F, hang=-1)
rect.hclust(tree.sing, k = k)#, border = Colors[1])
plot(tree.comp, labels=F, hang=-1)
rect.hclust(tree.comp, k = k)#, border = Colors[1])
plot(tree.avg, labels=F, hang=-1)
rect.hclust(tree.avg, k = k)#, border = Colors[1])
#plot(tree.cen, labels=F, hang=-1)
#rect.hclust(tree.cen, k = k)#, border = Colors[1])
@
\end{vbframe}

\begin{vbframe}{Properties: Centroid Linkage}
\begin{itemize}
  \item Centroid linkage defines the distance based on \textbf{artificial data points} (the cluster centers), which produces dendrograms \textbf{with inversions}, i.e., the distance between the clusters to be merged can be smaller in the next step.
  \item In single, complete and average linkage, the distance between the clusters to be merged increases in each step. \
  $\Rightarrow$ always produces dendrograms \textbf{without inversions}.
\end{itemize}

\lz

<<echo=FALSE, fig.width = 6, fig.height = 2.5, out.width="0.8\\textwidth">>=
simple.data = data.frame(
  x = c(0,1,0.5),
  y = c(0,0,0.9)
)

#simple.data = as.data.frame(rbind(c(1,1), c(5,1), c(3,1+2*sqrt(3))))
par(mfrow = c(1,2), mar = c(4,4,1,1))
plot(simple.data, xlim = c(0, 1), ylim = c(0, 1),
  xlab = "Dimension 1", ylab = "Dimension 2")
text(simple.data[1,], labels = 1, pos = 3)
text(simple.data[2,], labels = 2, pos = 3)
text(simple.data[3,], labels = 3, pos = 1)
centroid = colMeans(simple.data[1:2,])
points(centroid[1], centroid[2], col = "gray60", pch = 19)
text(centroid[1], centroid[2], labels = expression(C[12]), col = "gray60", pos = 4)
centroid = colMeans(simple.data[1:3,])
#points(centroid[1], centroid[2], col = "gray60", pch = 19)
#text(centroid[1], centroid[2], labels = expression(C[123]), col = "gray60", pos = 4)

agglo = hclust(dist(simple.data, method = "euclidean")^2, method = "centroid")
#cophenetic(agglo)
agglo$height = sqrt(agglo$height)

par(mar = c(2,4,1,1))
plot(as.dendrogram(agglo), main = "Cluster Dendrogram", ylab = "Height", ylim = c(0,1))
#rect.hclust(agglo, k = 2)
@
\end{vbframe}

\begin{vbframe}{Summary}
\begin{itemize}
  \item \textbf{Hierarchical agglomerative clustering methods} iteratively merge observations/clusters until all observations are in one single cluster.
  \item Results in a hierarchy of clustering assignments which can be visualized in a \textbf{dendrogram}.
Each node of the dendrogram represents a cluster and its \enquote{height} is proportional to the distance of its child nodes.
  \item The most common linkage functions are \textbf{single, complete, average} and \textbf{centroid} linkage.
There is no perfect linkage and each linkage has its own advantages and disadvantages.
\end{itemize}
\end{vbframe}

\section{Partitioning Clustering Methods}

\begin{vbframe}{Optimal Partitioning Clustering}
<<echo=FALSE, results='hide'>>=
Colors = c("#000000", "#E69F00", "#56B4E9", "#009E73",
           "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
palette(Colors)
add.alpha <- function(col, alpha=1){
  if(missing(col))
    stop("Please provide a vector of colours.")
  apply(sapply(col, col2rgb)/255, 2,
                     function(x)
                       rgb(x^(1), x[2], x[3], alpha=alpha))
}
@

\textbf{Hierarchical clustering:} \\
Stepwise merging (agglomerative methods) or dividing (divisive methods) of clusters based on distances and linkages.
The number of clusters are selected by splitting the dendrogram at a specific threshold for the \enquote{height} after visual inspection.

\lz

\textbf{Partitioning clustering:} \\
Partitions the $n$ observations into a predefined number of $k$ clusters by optimizing a numerical criterion. The most common partitioning methods are:

\begin{itemize}
  \item $k$-means
  \item $k$-medians
  \item $k$-medoids (Partitioning Around Medoids (PAM))
\end{itemize}
\end{vbframe}

\begin{vbframe}{$k$-means}
$k$-means partitions the $n$ observations into $k$ predefined clusters $C_1, C_2, \hdots, C_k$ by minimizing the \textbf{compactness}, i.e. the \textbf{within-cluster variation} of all clusters using
$$\textstyle\sum_{j=1}^k \sum_{i \in C_j} \|\xi-\bar{\xb}_j\|_2^2 \rightarrow \min,$$
where $\bar{\xb}_j = \frac{1}{n_j} \sum_{i \in C_j} \xi$ is the centroid of cluster $j$ and $n_j$ is the number of observations in cluster $j$.

\lz

<<echo=FALSE, fig.height=3, fig.width=6.5, out.width="0.7\\textwidth", message=FALSE>>=
Colors = colorspace::rainbow_hcl(3)
set.seed(123456)
n = 6
cl1 = mvrnorm(n = n, mu = c(6,0), Sigma = diag(2)*0.2)
cl2 = mvrnorm(n = n, mu = c(3,-1.5), Sigma = diag(2)*0.3)
cl3 = mvrnorm(n = n, mu = c(2,0.25), Sigma = diag(2)*0.6)

e1 = ellipsoidhull(cl1)
e2 = ellipsoidhull(cl2)
e3 = ellipsoidhull(cl3)
dat = rbind(cl1, cl2, cl3)
d1 = as.matrix(pdist(cl1, cl2))
d2 = as.matrix(pdist(cl1, cl3))
d3 = as.matrix(pdist(cl2, cl3))

par(mar = c(0,0,0,0))

pr = (rbind(predict(e1), predict(e2), predict(e3)))
plot(dat, pch = 19, col = rep(Colors, each = n),
  xlab = "", ylab = "", axes = F, xlim = range(pr[,1]), ylim = range(pr[,2]))
lines(predict(e1), col = Colors[1])
lines(predict(e2), col = Colors[2])
lines(predict(e3), col = Colors[3])


centers = as.data.frame(rbind(
  c("x" = e2$loc[1], "y" = e2$loc[2]),
  c("x" = e1$loc[1], "y" = e1$loc[2])))
#lines(centers, col = "black", lwd = 2)
arrows(x0 = centers$x[1], x1 = centers$x[2],
  y0 = centers$y[1], y1 = centers$y[2], code = 3, lwd = 2, length = 0.1, col = "gray60")
xy = colMeans(centers)
text(xy[1], xy[2], label = "between cluster variation", col = "black")

text(e1$loc[1], e1$loc[2], label = expression(C[1]), col = Colors[1], pos = 1)
text(e2$loc[1], e2$loc[2], label = expression(C[2]), col = Colors[2], pos = 1)
text(e3$loc[1], e3$loc[2], label = expression(C[3]), col = Colors[3], pos = 3)

d = predict(e1)
#summary(d)
xy = dat[c(14,16),]
#lines(xy, col = "black", lwd = 2)
arrows(x0 = xy[1,1], x1 = xy[2,1],
  y0 = xy[1,2], y1 = xy[2,2], code = 3, lwd = 2, length = 0.1, col = "gray60")
xy = colMeans(xy)
text(xy[1], xy[2], label = "within-cluster variation", col = "black")

@
\end{vbframe}

\begin{vbframe}{$k$-means}
Idea: Consider every possible partition of $n$ observations into $k$ clusters and select the one with the lowest \textbf{within-cluster variation}.

\lz 

Problem: Requires trying all possible assignments of $n$ observations into $k$ clusters, which in practice is nearly impossible (Hothorn et al., 2009, p. 322):

\begin{table}[]
\begin{tabular}{rcl}
$n$ & $k$ & Number of possible partitions \\
\hline
15  &  3  & 2.375.101                     \\
20  &  4  & 45.232.115.901                \\
100 &  5  & $10^{68}$ \\
\hline
\end{tabular}
\end{table}
\begin{center}
\tiny{Hothorn, T., Everitt, B. S. (2009). A handbook of statistical analyses using R. Chapman and Hall/CRC.}
\end{center}
\end{vbframe}

\begin{vbframe}{$k$-means}
Use an approximation:

\lz

\begin{enumerate}
\item \textbf{Initialization:} Choose $k$ arbitrary observations to be the initial cluster
   centers.
\item \textbf{Assignment:} Assign every observation to the cluster with the closest center.
\item \textbf{Update:} Compute the new center of each cluster as the mean of its members.
\item Repeat (2) and (3) until the centers do not move.
\end{enumerate}
\end{vbframe}


\begin{vbframe}{$k$-means Example}
<<echo=FALSE, fig.width = 7, fig.height = 5, fig.align="center", message=FALSE, results="asis", out.width="0.9\\textwidth">>=
set.seed(1234)
pts = iris[, 3:4]

iters = 3
K = 3
km = vector("list", iters)
centers = vector("list", iters + 1)

centers[[1]] = pts[sample.int(nrow(pts), K), , drop = FALSE] #matrix(c(1,0.5,2,1,7,2), ncol = 2, byrow = TRUE)

par(mar = c(4,4,3,0))
plot(pts, pch = 19, col = "gray70")
points(centers[[1]][, 1], centers[[1]][, 2], pch = 4, cex = 1.5, lwd = 2, col = "darkred")
title(main = "Iteration 0", line = 2)
title(main = "Choose k arbitrary observations to be the initial cluster centers", line = 1, cex.main = 0.8)

for (i in 1:iters) {
  cat("  \\end{vbframe}  \n  \\begin{vbframe}{$k$-means Example}  \n  \\addtocounter{framenumber}{-1}  \n")
  km[[i]] = kmeans(pts, centers = centers[[i]], nstart = 1, algorithm = "Lloyd", iter.max = 1)

  col = km[[i]]$cluster
  plot(pts, pch = 19, col = col)
  title(main = paste("Iteration", i), line = 2)
  title(main = "Assign observations to nearest cluster center", line = 1, cex.main = 0.8)
  points(centers[[i]][, 1], centers[[i]][, 2], pch = 4, cex = 1.5, lwd = 2, col = "darkred")

  cat("  \n")
  cat("  \\end{vbframe}  \n  \\begin{vbframe}{$k$-means Example}  \n  \\addtocounter{framenumber}{-1}  \n")

  centers[[i + 1]] = km[[i]]$centers
  plot(pts, pch = 19, col = col)
  points(centers[[i + 1]][, 1], centers[[i + 1]][, 2], pch = 4, cex = 1.5, lwd = 2, col = "darkred")
  title(main = paste("Iteration", i), line = 2)
  title(main = "Compute new cluster center", line = 1, cex.main = 0.8)
  cat("  \n")
}
@
\end{vbframe}

% \begin{vbframe}{$k$-means in R}
% The $k$-means algorithm is part of the base distribution in R, given by
% the \code{kmeans} function (using \code{algorithm $=$ "Lloyd"}):

% <<echo=FALSE>>=
% km = kmeans(iris[,3:4], centers = 3, nstart = 100,
%   iter.max = 100, algorithm = "Lloyd")
% str(km)
% @
% \end{vbframe}

% \begin{vbframe}{$k$-means in R}
% The final cluster assignments can be visualized:

% \lz

% <<echo=FALSE, fig.width = 7, fig.height = 5, out.width = "\\textheight", fig.align="center">>=
% par(mar = c(4,4,1,1))
% plot(iris[,3:4], pch = 19, col = km$cluster)
% @
% \end{vbframe}

\begin{vbframe}{Properties of $k$-means}
\begin{itemize}
  \item $k$-means is based on computing the mean, which is sensitive to outliers and can only be computed for numerical data. \\
  \item The \textbf{within-cluster variation} is reduced in each iteration.
  \item The final result is typically not the best result that globally minimizes the \textbf{within-cluster variation}.\\
  $\rightarrow$ would only be possible after trying all possible partitions! \\
  \item $k$-means can be restarted multiple times.
  The clustering with the smallest within-cluster variation is then selected as
  the best solution.
\end{itemize}

\framebreak

\begin{itemize}
  \item $k$-means produces different clusters depending on the initial centers and always converges, e.g.:

\lz

<<echo=FALSE, fig.width=6, fig.height=3, out.width="0.9\\textwidth">>=
set.seed(0)
x = rbind(
  scale(matrix(rnorm(2*20, sd = 0.4), ncol=2), center=c(1,1), scale = F),
  scale(matrix(rnorm(2*30, sd = 0.4), ncol=2), center=-c(1,1), scale = F)
  )
x = rbind(x, matrix(rep(seq(-1,1, length.out = 10), each = 2), ncol = 2, byrow = TRUE))
#x = rbind(x, matrix(runif(2*10, min(x), max(x)), ncol=2))
d = dist(x)

init.col = add.alpha("#000000", alpha = 0.5)
par(mfrow=c(1,3), mar = c(3,3,1,1))
for(i in 1:3) {
  rand = c(1, 25, 54)+i
  km1 = kmeans(x, centers = x[rand, ], nstart = 1, algorithm = "Lloyd")
  plot(x, pch = 19, col = km1$cluster, ylim = c(-1.5, 2))
  points(x[rand, 1], x[rand, 2], pch = 4, cex = 2, lwd = 2, col = init.col)
  if(i == 1) legend("topleft", pch = 4, pt.lwd = 2, pt.cex = 2, legend = "Initial Cluster Centers",
    col = init.col)
}
@

\end{itemize}
\end{vbframe}

\begin{vbframe}{Choice of $k$}
\begin{itemize}
  \item Many methods exist for choosing the number of clusters $k$ (there is no perfect solution).
  \item The easiest method is to apply $k$-means for different $k$ and plot the \textbf{within-cluster variation} for each number of $k$.
  \item The \textbf{within-cluster variation} always decreases with increasing number of clusters.
  \item An \textbf{"elbow"} in the plot might indicate a useful solution.
\end{itemize}

\lz

<<echo=FALSE, fig.height=3, fig.width=6, fig.align="center", out.width="0.8\\textheight">>=
set.seed(1)
K = 8
wss = numeric(K)
for (k in 1:K) {
  km = kmeans(iris[,3:4], centers = k, nstart = 10, iter.max = 100, algorithm = "Lloyd")
  wss[k] = km$tot.withinss
}
par(mar = c(4,4,1,1))
plot(1:K, wss, type = "b", xlab = "k (number of clusters)", ylab = "within-cluster variation")
@
\end{vbframe}

\begin{vbframe}{$k$-medoids}
<<echo=FALSE, results='hide'>>=
Colors = c("#000000", "#E69F00", "#56B4E9", "#009E73",
           "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
palette(Colors)
add.alpha <- function(col, alpha=1){
  if(missing(col))
    stop("Please provide a vector of colours.")
  apply(sapply(col, col2rgb)/255, 2,
                     function(x)
                       rgb(x[1], x[2], x[3], alpha=alpha))
}
@

\lz

\begin{itemize}
  \item is strongly related to $k$-means and is realized by the \textbf{Partitioning Around Medoids (PAM)} algorithm.
  \item uses cluster medoids as representative clusters, i.e. \textbf{real data points} instead of \textbf{artificial data points} (such as the cluster centers as in $k$-means) are used.
  \item is less sensitive to outliers and more robust than $k$-means.
  \item can handle categorical features ($k$-means does not because it is based on calculating the cluster centers by taking the mean in each dimension).
\end{itemize}
\end{vbframe}

\begin{vbframe}{The PAM algorithm}

\lz

\begin{enumerate}
  \item \textbf{Initialization:} Randomly select $k$ data points as the medoids.
  \item \textbf{Assignment:} Assign each data point $\xi$ to its closest medoid $m$ and calculate the within-cluster variation for each medoid (by summing up the distances of the current medoid $m$ to all other data points associated to $m$) .
  \item \textbf{Update:} Swap $m$ and $\xi$ and recompute the within-cluster variation to see if another medoid is more appropriate. Select the medoid $m$ with the lowest within-cluster variation.
  \item Repeat steps (2) and (3) until medoids do not change.
\end{enumerate}

\framebreak

\lz

The PAM algorithm typically uses the following two metrics to compute distances:
\begin{itemize}
  \item The euclidean distance (root sum-of-squares of differences).
  \item The Manhattan distance (the sum of absolute distances).
\end{itemize}

\lz

\textbf{Note:} The Manhattan distance should give more robust results if your data contains outliers.
In all other cases, the results will be similar for both metrics.
\end{vbframe}

\begin{vbframe}{$k$-means vs. $k$-medoids}

\lz

<<echo=FALSE, fig.width = 10, fig.height = 6, fig.align="center", out.width="\\textwidth", message=FALSE, results="asis">>=
kclust = function(x, centers=NULL, k=NULL, alg="kmeans", maxiter=100) {
  n = nrow(x)
  p = ncol(x)
  if (is.null(centers)) {
    if (is.null(k)) stop("Either centers or k must be specified.")
    if (alg=="kmeans") centers = matrix(runif(k*p,min(x),max(x)),nrow=k)
    else centers = x[sample(n,k),]
  }
  k = nrow(centers)
  cluster = matrix(0,nrow=0,ncol=n)

  for (iter in 1:maxiter) {
    cluster.new = clustfromcent(x,centers,k)
    centers.new = centfromclust(x,cluster.new,k,alg)

    cluster = rbind(cluster,cluster.new)
    j = is.na(centers.new[,1])
    if (sum(j)>0) centers.new[j,] = centers[j,]
    centers = centers.new

    if (iter>1 & sum(cluster[iter,]!=cluster[iter-1,])==0) break
  }
  return(list(centers=centers,cluster=cluster[iter,],iter=iter,cluster.history=cluster))
}

# Compute clustering assignments, given centers
clustfromcent = function(x, centers, k) {
  n = nrow(x)
  dist = matrix(0,n,k)
  for (i in 1:k) {
    dist[,i] = colSums((t(x)-centers[i,])^2)
  }
  return(max.col(-dist,ties="first"))
}

# Compute centers, given clustering assignments
centfromclust = function(x, cluster, k, alg) {
  if (alg=="kmeans") return(avgfromclust(x,cluster,k))
  else return(medfromclust(x,cluster,k))
}

avgfromclust = function(x, cluster, k) {
  p = ncol(x)
  centers = matrix(0,k,p)
  for (i in 1:k) {
    j = cluster==i
    if (sum(j)==0) centers[i,] = rep(NA,p)
    else centers[i,] = colMeans(x[j,,drop=FALSE])
  }
  return(centers)
}

medfromclust = function(x, cluster, k) {
  p = ncol(x)
  centers = matrix(0,k,p)
  for (i in 1:k) {
    j = cluster==i
    if (sum(j)==0) centers[i,] = rep(NA,p)
    else {
      d = as.matrix(dist(x[j,],diag=T,upper=T)^2)
      ii = which(j)[which.min(colSums(d))]
      centers[i,] = x[ii,]
    }
  }
  return(centers)
}

# Compute within-cluster variation
wcv = function(x, cluster, centers) {
  k = nrow(centers)
  wcv = 0
  for (i in 1:k) {
    j = cluster==i
    nj = sum(j)
    if (nj>0) {
      wcv = wcv + sum((t(x[j,])-centers[i,])^2)
    }
  }
  return(wcv)
}

set.seed(1234)
pts = as.matrix(iris[, 3:4])

iters = 2
K = 3
km = kmed = vector("list", iters)
centers = kmed.centers = vector("list", iters + 1)

ind = sample.int(nrow(pts), K)
centers[[1]] = kmed.centers[[1]] = pts[c(99,93,91), , drop = FALSE]

par(mar = c(4,4,3,0))
plot(pts, pch = 19, col = "gray70")
points(centers[[1]][, 1], centers[[1]][, 2], pch = 4, cex = 2, lwd = 2, col = "red")
title(main = "Iteration 0", line = 2)
title(main = "Choose k arbitrary observations to be the initial cluster centers", line = 1, cex.main = 0.8)

par(mfrow = c(1, 2))
for (i in 1:iters) {
  cat("  \\end{vbframe}  \n  \\begin{vbframe}{$k$-means vs. $k$-medoids}  \n  \\addtocounter{framenumber}{-1}  \n")
  # kmeans
  km[[i]] = kmeans(pts, centers = centers[[i]], nstart = 1, algorithm = "Lloyd", iter.max = 1)
  col = km[[i]]$cluster
  # kmedoids
  kmed[[i]] = kclust(pts, centers = kmed.centers[[i]], alg = "kmedoids", maxiter = 1)
  col.kmed = kmed[[i]]$cluster

  # kmeans
  par(mar = c(4,3,3,0))
  plot(pts, pch = 19, col = col)
  title(main = paste("k-means iteration", i), line = 2)
  title(main = "Assignment step", line = 1, cex.main = 0.8)
  points(centers[[i]][, 1], centers[[i]][, 2], pch = 4, cex = 2, lwd = 2, col = "red")

  # kmedoids
  par(mar = c(4,3,3,0))
  plot(pts, pch = 19, col = col.kmed)
  title(main = paste("k-medoids iteration", i), line = 2)
  title(main = "Assignment step", line = 1, cex.main = 0.8)
  points(kmed.centers[[i]][, 1], kmed.centers[[i]][, 2], pch = 4, cex = 2, lwd = 2, col = "red")

  cat("  \n")
  cat("  \\end{vbframe}  \n  \\begin{vbframe}{$k$-means vs. $k$-medoids}  \n  \\addtocounter{framenumber}{-1}  \n")

  centers[[i + 1]] = km[[i]]$centers
  kmed.centers[[i + 1]] = medfromclust(pts, kmed[[i]]$cluster, K)

  par(mar = c(4,3,3,0))
  plot(pts, pch = 19, col = col)
  points(centers[[i + 1]][, 1], centers[[i + 1]][, 2], pch = 4, cex = 2, lwd = 2, col = "red")
  title(main = paste("k-means iteration", i), line = 2)
  title(main = "Update step", line = 1, cex.main = 0.8)

  par(mar = c(4,3,3,0))
  plot(pts, pch = 19, col = col)
  points(kmed.centers[[i + 1]][, 1], kmed.centers[[i + 1]][, 2], pch = 4, cex = 2, lwd = 2, col = "red")
  title(main = paste("k-medoids iteration", i), line = 2)
  title(main = "Update step", line = 1, cex.main = 0.8)
  #
  # points(medfromclust(pts, kmed[[i]]$cluster.history[i,], K), col = 1:3, pch = 19, cex = 2)
  # points(medfromclust(pts, kmed[[i]]$cluster.history[i,], K), pch = 21, cex = 2)

  cat("  \n")
}
@
\end{vbframe}

% \begin{vbframe}{$k$-medoids in R}
% The $k$-medoids algorithm is implemented in the \code{pam} function included in the \pkg{cluster} R-package.

% \lz

% We compare $k$-means and $k$-medoids on a modified iris data with additional outliers:

% % <<echo = TRUE, eval = FALSE>>=
% % set.seed(5)
% % random = data.frame(Petal.Length = runif(2, 2.0, 3.5), Petal.Width = runif(2,1.0,10.5))
% % example = rbind(random, iris[,3:4])
% % @

% <<echo=FALSE>>=
% library(cluster)
% # add outliers to iris
% out = data.frame(Petal.Length = c(2.3, 3), Petal.Width = c(9.7, 3.7))
% iris2 = rbind(iris[, 3:4], out)

% # K-medoid vs. K-means
% kmedoid = pam(iris2, k = 3)
% km = kmeans(iris2, centers = 3, iter.max = 100, nstart = 100)
% @
% % \end{vbframe}

% % \begin{vbframe}{$k$-medoids in R}
% <<echo=FALSE, fig.width = 9, fig.height = 5, fig.align="center", out.width="0.85\\textwidth">>=
% set.seed(5)
% par(mfrow = c(1,2), mar = c(4,3,3,0))

% plot(iris2, col=kmedoid$clustering, pch=19, cex=1, main = "K-Medoids")
% points(kmedoid$medoids, col="red", pch=4, cex=3, lwd=3)

% plot(iris2, col = km$cluster, pch = 19, cex = 1, main = "K-Means")
% points(km$centers, col="red", pch=4, cex=3, lwd=3)
% @
% \vspace{-2cm}
% \end{vbframe}

\begin{vbframe}{Summary}

\lz

\begin{itemize}
  \item Minimizing the \textbf{within-cluster variation} exactly is not feasible and can be approximated by the $k$-means algorithm.
  \item $k$-means always converges, however, the cluster assignments strongly depend on the initial
  centers. \\
  $\rightarrow$ repeat it several times with different initial centers.
  \item A simple solution for choosing the number of clusters $k$ is to plot the
  \textbf{within-cluster variation} for several $k$ and look for an \textbf{"elbow"} which is a good guess for $k$.
\end{itemize}
\end{vbframe}

\endlecture
