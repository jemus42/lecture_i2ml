% Introduction to Machine Learning
% Day 3

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@

% Load all R packages and set up knitr
<<setup, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{Code demo for KNN}
\lecture{intro to ML}

\begin{vbframe}{kNN}

  Recap the functioning of k-NN:
  \begin{itemize}
    \item For each test data point calculate the distance to all train data points using any distance measures
    \item Most common measures
    \begin{itemize}
      \item Metric features:
      \begin{itemize}
      \item Euclidean distance: $D_2(a, b) = \sqrt{(a_1 - b_1)^2 + ... + (a_p - b_p)^2}$
      \item Manhattan distance: $D_1(a, b) = |a_1 - b_1| + ... + |a_p - b_p| $
      \item Mahalanobis distance: $D_{Mahalanobis}(a, b) = \sqrt{(a_i - b_i)^T \Sigma^{-1}(a_i - b_i)}$
      \end{itemize}
      \item Categorical features:
      \begin{itemize}
      \item[] Simple Matching Coefficient, Jaccard Coefficient, Rao's Coefficient
      \end{itemize}
      \item Select the k nearest neighbors for each test data point and use the most frequent neighborhood class as prediction
    \end{itemize}
  \end{itemize}

\framebreak

Use the dummy data set iris with only numeric features and split in test and train data:

<<echo = TRUE>>=
data(iris)
set.seed(1327)

trainSize = 3/4
trainIndices = sample(x = seq(1, nrow(iris), by = 1), size =
  ceiling(trainSize * nrow(iris)), replace = FALSE)
irisTrain = iris[ trainIndices, ]
irisTest = iris[ -trainIndices, ]
str(iris)
@

\framebreak

Imagine five data points:

<<echo = TRUE>>=
ill.df = iris[c(1, 2, 3, 55, 110), -4]
levels(ill.df[, "Species"]) = c(levels(ill.df[, "Species"]), "???")
ill.df[3, "Species"] = "???"
print(ill.df)
@

\framebreak

Which class would you select as prediction for the third observation?

<<echo = TRUE, tidy=TRUE, tidy.opts=list(width.cutoff=60), fig.height=3>>=

plot1 <- ggplot(ill.df, aes(x=Sepal.Length, y=Sepal.Width, color=Species, )) + geom_point(size=3)
plot2 <- ggplot(ill.df, aes(x=Sepal.Length, y=Petal.Length,color=Species)) + geom_point(size=3)
grid.arrange(plot1, plot2, ncol=2)

@
\framebreak

The function takes target Y, traindata, the testdata on which we want to perform knn classification as well as the k parameter. We also include the option to normalize the features (Why is that important?).

<<eval= TRUE, echo = FALSE, code=readLines("src/1/getknn.R")>>=
@

<<eval= FALSE, echo = TRUE, code=readLines("src/1/getknn.R", n=17)>>=
@

<<eval= FALSE, echo = TRUE, code=readLines("src/1/getknn.R", n=-1)[18:38]>>=
@

\framebreak
<<eval= TRUE, echo = TRUE, tidy=TRUE, tidy.opts=list(width.cutoff=58)>>=
# Test on 10 flowers
result = get.knn(Y = "Species", train.data = irisTrain, test.data = irisTest, k = 3, normalize = FALSE)
print(paste0("mean misclassififation error on test data: ", result$mmce))
head(cbind(irisTest, result$prediction))
@
\framebreak
\end{vbframe}

\begin{vbframe}{kNN}
Check the confusion matrix for the predictions

<<eval= TRUE, echo = TRUE, tidy=TRUE, tidy.opts=list(width.cutoff=58)>>=
print(table(result$prediction, irisTest$Species))
@

\framebreak

Run and test it for different k's:

<<eval= TRUE, echo = TRUE, tidy=TRUE, tidy.opts=list(width.cutoff=58)>>=
k.values = c(1, 2, 3, 5, 7, 9, 11, 15, 50)
storage = data.frame(matrix(NA, ncol = 2, nrow = length(k.values)))
colnames(storage) = c("mmce", "k")
for (i in 1:length(k.values)) {
    storage[i, "mmce"] = get.knn(Y = "Species", train.data = irisTrain,
                                 test.data = irisTest, k = k.values[i],
                                normalize = FALSE)$mmce
    storage[i, "k"] = k.values[i]
}
print(storage)
@

\framebreak
\begin{itemize}
\item We included the option to normalize features according to the rule

$$
x_{normalized} = \frac{x - max(x)}{max(x) - min(x)}
$$

\item This makes sense, if features are on totally different scales (e.g. centimeter vs. meter). In such a case we would compare apples with oranges and the distances would be weighted unequally.
\item Does it improve performannce with our data?
\end{itemize}
\framebreak
<<eval= TRUE, echo = TRUE, tidy=TRUE, tidy.opts=list(width.cutoff=58)>>=
k.values = c(1, 2, 3, 5, 7, 9, 11, 15, 50)
storage = data.frame(matrix(NA, ncol = 2, nrow = length(k.values)))
colnames(storage) = c("mmce", "k")
for (i in 1:length(k.values)) {
    storage[i, "mmce"] = get.knn(Y = "Species", train.data = irisTrain,
                                 test.data = irisTest, k = k.values[i],
                                normalize = TRUE)$mmce
    storage[i, "k"] = k.values[i]
}
print(storage)
@

\framebreak

The mlr package offers a unified interface to many different machine learning algorithms making complicated implementations as above unnecessary. Check the \href{https://mlr-org.github.io/mlr-tutorial/devel/html/learner/index.html#modifying-a-learner}{tutorial} and the \href{https://mlr-org.github.io/mlr-tutorial/release/html/integrated_learners/}{list of integrated learners}. It uses a simple syntax, as used below:

<<irisTask, eval= FALSE, echo = TRUE, tidy=TRUE, tidy.opts=list(width.cutoff=58)>>=
library(mlr)
# define task
irisTask = makeClassifTask(data = irisTrain, target = "Species")
# define learner and check possible models on mlr homepage
irisLearner = makeLearner("classif.kknn", k = 5)
# check available parameter settings
getParamSet(irisLearner)
@

<<irisTask, eval= TRUE, echo = FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=58)>>=
@

\framebreak

<<eval= TRUE, echo = TRUE>>=
# train the model
irisModel = train(learner = irisLearner, task = irisTask)
# predict on test data
irisPred = predict(irisModel, newdata = irisTest[, -5])
# check confusion matrix
print(table(irisPred$data$response, irisTest$Species))
# calculate mmce
(mmce =
  round(length(which(irisPred$data$response != irisTest$Species)) /
  length(irisPred$data$response), 5))
@

\end{vbframe}

\endlecture
