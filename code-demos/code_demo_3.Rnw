% Introduction to Machine Learning
% Day 3

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@

% Load all R packages and set up knitr
<<setup, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{999}{Code for Chapter 3}
\lecture{intro to ML}

\begin{vbframe}{Bias-Variance of holdout}

  Experiment:
  \begin{itemize}
    \item Data: simulate spiral data (sd = 0.1) from the \texttt{mlbench} package.
    \item Learner: CART (\texttt{classif.rpart} from \texttt{mlr}).
    \item Goal: estimate real performance of a model with $|\Dtrain| = 500$.
    \item Get the "true" estimator by repeatedly sampling 500 observations from the simulator,
      fit the learner, then evaluate on a really large number of observation.
    \item Analyse different types of holdout and subsampling (= repeated holdout), with different split rates:
    \begin{itemize}
    \item Sample $\D$ with $|\D| = 500$ and use split-rate $s \in \{0.05, 0.1, ..., 0.95\}$ for training with $|\Dtrain| = s \cdot 500$.
    \item Estimate performance on $\Dtest$ with $|\Dtest| = 500 \cdot (1 - s)$.
    \item Repeat the samping of $\D$ 50 times and the splitting with $s$ 50 times ($\Rightarrow$ 2500 experiments for each split-rate).
    \end{itemize}
  \end{itemize}

\framebreak

Visualize the perfomance estimator - and the MSE of the estimator - in relation to the true error rate.

<<>>=
load("rsrc/holdout-biasvar.RData")
@

<<echo = FALSE, fig.height = 5>>=
ggd1 = melt(res)
colnames(ggd1) = c("split", "rep", "ssiter", "mmce")
ggd1$split = as.factor(ggd1$split)
ggd1$mse = (ggd1$mmce -  realperf)^2
ggd1$type = "holdout"
ggd1$ssiter = NULL
mse1 = ddply(ggd1, "split", summarize, mse = mean(mse))
mse1$type = "holdout"

ggd2 = ddply(ggd1, c("split", "rep"), summarize, mmce = mean(mmce))
ggd2$mse = (ggd2$mmce -  realperf)^2
ggd2$type = "subsampling"
mse2 = ddply(ggd2, "split", summarize, mse = mean(mse))
mse2$type = "subsampling"

ggd = rbind(ggd1, ggd2)
gmse = rbind(mse1, mse2)

ggd$type = as.factor(ggd$type)
pl1 = ggplot(ggd, aes(x = split, y = mmce, col = type))
pl1 = pl1 + geom_boxplot()
pl1 = pl1 + geom_hline(yintercept = realperf)
#pl1 = pl1 + theme(axis.text.x = element_text(angle = 45))

gmse$split = as.numeric(as.character(gmse$split))
gmse$type = as.factor(gmse$type)

pl2 = ggplot(gmse, aes(x = split, y = mse, col = type))
pl2 = pl2 + geom_line()
pl2 = pl2 + scale_y_log10()
pl2 = pl2 + scale_x_continuous(breaks = gmse$split)

grid.arrange(pl1 + theme_minimal(), pl2 + theme_minimal(), layout_matrix = rbind(1,1,2))
@


% \framebreak
%
%   \begin{itemize}
%     \item The training error decreases with smaller training set size as it is easier for the model to learn the underlying structure in smaller training sets perfectly.
%     \item The test error (its bias) decreases with increasing training set size as the model generalizes better with more data, however, the variance increases as the test set size decreases at the same time.
%     \item The variance of the test error should decrease if we repeat the hold-out more often. %(here 10 vs. 100 repetitions):
%
% <<echo = FALSE, cache = TRUE, eval = FALSE, out.width="0.85\\textwidth", fig.height=3>>=
% res = rbind(cbind(res.rpart, repetitions = 100), cbind(res.rpart.small, repetitions = 20))
% res$repetitions = as.factor(res$repetitions)
%
% p1 = ggplot(data = subset(res, measure == "1"), aes(x = percentage, y = mmce)) +
%   geom_errorbar(aes(ymin = mmce - sd, ymax = mmce + sd, colour = repetitions), width = 0.025, position = position_dodge(width = 0.01)) +
%   geom_line(aes(colour = repetitions), position = position_dodge(width = 0.01)) +
%   geom_point(aes(colour = repetitions), position = position_dodge(width = 0.01)) +
%   ylab("Test error") +
%   xlab("Training set percentage") +
%   theme_minimal()
% p1
% @
%
% \end{itemize}

\end{vbframe}



\begin{vbframe}{Benchmarking in \texttt{mlr}}

\begin{itemize}
\item In a benchmark experiment different learning methods are applied to one or several data sets with the aim to compare and rank the algorithms with respect to one or more performance measures.
\item It is important that the train and test sets are synchronized, i.e. all learning methods see the same data splits so that they are better comparable.
\end{itemize}

We will first create a list of tasks and a list of learners:

<<echo = TRUE>>=
data("BostonHousing", "mtcars", "swiss", package = c("mlbench", "datasets"))
tasks = list(
  makeRegrTask(data = BostonHousing, target = "medv"),
  makeRegrTask(data = swiss, target = "Fertility"),
  makeRegrTask(data = mtcars, target = "mpg")
)
learners = list(
  makeLearner("regr.rpart"),
  makeLearner("regr.randomForest"),
  makeLearner("regr.lm")
)
@


\framebreak

The \texttt{benchmark} function from \texttt{mlr} allows you to compare all tasks and learners w.r.t. one or more measures based on the resampling method specified in the \texttt{resamplings} argument:

<<echo = -1, cache = TRUE>>=
set.seed(1)
(bmr = benchmark(learners, tasks, resamplings = cv10, measures = mlr::mse))
@

\end{vbframe}

\begin{vbframe}{Benchmarking in \texttt{mlr}: Access Data}

<<echo = TRUE, cache = TRUE>>=
head(getBMRAggrPerformances(bmr, as.df = TRUE), 3)
head(getBMRPerformances(bmr, as.df = TRUE), 3)
head(getBMRPredictions(bmr, as.df = TRUE), 3)
@

\end{vbframe}

\begin{vbframe}{Visualizing Performances}

Inspect the distribution of the performance measures using box plots:

<<echo = TRUE, cache = TRUE, out.width="0.8\\textwidth", fig.height=5>>=
plotBMRBoxplots(bmr, measure = mlr::mse)
@

\framebreak

The plot is a \texttt{ggplot2} object that can be changed (names, colors, etc.):

<<echo = TRUE, cache = TRUE, out.width="0.8\\textwidth", fig.height=5>>=
plotBMRBoxplots(bmr, measure = mlr::mse, style = "violin") +
  aes(color = learner.id)
@

\framebreak

Visualize the aggregated performance values as dot plot:

<<echo = TRUE, cache = TRUE, out.width="0.8\\textwidth", fig.height=5>>=
plotBMRSummary(bmr)
@

\framebreak

Plot the rankings of the aggregated performance values:

<<echo = TRUE, cache = TRUE, out.width="0.8\\textwidth", fig.height=5>>=
plotBMRSummary(bmr, trafo = "rank")
@

\end{vbframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%       d3s3
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{vbframe}{Nested Resampling}

  \begin{itemize}
    \item Regression task \pkg{mlbench.friedman1}.
    \item We jointly tune over a random forest and a neural net, with parameters \enquote{mtry}
      and \enquote{size} (with mlr's ModelMultiplexer).
    \item Random train and validation set with 100 + 100 observations.
    \item 50 iterations of random search tuning with holdout-splitting where we
      train on the training set and test on the validation set.
    \item After we are done, we store the best obtained configuration, and the obtained performance estimator
      on the validation set (biased).
    \item We sample another, small independent test set, with 100 observations, and a large one with 50K.
      We train the optimal model on the train set and test on both new test sets
      (that's the nested performance estimator and also the \enquote{real} performance estimator, which we usually do not have).
    \item This procedure was repeated 100 times.
  \end{itemize}

  \framebreak

<<fig.height=3, echo = FALSE>>=
ggd = load2("rsrc/nested-resample-example.RData")
pl = ggplot(ggd, aes(y = value, x = type)) + geom_boxplot()
pl = pl + scale_x_discrete(labels = c("wrong, biased resampling", "nested resampling", "'real' test error"))
pl = pl + ylab("MSE") + xlab("") + ylim(c(0,30))
pl = pl + theme(axis.text.x = element_text(angle = 0))
print(pl)
@
\begin{itemize}
  \item In reality we would train on the joint 100+100 train-valid-set observations.
    We don't do this here for a fair comparison with the naive strategy.
  \item Note the downward bias of \enquote{naive} tuning where model selected was not embedded in an
    outer resampling loop. Nested resampling get's it right.
\end{itemize}

\end{vbframe}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%          d3s4 ROC
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{vbframe}{ROC Analysis in R}
\begin{itemize}
  \item \texttt{generateThreshVsPerfData} calculates one or several performance measures for a sequence of decision thresholds from 0 to 1.
  \item It provides S3 methods for objects of class \texttt{Prediction}, \texttt{ResampleResult}
and \texttt{BenchmarkResult} (resulting from  \texttt{predict.WrappedModel}, \texttt{resample}
or \texttt{benchmark}).
  \item \texttt{plotROCCurves} plots the result of \texttt{generateThreshVsPerfData} using \texttt{ggplot2}.
  \item More infos \url{http://mlr-org.github.io/mlr-tutorial/release/html/roc_analysis/index.html}
\end{itemize}
\end{vbframe}

\begin{vbframe}{Example 1: Single predictions}
\scriptsize
<<echo=TRUE, message = FALSE>>=
set.seed(1)
# get train and test indices
n = getTaskSize(sonar.task)
train.set = sample(n, size = round(2/3 * n))
test.set = setdiff(seq_len(n), train.set)

# fit and predict
lrn = makeLearner("classif.lda", predict.type = "prob")
mod = train(lrn, sonar.task, subset = train.set)
pred = predict(mod, task = sonar.task, subset = test.set)
@
\normalsize
\end{vbframe}

\begin{vbframe}{Example 1: Single predictions}
We calculate fpr, tpr and compute error rates:

\scriptsize
<<echo = TRUE>>=
df = generateThreshVsPerfData(pred, measures = list(fpr, tpr, mmce))
@
\normalsize
\begin{itemize}
  \item \texttt{generateThreshVsPerfData} returns an object of class \texttt{ThreshVsPerfData},
which contains the performance values in the \texttt{\$data} slot.
  \item By default, \texttt{plotROCCurves} plots the performance values of the first two measures passed
to \texttt{generateThreshVsPerfData}.
  \item The first is shown on the x-axis, the second on the y-axis.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Example 1: Single predictions}
\scriptsize
<<echo = TRUE, fig.align="center", fig.width = 5, fig.height = 5, out.width="0.55\\textwidth">>=
df = generateThreshVsPerfData(pred, measures = list(fpr, tpr, mmce))
plotROCCurves(df)
@
\normalsize

\framebreak

The corresponding area under curve auc can be calculated by

\scriptsize
<<echo = TRUE>>=
performance(pred, auc)
@

\normalsize
\texttt{plotROCCurves} always requires a pair of performance measures that are plotted against
each other.

\framebreak

If you want to plot individual measures vs. the decision threshold, use

\scriptsize
<<echo = TRUE, fig.align="center", fig.height = 4, fig.width = 8, out.width="0.9\\textwidth">>=
plotThreshVsPerf(df)
@
\normalsize
\end{vbframe}


\begin{vbframe}{Example 2: Benchmark Experiment}
\scriptsize
<<>>=
options(width = 200)
@
<<echo = TRUE>>=
lrn1 = makeLearner("classif.randomForest", predict.type = "prob")
lrn2 = makeLearner("classif.rpart", predict.type = "prob")

cv5 = makeResampleDesc("CV", iters = 5)

bmr = benchmark(learners = list(lrn1, lrn2), tasks = sonar.task,
  resampling = cv5, measures = list(auc, mmce), show.info = FALSE)
bmr
@
\normalsize

Calling \texttt{generateThreshVsPerfData} and \texttt{plotROCCurves} on the \texttt{BenchmarkResult}
produces a plot with ROC curves for all learners in the experiment.

\framebreak

\scriptsize
<<echo = TRUE, fig.align="center", fig.height = 4, fig.width = 8, out.width="\\textwidth">>=
df = generateThreshVsPerfData(bmr, measures = list(fpr, tpr, mmce))
plotROCCurves(df)
@
\framebreak

\scriptsize
<<echo = TRUE, fig.align="center", fig.height = 4, fig.width = 8, out.width="\\textwidth">>=
plotThreshVsPerf(df)
@
\end{vbframe}







\endlecture
