\begin{vbframe}{Tree building algorithms and their implementation in R}
\begin{itemize}
\item CART (Breiman et al., 1984):
  
  Package \pkg{rpart} with fitting function \code{rpart()}.
\begin{itemize}
\item No nice plotting functionalities.
\item But \pkg{rattle} is able to create fancy visualizations.
\end{itemize}

\item Unbiased Recursive Partitioning (Hothorn et al., 2006):
  
  \pkg{partykit} (old: \pkg{party}) provides the function \code{ctree()} for recursive partitioning in a conditional inference framework.
\begin{itemize}
\item Supports continuous, censored, ordered, nominal and multivariate response variables.
\item \code{ctree()} uses unbiased split selection.
\item Nice plotting functionality.
\end{itemize}

\item C4.5 (Quinlan, 1993):
  

%\item \pkg{evtree} (function \code{evtree()}):
  
  %Uses evolutionary algorithms, which is a global optimization method to search over the parameter space of trees instead of performing a forward stepwise search like in CART.
\end{itemize}
\end{vbframe}


####################################d4s3



\begin{figure}
<<echo=FALSE, message=FALSE, warning=FALSE, fig.height=4.5>>=
  library(mlr)
set.seed(1)
lrn = makeLearner("regr.randomForest", predict.type = "response")
# remove this values from title
lrn$par.vals$se.boot = NULL
lrn$par.vals$ntree.for.se = NULL
lrn2 = makeLearner("classif.randomForest")
# remove this values from title
lrn2$par.vals$se.boot = NULL
lrn2$par.vals$ntree.for.se = NULL
task = convertMLBenchObjToTask("mlbench.friedman1", n = 500, sd = 0.1)
plotLearnerPrediction(lrn, task, cv = 0, ntree = 1)
@
  \caption{randomForest trained on the \enquote{friedman1} regression task
    from the \pkg{mlbench} \pkg{R}-package with increasing number of trees}
\end{figure}

\framebreak
<<echo=FALSE>>=
  plotLearnerPrediction(lrn, task, cv = 0, ntree = 10)
@
  \framebreak
<<echo=FALSE>>=
  plotLearnerPrediction(lrn, task, cv = 0, ntree = 500)
@
  \framebreak
\begin{figure}
<<echo=FALSE, fig.height=4.5>>=
  plotLearnerPrediction(lrn2, iris.task, cv = 0, ntree = 1)
@
  \caption{randomForest on \enquote{iris} for increasing number of trees}
\end{figure}
\framebreak
<<echo=FALSE>>=
  plotLearnerPrediction(lrn2, iris.task, cv = 0, ntree = 10)
@
  \framebreak
<<echo=FALSE>>=
  plotLearnerPrediction(lrn2, iris.task, cv = 0, ntree = 500)
@
  \framebreak

<<echo=FALSE>>=
  mod = train(lrn, task)$learner.model
plot(mod, main = "OOB error for different number of trees, call plot(model) on rf for this")
@
  
  \end{vbframe}

########################################d4s4
\begin{vbframe}{Benchmark: Random Forest vs. (bagged) rpart vs. (bagged) knn}

\begin{itemize}
\item Goal: Compare performance of random forest against (bagged) stable and (bagged) unstable methods
\item Algorithms:
  \begin{itemize}
\item classification tree (\code{rpart})
\item bagged classification tree using 50 bagging iterations (\code{bagged.rpart})
\item k-nearest neighbors (\code{knn})
\item bagged k-nearest neighbors using 50 bagging iterations (\code{bagged.knn})
\item random forest with 50 trees (\code{rF})
\end{itemize}
\item Method to evaluate performance: 10-fold cross-validation
\item Performance measure: mean missclassification error
\end{itemize}

\framebreak

\begin{itemize}
\item Datasets from \pkg{mlbench}:
  \end{itemize}

\begin{table}
\footnotesize
\begin{tabular}{p{1.5cm}p{2cm}p{0.5cm}p{0.5cm}p{5cm}}
Name & Kind of data &  n & p & Task\\
\hline
Glass & Glass identification data & 214 & 10 & Predict the type of glass on the basis of the chemical analysis of the glasses represented by the 10 features\\
Ionosphere & Radar data & 351 & 35 & Predict whether the radar returns show evidence of some type of structure in the ionosphere (\enquote{good}) or not (\enquote{bad}) \\
Sonar & Sonar data & 208 & 61 & Discriminate between sonar signals bounced off a metal cylinder (\enquote{M}) and those bounced off a cylindrical rock (\enquote{R})\\
Waveform & Artificial data & 100 & 21 & Simulated 3-class problem which is considered to be a difficult pattern recognition problem. Each class is generated by the waveform generator.\\
\hline
\end{tabular}
\end{table}

\framebreak

\begin{center}
\includegraphics[height = 7.2cm, keepaspectratio]{figure_man/bm_stable_vs_unstable.jpg}
\end{center}

\framebreak

Bagging knn does not improve performance because:
  
  \begin{itemize}
\item knn is stable w.r.t. perturbations
\item Each bootstrap sample contains about 63\% unique observations
$$ \P(\text{obs. drawn}) = \left(1 - \frac{1}{n}\right)^n = \ \stackrel{n \to \infty}{\longrightarrow} \ 1 - \frac{1}{e} \approx 0.63$$
  \item In a 2-class problem classification may only change if for a test case the nearest neighbor in the learning set is \textbf{not} in at least half of the bootstrap samples.
\item But probability that an observation is drawn into the bootstrap sample is 63\% which is greater than 50\%.
\end{itemize}

\end{vbframe}
