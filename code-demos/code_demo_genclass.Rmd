---
output: pdf_document
params:
  set_title: "Code demo for generative classification methods"
---
  
```{r, child = "../style/preamble_code_demos.Rmd", include = FALSE}

```

```{r, child = "../style/setup.Rmd", include = FALSE}

```  

```{r, echo=FALSE}
knitr::read_chunk("code_demo_knn.Rmd")
```  

```{r knn-plot_2D_classif, echo=FALSE}

```

# Code demo for generative classification methods

not modeling cond density directly

$$
\pi_k(x) \propto \pi_k \cdot p(y = k | x)
$$

$$
\hat{\pi}_k = \frac{n_k}{n}
$$

$$
1 = \sum^{g}_{j=1}\alpha \pi_j \cdot p(y = j | x) \iff \alpha = \frac{1}{\sum^{g}_{j=1}\pi_j \cdot p(y = j | x)}
$$

$$
\pi_k(x) = \frac{\pi_k \cdot p(y = k | x)}{\sum^{g}_{j=1}\pi_j \cdot p(y = j | x)}
$$

```{r genclass-plot_data}
library(mlr)

data(iris)
iris_train = iris[, c("Species","Sepal.Width","Sepal.Length")]
iris_task = makeClassifTask(data = iris_train, target = "Species")

ggplot(iris_train, aes(x=Sepal.Width, y=Sepal.Length)) + 
  geom_point(aes(color=Species))
```

## Linear discriminant analysis (LDA)

In LDA, we caclulate the posteriori class probabilities like that:
$$
p(y = k | x) = \frac{1}{\pi^{\frac{p}{2}} |\Sigma|^{\frac{1}{2}}}\exp(- \frac{1}{2} (x-\mu_k)^T\Sigma^{-1}(x-\mu_k) )
$$

* $\hat{\mu}_k = \frac{1}{n_k}\sum_{i: y^{(i)} = k} x^{(i)}$  
* $\hat{\Sigma} = \frac{1}{n - g} \sum_{k=1}^g\sum_{i: y^{(i)} = k} (x^{(i)} - \hat{\mu}_k)(x^{(i)} - \hat{\mu}_k)^T$  
    
```{r genclass-lda_plot}
iris_lda_learner = makeLearner("classif.lda")
iris_lda_model = train(learner = iris_lda_learner, task = iris_task)

plotLearnerPrediction(iris_lda_learner, iris_task)
```
    
## Quadratic discriminant analysis (QDA)

In QDA, we caclulate the posteriori class probabilities like that:
$$
p(y = k | x) = \frac{1}{\pi^{\frac{p}{2}} |\Sigma_k|^{\frac{1}{2}}}\exp(- \frac{1}{2} (x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k) )
$$

* $\hat{\mu}_k = \frac{1}{n_k}\sum_{i: y^{(i)} = k} x^{(i)}$  
* $\hat{\Sigma}_k = \frac{1}{n_k - 1} \sum_{i: y^{(i)} = k} (x^{(i)} - \hat{\mu}_k)(x^{(i)} - \hat{\mu}_k)^T$  
    
```{r genclass-qda_plot}
iris_qda_learner = makeLearner("classif.qda")
iris_qda_model = train(learner = iris_qda_learner, task = iris_task)

plotLearnerPrediction(iris_qda_learner, iris_task)
```
  
## Naive Bayes

Here, we make a “naive” conditional independence assumption.

$$
p(y = k | x) = \prod^{p}_{j=1}p(x_j | y=k)
$$


```{r genclass-nb_priors}
get_priors <- function(df,target){
  as.list(prop.table(table(df[,target])))
}
```


```{r genclass-nb_lhoods}
# this can be written way more elegantly with S3 methods

likelihood <- function(likelihood_def, data) {
   switch(likelihood_def$type,
          gaussian = list(mean = mean(data), sd = sd(data), type="gaussian"),
          categorical = list(freq_table = prop.table(table(data)), type = "categorical")
   )
}

predict_likelihood <- function(likelihood, x){
   switch(likelihood$type,
          gaussian = dnorm(x, mean = likelihood$mean, sd = likelihood$sd),
          categorical = likelihood$freq_table[
            which(x == names(likelihood$freq_table))]
   )  
}

get_likelihoods <- function(df,target, likelihood_defs = NULL) {
  X <- df[,!(names(df) %in%target)]
  features <- colnames(X)
  target_levels <- levels(df[,target])
  # default to gaussian density estimates:
  if (is.null(likelihood_defs)) {
     likelihood_defs  <- sapply(features, function(feature)
                           list(args = list(), type = 
                                  ifelse(is.factor(df[,feature]),
                                                   "categorical",
                                                   "gaussian")), 
                           simplify = FALSE,
                           USE.NAMES = TRUE)
  }
  
  
  # loop for all target levels over all features and compute conditional density
  likelihoods <- lapply(
    target_levels, 
    function(level) {
      sapply(features, function(feature) {
        feature_data <- X[df[[target]] == level, feature]
        likelihood(likelihood_defs[[feature]],feature_data)
      }, simplify = FALSE)
    })
  names(likelihoods) <- target_levels
  likelihoods
}
```

```{r genclass-naivebayes}
norm1 <- function(x) x / sum(abs(x))

naivebayes <- function(X, priors, likelihoods){
  res <- matrix(data=NA_real_,nrow=nrow(X),ncol=length(priors))
  features <- colnames(X)
  # for level
  #   for feature
  #     compute res[[level]][, feature] <-  f(feature|level)
  # dann jeweils exp(rowSums(log(res)))
  
  for (i in 1:nrow(X)){
    res[i,] <-sapply(
      names(priors), 
      function(level) {
        priors[[level]] * 
          prod(
            sapply(features, 
                   FUN = function(feature) {
                     likelihood <- likelihoods[[level]][[feature]]
                     predict_likelihood(likelihood,X[i,feature])
                   }))
      }
      
    )
    res[i,] <- norm1(res[i,])
    
  }
  colnames(res) <- names(priors)
  res                          
}
```

```{r genclass-nb_example}
target <- "Species"
likelihoods <- get_likelihoods(iris_train,target)
priors <- get_priors(iris_train,target)

X <- iris_train[,!(names(iris_train) %in%target)]
head(naivebayes(X, priors, likelihoods))
```

```{r genclass-classify_naivebayes}
classify_naivebayes <- function(X, priors, likelihoods){
  nb <- naivebayes(X, priors, likelihoods)
  
  list(
    prediction =   names(sapply(1:nrow(nb), function(i)
    which.max(nb[i,])
  )),
    levels = names(priors)
  )
}
```

```{r genclass-plot_2D_naivebayes}
# function for naive bayes with two features to visualize class
# boundaries X1 and X2 are the names of the two features to use.

plot_2D_naivebayes <- function(priors, likelihoods, X1 = "X1", X2 = "X2",
                           # by default, we "predict" the class of the training data:
                           to_classify_labels = as.character(Y),
                           to_classify_data = data.frame(
                             "X1" = X[, 1],
                             "X2" = X[, 2]
                           ),
                           lengthX1 = 100, lengthX2 = 100,
                           title = '"Naive Bayes" ~',
                           plot_class_rate = TRUE) {
  plot_2D_classify(
    to_classify_labels = to_classify_labels,
    to_classify_data = to_classify_data,
    classify_method = function(to_classify_data)
      classify_naivebayes(
        X = to_classify_data,
        priors, likelihoods
      ),
    X1, X2,
    lengthX1 = lengthX1, lengthX2 = lengthX2,
    title = title,
    plot_class_rate = plot_class_rate
  )
}
```
```{r}

plot_2D_naivebayes(priors, likelihoods, "Sepal.Width", "Sepal.Length",
                   iris_train$Species, X)
```

```{r genclass-naivebayes_plot}
iris_nb_learner = makeLearner("classif.naiveBayes")
iris_nb_model = train(learner = iris_nb_learner, task = iris_task)

plotLearnerPrediction(iris_nb_learner, iris_task)
```

https://en.wikipedia.org/wiki/Kernel_(statistics)#Kernel_functions_in_common_use

```{r genclass-kde_kernels}
library("kdensity")
#library("EQL")
par(mfrow=c(1,2))
for (kernel in c("gaussian","triangular") )
  plot(kdensity(iris_train[,"Sepal.Width"], start = "gumbel", kernel = kernel),
       main=paste0("Kernel: ", kernel))
```

```{r genclass-kde_bw}
par(mfrow=c(2,2))
for (bw in seq(0.1,0.5,length.out = 4) )
  plot(kdensity(iris_train[,"Sepal.Length"], start = "gumbel", 
                kernel = "gaussian", bw = bw),
       main=paste0("KDE of Sepal.Lenght"))
```

```{r genclass-nb_kde}

likelihood <- function(likelihood_def, data) {
   switch(likelihood_def$type,
          gaussian = list(mean = mean(data), sd = sd(data), type="gaussian"),
          categorical = list(freq_table = prop.table(table(data)), type = "categorical"),
          kde = list(kde =
            do.call(kdensity, append(likelihood_def$args,structure(list(data),names="x"))),
             type="kde"), 
   )
}

predict_likelihood <- function(likelihood, x){
   switch(likelihood$type,
          gaussian = dnorm(x, mean = likelihood$mean, sd = likelihood$sd),
          categorical = likelihood$freq_table[
            which(x == names(likelihood$freq_table))],
          kde = likelihood$kde(x)
   )  
}

likelihoods_kde <- get_likelihoods(iris_train,target,sapply(colnames(iris_train[,!(names(iris_train) %in% target)]), function(feature)
                           list(args = list(), type = "kde"), 
                           simplify = FALSE,
                           USE.NAMES = TRUE))

predict_likelihood(likelihoods_kde$setosa$Sepal.Width,c(10,20,5))

plot_2D_naivebayes(priors, likelihoods_kde, "Sepal.Width", "Sepal.Length",
                   iris_train$Species, X)
```

