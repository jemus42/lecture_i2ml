---
output: pdf_document
params:
  set_title: "Code demo for generative classification methods"
---
  
```{r, child = "../style/preamble_code_demos.Rmd", include = FALSE}

```

```{r, child = "../style/setup.Rmd", include = FALSE}

```  

```{r, echo=FALSE}
knitr::read_chunk("code_demo_knn.Rmd")
```  

```{r knn-plot_2D_classif, echo=FALSE}

```

# Code demo for generative classification methods

not modeling cond density directly

$$
\pi_k(x) \propto \pi_k \cdot p(x | y = k)
$$

$$
\hat{\pi}_k = \frac{n_k}{n}
$$

$$
1 = \sum^{g}_{j=1}\alpha \pi_j \cdot p(x | y = j) \iff \alpha = \frac{1}{\sum^{g}_{j=1}\pi_j \cdot p (x | y = j)}
$$

$$
\pi_k(x) = \frac{\pi_k \cdot p(x | y = k)}{\sum^{g}_{j=1}\pi_j \cdot p(x | y = j)}
$$

```{r genclass-plot_data}
library(mlr)

data(iris)
target <- "Species"
features <- c("Sepal.Width","Sepal.Length")
iris_train = iris[, c(target, features)]
target_levels <- levels(iris_train[,target])
iris_task = makeClassifTask(data = iris_train, target = "Species")

ggplot(iris_train, aes(x=Sepal.Width, y=Sepal.Length)) + 
  geom_point(aes(color=Species))
```

## Linear discriminant analysis (LDA)

In LDA, we caclulate the posteriori class probabilities like that:
$$
p(x | y = k) = \frac{1}{\pi^{\frac{p}{2}} |\Sigma|^{\frac{1}{2}}}\exp(- \frac{1}{2} (x-\mu_k)^T\Sigma^{-1}(x-\mu_k) )
$$

* $\hat{\mu}_k = \frac{1}{n_k}\sum_{i: y^{(i)} = k} x^{(i)}$  
* $\hat{\Sigma} = \frac{1}{n - g} \sum_{k=1}^g\sum_{i: y^{(i)} = k} (x^{(i)} - \hat{\mu}_k)(x^{(i)} - \hat{\mu}_k)^T$  
    
```{r genclass-lda_plot}
iris_lda_learner = makeLearner("classif.lda")
iris_lda_model = train(learner = iris_lda_learner, task = iris_task)


plotLearnerPrediction(iris_lda_learner, iris_task)
```
```{r genclass-lda_likelihood}
library(mvtnorm)


get_mvgaussian_lda <- function(data, target, level, features){
  classif_task = makeClassifTask(data = data[,c(features, target)], target = target)
  lda_learner = makeLearner("classif.lda")
  lda_model = train(learner = lda_learner, task = classif_task)  
  
  list(mean = lda_model$learner.model$means[level,], 
       sigma = lda_model$learner.model$scaling %*% t(lda_model$learner.model$scaling), 
       type="mv_gaussian", 
       features = features)
}

# this can be written way more elegantly with S3 methods


likelihood <- function(likelihood_def, data) {
  # will be extended
   switch(likelihood_def$type,
          mvgaussian_lda = get_mvgaussian_lda(data, likelihood_def$target,
                                              likelihood_def$level, 
                                              likelihood_def$features)
   )
}

predict_likelihood <- function(likelihood, x){
   switch(likelihood$type,
         mv_gaussian = dmvnorm(x, mean=likelihood$mean, 
                               sigma = likelihood$sigma)
   )  
}

```

```{r genclass-plot2D_lik_mv}
library(reshape2)

plot_2D_likelihood <- function(likelihoods, data, X1, X2, target, lengthX1 = 100, 
                               lengthX2 = 100){
  gridX1 <- seq(
    min(data[, X1]),
    max(data[, X1]),
    length.out = lengthX1
  )
  gridX2 <- seq(
    min(data[, X2]),
    max(data[, X2]),
    length.out = lengthX2
  )

  # compute grid coordinates with cartesian product
  grid_data <- expand.grid(gridX1, gridX2)  
  features <-  c(X1, X2)
  target_levels <- names(likelihoods)
  names(grid_data) <- features
  
  lik <- sapply(target_levels, function(level) { 
    likelihood <- likelihoods[[level]]
    predict_likelihood(likelihood, grid_data[,likelihood$features])
  })
  
  grid_data <- cbind(grid_data, lik)
  
  to_plot <- melt(grid_data, id.vars = features)
  
    ggplot() +
    geom_contour(
      data = to_plot,
      aes_string(x = X1, y = X2, z = "value", color = "variable")
    ) + 
    geom_point(data=data, aes_string(x=X1,y=X2, color= target))
}

likelihoods <- sapply(target_levels, function(level) 
  likelihood(list(type="mvgaussian_lda", target=target,
                level=level, features = features), iris_train), simplify = FALSE)

plot_2D_likelihood(likelihoods,iris_train, "Sepal.Width", "Sepal.Length", target)

```
    
## Quadratic discriminant analysis (QDA)

In QDA, we caclulate the posteriori class probabilities like that:
$$
p(x | y = k) = \frac{1}{\pi^{\frac{p}{2}} |\Sigma_k|^{\frac{1}{2}}}\exp(- \frac{1}{2} (x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k) )
$$

* $\hat{\mu}_k = \frac{1}{n_k}\sum_{i: y^{(i)} = k} x^{(i)}$  
* $\hat{\Sigma}_k = \frac{1}{n_k - 1} \sum_{i: y^{(i)} = k} (x^{(i)} - \hat{\mu}_k)(x^{(i)} - \hat{\mu}_k)^T$  
    
```{r genclass-qda_plot}
iris_qda_learner = makeLearner("classif.qda")
iris_qda_model = train(learner = iris_qda_learner, task = iris_task)

iris_qda_model$learner.model$means
iris_qda_model$learner.model$scaling

plotLearnerPrediction(iris_qda_learner, iris_task)
```

```{r genclass-qda_likelihood}

get_mvgaussian_qda <- function(data, target, level, features){
  classif_task = makeClassifTask(data = data[,c(features, target)], target = target)
  qda_learner = makeLearner("classif.qda")
  qda_model = train(learner = qda_learner, task = classif_task)  
  
  list(mean = qda_model$learner.model$means[level,], 
       sigma = qda_model$learner.model$scaling[,,level]
          %*% t(qda_model$learner.model$scaling[,,level]), 
       type="mv_gaussian", 
       features = features)
}

# this can be written way more elegantly with S3 methods


likelihood <- function(likelihood_def, data) {
  # will be extended
   switch(likelihood_def$type,
          mvgaussian_lda = get_mvgaussian_lda(data, likelihood_def$target,
                                              likelihood_def$level, 
                                              likelihood_def$features),
          mvgaussian_qda = get_mvgaussian_qda(data, likelihood_def$target,
                                              likelihood_def$level, 
                                              likelihood_def$features)          
   )
}

likelihoods <- sapply(target_levels, function(level) 
  likelihood(list(type="mvgaussian_qda", target=target,
                level=level, features = features), iris_train), simplify = FALSE)

plot_2D_likelihood(likelihoods,iris_train, "Sepal.Width", "Sepal.Length", target)
```
  
## Naive Bayes

Here, we make a “naive” conditional independence assumption.

$$
p(x | y = k) = \prod^{p}_{j=1}p(x_j | y=k)
$$


```{r genclass-nb_priors}
get_priors <- function(df,target){
  as.list(prop.table(table(df[,target])))
}
```


```{r genclass-nb_lhoods}
# this can be written way more elegantly with S3 methods

likelihood <- function(likelihood_def, data) {
  # will be extended
   switch(likelihood_def$type,
          mvgaussian_lda = get_mvgaussian_lda(data, likelihood_def$target,
                                              likelihood_def$level, 
                                              likelihood_def$features),
          mvgaussian_qda = get_mvgaussian_qda(data, likelihood_def$target,
                                              likelihood_def$level, 
                                              likelihood_def$features),
          gaussian = list(mean = mean(data[,likelihood_def$features]), sd = sd(data[,likelihood_def$features]),
                          features = likelihood_def$features, type="gaussian"),
          categorical = list(freq_table = prop.table(table(data)), 
                           features = likelihood_def$features, type = "categorical"),
          naivebayes = list(likelihoods = sapply(likelihood_def$likelihood_defs,
                                                 function(likelihood_def)
                                                   likelihood(likelihood_def, data),
                                                   simplify = FALSE),
                            features =  likelihood_def$features, type="naivebayes")
   )
}

get_naivebayes_likelihoods <- function(df,target, likelihood_defs = NULL) {
  X <- df[,!(names(df) %in%target)]
  features <- colnames(X)
  target_levels <- levels(df[,target])
  # default to gaussian density estimates:
  if (is.null(likelihood_defs)) {
     likelihood_defs  <- sapply(features, function(feature)
                           list(args = list(), 
                                features = feature,
                                  type = 
                                  ifelse(is.factor(df[,feature]),
                                                   "categorical",
                                                   "gaussian")), 
                           simplify = FALSE,
                           USE.NAMES = TRUE)
  }
  
  naivebayes_def <- list(type= "naivebayes",likelihood_defs = likelihood_defs,
                         features = features)
  # loop for all target levels over all features and compute conditional density
  print(naivebayes_def)
  sapply(
    target_levels, 
    function(level) {
      likelihood(naivebayes_def , X[df[[target]] == level,features])
    }, simplify = FALSE)
}

```

```{r genclass-plot2D_lik}
predict_likelihood <- function(likelihood, x){
  if(is.data.frame(x)) x = x[,likelihood$features]
   switch(likelihood$type,
          mv_gaussian = dmvnorm(x, mean=likelihood$mean, 
                               sigma = likelihood$sigma),
          gaussian = dnorm(x, mean = likelihood$mean, sd = likelihood$sd),
          categorical = likelihood$freq_table[
            which(x == names(likelihood$freq_table))],
          naivebayes =  exp(rowSums(log(sapply(likelihood$likelihoods, function(likelihood)
            predict_likelihood(likelihood, x)))))
   )  
}

likelihoods <- get_naivebayes_likelihoods(iris_train, target)
plot_2D_likelihood(likelihoods,iris_train, "Sepal.Width", "Sepal.Length", target)

```

```{r genclass-naivebayes}
norm1 <- function(x) x / sum(abs(x))

naivebayes <- function(X, priors, nb_likelihood, normalize=FALSE){

  features <- colnames(X)
  target_levels <- names(priors)
  
  lik <- sapply(target_levels, function(level) { 
    likelihood <- likelihoods[[level]]
    predict_likelihood(likelihood, X[,likelihood$features])
  })
  
  joint_density <- unlist(priors) * lik
  
  if(normalize){
    t(apply(joint_density, 1, norm1))
  } else{
    joint_density
  }
}
```

```{r genclass-nb_example}
priors <- get_priors(iris_train,target)

X <- iris_train[,!(names(iris_train) %in%target)]
head(naivebayes(X, priors, likelihoods, normalize = TRUE))
```

```{r genclass-classify_naivebayes}
classify_naivebayes <- function(X, priors, likelihoods){
  nb <- naivebayes(X, priors, likelihoods)
  
  list(
    prediction =   names(sapply(1:nrow(nb), function(i)
    which.max(nb[i,])
  )),
    levels = names(priors)
  )
}
```

```{r genclass-plot_2D_naivebayes}
# function for naive bayes with two features to visualize class
# boundaries X1 and X2 are the names of the two features to use.

plot_2D_naivebayes <- function(priors, likelihoods, X1 = "X1", X2 = "X2",
                           # by default, we "predict" the class of the training data:
                           to_classify_labels = as.character(Y),
                           to_classify_data = data.frame(
                             "X1" = X[, 1],
                             "X2" = X[, 2]
                           ),
                           lengthX1 = 100, lengthX2 = 100,
                           title = '"Naive Bayes" ~',
                           plot_class_rate = TRUE) {
  plot_2D_classify(
    to_classify_labels = to_classify_labels,
    to_classify_data = to_classify_data,
    classify_method = function(to_classify_data)
      classify_naivebayes(
        X = to_classify_data,
        priors, likelihoods
      ),
    X1, X2,
    lengthX1 = lengthX1, lengthX2 = lengthX2,
    title = title,
    plot_class_rate = plot_class_rate
  )
}
```
```{r}

plot_2D_naivebayes(priors, likelihoods, "Sepal.Width", "Sepal.Length",
                   iris_train$Species, X)
```

```{r genclass-naivebayes_plot}
iris_nb_learner = makeLearner("classif.naiveBayes")
iris_nb_model = train(learner = iris_nb_learner, task = iris_task)

plotLearnerPrediction(iris_nb_learner, iris_task)
```

https://en.wikipedia.org/wiki/Kernel_(statistics)#Kernel_functions_in_common_use

```{r genclass-kde_kernels}
library("kdensity")
#library("EQL")
par(mfrow=c(1,2))
for (kernel in c("gaussian","triangular") )
  plot(kdensity(iris_train[,"Sepal.Width"], start = "gumbel", kernel = kernel),
       main=paste0("Kernel: ", kernel))
```

```{r genclass-kde_bw}
par(mfrow=c(2,2))
for (bw in seq(0.1,0.5,length.out = 4) )
  plot(kdensity(iris_train[,"Sepal.Length"], start = "gumbel", 
                kernel = "gaussian", bw = bw),
       main=paste0("KDE of Sepal.Lenght"))
```

```{r genclass-nb_kde}
likelihood <- function(likelihood_def, data) {
  # will be extended
   switch(likelihood_def$type,
          mvgaussian_lda = get_mvgaussian_lda(data, likelihood_def$target,
                                              likelihood_def$level, 
                                              likelihood_def$features),
          mvgaussian_qda = get_mvgaussian_qda(data, likelihood_def$target,
                                              likelihood_def$level, 
                                              likelihood_def$features),
          gaussian = list(mean = mean(data[,likelihood_def$features]), sd = sd(data[,likelihood_def$features]),
                          features = likelihood_def$features, type="gaussian"),
          categorical = list(freq_table = prop.table(table(data)), 
                           features = likelihood_def$features, type = "categorical"),
          kde = list(kde =
            do.call(kdensity, append(likelihood_def$args,structure(list(data),names="x"))),
             type="kde", features = likelihood_def$features),           
          naivebayes = list(likelihoods = sapply(likelihood_def$likelihood_defs,
                                                 function(likelihood_def)
                                                   likelihood(likelihood_def, data),
                                                   simplify = FALSE),
                            features =  likelihood_def$features, type="naivebayes")
   )
}

predict_likelihood <- function(likelihood, x){
  if(is.data.frame(x)) x = x[,likelihood$features]
   switch(likelihood$type,
          mv_gaussian = dmvnorm(x, mean=likelihood$mean, 
                               sigma = likelihood$sigma),
          gaussian = dnorm(x, mean = likelihood$mean, sd = likelihood$sd),
          categorical = likelihood$freq_table[
            which(x == names(likelihood$freq_table))],
          kde = likelihood$kde(x),
          naivebayes =  exp(rowSums(log(sapply(likelihood$likelihoods, function(likelihood)
            predict_likelihood(likelihood, x)))))
   )  
}


likelihoods_kde <- get_naivebayes_likelihoods(iris_train,target,sapply(features, function(feature)
                           list(args = list(), 
                                features = feature,
                                  type = "kde"), 
                           simplify = FALSE,
                           USE.NAMES = TRUE))

plot_2D_likelihood(likelihoods_kde,iris_train, "Sepal.Width", "Sepal.Length", target)

plot_2D_naivebayes(priors, likelihoods_kde, "Sepal.Width", "Sepal.Length",
                   iris_train$Species, X)
```


```{r genclass-nb_kde_bws}

bw_values <- seq(0.05,0.3,length.out = 4)
ggplot_list <- lapply(
  bw_values,
  function(bw) {
  likelihoods_kde_bw <- get_likelihoods(iris_train,target,sapply(colnames(iris_train[,!(names(iris_train) %in% target)]), 
                           function(feature)  list(args = list(bw = bw), type = "kde"), 
                           simplify = FALSE,
                           USE.NAMES = TRUE))
  plot_2D_naivebayes(priors, likelihoods_kde_bw, "Sepal.Width", "Sepal.Length",
                   iris_train$Species, X, title='"NB" ~')
  }
)
do.call(grid.arrange, ggplot_list)
```