---
output: pdf_document
params:
  set_title: "Code demo for Kaggle Challenge"
---

```{r kaggle-preamble, child = "../style/preamble_code_demos.Rmd", include = FALSE, purl = FALSE}

```

```{r kaggle-setup, child = "../style/setup.Rmd", include = FALSE, purl = FALSE}

```  

# Code demo for Kaggle Challenge

In this code demo we

* use cart to compete in a kaggle challenge,
* learn how to make a submission for the challenge,
* improve the model by using feature engineering.

## Introductory kaggle challenge

We will compete in our first [kaggle challenge](https://www.kaggle.com/c/titanic) on the prediction of titanic survivors. 

### Preprocessing and Data check

```{r kaggle_data}
### Data preprocess

# load and check the data
all_train <- read.csv(file = "data/train_titanic.csv")
str(all_train)
# no target column "survived" on test dataset
all_test <- read.csv(file = "data/test_titanic.csv")

# can we use all features?
# Nope: delete those with too many levels as this would inflate the model
# also kill the ID

train <- all_train[, -c(
  which(colnames(all_train) == "Cabin"),
  which(colnames(all_train) == "Name"),
  which(colnames(all_train) == "Ticket"),
  which(colnames(all_train) == "PassengerId")
)]

test <- all_test[, -c(
  which(colnames(all_test) == "Cabin"),
  which(colnames(all_test) == "Name"),
  which(colnames(all_test) == "Ticket"),
  which(colnames(all_test) == "PassengerId")
)]
```

### Build a first simple model with mlr and check the performance via CV

```{r kaggle-cv_cart, warning=FALSE}
### model corner
library(mlr)

# choose specific model and parameters
task <- makeClassifTask(data = train, target = "Survived")

learner <- makeLearner(cl = "classif.rpart")
# check choosable parameters and set accordingly
getLearnerParamSet(learner)
# check available settings here:
# https://www.rdocumentation.org/packages/rpart/versions/4.1-12/topics/rpart.control
learner <- makeLearner(
  cl = "classif.rpart",
  par.vals = list(minsplit = 10, cp = 0.05)
)

# make sure to assign mlr package to train
mod <- mlr::train(learner = learner, task = task)

### performance estimate via CV
cv <- makeResampleDesc(method = "CV", iters = 10)
# use mlr::listMeasures() to get list of possible measures
# important: always check on which measure they evaluate you!
mlr::crossval(learner = learner, task = task, iters = 10, 
              measures = list(mmce, acc))
```

Store and submit your predictions

```{r kaggle-cv_cart_sub}
# predict for submission
pred <- predict(mod, newdata = test)
submission <- pred$data

submission$PassengerId <- all_test$PassengerId

colnames(submission) <- c("Survived", "PassengerId")

write.csv(submission, file = "data/submissionTitanic_1.csv", row.names = FALSE)
```

### Tune the Hyperparameters of the algorithm

```{r kaggle-cv_cart_tune}
### Tune the model
# we chose two numeric parameters above and now search for optimal values
# check available parameters
set.seed(1337)
getLearnerParamSet(learner)
# make parameter set
paramSet <- makeParamSet(
  makeDiscreteParam("minsplit", 
                    values = c(1, 3, 5, 7, 10, 15, 20, 
                               30, 40, 45, 50, 60, 70, 100)),
  makeNumericParam("cp", lower = 0.0001, upper = 0.1)
)
# choose random search - why not grid search?
ctrl <- makeTuneControlRandom(maxit = 100)
res_desc <- makeResampleDesc("CV", iters = 10L, predict = "both")
tunes <- mlr::tuneParams(
  learner = learner, task = task, resampling = res_desc,
  par.set = paramSet, control = ctrl, measures = list(mmce, acc)
)
```

Visualize the random search over both parameters:
  
```{r kaggle-vis_rand_search}
vis_hyper <- generateHyperParsEffectData(tunes)
plt <- plotHyperParsEffect(vis_hyper, x = "minsplit", 
                           y = "cp", z = "acc.test.mean")
plt
# tuning result
tunes
```

Store and submit those results to kaggle

```{r kaggle-tune_cv_cart_sub}
# use those param settings for the CART
learner <- makeLearner(
  cl = "classif.rpart",
  par.vals = list(minsplit = 10, cp = 0.0923)
) # inspect the learner
# learner
mod <- mlr::train(learner = learner, task = task)

# predict for submission
pred <- predict(mod, newdata = test)
submission <- pred$data

submission$PassengerId <- all_test$PassengerId

colnames(submission) <- c("Survived", "PassengerId")

write.csv(submission, file = "data/submissionTitanic_2.csv", row.names = FALSE)
```

#### Check variable importances

```{r kaggle-cv_cart_var_imp}
library(ggplot2)
var_imp <- getFeatureImportance(mod)
var <- as.data.frame(t(var_imp$res))
var$names <- names(var_imp$res)
p <- ggplot(data = var, aes(x = names, y = V1)) + geom_bar(stat = "identity") +
  ggtitle(label = "Variable Importances")
p
```

### Feature engineering

Can we further condense the information from the multi-level factors and use it for our model?
  
  We take a closer look at the names of the guests. 

```{r kaggle-feat_eng_data, message=FALSE, warning=FALSE}
### feature engineering
library(dplyr)

# indicator for train or test set
all_train$train <- 1
all_test$train <- 0
all_test$Survived <- NA

# compute once for all data and split again for training with ID
all_data <- rbind(all_train, all_test)
eng_data <- all_data

head(all_data$Name)
```

We can see, that there is information on the title of the people in their names. We use that information as a new feature!
  
```{r kaggle-feat_eng_title}
# use regular expressions via strplit to extract the title of the people
# temporary storage
temp <- sapply(strsplit(as.character(all_data$Name), split = ","), 
               function(x) x[2])
title <- strsplit(temp, split = " ")
eng_data$title <- sapply(title, function(x) x[2])
# unfortunately still too many titles with too few observations
table(eng_data$title)
```

Btw.: we found the Captain:
  
```{r kaggle-feat_eng_captain}
# btw.: we found the captain:
all_data[which(eng_data$title == "Capt."), "Name"]
```

condense those with obs < 5 to class "other"

```{r kaggle-feat_eng_other_title}
freqs <- as.data.frame(table(eng_data$title))
other_titles <- freqs[which(freqs$Freq < 5), "Var1"]
eng_data[which(eng_data$title %in% other_titles), "title"] <- "other"
eng_data$title <- as.factor(eng_data$title)
# looks better now
table(eng_data$title)
```

### Build updated model

```{r kaggle-feat_eng_model}
### model corner 2 with engineered feature
library(mlr)
library(dplyr)

train <- eng_data %>%
  filter(train == 1) %>%
  select(-c(PassengerId, Name, Ticket, train, Cabin))

test <- eng_data %>%
  filter(train == 0) %>%
  select(-c(PassengerId, Name, Ticket, train, Cabin, Survived))

# choose specific model and parameters
task <- makeClassifTask(data = train, target = "Survived")

learner <- makeLearner(cl = "classif.rpart")
# we chose two numeric parameters above and now search for optimal values
paramSet <- makeParamSet(
  makeDiscreteParam("minsplit", values = c(1, 3, 5, 7, 10, 15, 20, 
                                           30, 40, 45, 50, 60, 70, 100)),
  makeNumericParam("cp", lower = 0.0001, upper = 0.1)
)
# choose random search - why not grid search?
ctrl <- makeTuneControlRandom(maxit = 100)
res_desc <- makeResampleDesc("CV", iters = 10L, predict = "both")
tunes <- mlr::tuneParams(
  learner = learner, task = task, resampling = res_desc,
  par.set = paramSet, control = ctrl, measures = list(mmce, acc)
)
```

Check tuning result

```{r kaggle-feat_eng_res}
tunes
```

Write and store the submission

```{r kaggle-feat_eng_sub}
# use those param settings for the CART
learner <- makeLearner(
  cl = "classif.rpart",
  par.vals = list(minsplit = 7, cp = 0.00601)
)

mod <- mlr::train(learner = learner, task = task)

# predict for submission
pred <- predict(mod, newdata = test)
submission <- pred$data

submission$PassengerId <- eng_data %>%
  filter(train == 0) %>%
  select(PassengerId)

colnames(submission) <- c("Survived", "PassengerId")

write.csv(submission, file = "data/submissionTitanic_3.csv", row.names = FALSE)
```

#### Check Variable Importances

```{r kaggle-feat_eng_var_imp}
library(ggplot2)
var_imp <- getFeatureImportance(mod)
var <- as.data.frame(t(var_imp$res))
var$names <- names(var_imp$res)
p <- ggplot(data = var, aes(x = names, y = V1)) + geom_bar(stat = "identity") +
  ggtitle(label = "Variable Importances")
p
```

What can we see? How could we critisize that result? Is there a way to detect the problem?
  
