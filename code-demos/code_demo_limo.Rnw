% Introduction to Machine Learning
% Day 3

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@

% Load all R packages and set up knitr
<<setup, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{Code demo for linear model}
\lecture{intro to ML}

\begin{vbframe}{Linear model and loss minimization }
We learned that we can estimate the $\hat \beta$ coefficients by minimizing the emiprical risk $$R_{emp}(f) = \frac{1}{n}\sum_{i=1}^n L(y_i, f(x_i | \beta))$$ of our estimator over $\beta$ with quadratic loss such that: $$ R_{emp}(f) = \frac{1}{n}\sum_{i=1}^n (y_i - x_i^T \hat\beta)^2.$$
   This can be written in matrix notation as: $$R_{emp}(f) = \frac{1}{n}(X \beta - Y)^T (X\beta - Y) = \frac{1}{n}[\beta^T X^T X \beta - 2 \beta^TX^TY + Y^TY]$$

We use the quadratic loss and yield this minimization problem w.r.t. $\beta$:

$$\hat \beta = \arg \min_{\beta} R_{emp}(f)$$

\framebreak

\begin{itemize}
\item We can solve this kind of minimization problem using an iterative technique termed \textbf{Gradient Descent}
\item \textbf{Note}: An analytic solution exists for the quadratic loss, s.t. $$\hat \beta = (X^TX)^{-1}X^Ty.$$
\item The Gradient Descent method follows this algorithm:
\begin{enumerate}
\item Initialize $\beta_0$ randomly
\item Calculate the Gradient of our loss function with respect to the current $\beta$:
$$
\frac{\partial R_{emp}(f)}{\partial \beta} = \nabla_{\beta} \frac{1}{n}[\beta^T X^T X \beta - 2 \beta^TX^TY + Y^TY] = \frac{1}{n}X^T[X\beta - Y]
$$ \newpage
\item in each step, we update the estimate for $\beta$ using this formula:
$$
\beta_{t+1} = \beta_{t} - \lambda \frac{\partial R_{emp}(f)}{\partial \beta}
$$
\item We stop when the updates of $\beta$ are beyond a certain threshold or the maximum iterations are reached.
\end{enumerate}
\end{itemize}
\framebreak

Think of it as mountain from which we try to find the way to the valley. In each step, we check for the steepest descent and walk in that direction:

\includegraphics[height = 7.2cm, keepaspectratio]{figures_man/gradient_mountain.png}

\framebreak
Implementation
<<limo, eval= FALSE, echo = 1:19, tidy=TRUE, tidy.opts=list(width.cutoff=58)>>=
set.seed(1337)
#### simluated data with 3 tricky features
# X = as.matrix(cbind(runif(100, -3, 5), rnorm(100, -2, 10), rnorm(100, 5, 2)))
# Y = as.matrix(0.4 * X[, 1] * 0.3 * X[, 2] + 0.3 * X[, 3]+ rnorm(100) + 2)

#### simluated data with 2 simple features
X = as.matrix(cbind(runif(100, -3, 5), runif(100, -2, 10)))
Y = as.matrix(0.5*X[, 1] + 0.5*X[, 2] + rnorm(100) + 2)
# add intercept
X = cbind(1, X)
n = nrow(X)

# initialize beta with 0
beta = as.matrix(rep(0, ncol(X)))

# set maximum of updates
max.iter = 10000

# function that calculates the squared loss
error = function(Y, X, beta) {
  1 / nrow(X) * t(X %*% beta - Y) %*% (X %*% beta - Y)
}

# initialize empty data frames for storage
error.storage = data.frame(matrix(0, ncol = 2, nrow = max.iter))
beta.storage = data.frame(matrix(0, ncol = 1 + ncol(X), nrow = max.iter))
error.storage[, 1] = seq(1, max.iter)
beta.storage[, 1] = seq(1, max.iter)

# learning rate
lambda = 0.01

#  loop over gradient updates
for (i in 1:max.iter) {
  beta = beta - lambda * (1/n * (t(X) %*% (X %*% beta - Y)))
  error.storage[i , 2] = error(Y = Y, X = X, beta = beta)
  beta.storage[i , -1] = t(beta)
}

# Plot stuff
for (i in 1:length(beta)) {
  plot(x = beta.storage[, 1], y = beta.storage[, i + 1], ylab = "coefficient value",
    xlab = "iteration", type = "l", col = "blue", main = paste0("beta ", i - 1))
}
plot(x = error.storage[, 1], y = error.storage[, 2], ylab = "error value",
  xlab = "iteration", type = "l", col = "red", main = "squared error loss")
@
\framebreak

<<limo, eval= FALSE, echo = 20:33, tidy=TRUE, tidy.opts=list(width.cutoff=58)>>=
@
\framebreak

<<limo, eval= FALSE, echo = 34:37, tidy=TRUE, tidy.opts=list(width.cutoff=58)>>=
@
<<limo, eval= TRUE, echo = FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=58)>>=
@

\framebreak

Comparison with mlr's lm

<<eval= TRUE, echo = TRUE, tidy=TRUE, tidy.opts=list(width.cutoff=58)>>=
library(mlr)
df = data.frame(cbind(Y, X[,-1 ]))
colnames(df) = c("Y", paste0("X",seq(1, length(beta) -1 )))
simTask = makeRegrTask(data = df, target = "Y")
simLm = makeLearner("regr.lm")
# ngetParamSet(simLm)
simModel = train(learner = simLm, task = simTask)
simModel$learner.model
@

\end{vbframe}

\endlecture
