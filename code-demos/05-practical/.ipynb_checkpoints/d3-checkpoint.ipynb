{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3rd Notebook for Machine Learning Intro Lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How not to split: the good, the bad, the ugly\n",
    "\n",
    "How does the choice of the split in train and test data affect the model?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the good split\n",
    "\n",
    "we train on the data 1:30 and test on 31:50. Remember, that iris is an ordered data set with the first 50 obsverations being setosa, the next 50 versicolor and the last 50 virginica. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>mmce:</strong> 0"
      ],
      "text/latex": [
       "\\textbf{mmce:} 0"
      ],
      "text/markdown": [
       "**mmce:** 0"
      ],
      "text/plain": [
       "mmce \n",
       "   0 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "            predicted\n",
       "true         setosa versicolor virginica -err.-\n",
       "  setosa         20          0         0      0\n",
       "  versicolor      0          0         0      0\n",
       "  virginica       0          0         0      0\n",
       "  -err.-          0          0         0      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(mlr)\n",
    "task = makeClassifTask(data = iris, target = \"Species\")\n",
    "learner = makeLearner(\"classif.kknn\", k = 3)\n",
    "model = train(learner, task, subset = c(1:30))\n",
    "pred = predict(model, task = task, subset = 31:50)\n",
    "performance(pred, measures = mmce)\n",
    "calculateConfusionMatrix(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the bad split\n",
    "\n",
    "we train on the data 1:100 and test on 101:150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>mmce:</strong> 1"
      ],
      "text/latex": [
       "\\textbf{mmce:} 1"
      ],
      "text/markdown": [
       "**mmce:** 1"
      ],
      "text/plain": [
       "mmce \n",
       "   1 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "            predicted\n",
       "true         setosa versicolor virginica -err.-\n",
       "  setosa          0          0         0      0\n",
       "  versicolor      0          0         0      0\n",
       "  virginica       0         50         0     50\n",
       "  -err.-          0         50         0     50"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task = makeClassifTask(data = iris, target = \"Species\")\n",
    "learner = makeLearner(\"classif.kknn\", k = 3)\n",
    "model = train(learner, task, subset = c(1:100))\n",
    "pred = predict(model, task = task, subset = 101:150)\n",
    "performance(pred, measures = mmce)\n",
    "calculateConfusionMatrix(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the ugly split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>mmce:</strong> 0.18"
      ],
      "text/latex": [
       "\\textbf{mmce:} 0.18"
      ],
      "text/markdown": [
       "**mmce:** 0.18"
      ],
      "text/plain": [
       "mmce \n",
       "0.18 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "            predicted\n",
       "true         setosa versicolor virginica -err.-\n",
       "  setosa          5          0         0      0\n",
       "  versicolor      0          5         0      0\n",
       "  virginica       0          9        31      9\n",
       "  -err.-          0          9         0      9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task = makeClassifTask(data = iris, target = \"Species\")\n",
    "learner = makeLearner(\"classif.kknn\", k = 3)\n",
    "model = train(learner, task, subset = c(1:45, 51:95, 101:110))\n",
    "pred = predict(model, task = task, subset = c(46:50, 96:100, 111:150))\n",
    "performance(pred, measures = mmce)\n",
    "calculateConfusionMatrix(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "Check the performance of our knn-classifier on the test and train set depending on the hyperparameter k:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in get.knn(Y = \"Species\", train.data = irisTrain, test.data = irisTest, : could not find function \"get.knn\"\n",
     "output_type": "error",
     "traceback": [
      "Error in get.knn(Y = \"Species\", train.data = irisTrain, test.data = irisTest, : could not find function \"get.knn\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "k.values = c(1, 2, 3, 5, 10, 15, 20, 25, 30, 50, 80, 100)\n",
    "storage = data.frame(matrix(NA, ncol = 3, nrow = length(k.values)))\n",
    "colnames(storage) = c(\"mmce_train\", \"mmce_test\", \"k\")\n",
    "for (i in 1:length(k.values)) {\n",
    "  storage[i, \"mmce_test\"] = get.knn(Y = \"Species\", train.data = irisTrain, \n",
    "    test.data = irisTest, k = k.values[i], \n",
    "    normalize = FALSE)$mmce\n",
    "  storage[i, \"mmce_train\"] = get.knn(Y = \"Species\", train.data = irisTrain, \n",
    "    test.data = irisTrain, k = k.values[i], \n",
    "    normalize = FALSE)$mmce\n",
    "  storage[i, \"k\"] = k.values[i]\n",
    "}\n",
    "\n",
    "\n",
    "plot(y = storage$mmce_train, x = storage$k, type = \"lty\", \n",
    "  ylab = \"mmce\", xlab = \"k\", col = \"turquoise\", main = \"Overfitting knn\")\n",
    "lines(y = storage$mmce_test, x = storage$k, type = \"lty\", col = \"orange\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Performance Estimation\n",
    "\n",
    "- show overfitting with practical data set\n",
    "- explain early stopping with validation data as strategy to overcome overfitting?\n",
    "- the good, the bad, the ugly from [Lars Kotthof](http://www.cs.uwyo.edu/%7Elarsko/ml-fac/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Resampling \n",
    "- hard code CV-algorithm\n",
    "- compare with mlr's inmplementation\n",
    "- FCIM exercise mlr_a_2.Rnw on small benchmark study\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Nested Resampling \n",
    "- Update hard-coded algrithm from part2\n",
    "- show example for overtuning.\n",
    "    - 1) tune with nested resampling\n",
    "    - 2) tune without - overtuning\n",
    "    - -> test performance on super external test data set\n",
    "- tuning_a_1.Rnw from FCIM for tuning \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Confusion matrices, ROC etc.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
