---
output: pdf_document
params:
    set_title: "Code demo for logistic regression and Newton-Raphson"
---

```{r, child = "../style/preamble_code_demos.Rmd", include = FALSE}

```

```{r, child = "../style/setup.Rmd", include = FALSE}

```  

```{r, echo=FALSE}
knitr::read_chunk("code_demo_limo.Rmd")
knitr::read_chunk("code_demo_knn.Rmd")
```  

```{r grad_desc_opt-def, echo=FALSE}

```
```{r knn-plot_2D_classif, echo=FALSE}

```

# Logistic regression

## Idea

In the lecture we learnt that logistic regression is a discriminant approach,
i.e., we are modeling the posterior probabilities $\pi(x)$ of the labels
directly using the logistic function, s.t.

$$\pi(x) = \mathbb{P}(y=1 |x) = \frac{1}{1+\exp(-\theta^Tx)}.$$
**Note** As in the lecture we suppress the intercept in notation, i.e., 
$\theta^Tx \equiv \theta_0 + \theta^Tx$.

This can be easily implemented as:
```{r logreg-logistic}
logistic <- function(X, theta) {
  1 / (1 + exp(-X %*% theta))
}
```

And we can observe that logistic regression “squashes” the estimated linear 
scores $\theta^Tx$ to $[0, 1]$:

```{r logreg-plot_logistic}
num_data <- 100
theta <- c(0, 1)

ggplot(data = data.frame(x = seq(-5, 5, length.out = num_data)), aes(x)) +
  stat_function(fun = function(x) logistic(cbind(rep(1, length(x)), x), theta))
```

As shown in the lecture from the definition of accuracy and the encoding of the 
labels to $\{-1, 1\}$ one can derive the Bernoulli loss:
$$L(y,f(x)) = \log(1 + \exp(-yf(x)))$$
For the logistic regression it holds, that $f(x) = \theta^Tx$.
```{r logreg-ber_loss}
loss_bernoulli <- function(Y, X, theta) {
  log(1 + 1 / exp(Y * (X %*% theta)))
}
```
We can plot the loss for $y = 1$ and $y = -1$:
```{r logreg-ber_loss_plot}
library(reshape2)
num_data <- 100
theta <- c(0, 1)
y <- c(1, -1)

x <- seq(-4, 4, length.out = num_data)
L <- sapply(y, function(y) loss_bernoulli(y, cbind(rep(1, length(x)), x), theta))
plot_data <- melt(data.frame(x = x, L1 = L[, 1], L2 = L[, 2]), id.vars = "x")

ggplot(plot_data, aes(x = x, y = value, colour = variable)) +
  geom_line() +
  labs(x = expression(f(x)), y = expression(L(y, f(x)))) +
  scale_color_manual(name = "", values = c("purple", "yellow"), 
                     labels = c("y = 1", "y = -1"))
```



Which means in order to minimize the loss, we should predict 
$y = 1$ if 
$$\theta^Tx \geq 0 \iff \pi(x) = \mathbb{P}(y=1 |x) = \frac{1}{1+\exp(-\theta^Tx)} \geq 0.5.$$ 
```{r logreg-classify}
classify_logreg <- function(X, theta, conv = as.numeric) {
  probability <- logistic(X, theta)
  list(
    prediction = conv(ifelse(probability > .5, 1, -1)),
    levels = c("1", "-1")
  )
}
```

With the help of the loss function, we can get the empirical risk as usual:
$$R_{emp} = \sum_{i=1}^nL(y^{(i)}, f(x^{(i)})$$

```{r logreg-ber_risk}
risk_bernoulli <- function(Y, X, theta) {
  sum(loss_bernoulli(Y, X, theta))
}
```

## Logistic regression - example

To investigate how the parameter $\theta$ of
the logistic regression can be estimated, we create some artificial data:

```{r logreg-dgp}
set.seed(1337)

n <- 1000
p <- 2

# model without intercept
X <- matrix(runif(n * p, -5, 5), nrow = n)
theta <- c(3, -1)

Y <- 2 * rbinom(n = n, size = 1, prob = logistic(X, theta)) - 1
```

We use the *plot_2D_classify* function from the [KNN code demo](./code_demo_knn.pdf) as we want to plot how well we 
can classify the data:

```{r logreg-plot_2D_logreg-def}  
# function for logistic regression with two features to visualize class
# boundaries X1 and X2 are the names of the two features to use.

plot_2D_logreg <- function(theta, X1 = "X1", X2 = "X2",
                           # by default, we "predict" the class of the training data:
                           to_classify_labels = as.character(Y),
                           to_classify_data = data.frame(
                             "X1" = X[, 1],
                             "X2" = X[, 2]
                           ),
                           lengthX1 = 100, lengthX2 = 100,
                           title = '"Logistic regression" ~',
                           plot_class_rate = TRUE) {
  plot_2D_classify(
    to_classify_labels = to_classify_labels,
    to_classify_data = to_classify_data,
    classify_method = function(to_classify_data)
      classify_logreg(
        X = cbind(as.matrix(to_classify_data)),
        theta = theta, conv = as.character
      ),
    X1, X2,
    lengthX1 = lengthX1, lengthX2 = lengthX2,
    title = title,
    plot_class_rate = plot_class_rate
  )
}
```

Let's classify the data with our *real* parameter $\theta$:

```{r logreg-plot_realtheta}
plot_2D_logreg(theta)
risk_bernoulli(Y, X, theta)
```


We notice that our data is linearly separated as one would expect, since 
our decision boundary is defined by 
$$\theta^Tx = 0,$$
which defines a hyperplane. Also we note that $\theta$ represents a vector normal to
this plane and is pointing to "1"-side. But doesn't that mean that for 
$\lambda \in \mathbb{R}^+$ $\lambda\cdot\theta$ separates 
the data equally well? Check for $\lambda \in \{0.5, 2,3\}$ the classification and the 
empirical risk. Can you conclude what determines the *optimal* length of $\theta$?
To gain further understanding of this behavior let's look at a data scenario, where we
encounter the so-called complete separation, i.e., the data can be perfectly 
classified by our classification method:


```{r logreg-compl_sep}
Y_compl_sep <- classify_logreg(X, theta)$prediction

thetas <- list(0.5 * theta, 1 * theta, 2 * theta, 10 * theta)
ggplot_list <- lapply(
  thetas,
  function(theta) {
    plot_2D_logreg(theta,
      to_classify_labels = as.character(Y_compl_sep),
      title = paste0(
        "theta ==",
        paste0('~ "(', paste0(theta, collapse = ","), '), " ~ '),
        "R[emp] == ",
        risk_bernoulli(Y_compl_sep, X, theta = theta)
      ),
      plot_class_rate = FALSE
    )
  }
)
do.call(grid.arrange, ggplot_list)
```

Remember the Bernoulli loss was derived, s.t.
$$L(y,f(x)) = \log(1 + \exp(-y \theta^Tx).$$

In the case of complete separation it holds for every observation that
$$L(y,f(x)) = \log(1 + \exp(-|\theta^Tx|),$$
since every observation is classified correctly.
Hence the empirical risk is monotonously decreasing as the norm of theta increases,
which means that by only minimizing the empirical risk we can not converge to a 
solution. In this situation additional artificial constraints -such as regularization- 
can be introduced to find a solution. Since in our example we have not to deal
with complete separation, we can apply the *gradient_descent_opt_stepsize* function of the 
[code demo to the linear model](./code_demo_limo.pdf) directly to our 
empirical risk we derived from the Bernoulli loss. To do so we need the gradient
of our empirical risk, for which it can be found that

$$\frac{\partial R_{emp}(f)}{\partial \theta} = \sum_{i=1}^n \frac{-y^{(i)}}{1+\exp(y^{(i)}\theta^Tx^{(i)})}x^{(i)}.$$
Which can be implemented as:
```{r logreg-ber_grad}
gradient_bernoulli <- function(Y, X, theta) {
  colSums((1 / (1 + exp(Y * (X %*% theta))) * -Y)[, 1] * X)
}
```

Now we can apply the gradient descent method to find a numerical solution:

```{r, logreg_grad_desc}
res <- gradient_descent_opt_stepsize(Y, X, c(-1, -1),
  risk_bernoulli,
  gradient_bernoulli,
  max_learning_rate = 0.1,
  max_iterations = 130,
  epsilon = 0.00001,
  include_storage = TRUE
)
title(main = "Gradient Descent", outer = TRUE, line = -1)
print(paste("Risk: ", res$risk))
res$theta
```
To see what happens when we use gradient descent, we write a function which plots
how we *traverse* through the parameter space as the algorithm converges:

```{r logreg-plot_emp_risk}
plot2D_emp_risk <- function(min_theta = c(0, 0),
                            max_theta = c(1, 1),
                            theta_length = c(100, 100),
                            emp_risk,
                            thetas,
                            log_emp_risk = FALSE) {
  grid_thetas <- sapply(1:2, function(i)
    seq(min_theta[i],
      max_theta[i],
      length.out = theta_length[i]
    ))

  # compute grid coordinates with cartesian product
  grid_theta <- expand.grid(grid_thetas[, 1], grid_thetas[, 2])

  emp_risk <- if (log_emp_risk) {
    apply(grid_theta, 1, function(theta)
      log(emp_risk(theta)))
  } else {
    apply(grid_theta, 1, function(theta)
      emp_risk(theta))
  }

  grid_theta$emp_risk <- emp_risk



  ggplot() +
    geom_raster(
      data = grid_theta,
      aes(x = Var1, y = Var2, fill = emp_risk)
    ) +
    geom_contour(
      data = grid_theta, colour = "white",
      aes(x = Var1, y = Var2, z = emp_risk)
    ) +
    geom_line(
      data = data.frame(thetas), aes(x = X1, y = X2),
      colour = "red"
    ) +
    labs(
      fill = if (log_emp_risk) {
        expression(log(R[emp](theta)))
      } else {
        expression(R[emp](theta))
      },
      title = if (log_emp_risk) {
        expression(log(R[emp](theta)))
      } else {
        expression(R[emp](theta))
      },
      x = expression(theta[1]),
      y = expression(theta[2])
    )
}
```

```{r logreg-plot_emprisk_grad_desc}
plot2D_emp_risk(
  emp_risk = function(theta) (risk_bernoulli(Y, X, theta)),
  max_theta = c(10, 10), min_theta = c(-5, -10),
  log_emp_risk = TRUE,
  thetas = res$storage$theta
)
```
With gradient descent we are able to find the minimum, but also we note that
the algorithm needs a lot of steps in the valley of the empirical risk function.
This happens since it can be shown that for gradient descent with exact line search
two consecutive search directions are orthogonal to each other. To counter this 
we look at another approach in the next chapter.

## Newton-Raphson

The Newton-Raphson method follows this algorithm:

0. Initialize $\theta^{[0]}$ (randomly) and calculate the gradient and the Hessian matrix, which is a square matrix of second-order partial derivatives, of the empirical risk with respect to $\theta$, for example for the Bernoulli loss:

$$\frac{\partial R_{emp}(f)}{\partial \theta} = \sum_{i=1}^n \frac{-y^{(i)}}{1+\exp(y^{(i)}\theta^Tx^{(i)})}x^{(i)},$$
$$
\frac{\partial^2 R_{emp}(f)}{\partial \theta^2} = \sum_{i=1}^n \frac{\exp(y^{(i)}\theta^Tx^{(i)})}{(1+\exp(y^{(i)}\theta^Tx^{(i)}))^2}x^{(i)}{x^{(i)}}^T.
$$



Now iterate these two steps: 

1. Evaluate the gradient and the Hessian matrix at the current value of the parameter vector $\theta^{[t]}$:

$$\frac{\partial R_{emp}(f)}{\partial \theta}\bigg|_{\theta=\theta^{[t]}}  = \sum_{i=1}^n \frac{-y^{(i)}}{1+\exp(y^{(i)}{\theta^{[t]}}^T x^{(i)})}x^{(i)},$$
$$\frac{\partial^2 R_{emp}(f)}{\partial \theta^2}\bigg|_{\theta=\theta^{[t]}}  =  \sum_{i=1}^n \frac{\exp(y^{(i)}{\theta^{[t]}}^T x^{(i)})}{(1+\exp(y^{(i)}{\theta^{[t]}}^T x^{(i)}))^2}x^{(i)}{x^{(i)}}^T.$$
2. update the estimate for $\theta$ using this formula:
$$
\theta^{[t+1]} = \theta^{[t]} - \lambda \bigg[\frac{\partial^2 R_{emp}(f)}{\partial \theta^2}\bigg|_{\theta=\theta^{[t]}}\bigg]^{-1} \frac{\partial R_{emp}(f)}{\partial \theta}\bigg|_{\theta = \theta^{[t]}}
$$

- The \emph{stepsize} or \emph{learning rate} parameter $\lambda$ controls the size of the updates per iteration $t$.
- We stop if the differences between successive updates of $\theta$ are below a certain threshold or once a maximum number of iterations is reached.
- Note: The classic Newton-Raphson method uses $\lambda=1$, but we can enlarge its radius of convergence by optimizing $\lambda$ in every step.

We can implement this quite easily by realizing that we are principally doing the same as with the gradient descent method - except that
now we use a gradient which is weighted with the inverse of second derivatives. The second derivatives represent how curved our surface is at our candidate point. Intuitively this means when we are at a point, where the surface is strongly curved, we want to make small steps, since we do not want to *pass* the minimum,  as for a relatively flat surface bigger steps can be made.

```{r logreg-newton}
library(MASS)

newton_opt_stepsize <- function(Y, X, theta,
                                risk,
                                gradient,
                                hessian,
                                lambda = 0.005,
                                epsilon = 0.0001,
                                max_iterations = 2000,
                                min_learning_rate = 0,
                                max_learning_rate = 1000,
                                plot = TRUE,
                                include_storage = FALSE) {
  gradient_descent_opt_stepsize(Y, X, theta,
    risk = risk,
    gradient = function(Y, X, theta)
      ginv(hessian(Y, X, theta)) %*%
        gradient(Y, X, theta),
    lambda = lambda,
    epsilon = epsilon,
    max_iterations = max_iterations,
    min_learning_rate = min_learning_rate,
    max_learning_rate = max_learning_rate,
    plot = plot,
    include_storage = include_storage
  )
}
```


## Idea of Newton-Raphson

In principle Newton-Raphson is a method for finding a root of a function. To understand
this we look at a scalar function $f$, from which we want to find its root:

```{r logreg_idea_newton, echo=FALSE}
num_data <- 100

x <- seq(from = 0, to = 7, length.out = 100)

f <- function(x) x^2 - 1
fx <- function(x) 2 * x

func_data <- data.frame(x = x, y = f(x))

x0 <- 6.5
slope <- fx(x0)
intercept <- f(x0) - slope * x0

x1 <- x0 - slope^-1 * f(x0)

ggplot() +
  geom_line(data = func_data, aes(x = x, y = y), color = "purple") +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = x0, color = "yellow") +
  geom_abline(slope = slope, intercept = intercept, color = "turquoise") +
  geom_curve(data = data.frame(x = x1 + 0.3, y = 0, x_to = x1 + 0.2, y_to = 
                                 slope * (x1 + 0.2) + intercept), 
             aes(x = x, y = y, xend = x_to, yend = y_to), angle = 90, 
             curvature = 0.3, color = "black") +
  geom_text(aes(label = 'x^{~"[t+1]" }', x = x1, y = -1.5), parse = TRUE) +
  geom_text(aes(label = 'x^{~"[t]" }', x = x0, y = -1.5), parse = TRUE) +
  geom_text(aes(label = 'tan^-1 ~ ( ~ "f\'"(x^{~"[t]" }))  ', x = x1 + 1, y = 3),
    parse = TRUE,
    color = "black"
  ) +
  geom_text(aes(label = "f(x)", x = 4, y = 20),
    parse = TRUE,
    color = "purple"
  ) +
  geom_text(aes(label = 'f(x^{~"[t]" })', x = x0 + 0.3, y = f(x0)),
    parse = TRUE,
    color = "black"
  )
```

The idea for finding a root candidate consists in following the negative gradient from our starting point.
By applying simple trigonometry one derives that
$$
f'(x^{[t]}) = \frac{f(x^{[t]})}{x^{[t]}-x^{[t+1]}}.
$$

By solving for $x^{[t+1]}$ we get that
$$
x^{[t+1]} = x^{[t]} - f'(x^{[t]})^{-1}\cdot f(x^{[t]}),
$$
which is the one-dimensional Newton-Raphson method. But how does finding the root of a function help in minimizing the empirical risk function?
Remember that a necessary condition for a minimum is, that the gradient is zero, i.e., for the empirical risk that
$$
\frac{\partial R_{emp}(f)}{\partial \theta} = 0.
$$
This means by applying the multidimensional version of the Newton-Raphson method to the gradient function, we 
are minimizing the empirical risk function. This is also the reason why the second derivatives appear, since we are taking the derivative
of the gradient function.

## Example continued with Newton-Raphson

As already mentioned for using Newton-Raphson we need the Hessian matrix:

```{r logreg-ber_hessian}
hessian_bernoulli <- function(Y, X, theta) {
  exp_yxt <- exp(Y * (X %*% theta))
  t((exp_yxt / (1 + exp_yxt)^2)[, 1] * X) %*% X
}
```

Now we can use the Newton-Raphson for solving our example:

```{r logreg-example_newton}
res <- newton_opt_stepsize(Y, X, c(-1, -1),
  risk_bernoulli,
  gradient_bernoulli,
  hessian_bernoulli,
  min_learning_rate = 0.0,
  max_learning_rate = 10,
  max_iterations = 5,
  include_storage = TRUE
)
title(main = "Newton Raphson", outer = TRUE, line = -1)
print(paste("Risk: ", res$risk))
res$theta
```

We see that we need less steps to achieve a similar good result as when we used the gradient descent method.

```{r logreg-plot_emprisk_newton}
plot2D_emp_risk(
  emp_risk = function(theta) (risk_bernoulli(Y, X, theta)),
  max_theta = c(10, 10), min_theta = c(-5, -10),
  log_emp_risk = TRUE,
  thetas = res$storage$theta
)
```

We see that by using Newton-Raphson we are incorporating more knowledge of our function as the Hessian matrix represents the curvature of a function, which allows to make bigger steps in the right direction. Gradient descent is a so-called first-order iterative optimization algorithm, while Newton-Raphson is second-order one since we're computing second derivatives. The behaviors of both algorithms we observed are typical for their order:

- First-order iterative optimization algorithms usually need more steps than second-order one.
- The computation of step for a first-order one is less *expensive*, since we do not need to compute the Hessian matrix and invert it.

Note: In our toy example we did not use an intercept, s.t. we could visualize the algorithms better. For real data an intercept should definitively be included, since it represents the base-line probability for class "1".




