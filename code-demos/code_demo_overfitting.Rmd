---
output: pdf_document
params:
  set_title: "Code demo about overfitting"
---
  
```{r overfitting-preamble, child = "../style/preamble_code_demos.Rmd", include = FALSE, purl = FALSE}

```

```{r overfitting-setup, child = "../style/setup.Rmd", include = FALSE, purl = FALSE}

```  

# Code demo about overfitting

##  Overfitting kNN

Why do we have to split in train and test?

Check the performance of our knn-classifier on the test and train set depending on the hyperparameter k:

```{r overfitting-knn, message=FALSE, warning=FALSE}
library(mlr)
library(mlbench)

set.seed(13)
spiral <- as.data.frame(mlbench.spirals(n = 500, sd = 0.1))
plot(x = spiral$x.1, y = spiral$x.2, col = spiral$classes)

train_size <- 3 / 4
train_indices <- sample(x = seq(1, nrow(spiral), by = 1), size = 
                          ceiling(train_size * nrow(spiral)), replace = FALSE)
spiral_train <- spiral[ train_indices, ]
spiral_test <- spiral[ -train_indices, ]



# run experiment
k_values <- rev(c(1, 2, 3, 4, 5, 7, 8, 9, 10, 15, 20, 25, 30, 35, 45, 50, 60, 70, 80, 90, 100))
storage <- data.frame(matrix(NA, ncol = 3, nrow = length(k_values)))
colnames(storage) <- c("mmce_train", "mmce_test", "k")

for (i in 1:length(k_values)) {
  spiral_task <- makeClassifTask(data = spiral_train, target = "classes")
  spiral_learner <- makeLearner("classif.kknn", k = k_values[i])
  spiral_model <- train(learner = spiral_learner, task = spiral_task)

  # test data
  # choose additional measures from: listMeasures(irisTask)
  spiral_pred <- predict(spiral_model, newdata = spiral_test[])
  storage[i, "mmce_test"] <- performance(pred = spiral_pred, measures = mmce)

  # train data
  spiral_pred <- predict(spiral_model, newdata = spiral_train[])
  storage[i, "mmce_train"] <- performance(pred = spiral_pred, measures = mmce)

  storage[i, "k"] <- k_values[i]
}

storage <- storage[rev(order(storage$k)), ]

plot(
  x = storage$k, y = storage$mmce_train, main = "Overfitting behavior KNN",
  xlab = "k", ylab = "mmce", col = "blue", type = "l",
  xlim = rev(range(storage$k)),
  ylim = c(
    min(storage$mmce_train, storage$mmce_test),
    max(storage$mmce_train, storage$mmce_test)
  )
)
lines(x = storage$k, y = storage$mmce_test, col = "orange")
legend("bottomleft", c("test", "train"), col = c("orange", "blue"), lty = 1)
```

## How not to split: the good, the bad, the ugly

How does the choice of the split in train and test data affect our estimation of the model performance?

### the good split

we train on the data 1:30 and test on 31:50. Remember, that iris is an ordered data set with the first 50 obsverations being setosa, the next 50 versicolor and the last 50 virginica. 

```{r overfitting-good_split}
library(mlr)
library(ggplot2)

task <- makeClassifTask(data = iris, target = "Species")
learner <- makeLearner("classif.kknn", k = 3)
model <- train(learner, task, subset = c(1:30))
pred <- predict(model, task = task, subset = 31:50)
performance(pred, measures = mmce)
calculateConfusionMatrix(pred)
```

### the bad split

we train on the data 1:100 and test on 101:150.

```{r overfitting-bad_split}
task <- makeClassifTask(data = iris, target = "Species")
learner <- makeLearner("classif.kknn", k = 3)
model <- train(learner, task, subset = c(1:100))
pred <- predict(model, task = task, subset = 101:150)
performance(pred, measures = mmce)
calculateConfusionMatrix(pred)
```

### the ugly split

```{r overfitting-ugly_split}
task <- makeClassifTask(data = iris, target = "Species")
learner <- makeLearner("classif.kknn", k = 3)
model <- train(learner, task, subset = c(1:45, 51:95, 101:110))
pred <- predict(model, task = task, subset = c(46:50, 96:100, 111:150))
performance(pred, measures = mmce)
calculateConfusionMatrix(pred)
```
