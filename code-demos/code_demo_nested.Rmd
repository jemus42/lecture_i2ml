---
output: pdf_document
params:
  set_title: "Code demo for Nested Resampling"
---
  
```{r nested-preamble, child = "../style/preamble_code_demos.Rmd", include = FALSE, purl = FALSE}

```

```{r nested-setup, child = "../style/setup.Rmd", include = FALSE, purl = FALSE}

```  

```{r nested-load_chunk, echo=FALSE, cache=FALSE, purl=FALSE}
# see https://stackoverflow.com/a/28151571
run_chunks <- function(chunkNames, envir = .GlobalEnv) {
  for (chunkName in chunkNames) {
    chunkName <- unlist(lapply(as.list(substitute(.(chunkName)))[-1], as.character))
    eval(parse(text = knitr:::knit_code$get(chunkName)), envir = envir)
  }
}

load_chunks <- function(rmd_files) {
  for (rmd_file in rmd_files) {
    knitr::purl(paste0(rmd_file, ".Rmd"), paste0(rmd_file, ".R"), documentation = 2)
    knitr::read_chunk(paste0(rmd_file, ".R"))
  }
}

load_chunks(c("code_demo_resampling"))
run_chunks(c("resampling-knn_cv"))
```

# Code demo for Nested Resampling 

In this code demo we

* compare resampling and nested resampling,
* see how we can tune hyperparemeters,
* learn how to evaluate the generalization error in this scheme.

## Setup

We take a look at the spirals data again:

```{r nested-data}
library(mlr)
library(mlbench)

set.seed(13)
spiral <- as.data.frame(mlbench.spirals(n = 500, sd = 0.1))
plot(x = spiral$x.1, y = spiral$x.2, col = spiral$classes)
```

No we want to find the optimal hyperparameter k for a knn model. Therefore we use nested resampling with the following options:
  
* 10 outer CV loops
* 10 inner CV loops
* 7 Candidates 

Basically, we want to do two things: find the optimal hyperparameter and estimate its generalization error. We can not do two operations in one CV and therefore use two nested CVs, thus we apply __nested resampling__.

```{r nested-knn_nr}

knn_nr <- function(data, target, outer_folds = 3, inner_folds = 4, k_candidates,
                   inform = FALSE) {
  # data: whole data set including the target variable
  # target: string indicating the target variable of the task
  # outer_folds: the amount of folds in the outer loop of the nested resampling
  # inner_folds: the amount of folds in the inner loop of the nested resampling
  # k_candidates: the potential values for k from which we want to select
  #              the optimal
  # inform: boolean controling if the user is getting informed about the
  #         progress of the procedure or not


  # counter for inform functionality
  counter <- 0

  # indices for the outer loops
  outer_indices <- sample(x = seq(1, nrow(data), by = 1), 
                          size = nrow(data), replace = FALSE)
  index_outer_mat <- matrix(data = outer_indices, byrow = TRUE, 
                            nrow = outer_folds)

  # frame to store the winner and its test-gen_error from all outer folds
  winner_cv <- as.data.frame(matrix(0, nrow = outer_folds, ncol = 2))
  colnames(winner_cv) <- c("k", "gen_error")

  for (i in 1:outer_folds) {

    # split in validation and data for the inner loop
    test_data <- data[index_outer_mat[i, ], ]
    inner_data <- data[ -index_outer_mat[i, ], ]

    # frame to store the CV-gen_errors for each candidate
    candidate_gen_error <- 
      as.data.frame(matrix(0, nrow = length(k_candidates), ncol = 2))
    colnames(candidate_gen_error) <- c("k", "gen_error")

    # calculate gen_error for each of the candidates via CV
    for (l in 1:length(k_candidates)) {
      inner_indices <- 
        sample(x = seq(1, nrow(inner_data), by = 1), 
               size = nrow(inner_data), replace = FALSE)

      # index matrix for inner folds
      index_inner_mat <- matrix(data = inner_indices, 
                                byrow = TRUE, nrow = inner_folds)

      # storage for CV errors for one candidate
      storage_inner_cv <- numeric(inner_folds)

      for (j in 1:inner_folds) {

        # data split in validation and train data
        eval_data <- inner_data[index_inner_mat[j, ], ]
        train_data <- inner_data[ -index_inner_mat[j, ], ]

        # model
        task <- makeClassifTask(data = train_data, target = target)
        learner <- makeLearner("classif.kknn", k = k_candidates[l])
        model <- train(learner = learner, task = task)
        # inform user about progress
        counter <- counter + 1
        if (inform) print(paste0("model: ", counter, 
                                 " inner fold: ", j, " outer fold: ", i))
        # store gen_error estimates from test on validation data
        storage_inner_cv[j] <- performance(predict(model, newdata = eval_data),
          measures = mmce)[[1]]
      }

      # CV gen_error for candidate l
      candidate_gen_error[l, "gen_error"] <- mean(storage_inner_cv)
      candidate_gen_error[l, "k"] <- k_candidates[l]
    }

    # get gen_error for best candidate in this outer_fold
    # in case of equally good Candidates, choose the first one
    best_candidate <- candidate_gen_error[
      which(candidate_gen_error$gen_error ==
        min(candidate_gen_error$gen_error)), "k"][1]
    winner_cv[i, "k"] <- best_candidate


    # model
    task <- makeClassifTask(data = inner_data, target = target)
    learner <- makeLearner("classif.kknn", k = k_candidates[l])
    model <- train(learner = learner, task = task)
    # store gen_error estimates from test on test data
    winner_cv[i, "gen_error"] <- performance(predict(model, newdata = test_data),
      measures = mmce
    )[[1]]
  }
  return(winner_cv[order(winner_cv$gen_error), ])
}
```

Run it and check the results of the outer CV:
  
```{r nested-results_knn_nr}
# let's run it
set.seed(1337)
result_nr <- knn_nr(
  data = spiral, target = "classes", outer_folds = 10, inner_folds = 10,
  k_candidates = c(1, 3, 5, 10, 20, 30, 40, 100), inform = FALSE
)
# order results by gen_error
result_nr
```

### Use only CV for Hyperparameter Selection:

We use the *knn_cv* function from the [resampling code demo](./code_demo_resampling.pdf)

```{r nested-cv_tune_knn}
cv_tune_knn <- function(data, target, folds, k_candidates) {
  candidatesgen_error <- 
    as.data.frame(matrix(data = 0, nrow = length(k_candidates), ncol = 2))
  colnames(candidatesgen_error) <- c("k", "gen_error")

  for (l in 1:length(k_candidates)) {
    candidatesgen_error[l, "k"] <- k_candidates[l]
    candidatesgen_error[l, "gen_error"] <- knn_cv(
      data = data, target = target, folds = folds,
      k = k_candidates[l]
    )$gen_error
  }
  return(candidatesgen_error[order(candidatesgen_error$gen_error), ])
}


result_cv <- cv_tune_knn(data = spiral, target = "classes", 
                        folds = 10, 
                        k_candidates = c(1, 3, 5, 10, 20, 30, 40, 100))
result_cv
```

### Test the hyperparameters on completely unseen data

Test the hyperparameters proposed by nested resampling and cross-validation on completely new data, that none of the two algorithms have seen before. Neat thing about the spiral data: we can simulate new data points whenever we want!
  
  What would we expect based on the lecture?
  
  An overly optimistic estimate for the perfect hyperparameter by simple CV (overtuning effect) and a more realistic estimate from the nested resampling algorithm. 

### Nested Resampling Hyperparameter

```{r nested-nr_hparam}
set.seed(1337)
unseen_spiral <- as.data.frame(mlbench.spirals(n = 200, sd = 0.1))

task <- makeClassifTask(data = spiral, target = "classes")
learner <- makeLearner("classif.kknn", k = 20)
model <- train(learner = learner, task = task)
print(paste0(
  "nested RS k = 20: ", 
  performance(predict(model, newdata = unseen_spiral), measures = mmce)[[1]],
  " with NR gen_error estimate: ", result_nr[2, 2]
))
```

### Simple Cross-Validation Hyperparameter

```{r nested-cv_hparam}
set.seed(1337)
unseen_spiral <- as.data.frame(mlbench.spirals(n = 200, sd = 0.1))

task <- makeClassifTask(data = spiral, target = "classes")
learner <- makeLearner("classif.kknn", k = 5)
model <- train(learner = learner, task = task)
print(paste0(
  "CV tuned k = 5: ", 
  performance(predict(model, newdata = unseen_spiral), measures = mmce)[[1]],
  " with CV gen_error estimate: ", result_cv[2, 2]
))
```

## mlr implementation

```{r nested-mlr_impl}
ps <- makeParamSet(
  makeDiscreteParam("k", values = c(1, 3, 5, 10, 20, 30, 40))
)

# inner loop
ctrl <- makeTuneControlGrid()
inner <- makeResampleDesc("CV", iters = 10L)
learner <- makeTuneWrapper("classif.kknn", 
                           resampling = inner, 
                           par.set = ps, control = ctrl, show.info = FALSE)

# outer loop
outer <- makeResampleDesc("CV", iters = 10)
r <- resample(learner, task = task, resampling = outer,
              extract = getTuneResult, show.info = FALSE)
r
```

Get the tuning results for the outer loops

```{r nested-mlr_res1}
# tune results
r$extract
```

```{r nested-mlr_res2}
getNestedTuneResultsX(r)
```
