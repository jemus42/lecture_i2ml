---
output: pdf_document
params:
  set_title: "Code demo for ROC"
---
  
```{r roc-preamble, child = "../style/preamble_code_demos.Rmd", include = FALSE, purl = FALSE}

```

```{r roc-setup, child = "../style/setup.Rmd", include = FALSE, purl = FALSE}

```  

# Code demo for ROC

We use the Breast Cancer data set from [UCI database](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)) which is an __unbalanced data set__ within which we want to predict the class of the cancer. We even manipulate the data set further to make it more unbalanced. 

The data looks like that:

```{r roc-data, message=FALSE, warning=FALSE}
library("dplyr")
library("mlbench")
library("mlr")

data("BreastCancer")

target_label <- "Class"

# delete one column with missing values
bc <- BreastCancer[, -c(1, 7)]
# mutate all factors to numeric, simlification but ok here
mut <- bc[, -9] %>%
  mutate_all(as.character) %>%
  mutate_all(as.numeric)
bc_data <- cbind(mut, bc[,target_label])
colnames(bc_data) <- c(colnames(mut), target_label)
# make it more extreme and kill 50% of the malignant data
bc_data <- bc_data[ -sample(which(bc_data[,target_label] == "malignant"), 150, replace = FALSE), ]
head(bc_data)
table(bc_data[,target_label]) / sum(table(bc_data[,target_label]))
```

We split the data again in train and test:

```{r roc-split_data}
# Data split
set.seed(1337)
train_size <- 3 / 4
train_indices <- sample(x = seq(1, nrow(bc_data), by = 1), 
                        size = ceiling(train_size * nrow(bc_data)), replace = FALSE)
bc_train <- bc_data[ train_indices, ]
bc_test <- bc_data[ -train_indices, ]

task <- makeClassifTask(data = bc_train, target = target_label)
```

We check the performance of three classifiers:

1. logreg regression

```{r roc-logreg}
# logreg
learner <- makeLearner("classif.logreg", predict.type = "prob")
model <- mlr::train(learner, task)
pred_logreg <- predict(model, newdata = bc_test)
performance(pred_logreg, measures = list(mmce, auc))
calculateConfusionMatrix(pred_logreg)
```

2. knn

```{r roc-knn}
# knn
learner <- makeLearner("classif.kknn", k = 5, predict.type = "prob")
model <- mlr::train(learner, task)
pred_knn <- predict(model, newdata = bc_test)
performance(pred_knn, measures = list(mmce, auc))
calculateConfusionMatrix(pred_knn)
```

3. A stupid learner that simply predicts the majority of the two classes or each observation. 

```{r roc-maj_vote}
# learner that uses simple majority vote for classification
stupidLearner <- makeLearner("classif.featureless", method = "majority", 
                              predict.type = "prob")
model <- mlr::train(stupidLearner, task)
pred_stupid <- predict(model, newdata = bc_test)
performance(pred_stupid, measures = list(mmce, auc))
calculateConfusionMatrix(pred_stupid)
```

By looking at the confusion matrices we see that the problem is now, that even the stupid approach yields a reasonable mmce performance. Thus, we need additional measure: Let's compare the logistic regression and the stupid learner in terms of sensitivity[^1] and specificity[^2] (check if you can compute these values by hand): 

[^1]: Also called *true positive rate* or *recall*.
[^2]: Also called *true negative rate*


```{r roc-conf}
library(caret)

target <- bc_data[,target_label]
target_levels <- levels(target)
target_test <- bc_test[,target_label]

pred <- function(threshold=0.5) factor(ifelse(pred_logreg$data$prob.benign > 
                                                threshold, target_levels[1], 
                                                target_levels[2]))


cm <- confusionMatrix(pred(0.5),reference = target_test)
cm$table
cm$byClass[["Sensitivity"]]
cm$byClass[["Specificity"]]

cm <- confusionMatrix(pred_stupid$data$response,reference = target_test)
cm$table
cm$byClass[["Sensitivity"]]
cm$byClass[["Specificity"]]

```

A specificity of 0 means that all ill persons would be told they are healthy, which is certainly not what the 
test is intended for. On the other hand can we do better with the logistic regression in terms of those measures?
Remember with our classification methods we try to estimate the posterior probabilities. Until now in the case of two classes we classified the observation as the first class if its posterior probability is greater or equal to 50% and otherwise as the second class. So what happens if we move this threshold?

```{r roc-thrsh}
cm <- confusionMatrix(pred(0.01),reference = target_test)
cm$table
cm$byClass[["Sensitivity"]]
cm$byClass[["Specificity"]]

cm <- confusionMatrix(pred(0.7),reference = target_test)
cm$table
cm$byClass[["Sensitivity"]]
cm$byClass[["Specificity"]]

```

We see that in our case with a higher threshold value the specificity improves and 
the sensitivity degrades and vice versa. We can investigate this relationship with 
ROC curves. Compare the ROC curves:

```{r roc-roc_curves}
rocs <- generateThreshVsPerfData(list(logreg = pred_logreg, knn = pred_knn,
                                      stupid = pred_stupid), 
                                 measures = list(fpr, tpr, mmce))
plotROCCurves(rocs)
```
