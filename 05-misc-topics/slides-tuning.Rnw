%This file is a child of preamble.Rnw in the style folder
%if you want to add stuff to the preamble go there to make
%your changes available to all childs

<<setup-child, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@
% Load all R packages and set up knitr
<<setup, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{17}{Tuning}
\lecture{Introduction to Machine Learning}
\sloppy


\begin{vbframe}{Introduction}

  \begin{itemize}
    \item Many parameters or decisions for an ML algorithm are not decided by the
      (usually loss-minimizing) fitting procedure.
    \item Our goal is to optimize these w.r.t. the estimated prediction error (often this implies an independent test set), or by cross-validation.
    \item The same applies to preprocessing, feature construction and other model-relevant operations.
      In general we might be interested in optimizing a machine learning \enquote{pipeline}.
  \end{itemize}

  \framebreak

  \begin{blocki}{Model parameters vs. hyperparameters}
    \item Model parameters are optimized during training.
    \item In a simple regression model $ y = \theta^T x $, the model parameter $\theta$ is learned from the training set.
    \item Hyper parameters are values that must be specified outside of the
    training phase and need to be set according to the problem.
    \item Simple linear regression doesn't have hyperparameters, but variants do:
    \begin{itemize}
      \item One can add a regularization parameter $\lambda$ to the loss function to control the size of $\theta$.
      \item The type of the regularization (e.g. Lasso or Ridge regression)
    \end{itemize}
  \end{blocki}

\framebreak

  Alternatives are:

  \begin{itemize}
    \item The algorithm is really insensitive w.r.t. to changes of a parameter, so we don't really have to do anything as long as we stay in a broad range of reasonable values.
    \item Constant default: we can benchmark the algorithm across a broad range of data sets and scenarios and try to find a constant value that works well in many different situations. Quite optimistic?
    \item Dynamic (heuristic) default: We can benchmark the algorithm across a broad range of data sets and scenarios and try to find an easily computable function that sets the parameter in a data dependent way,
    e.g. mtry = p/3 by setting the kernel width of an RBF SVM w.r.t. the distance distribution of training data points.
      How to construct or learn that heuristic function?
    \item We can try to set the parameter by extracting more info from the fitted model. E.g.
      early stopping in boosting or ntree for a random forest (does OOB error increase or stagnate?),
      or some regularized models allow full-path computation for whole sequences of $\lambda$ values.
  \end{itemize}

  \framebreak

  \begin{blocki}{Why tuning is important:}
  \item Hyperparameters control the capacity of a model, i.e., how flexible the model is, how many degrees of freedom it has in fitting the data.
  \item If our model is to flexible and adapts to the training data too much
  we will face the problem of overfitting.
  \item Hence, control of capacity, i.e proper setting of hyperparameters
  prevents from overfitting the model w.r.t. the training set.
  \item Many other choices like the type of kernel, preprocessing, etc., can
    heavily influence model performance in non-trivial ways. It is extremely hard to guess the
    correct choices here.
  \end{blocki}

  \framebreak

  \begin{blocki}{Types of hyper parameters:}
    \item Numerical parameters (real valued / integers)
    \begin{itemize}
      \item Cost parameter of an SVM
      \item Depth, node-size of a tree
    \end{itemize}
    \item Categorical parameters:
    \begin{itemize}
      \item Which split criterion for trees?
      \item Which SVM-kernel?
    \end{itemize}
    \item Ordinal parameters:
    \begin{itemize}
      \item $\{$\texttt{low}, \texttt{medium}, \texttt{high}$\}$
    \end{itemize}
    \item Dependent parameters:
    \begin{itemize}
      \item Kernel parameter, according to the kernel
    \end{itemize}
  \end{blocki}

  \framebreak

  \begin{blocki}{What our tuning problem consists of:}
    \item Our learning method (or are there actually several?)
    \item The performance measure. Determined by the application. It gets especially interesting if we have to deviate from the standard cases like classification error or MSE. In general, we could be interested in multiple measures at once.
    \item Resampling procedure for measuring the performance. How do we choose it?
    \item Our parameters plus their regions-of-interest for optimization.
  \end{blocki}

  \framebreak

  \begin{blocki}{Some general remarks on tuning}
    \item Lots of literature exists for models, far less on efficient tuning.
    \item Our optimization problem is derivative-free, we can only ask for the
      quality of selected points (black-box problem).
    \item Our optimization problem is stochastic in principle.
      We want to optimize expected performance and use resampling.
    \item Evaluation of our target function will probably take quite some time.
      Imagine we are cross-validating a complex model on a larger data set.
    \item Categorical and dependent parameters complicate the problem.
    \item For difficult problems parallelizing the computation seems relevant.
  \end{blocki}

\end{vbframe}


\begin{vbframe}{Offline- vs. online-tuning}

  \begin{blocki}{Offline-tuning:}
    \item Learn optimal parameter settings before solving an instance.
    \item Tuning on training instances
  \end{blocki}

  \lz

  \begin{blocki}{Online-tuning:}
    \item Learn optimal parameter settings during solving.
    \item No training phase
  \end{blocki}

\end{vbframe}

\begin{vbframe}{Offline configuration}
\begin{center}
\includegraphics{figure_man/offline_configuration.png}

{\tiny St√ºtzle and Lopez-Ibanez, Automatic (Offline) Configuration of Algorithms, 2014.}
\end{center}
\end{vbframe}

\begin{vbframe}{Grid search}

  \begin{itemize}
    \item Offline tuning technique which is still quite popular.
    \item For each parameter a finite set of candidates is predefined.
    \item Then, one we simply search the Cartesian product of all possible combinations.
    \item All solutions are searched in an arbitrary order.
    \item Extension: Start with a coarse grid and iteratively refine it around a detected optimum.
  \end{itemize}

\framebreak

<<>>=
lrn = makeLearner("classif.ksvm", predict.type = "prob")
# ps = makeParamSet(
#   makeDiscreteParam("C", values = 2^(-3:3)),
#   makeDiscreteParam("sigma", values = 2^(-3:3)))
set.seed(1)

ps = makeParamSet(
  makeNumericParam("C", lower = -3 , upper = 3, trafo = function(x) 2^x),
  makeNumericParam("sigma", lower = -3 , upper = 3, trafo = function(x) 2^x)
)

ctrl = makeTuneControlGrid()

res = tuneParams(lrn, task = sonar.task,
  par.set = ps, resampling = hout, control = ctrl)
opt.grid = as.data.frame(res$opt.path)

gridSearch <- ggplot(opt.grid, aes(x = C, y = sigma, size = mmce.test.mean))
gridSearch +  geom_point(shape = 21 , col = "black", fill = "#56B4E9" , alpha = .6) + scale_size(range = c(3,15)) + scale_x_continuous("Parameter 1") + scale_y_continuous("Parameter 2")
@

\framebreak

  \begin{blocki}{Advantages:}
    \item Very easy to implement, therefore very popular.
    \item All parameter types possible.
    \item Parallelization is trivial.
  \end{blocki}

  \begin{blocki}{Disadvantages}
    \item Combinatorial explosion, inefficient
    \item Searches large irrelevant areas.
    \item Which values / discretization?
  \end{blocki}

\end{vbframe}

\begin{vbframe}{Random search}

  \begin{itemize}
    \item Small variation of grid search.
    \item Instead of evaluating all parameter configurations,
      we randomly sample from the region-of-interest.
    \item This often drastically reduces optimization time.
    \item Performs just as well as normal grid search in surprisingly
    many applications.
  \end{itemize}

\framebreak

<<>>=
set.seed(1)
ps = makeParamSet(
  makeNumericParam("C", lower = -3 , upper = 3, trafo = function(x) 2^x),
  makeNumericParam("sigma", lower = -3 , upper = 3, trafo = function(x) 2^x)
)

ctrl = makeTuneControlRandom(maxit = 50)

res = tuneParams(lrn, task = sonar.task,
  par.set = ps, resampling = hout, control = ctrl)
opt.grid = as.data.frame(res$opt.path)

rndSearch <- ggplot(opt.grid, aes(x = C, y = sigma, size = mmce.test.mean))
rndSearch + geom_point(shape = 21 , col = "black", fill = "#56B4E9" , alpha = .6) + scale_size(range = c(3,15)) + scale_x_continuous("Parameter 1") + scale_y_continuous("Parameter 2")
@


  % \framebreak
  %
  % \begin{blocki}{Why it works:}
  %   \item Imagine the $5\%$ interval around the optimum.
  %   \item Now we sample points out of the parameter space.
  %   \item Each point has a $5\%$-chance of falling into this interval.
  %   \item If we draw $n$-points, then the probability that at least one of the points falls into the desired interval is:
  %   $$ 1 - (1 - 0.95)^n $$
  %   \item We can calculate the number of draws it takes to have a $95\%$-chance of success:
  %   $$
  %   1 - (1 - 0.95)^n > 0.95 \quad \Longleftrightarrow \quad n \geq 60
  %   $$
  % \end{blocki}

\end{vbframe}

\endlecture
