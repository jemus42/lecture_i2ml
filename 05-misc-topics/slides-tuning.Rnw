%This file is a child of preamble.Rnw in the style folder
%if you want to add stuff to the preamble go there to make
%your changes available to all childs

<<setup-child, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@
% Load all R packages and set up knitr
<<setup, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{Hyperparameter Tuning}
\lecture{Introduction to Machine Learning}
\sloppy

\begin{vbframe}{Hyperparameter tuning}
\begin{itemize}
\item Many parameters or decisions for an ML algorithm are not decided by the fitting procedure
\item \textbf{Model parameters} are optimized during training, by some form of loss minimization. They are an \textbf{output} of the training.
E.g., the coefficients of a linear model or the optimal splits of a tree learner.
\item \textbf{Hyperparameters} must be specified before the training phase. They are an \textbf{input} of the training.
E.g., how small a leaf can become for a tree; $k$ and which distance measure to use for kNN
\item HPs have to be set either by the user or by (smart) default values
\item Our goal is to optimize these w.r.t. the estimated prediction error; this implies an independent test set, or cross-validation
\end{itemize}
% - The same applies to preprocessing, feature construction and other model-relevant operations.
  % In general we might be interested in optimizing a machine learning \enquote{pipeline}
    % \item This also extends all kinds of preprocessing, feature construction and other model-relevant operations.
      % In general, we might be interested in optimizing an entire machine learning \enquote{pipeline}.
\end{vbframe}



% \framebreak

  % Possible scenarios for finding default hyperparameters:

  % \begin{itemize}
  %   \item If the learner's performance is fairly insensitive to changes of a hyperparameter, we don't really have to worry as long as we remain within the range of reasonable values.
  %   \item Constant default: we can benchmark the learner across a broad range of data sets and scenarios and try to find hyperparameter values that work well in many different situations. Quite optimistic?
  %   \item Dynamic (heuristic) default: We can benchmark the learner across a broad range of data sets and scenarios and try to find an easily computable function that sets the hyperparameter in a data dependent way,
  %   e.g. using \texttt{mtry}$ = p/3$ for RF.\\
  %     How to construct or learn that heuristic function, though...?
  %   \item In some cases, can try to set hyperparameters optimally by extracting more info from the fitted model. E.g. \texttt{ntrees} for a random forest (does OOB error increase or decrease if you remove trees from the ensemble?).
  % \end{itemize}
% \end{vbframe}

\begin{vbframe}{Types of hyperparameters}
    \begin{itemize}
    \item Numerical parameters (real valued / integers)
    \begin{itemize}
      \item $mtry$ in a random forest
      \item Neighborhood size $k$ for kNN
    \end{itemize}
    \item Categorical parameters:
    \begin{itemize}
      \item Which split criterion for classification trees?
      \item Which distance measure for kNN?
    \end{itemize}
    \item Ordinal parameters:
    \begin{itemize}
      \item $\{$\texttt{low}, \texttt{medium}, \texttt{high}$\}$
    \end{itemize}
    \item Dependent parameters:
    \begin{itemize}
      \item If we use the Gaussian kernel for the SVM, what is its width?
    \end{itemize}
  \end{itemize}
\end{vbframe}

\begin{vbframe}{Why tuning is important}
\begin{itemize}
\item Hyperparameters control the capacity of a model, i.e., how flexible the model is, how many degrees of freedom it has in fitting the data.
\item If a model is too flexible so that it simply \enquote{memorizes} the training data, we will face the dreaded problem of overfitting .
\item Hence, control of capacity, i.e., proper setting of hyperparameters
 can prevent overfitting the model on the training set.
\item Many other factors like preprocessing or feature selection can
  heavily influence model performance in non-trivial ways. It is extremely hard to guess the correct choices here.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Components of tuning problem}
\begin{itemize}
\item The learner (possibly: several competing learners?)
\item The performance measure. Determined by the application. Not necessarily identical to the loss function that the learner tries to minimize
  We could even be interested in multiple measures simultaneously, 
  e.g. accuracy and sparseness of our model, TRP and PPV, etc. 
\item A (resampling) procedure for estimating the predictive performance 
\item The learner's hyperparameters and their respective regions-of-interest over which we optimize
\end{itemize}
\end{vbframe}


\begin{vbframe}{Why is tuning so hard?}
\begin{itemize}
%\item Lots of literature exists for models, far less on efficient tuning.
\item Tuning is derivative-free (\enquote{black box problem}): It is usually impossible to compute derivatives of the objective (i.e., the resampled performance measure) that we optimize with regard to the HPs. All we can do is evaluate the performance for a given hyperparameter configuration.
\item Every evaluation requires one or multiple train and predict steps of the learner. I.e., every evaluation is very \textbf{expensive}.
\item Even worse: the answer we get from that evaluation is \textbf{not exact, but stochastic} in most settings, as we use resampling.
% \item Even worse: the function value we get from that evaluation is \textbf{likely also biased} -- it is difficult to evaluate the tested hyperparameter settings \emph{honestly}, i.e., in such a way that we neither over- nor underestimate their performance if we only have a limited amount of data available. (Remember resampling-based performance evaluation \& its problems -- this gets worse where tuning comes into play.)
\item Categorical and dependent hyperparameters aggravate our difficulties: the space of hyperparameters we optimize over has a non-metric, complicated structure.
\item For large and difficult problems parallelizing the computation seems relevant, to evaluate multiple HP configurations in parallel or to speed up the resampling-based performance evaluation
\end{itemize}
\end{vbframe}

\begin{vbframe}{Grid search}

\begin{itemize}
\item Simple technique which is still quite popular, tries all 
  HP combinations on a multi-dim. discretized grid
\item For each hyperparameter a finite set of candidates is predefined
\item Then, we simply search all possible combinations in arbitrary order
\end{itemize}

<<fig.height=3.7>>=
plotTune = function(d) {
  d$Accuracy = mvtnorm::dmvnorm(x = d, mean = c(5,5), sigma = 40 * diag(2)) * 120 + 0.4
  pl = ggplot(data = d, aes(x = x, y = y, color = Accuracy))
  pl = pl + geom_point(size = d$Accuracy * 4)
  pl = pl + xlab("Parameter 1") + ylab("Parameter 2") + coord_fixed()
}
x = y = seq(-10, 10, length.out = 10)
d = expand.grid(x = x, y = y)
pl = plotTune(d)
print(pl)
@

Grid search over 10x10 points

\framebreak

\begin{blocki}{Advantages}
\item Very easy to implement, therefore very popular
\item  All parameter types possible
\item  Parallelization is trivial
\end{blocki}

\begin{blocki}{Disadvantages}
\item  Combinatorial explosion, inefficient
\item  Searches large irrelevant areas
\item  Which values / discretization?
\end{blocki}
\end{vbframe}


\begin{vbframe}{Random search}

\begin{itemize}
\item Small variation of grid search
\item Uniformly sample from the region-of-interest
\end{itemize}

<<fig.height=3.8>>=
x = runif(40, -10, 10)
y = runif(40, -10, 10)
d = data.frame(x = x, y = y)
pl = plotTune(d)
print(pl)
@

Random search over 100 points

\framebreak

\begin{blocki}{Advantages}
\item Very easy to implement, therefore very popular
\item  All parameter types possible
\item  Parallelization is trivial
\item Anytime algorithm - we can always increase the budget when we are not satisfied
\item Often better than grid search, as each individual parameter has been tried with $m$ different values, when the search budget was $m$. Mitigates the problem of discretization
\end{blocki}

\begin{blocki}{Disadvantages}
\item As for grid search, many evaluations in areas with low likelihood for improvement
\end{blocki}
\end{vbframe}


  % \framebreak
  %
  % \begin{blocki}{Why it works:}
  %   \item Imagine the $5\%$ interval around the optimum.
  %   \item Now we sample points out of the parameter space.
  %   \item Each point has a $5\%$-chance of falling into this interval.
  %   \item If we draw $n$-points, then the probability that at least one of the points falls into the desired interval is:
  %   $$ 1 - (1 - 0.95)^n $$
  %   \item We can calculate the number of draws it takes to have a $95\%$-chance of success:
  %   $$
  %   1 - (1 - 0.95)^n > 0.95 \quad \Longleftrightarrow \quad n \geq 60
  %   $$
  % \end{blocki}


\endlecture
