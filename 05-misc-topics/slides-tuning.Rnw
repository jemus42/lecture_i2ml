%This file is a child of preamble.Rnw in the style folder
%if you want to add stuff to the preamble go there to make
%your changes available to all childs

<<setup-child, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@
% Load all R packages and set up knitr
<<setup, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{16}{Tuning}
\lecture{Introduction to Machine Learning}
\sloppy


\begin{vbframe}{Introduction}

  \begin{itemize}
    \item For many learners, a lot of parameters or properties need to be set manually \emph{before} the optimization over the hypothesis space by empirical risk minimization can be performed.\\
    (For example, setting the maximal depth of a tree or deciding which distance measure to use for kNN.)
    \item Our goal is to optimize these with regard to the expected model performance on new data (often this implies an independent test set, or cross-validation).
    \item This also extends all kinds of preprocessing, feature construction and other model-relevant operations.
      In general, we might be interested in optimizing an entire machine learning \enquote{pipeline}.
  \end{itemize}

  \framebreak

  \begin{blocki}{Model parameters vs. hyperparameters}
    \item \textbf{Model parameters} are optimized during training, usually by loss minimization.\\
     For example, a tree learner finds optimal splits by maximizing impurity reduction and optimal predictions by minimizing the average loss in its leaves.
     \lz
    \item \textbf{Hyper parameters} are values that must be specified outside of the
    training phase and that need to be chosen dependent on the problem.\\
    For example, the stopping criteria of a tree learner (how deep the tree can become, how small the number of observations in each node can become, etc) need to be set before optimization can be performed.
  \end{blocki}

\framebreak

  Alternatives are:

  \begin{itemize}
    \item The learner's performance is really insensitive to changes of a parameter, so we don't really have to do anything as long as we stay in a broad range of reasonable values.
    \item Constant default: we can benchmark the learner across a broad range of data sets and scenarios and try to find a constant value that works well in many different situations. Quite optimistic?
    \item Dynamic (heuristic) default: We can benchmark the learner across a broad range of data sets and scenarios and try to find an easily computable function that sets the parameter in a data dependent way,
    e.g. using \texttt{mtry}$ = p/3$ for RF.\\
      How to construct or learn that heuristic function, though...?
    \item We can try to set the parameter by extracting more info from the fitted model. E.g.
      \texttt{ntrees} for a random forest (does OOB error increase or stagnate?).
  \end{itemize}

  \framebreak

  \begin{blocki}{Why tuning is important:}
  \item Hyperparameters control the capacity of a model, i.e., how flexible the model is, how many degrees of freedom it has in fitting the data.
  \item If a model is too flexible so that it simply \enquote{memorizes} the training data,
  we will face the dreaded problem of overfitting .
  \item Hence, control of capacity, i.e., proper setting of hyperparameters
  can prevent overfitting the model on the training set.
  \item Many other factors like preprocessing or feature selection can
    heavily influence model performance in non-trivial ways. It is extremely hard to guess the
    correct choices here.
  \end{blocki}

  \framebreak

  \begin{blocki}{Types of hyper parameters:}
    \item Numerical parameters (real valued / integers)
    \begin{itemize}
      \item Number of trees in a random forest
      \item Neighborhood size $k$ for kNN
    \end{itemize}
    \item Categorical parameters:
    \begin{itemize}
      \item Which split criterion for classification trees?
      \item Which distance measure for kNN?
    \end{itemize}
    \item Ordinal parameters:
    \begin{itemize}
      \item $\{$\texttt{low}, \texttt{medium}, \texttt{high}$\}$
    \end{itemize}
    \item Dependent parameters:
    \begin{itemize}
      \item Given Euclidean distance for kNN, use rescaled features or not?
    \end{itemize}
  \end{blocki}

  \framebreak

  \begin{blocki}{What our tuning problem consists of:}
    \item Our learning method (or are there actually several?)
    \item The performance measure. Determined by the application. Not necessarily identical
    to the loss function that the learner tries to minimize.
    %It gets especially interesting if we have to deviate from the standard cases like classification error or MSE. 
     We could even be interested in multiple measures at once, e.g. the accuracy of a predicted value \emph{and} how long it took to compute it.
    \item A resampling procedure for measuring the performance. How do we choose it?
    \item The hyperparameters with their respective regions-of-interest over which to optimize.
  \end{blocki}

  \framebreak

  \begin{blocki}{Some general remarks on tuning}
    %\item Lots of literature exists for models, far less on efficient tuning.
    \item Tuning is a \enquote{black box problem}: It is usually impossible to compute
    derivative of the function we try to optimize, we can only evaluate it at given points.
%%CONTINUE HERE    
    find out the
      performance of selected hyperparamter configurations by training .
    \item Our optimization problem is stochastic in principle.
      We want to optimize expected performance and use resampling.
    \item Evaluation of our target function will probably take quite some time.
      Imagine we are cross-validating a complex model on a larger data set.
    \item Categorical and dependent parameters complicate the problem.
    \item For difficult problems parallelizing the computation seems relevant.
  \end{blocki}

\end{vbframe}


\begin{vbframe}{Offline- vs. online-tuning}

  \begin{blocki}{Offline-tuning:}
    \item Learn optimal parameter settings before solving an instance.
    \item Tuning on training instances
  \end{blocki}

  \lz

  \begin{blocki}{Online-tuning:}
    \item Learn optimal parameter settings during solving.
    \item No training phase
  \end{blocki}

\end{vbframe}

\begin{vbframe}{Offline configuration}
\begin{center}
\includegraphics{figure_man/offline_configuration.png}

{\tiny St√ºtzle and Lopez-Ibanez, Automatic (Offline) Configuration of Algorithms, 2014.}
\end{center}
\end{vbframe}

\begin{vbframe}{Grid search}

  \begin{itemize}
    \item Offline tuning technique which is still quite popular.
    \item For each parameter a finite set of candidates is predefined.
    \item Then, one we simply search the Cartesian product of all possible combinations.
    \item All solutions are searched in an arbitrary order.
    \item Extension: Start with a coarse grid and iteratively refine it around a detected optimum.
  \end{itemize}

\framebreak

<<>>=
lrn = makeLearner("classif.ksvm", predict.type = "prob")
# ps = makeParamSet(
#   makeDiscreteParam("C", values = 2^(-3:3)),
#   makeDiscreteParam("sigma", values = 2^(-3:3)))
set.seed(1)

ps = makeParamSet(
  makeNumericParam("C", lower = -3 , upper = 3, trafo = function(x) 2^x),
  makeNumericParam("sigma", lower = -3 , upper = 3, trafo = function(x) 2^x)
)

ctrl = makeTuneControlGrid()

res = tuneParams(lrn, task = sonar.task,
  par.set = ps, resampling = hout, control = ctrl)
opt.grid = as.data.frame(res$opt.path)

gridSearch <- ggplot(opt.grid, aes(x = C, y = sigma, size = mmce.test.mean))
gridSearch +  geom_point(shape = 21 , col = "black", fill = "#56B4E9" , alpha = .6) + scale_size(range = c(3,15)) + scale_x_continuous("Parameter 1") + scale_y_continuous("Parameter 2")
@

\framebreak

  \begin{blocki}{Advantages:}
    \item Very easy to implement, therefore very popular.
    \item All parameter types possible.
    \item Parallelization is trivial.
  \end{blocki}

  \begin{blocki}{Disadvantages}
    \item Combinatorial explosion, inefficient
    \item Searches large irrelevant areas.
    \item Which values / discretization?
  \end{blocki}

\end{vbframe}

\begin{vbframe}{Random search}

  \begin{itemize}
    \item Small variation of grid search.
    \item Instead of evaluating all parameter configurations,
      we randomly sample from the region-of-interest.
    \item This often drastically reduces optimization time.
    \item Performs just as well as normal grid search in surprisingly
    many applications.
  \end{itemize}

\framebreak

<<>>=
set.seed(1)
ps = makeParamSet(
  makeNumericParam("C", lower = -3 , upper = 3, trafo = function(x) 2^x),
  makeNumericParam("sigma", lower = -3 , upper = 3, trafo = function(x) 2^x)
)

ctrl = makeTuneControlRandom(maxit = 50)

res = tuneParams(lrn, task = sonar.task,
  par.set = ps, resampling = hout, control = ctrl)
opt.grid = as.data.frame(res$opt.path)

rndSearch <- ggplot(opt.grid, aes(x = C, y = sigma, size = mmce.test.mean))
rndSearch + geom_point(shape = 21 , col = "black", fill = "#56B4E9" , alpha = .6) + scale_size(range = c(3,15)) + scale_x_continuous("Parameter 1") + scale_y_continuous("Parameter 2")
@


  % \framebreak
  %
  % \begin{blocki}{Why it works:}
  %   \item Imagine the $5\%$ interval around the optimum.
  %   \item Now we sample points out of the parameter space.
  %   \item Each point has a $5\%$-chance of falling into this interval.
  %   \item If we draw $n$-points, then the probability that at least one of the points falls into the desired interval is:
  %   $$ 1 - (1 - 0.95)^n $$
  %   \item We can calculate the number of draws it takes to have a $95\%$-chance of success:
  %   $$
  %   1 - (1 - 0.95)^n > 0.95 \quad \Longleftrightarrow \quad n \geq 60
  %   $$
  % \end{blocki}

\end{vbframe}

\endlecture
