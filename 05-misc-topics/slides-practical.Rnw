%This file is a child of preamble.Rnw in the style folder
%if you want to add stuff to the preamble go there to make
%your changes available to all childs

<<setup-child, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@
% Load all R packages and set up knitr
<<setup, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{18}{Practical tips}
\lecture{Introduction to Machine Learning}
\sloppy



\begin{vbframe}{Practical tips}
Practical Tips and best practices for doing machine learning. 
\vfill
\begin{quote}
  An approximate answer to the right problem is worth a good deal more than an exact answer to an approximate problem. \\
  \hfill -- John Tukey
\end{quote}
\end{vbframe}

\begin{vbframe}{Sources}
    
    This lecture is based on the following books:
    \begin{itemize}
    \item Leek, Jeff: \emph{The Elements of Data Analytic Style}. Ebook, \url{https://leanpub.com/datastyle}, 2015.
    \item Hastie/Tibshirani/Friedman: \emph{The Elements of Statistical Learning}. Springer, 2009.
    \end{itemize}
\end{vbframe}

\begin{vbframe}{The Data Science Workflow}
  %  http://docs.aws.amazon.com/machine-learning/latest/dg/building-machine-learning.html
  \begin{enumerate}
  \item (Re-)formulate the problem as a prediction problem. What is observed? What should be predicted?
  \item Collect, clean and prepare the data for the machine learning model.
  \item Explore the data to assess the quality and understand it.
  \item Transform the input and output variables if necessary. Feature engineering is usually a manual job. 
  \item Train the model, evaluate its performance on independent test data, which was not used for learning.
  \item Predict new data points with the model
 \end{enumerate}
\end{vbframe}

\begin{vbframe}{Missing data}
  \begin{itemize}
  \item In real data sets, missing values are common.
  \item For some observations only a part of the variables might be observed.
  \item Most algorithms can't handle missing values (e.g. linear models).
  \item Missing data can be missing completely at random (MCAR) or missing not a at random (MNAR). 
  \item Example MCAR: A blood test get's lost in the lab / Survey questions are not filled out because of internet connectivity problems. 
  \item Example MNAR: People with a high income leave the question about their income empty (because it's high). 
  \end{itemize}
\end{vbframe}

\begin{vbframe}{Missing data}
  \begin{itemize}
  \item If only a small portion of the data contains missing values and the values are missing at random (MCAR), these observations can be deleted. 
  \item Deleting data that is not missing at random (MNAR) might bias the machine learning model. 
  \item If the missing values have a structure (MNAR), adding a feature indicating missingness can help (e.g. 'Refused to answer'). 
  \item For randomly missing data (MCAR) \textbf{Imputation} is an option.
  \item Imputation is more art than science -- the choice of method depends on the data as well as the prediction problem. 
  \end{itemize}
\end{vbframe}

\begin{vbframe}{Imputation}
  Imputation methods: 
  \begin{itemize}
  \item The ``hot deck'' imputation chooses the most similar data point to replace missing values (1-nearest neighbour method).
  \item The mean imputation replace missing feature values with their mean. Problematic: changes the feature distribution.
  \item The regression imputation fits a regression model with the missing value feature as target and all other features as input. The predictions are used to impute the missing values. 
  \item The EM-algorithm is an iterative method to estimate parameters that were not observed or cannot be observed.
  \end{itemize}
\end{vbframe}

\begin{vbframe}{Feature Engineering}
% http://docs.aws.amazon.com/machine-learning/latest/dg/data-transformations-for-machine-learning.html
  \begin{itemize}
  \item \textbf{Feature Engineering} is the combination and transformation of features to improve the predictive performance.
  \item Example: The \texttt{flights}-dataset contains flights: departure and arrival time and the delays in minutes. Additionally to knowing departure and arrival, the flight duration might be useful to predict delays.
  \item $\Rightarrow$ Create a new feature ``flight duration'' as the time between departure and planned arrival time.
  \end{itemize}
\end{vbframe}

\begin{vbframe}{Feature Engineering}
  % http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/
  More examples:
  \begin{itemize}
  \item Calculate the logarithm of a feature (useful for GLMs, but not trees).
  \item Create a ``County'' feature from the zip code.
  \item Split a time variable like \texttt{2016-06-01\_13:49} into the feature ``year'', ``month'', ``day'', ``weekday'' and ``time''.
  \end{itemize}
\end{vbframe}

\begin{vbframe}{Model class selection}
  Which model class should I use?
  \begin{itemize}
  \item If you prioritize good \textbf{prediction} over understanding the underlying model, choose a machine learning algorithm. Example: Amazon product recommendations. 
  \item If you prioritize valid \textbf{insights} over good prediction (e.g. understanding the influence of a feature), choose statistical models. Example: Clinical trials. 
  \end{itemize}
\end{vbframe}

\begin{vbframe}{Retraining prediction models}
  %  http://docs.aws.amazon.com/machine-learning/latest/dg/retraining-models-on-new-data.html
  \begin{itemize}
  \item Prediction models require new data to have the same distribution as the the training data to yield good predictions. 
  \item If the data generating process changes, the model must be retrained. 
  \item Ideally, the distribution of new data should be monitored. 
  \item Alternatively, the model could be retrained regularly, e.g. monthly.
  \end{itemize}
\end{vbframe}

\begin{vbframe}{kaggle}
  \begin{itemize}
  \item Companies and research institutions create machine learning competitions on the online platform kaggle.
  \item They provide datasets and a clear problem formulation.
  \item Anyone with ML knowledge, an internet connection and a computer can participate and learn models. 
  \item The participants with the best scores (predictive performance) on an independent test dataset win prizes. 
  \item Among the winners certain algorithms are used often. 
  \end{itemize}
\end{vbframe}
  
\begin{vbframe}{Which algorithms work best?}
  A careful generalization:
  \begin{itemize}
  \item \textbf{GLMs} are the classic choice for parameterized models. \textbf{Lasso} and \textbf{Elastic Net} can be used to avoid overfitting and reduce dimensionality. 
  \item For good prediction on tabular data, tree ensembles like \textbf{Random Forest} and \textbf{xgboost} (boosted tree stumps) work generally well. 
  \item For audio and image data, \textbf{Deep Learning} is the strongest method. 
  \item But: There is no single algorithm that is the best for all settings. Try different methods and choose the one that works best in your specific problem setting. 
  \end{itemize}
\end{vbframe}

\begin{vbframe}{What are important factors for a good model?}
  % https://www.quora.com/What-do-top-Kaggle-competitors-focus-on
  \begin{itemize}
  \item A first explorative analysis to discover oddities and understand the data.
  \item Data preparation and feature engineering are often more impactful then the choice of algorithm. This requires creativity. Usually having more features is preferable - many algorithms can ignore irrelevant features. 
  \item Always be wary of overfitting.
  \item Blending/ensembling multiple models can improve the predictions
  \end{itemize}
\end{vbframe}

\begin{vbframe}{Summary}
  \begin{itemize}
  \item Machine Learning: Less assumtions $\Rightarrow$ better predictions
  \item Statistik: Assumptions $\Rightarrow$ better interpretability
  \item The model choice depends on the problem. 
  \item Data preparation and feature engineering are even more important. 
  \end{itemize}

\begin{quote}
  People worry that computers will get too smart and take over the world, but the real problem is that they're too stupid and they've already taken over the world.
  \hfill -- Pedro Domingos
\end{quote}
\end{vbframe}
\endlecture
