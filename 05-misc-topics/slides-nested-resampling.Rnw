% Introduction to Machine Learning
% Day 3

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
library(smoof)
@

% Load all R packages and set up knitr
<<setup, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{17}{Nested Resampling}
\lecture{Fortgeschrittene Computerintensive Methoden}

\begin{vbframe}{Training set vs Test Set}
\includegraphics{figure_man/test_train.png}
\begin{itemize}

\item The training set contains a known output and our model learns on this data. 
\item We evaluate our model’s predictive skill on the test set.

\item But what if our train/test split isn’t random? What if one subset of our data has only people from a certain category or is constant in one of the features? 

\item What if we are sure that we want to apply a certain model, but don't know which hyperparameters to use?

\end{itemize}
\end{vbframe}

\begin{vbframe}{Train - Validate - Test}
\includegraphics{figure_man/test_valid_train.png}
\begin{itemize}

\item Train: Used to train your models. 

\item Validation: used for model tuning (evaluate different hyperparameters) or model selection

\item Test: used for final estimate of the generalization performance.

\end{itemize}
\end{vbframe}


\begin{vbframe}{Nested Resampling}

Imagine this situation:

\begin{itemize}
  \item We want to use the k-NN algorithm for classification on the iris data set
  \item As we learned before, we use Cross-Validation to tune the hyperparameter $k$ (the size of the neighborhood)
  \item Therefore, we split our data in a validation and a train data set
  \item Now that we found the perfect $k^*$, we want to evaluate the performance of our model
  \item Problem: we already spent all our data in the Cross-Validation and do not have any unseen data left
  \item Solution: \textbf{nested resampling}
\end{itemize}

\end{vbframe}

\begin{vbframe}{Cross validation}
\includegraphics{figure_man/cross-validation.pdf}
\end{vbframe}

\begin{vbframe}{Nested resampling}
In model selection, we are interested in selecting the best model from a set of potential candidate models (e.g., different model classes, different hyperparameter settings, different feature sets).


\begin{blocki}{Problem}
    \item We cannot evaluate our finally selected leaner on the same resampling splits that we have used to
      perform model selection for it, e.g., to tune its hyperparameters.
    \item By repeatedly evaluating the learner on the same test set, or the same CV splits, information
      about the test set \enquote{leaks} into our evaluation.
    \item Danger of overfitting to the resampling splits / overtuning!
    \item The final performance estimate will be optimistically biased.
    \item One could also see this as a problem similar to multiple testing.

\end{blocki}

\framebreak

\begin{blocki}{Instructive and problematic example}
    \item Assume a binary classification problem with equal class sizes.
    \item Assume a learner with hyperparameter $\lambda$.
    \item Here, the learner is a (non-sensical) feature-independent classifier, 
          where $\lambda$ has no effect. The learner simply 
          predicts random labels with equal probability.
    \item Of course, it's true generalization error is 50\%.
    \item A cross-validation of the learner (with any fixed $\lambda$) will easily show this
      (given that the partitioned data set for CV is not too small).
    \item Now lets \enquote{tune} it, by trying out 100 different $\lambda$ values.
    \item We repeat this experiment 50 times and average results.
\end{blocki}

<<fig.height=3.5, echo = FALSE>>=
# rsrc data from rsrc/overtuning-example.R
ggd = BBmisc::load2("rsrc/overtuning-example.RData", "res2.mean")
ggd = melt(ggd, measure.vars = colnames(res2), value.name = "tuneperf");
colnames(ggd)[1:2] = c("data.size", "iter")
ggd = sortByCol(ggd, c("data.size", "iter"))
ggd$data.size = as.factor(ggd$data.size)


pl = ggplot(ggd, aes(x = iter, y = tuneperf, col = data.size))
pl =  pl + geom_line()
print(pl)
@

\begin{itemize}
  \item Plotted is the best \enquote{tuning error} after $k$ tuning iterations
    \item We have performed the experiment for different sizes of learning data
      that where cross-validated.
    \item Experiment was simulated with mlr's \enquote{classif.featureless} learner.
    \item Quiz: How to mathematically calculate the shape of the curves?
\end{itemize}

\framebreak

\begin{blocki}{Another perspective}
  \item Tune the same \enquote{fake} parameter $\lambda$ with CV
  \item Run the CV for 10, 100 and 500 different values of $\lambda$
  \item The \enquote{best} $\lambda$ error is getting smaller with increasing iterations
  \item We would choose the lambda value that yields a mmce = 30\%
  \item Nonsense as our model could never get better than mmce = 50\%
  \item The result is based on pure randomness
  \item The chance of getting such a random \enquote{outlier-split} increases with the number of CV iterations
  \item We \textbf{overtune} the model
\end{blocki}

<<fig.heigh = 5>>=

  set.seed(1342)

  makeMyTask = function(n) {
    y = sample(c(0,1), size = n, replace = TRUE)
    d = data.frame(x = 1, y = as.factor(y))
    makeClassifTask(data = d, target = "y")
  }

  task.size = 100
  task = makeMyTask(task.size)
  cv.iters = 2
  # "tuning" featureless learner by resampling it very often
  lrn = makeLearner("classif.featureless", method = "sample-prior")
  rin = makeResampleInstance("CV", iters = cv.iters, task = task)
  tune.list = c(10, 100, 500)

  # storage
  storage.list = list()
  for (i in 1:length(tune.list)) {
    storage.list[[i]] = replicate(tune.list[[i]], resample(lrn, task, rin, show.info = FALSE)$aggr)
  }
  boxplot(storage.list[[1]], storage.list[[2]], storage.list[[3]],
    names =  tune.list, xlab = "CV iterations", ylab = "mmce", col = "#E69F00AA")
  abline(h = min(storage.list[[1]]), lty = 3)
  abline(h = min(storage.list[[2]]), lty = 3)
  abline(h = min(storage.list[[3]]), lty = 3)

@


\begin{blocki}{Simple solution}
\item Again, simply simulate what happens in the model's application.
\item All parts of model building (including model selection, preprocessing) should be embedded
  in the resampling, i.e., repeated for every pair of training/test data.
\item For steps that themselves require resampling (e.g. hyperparameter tuning) this results
  in two \textbf{nested resampling} loops, i.e. a resampling strategy for both tuning and outer evaluation.
\item Simplest form is a 3-way split into a training, optimization and test set.
  Inducers are trained on the training set, evaluated on the optimization set.
  After the final model is selected, we fit on joint training+optimization set and evaluate
  a final time on the test set. Note that we touch the test set only once, and have no way of \enquote{cheating}.
\end{blocki}

  \framebreak

Example: Outer loop with 3-fold CV and inner loop with 4-fold CV

\begin{center}
  \includegraphics[width=9.5cm]{figure_man/Nested_Resampling.png}
\end{center}

\framebreak

\begin{itemize}
  \item We evaluate hyperparameter candidates $\theta_i; i = 1, \dots$ with 4-fold CV in the inner resampling and 3-fold CV in the outer:
  \begin{itemize}
    \item Evaluate each $\theta_i$ on the 4 inner test samples
    \item Calculate the corresponding preliminary $\hat GE(\theta_i)$ for $\theta_i$
    \item Choose the winner $\theta^*$ for each of the 3 inner samples
    \item If we would take this $\hat GE(\theta^*)$ as the overall generalization error we would get an optimistic bias (Why?)
    \item We have (at most) 3 winners from the inner resampling rounds
    \item For each of them, we re-train the model on the outer train sets (darkgreen), calculate the "real" $GE(\theta^{*})$ on the outer test sets (lightgreen), and choose the best overall hyperparameter setting $\theta^{**}$
  \end{itemize}
  \item The error estimates on the outer samples (lightgreen) are unbiased because this data was strictly excluded from the validation process
\end{itemize}
\end{vbframe}

% \begin{vbframe}{Nested resampling with mlr}

% <<size="footnotesize">>=
% ## Tuning in inner resampling loop
% ps = makeParamSet(
%   makeDiscreteParam("C", values = 2^(-2:2)),
%   makeDiscreteParam("sigma", values = 2^(-2:2))
% )
% ctrl = makeTuneControlGrid()
% inner = makeResampleDesc("Subsample", iters = 2L)
% lrn = makeTuneWrapper("classif.ksvm", resampling = inner,
%   par.set = ps, control = ctrl, show.info = FALSE)
% ## Outer resampling loop
% outer = makeResampleDesc("CV", iters = 3L)
% @

%   \framebreak

% <<size="footnotesize">>=
% r = resample(lrn, iris.task, resampling = outer,
%   extract = getTuneResult, show.info = FALSE); print(r)

% print(r$extract[[1L]])
% @
% \end{vbframe}


\endlecture
