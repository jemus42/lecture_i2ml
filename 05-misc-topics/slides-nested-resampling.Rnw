% Introduction to Machine Learning
% Day 3

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
library(smoof)
@

% Load all R packages and set up knitr
<<setup, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{17}{Nested Resampling}
\lecture{Fortgeschrittene Computerintensive Methoden}

\begin{vbframe}{Training set vs Test Set}
\includegraphics{figure_man/test_train.png}
\begin{itemize}

\item The training set contains a known output and our model learns on this data. 
\item We test our model’s prediction on test subset.

\item But what if our train/test split isn’t random? What if one subset of our data has only people from a certain category or is constant in one of the features? 

\item What if we are sure, that we want to apply a certain model, but are undecided on which parameters to use?

\end{itemize}
\end{vbframe}

\begin{vbframe}{Train - Validate - Test}
\includegraphics{figure_man/test_valid_train.png}
\begin{itemize}

\item Train: Train your models 

\item Validation: for model tuning (validate models with different parameters) or model selection

\item Test: make a final estimate of the generalization 

\end{itemize}
\end{vbframe}






\begin{vbframe}{Nested Resampling}

Imagine this situation:

\begin{itemize}
  \item We want to use the k-NN algorithm for classification on the iris data set
  \item As we learned before, we use Cross-Validation to tune the hyperparameter $k$ (the size of the neighborhood)
  \item Therefore, we split our data in a validation and a train data set
  \item Now that we found the perfect $k^*$, we want to evaluate the performance of our model
  \item Problem: we already spent all our data in the Cross-Validation and do not have any unseen data left
  \item Solution: \textbf{nested resampling}
\end{itemize}

\end{vbframe}

\begin{vbframe}{Cross validation}
\includegraphics{figure_man/cross-validation.pdf}
\end{vbframe}

\begin{vbframe}{Nested resampling}
In model selection, we are interested in selecting the best model from a set of potential candidate models (e.g., different model classes, different hyperparameter settings, different feature sets).


\begin{blocki}{Problem}
    \item We cannot evaluate our finally selected inducer on the same resampling splits that we have used to
      perform model selection for it, e.g., to tune its hyperparameters.
    \item By repeatedly evaluating the inducer on the same test set, or the same CV splits, information
      about the test can enter the algorithm.
    \item Danger of overfitting to the resampling splits / overtuning!
    \item The final performance estimate will be optimistically biased.
    \item One could also see this as a problem similar to multiple testing.

\end{blocki}

\framebreak

\begin{blocki}{Instructive and problematic example}
    \item Assume a binary classif. problem with equal apriori class sizes.
    \item Assume an inducer $a(\D, \lambda)$, with hyperparameter $\lambda$.
    \item $a$ shall be a (non-sensical) feature-independent classifier, where $\lambda$ has no effect.
      $a$ predicts random labels with equal probability.
    \item Of course, a's generalization error is 50\%.
    \item A cross-validation of $a$ (with any fixed $\lambda$) will easily show this
      (given that the partitioned data set for CV is not too small).
    \item Now lets \enquote{tune} $a$, by trying out 100 different $\lambda$ values.
    \item We repeat the experiment 50 times and average results.
\end{blocki}

<<fig.height=3.5, echo = FALSE>>=
# rsrc data from rsrc/overtuning-example.R
ggd = BBmisc::load2("rsrc/overtuning-example.RData", "res2.mean")
ggd = melt(ggd, measure.vars = colnames(res2), value.name = "tuneperf");
colnames(ggd)[1:2] = c("data.size", "iter")
ggd = sortByCol(ggd, c("data.size", "iter"))
ggd$data.size = as.factor(ggd$data.size)


pl = ggplot(ggd, aes(x = iter, y = tuneperf, col = data.size))
pl =  pl + geom_line()
print(pl)
@

\begin{itemize}
  \item Plotted is the best \enquote{tuning error} after $k$ tuning iterations
    \item We have performed the experiment for different sizes of learning data
      that where cross-validated.
    \item Experiment was simulated with mlr's \enquote{classif.featureless} learner.
    \item Quiz: How to mathematically calculate the shape of the curves?
\end{itemize}

\framebreak

\begin{blocki}{Another perspective}
  \item Tune the same "fake" parameter $\lambda$ with CV
  \item Run the CV for 10, 100 and 500 different values of $\lambda$
  \item The "best" $\lambda$ error is getting smaller with increasing iterations
  \item We would choose the lambda value that yields a mmce = 30\%
  \item Nonsense as our model could net get better than mmce = 50\%
  \item The result is based on pure randomness
  \item The chance of getting such a random "outlier-split" increases with the CV iterations
  \item We \textbf{overtune} the model
\end{blocki}

<<fig.heigh = 5>>=

  set.seed(1342)

  makeMyTask = function(n) {
    y = sample(c(0,1), size = n, replace = TRUE)
    d = data.frame(x = 1, y = as.factor(y))
    makeClassifTask(data = d, target = "y")
  }

  task.size = 100
  task = makeMyTask(task.size)
  cv.iters = 2
  # "tuning" featureless learner by resampling it very often
  lrn = makeLearner("classif.featureless", method = "sample-prior")
  rin = makeResampleInstance("CV", iters = cv.iters, task = task)
  tune.list = c(10, 100, 500)

  # storage
  storage.list = list()
  for (i in 1:length(tune.list)) {
    storage.list[[i]] = replicate(tune.list[[i]], resample(lrn, task, rin, show.info = FALSE)$aggr)
  }
  boxplot(storage.list[[1]], storage.list[[2]], storage.list[[3]],
    names =  tune.list, xlab = "CV iterations", ylab = "mmce", col = "#E69F00AA")
  abline(h = min(storage.list[[1]]), lty = 3)
  abline(h = min(storage.list[[2]]), lty = 3)
  abline(h = min(storage.list[[3]]), lty = 3)

@


\begin{blocki}{Simple solution}
\item Again, simply simulate what happens in model application.
\item All parts of the model building (including model selection, preprocessing) should be embedded
  in the resampling, i.e., repeated for every pair of training/test data.
\item For steps that themselves require resampling (e.g. hyperparameter tuning) this results
  in two nested resampling loops, i.e. a resampling strategy for both tuning and outer evaluation.
\item Simplest form is a 3-way split into a training, optimization and test set.
  Inducers are trained on the training set, evaluated on the optimization set.
  After the final model is selected, we fit on joint training+optimization set and evaluate
  a final time on the test set. Note that we touch the test set only once, and have no way of \enquote{cheating}.
\end{blocki}

  \framebreak

Example: Outer loop with 3-fold CV and inner loop with 4-fold CV

\begin{center}
  \includegraphics[width=9.5cm]{figure_man/Nested_Resampling.png}
\end{center}

\framebreak

\begin{itemize}
  \item Imagine we want to tune a model with $i = 1, ..., 7$ possible hyperparameter settings $\theta_i \in \Theta$
  \item We use 4-fold CV in the inner resampling and 3-fold CV in the outer and proceed like this:
  \begin{itemize}
    \item Test each $\theta_i$ on the 4 inner samples
    \item Calculate the corresponding preliminary $\hat GE(\theta_i)$ for all settings
    \item Choose the winner $\theta^*$ for each of the 3 inner samples
    \item If we would take this $\hat GE(\theta^*)$ as overall generalization error we would yield an optimistic bias (Why?)
    \item We receive 3 winners from the inner samples
    \item For each of them, we calculate the "real" $GE(\theta^{**})$ and choose the best overall hyperparameter setting $\theta^{**}$
  \end{itemize}
  \item The error estimates on the outer samples (lightgreen) are unbiased because this data was strictly excluded from the validation process
\end{itemize}
\end{vbframe}

% \begin{vbframe}{Nested resampling with mlr}

% <<size="footnotesize">>=
% ## Tuning in inner resampling loop
% ps = makeParamSet(
%   makeDiscreteParam("C", values = 2^(-2:2)),
%   makeDiscreteParam("sigma", values = 2^(-2:2))
% )
% ctrl = makeTuneControlGrid()
% inner = makeResampleDesc("Subsample", iters = 2L)
% lrn = makeTuneWrapper("classif.ksvm", resampling = inner,
%   par.set = ps, control = ctrl, show.info = FALSE)
% ## Outer resampling loop
% outer = makeResampleDesc("CV", iters = 3L)
% @

%   \framebreak

% <<size="footnotesize">>=
% r = resample(lrn, iris.task, resampling = outer,
%   extract = getTuneResult, show.info = FALSE); print(r)

% print(r$extract[[1L]])
% @
% \end{vbframe}


\endlecture
