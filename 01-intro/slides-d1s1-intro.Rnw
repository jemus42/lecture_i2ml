% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@
% Load all R packages and set up knitr
<<setup-r, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{1}{Introduction}
\lecture{Introduction to Machine Learning}




<<include=FALSE>>=
set.seed(19042011)
runifCirc <- function(n, radius = 1, d = 2)
  t(sapply(seq_len(n), function(i) HI::rballunif(d, radius)))
@
\sloppy

\begin{frame}{Data Science and Machine Learning}

\begin{columns}
  \column{.3\textwidth}
    \begin{figure}
      \includegraphics[width=\textwidth]{figure_man/gears.png}
    \end{figure}
  \column{0.6\textwidth}
  \begingroup
  \centering
    \fontsize{20pt}{22pt}\selectfont
    \vspace{1cm}
    \\
 % Machine Learning is a method of teaching computers to make predictions based on some data.
 %Machine learning is the study of algorithms that can automatically learn from and make predictions on data.
  Machine Learning is a method of teaching computers to make predictions based on some data.
  \endgroup
\end{columns}

\end{frame}

\begin{frame}{Data Science and Machine Learning}

\scriptsize

\begin{center}\includegraphics[width=0.95\textwidth]{figure_man/learning} \end{center}

\normalsize 

\end{frame}

\begin{frame}{Machine Learning as Black-Box Modeling}

\begin{itemize}

\item
  Many concepts in ML can be explained without referring to the inner
  workings of a certain algorithm or model, especially things like model
  evaluation and tuning.
\item
  ML also nowadays consists of dozens (or hundreds?) of different
  modelling techniques, where it is quite unclear which of these are
  really needed (outside of pure research) and which are really best.
\item
  Understanding model-agnostic techniques is really paramount and can be
  achieved in a limited amount of time.
\end{itemize}

\end{frame}

\begin{frame}{ML as Black-Box Modeling}

\begin{itemize}

\item
  Really studying the inner workings of each and every ML model can take
  years. Do we even need to do this at all for some models?
\item
  No: They exist in software. We can simply try them out, in the best
  case with an intelligent program that iterates over them and optimizes
  them.
\item
  Yes: At least some basic knowledge helps to make right choices.
  Actually knowing what you are doing is always good, also outside of
  science. And often stuff goes wrong, then understanding helps, too.
\end{itemize}

\end{frame}

\begin{frame}{ML as Black-Box Modeling}

\begin{itemize}
\item
  In the follwoing slides we will go through really fundamental terms
  and concepts in ML, that are relevant for everything that comes next.
\item
  We will also learn a couple of extremely simple models to obtain a
  basic understanding.
\item
  More complex stuff comes later.
\end{itemize}

Imagine you want to investigate how salary and workplace conditions
affect productivity of employees. Therefore, you collect data about
worked minutes per week (productivity), how many people work in the 
same office as the employees in question and the employees' salary.

\end{frame}


\begin{frame}{Data, Target and Input Features}

\scriptsize

\begin{center}\includegraphics[width=0.8\textwidth]{figure_man/data_table} \end{center}

\normalsize 

\vspace{-0.5cm}

The whole data set is expressed by \[
\D = \Dset
\] with the \(i\)-th observation \(\xyi\) $\in \mathcal{X}x\mathcal{Y}$.

$\mathcal{X}$ is called input and $\mathcal{Y}$ output or target space.

\end{frame}


\begin{frame}{Target and Features Relationship}

\begin{itemize}

\item
  For our observed data we know which outcome is produced:
\end{itemize}

\vspace{-0.5cm}

\scriptsize

\begin{center}\includegraphics[width=0.9\textwidth]{figure_man/new_data0_web} \end{center}

\normalsize 

\end{frame}

\begin{frame}{Target and Features Relationship}

\begin{itemize}

\item
  For new employees we can just observe the features:
\end{itemize}

\vspace{-0.5cm}

\scriptsize

\begin{center}\includegraphics[width=0.9\textwidth]{figure_man/new_data1_web} \end{center}

\normalsize 

\vspace{-0.5cm}

\(\Rightarrow\) The goal is to predict the target variable for
\textbf{unseen new data} by using a \textbf{model} trained on the
already seen \textbf{train data}.

\end{frame}

\begin{frame}{Supervised Learning Task}

\begin{itemize}
\item \textbf{Regression}: Given an input $x$, predict corresponding output from $\mathcal{Y} \in \mathbb{R}^m, 1 \leq m < \infty$.
\item \textbf{Classification}: Assigning an input $x$ to one class of a finite set of classes $\mathcal{Y} = \{C_1,...,C_m\}, 2 \leq m < \infty$.
\item \textbf{Density estimation}: Given an input $x$, predict the probability distribution $p(y|x)$ on $\mathcal{Y}$.


%\item 
  %\textbf{Regression task} if we have to predict a numeric target variable, e.g., the minutes an employee works per week.
%\item
  %\textbf{Classification task} if we have to predict a categorical
 % target state, e.g., if an employee is happy with her job or not.
 
\end{itemize}

\end{frame}

\begin{frame}{Regression Task}

\begin{itemize}
\item
  \textbf{Goal}: Predict a continuous output
\item
  \(y\) is a metric variable (with values in \(\R\))
\item
  Regression model can be constructed by different methods, e.g., linear
  regression, trees or splines
\end{itemize}

<<echo=FALSE, fig.height=4>>=
library(party)
library(ggplot2)

set.seed(1)
f = function(x) 0.5 * x^2 + x + sin(x)
x = runif(40, min = -3, max = 3)
y = f(x) + rnorm(40)
df = data.frame(x = x, y = y)
ggplot(df, aes(x, y)) + geom_point(size = 3) + stat_function(fun = f, color = "#FF9999", size = 2)
@

\end{frame}


\begin{frame}{Target and Features Relationship}

\scriptsize

\begin{center}\includegraphics[width=\textwidth]{figure_man/what_is_a_model_web} \end{center}

\normalsize 

\end{frame}

\begin{frame}{Target and Features Relationship}

\begin{itemize}
\item
  In ML, we want to be \enquote{lazy}. We do not want to specify \(f\)
  manually.
\item
  We want to learn it \textbf{automatically from labeled data}.
\item
  Later we will see that we do have to specify something, like \(f\)'s
  functional form and other stuff.
\item
  Mathematically, we face a problem of function approximation: search
  for an \(f\), such that, for all points in the training data and also
  all newly observed points
\end{itemize}

\begin{center}
  \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1cm,
      thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}]
    \node[punkt] (natur) {$y \approx f(x)$};
    \node[left=of natur] (x) {x};
    \node[right=of natur] (y) {y};
    \path[every node/.style={font=\sffamily\small}]
    (natur) edge node {} (y)
    (x) edge node  {} (natur)
    ;
  \end{tikzpicture}
\end{center}

\begin{itemize}

\item
  We call this \textbf{supervised learning}.
\end{itemize}

\end{frame}

\begin{frame}{What is a Model?}

%A model takes the features of new observations and produces a prediction
%\(\hat{y}\) of our target variable \(y\):

A model (or hypothesis) $f : \mathcal{X} \rightarrow \mathcal{Y}$ maps inputs (or input features) to outputs (or targets).

A hypothesis class $\mathcal{H}$ is a set of such functions.

\scriptsize

\begin{center}\includegraphics[width=0.9\textwidth]{figure_man/the_model_web} \end{center}

\normalsize 

\end{frame}

\begin{frame}{What is an Inducer?}

The \textbf{inducer} (learner, algorithm) takes our labeled data set
(\textbf{training set}) and produces a model (which again is a
function):

Applying a learning algorithm means coming up with a hypothesis given sample data (formally, it maps from $\{((x^{(1)},y^{(1)}),...,(x^{(n)},y^{(n)}))|1 \leq i \leq n < \infty ,x^{(i)} \in \mathcal{X},y^{(i)}, \in \mathcal{Y}\} to \mathcal{H}$).

\vspace{-0.5cm}

\scriptsize

\begin{center}\includegraphics[width=0.7\textwidth]{figure_man/the_inducer_web} \end{center}

\normalsize 

\end{frame}

\begin{frame}{How to Evaluate Models}

\begin{itemize}

\item
  Simply compare predictions from model with truth:
\end{itemize}

\scriptsize

\begin{center}\includegraphics[width=0.8\textwidth]{figure_man/eval_inducer1_web} \end{center}

\normalsize 
\end{frame}

\begin{frame}{Inducer Decomposition}

Nearly all ML supervised learning training algorithms can be described
by three components:

\begin{center}
\textbf{Learning = Representation + Evaluation + Optimization}
\end{center}

\begin{itemize}
\item
  \textbf{Representation / Hypothesis Space:} Defines functional
  structures of \(f\) we can learn.
\item
  \textbf{Evaluation:} How well does a certain hypothesis score on a
  given data set? Allows us to choose better candidates over worse ones.
\item
  \textbf{Optimization:} How do we search the hypothesis space? Guided
  by the evaluation metric.
\item
  All of these components represent important choices in ML which can
  have drastic effects:
  \newline
  If we make smart choices here, we can tailor our inducer to our needs
  - but that usually requires quite a lot of experience and deeper
  insights into ML.
\end{itemize}

\end{frame}

\begin{frame}{Inducer Decomposition}

\begin{table}[]
\begin{tabular}{lllll}
 \textbf{Representation} & \textbf{Evaluation} &  \textbf{Optimization}&  &  \\
Instances / Neighbours & Squared error & Gradient descent &  \\
Linear functions & Likelihood & Stochastic gradient descent  &  \\
Decision trees & Information gain & Quadratic programming & \\
Set of rules & K-L divergence & Greedy optimization & \\
Neural networks & & Combinatorial optimization & \\
Graphical models & & \\
\end{tabular}
\end{table}

Note: What is on the same line above does not belong together!

\end{frame}

\begin{frame}{Machine Learning is changing our world}
\begin{itemize}
   \item Search engines learn what you want
   \item Recommender systems learn your taste in books, music, movies,...
   \item Algorithms do automatic stock trading
   \item Elections are won by understanding voters
   \item Google Translate learns how to translate text
   \item Siri learns to understand speech
   \item DeepMind beats humans at Go
   \item Cars drive themselves
   \item Medicines are developed faster
   \item Smartwatches monitor your health
   \item Data-driven discoveries are made in Physics, Biology, Genetics, Astronomy, Chemistry, Neurology,...
\end{itemize}
\end{frame}

\begin{vbframe}{Quotes}
\begin{itemize}
  \item New York Times (August 2009): \emph{\enquote{I keep saying that the
    sexy job in the next 10 years will be statisticians,} said Hal
    Varian, chief economist at Google. \enquote{And I'm not kidding.}}\\[0.1cm]
    http://www.nytimes.com/2009/08/06/technology/06stats.html
  \item \emph{\enquote{We can say with complete confidence that in the coming century, high-dimensional data analysis will be a very significant activity, and completely new methods of high-dimensional data analysis will be developed; \ldots}}\\[0.1cm]
    David Donoho in his lecture 'Math Challenges of the 21st Century' to the American Mathematical Society (2000)
\end{itemize}

\end{vbframe}

\begin{vbframe}{Motivation}
\lz
\centering
\includegraphics[width=0.98\textwidth]{figure_man/food.png}
\end{vbframe}

\begin{vbframe}{Machine Learning Workflow}
\vspace{1cm}
\centering
\includegraphics[width=1\textwidth]{figure_man/ml-workflow.png}
\end{vbframe}


\begin{vbframe}{Supervised Learning}
\lz
\begin{itemize}
  \item One tries to learn the relationship between \enquote{input} $x$ and \enquote{output} $y$.
  \item For learning, there is training data with labels available
 \item Mathematically, we face a problem of function approximation: search for an $f$, such that,
  for all points in the training data, and also all newly observed points,
$$ y \approx f(x). $$
\end{itemize}

\framebreak

\textbf{Regression Task}
\lz
\begin{columns}[T]
  \begin{column}{0.5\textwidth}
    \structure{Goal}: Predict a continuous output
    \begin{itemize}
      \item $y$ is metric variable (with values in $\R$)
      \item Regression model can be constructed by different methods, e.g. trees or splines
    \end{itemize}
  \end{column}
  \begin{column}{0.5\textwidth}
<<regression-task-plot, fig.height=4>>=
set.seed(1)
f = function(x) 0.5 * x^2 + x + sin(x)
x = runif(40, min = -3, max = 3)
y = f(x) + rnorm(40)
df = data.frame(x = x, y = y)
ggplot(df, aes(x, y)) + geom_point(size = 3) + stat_function(fun = f, color = "#FF9999", size = 2)
@
\lz
<<regression-task-plot-tree, fig.height=4>>=
tree = ctree(y ~ ., data = df,
  controls = ctree_control(maxdepth = 3))
plot(tree)
@
  \end{column}
\end{columns}

\framebreak

\textbf{Binary Classification Task}
\lz
\begin{columns}[T]
  \begin{column}{0.5\textwidth}
    \structure{Goal}: Predict a class (or membership probabilities)
    \begin{itemize}
      \item $y$ is a categorical variable (with two different unordered discrete values)
    \end{itemize}
  \end{column}
  \begin{column}{0.5\textwidth}
<<classification-task-plot, fig.height=6, fig.width=6>>=
set.seed(1)
df2 = data.frame(x1 = c(rnorm(10, mean = 3), rnorm(10, mean = 5)), x2 = runif(10), class = rep(c("a", "b"), each = 10))
ggplot(df2, aes(x = x1, y = x2, shape = class, color = class)) + geom_point(size = 3) + geom_vline(xintercept = 4, linetype = "longdash")
@
  \end{column}
\end{columns}

\framebreak

\textbf{Multiclass Classification Task}
\lz
\begin{columns}[T]
  \begin{column}{0.6\textwidth}
    \structure{Goal}: Predict a class (or membership probabilities)
    \begin{itemize}
      \item $y$ is a categorical variable with more than two different unordered discrete values
      \item Each instance belongs only to one class!
    \end{itemize}
  \end{column}
  \begin{column}{0.4\textwidth}
<<multi-classification-task-plot, fig.height=4, fig.width=6>>=
plotLearnerPrediction(makeLearner("classif.svm"), iris.task, c("Petal.Length", "Petal.Width")) +
  ggtitle("")
@
\lz
<<multi-classification-task-plot-tree, fig.height=4, fig.width=6>>=
tree = ctree(Species ~ ., data = iris,
  controls = ctree_control(maxdepth = 3))
plot(tree)
@
  \end{column}
\end{columns}


\framebreak
\lz
\textbf{Other supervised learning tasks}
\begin{itemize}
  \item Multilabel classification
  \item Forecasting
  \item Survival prediction
  \item Cost-sensitive classification
\end{itemize}
\end{vbframe}


% \textbf{Multilabel Classification Task}
% \lz
% \begin{columns}[T]
%   \begin{column}{0.5\textwidth}
%     \structure{Goal}: Predict multiple classes (or membership probabilities) for a single observation
%     \begin{itemize}
%       \item $Y$ is a matrix containing one dummy coded vector for each categorical label
%       \item Example of $Y$:
%     \end{itemize}
%   \end{column}
%   \begin{column}{0.5\textwidth}
% <<multilabel-task-plot, echo=FALSE,fig.height=6>>=
% set.seed(1)
% df3 = data.frame(x = c(rnorm(10, mean = 2, sd = 1), rnorm(10, mean = 4, sd = 1), rnorm(10, mean = 6, sd = 1)),
%   y = runif(10), class = c(rep("a", 20), rep("b", 10)), fill = c(rep("a", 10), rep("b", 20)))
% ggplot(df3, aes(x = x, y = y)) + geom_point(aes(colour = class),size = 6) + geom_point(aes(colour = fill), size = 3) +
%   geom_vline(xintercept = 3, linetype = "longdash") + geom_vline(xintercept = 5, linetype = "longdash") +
%   theme(legend.position = "none")
% @
%   \end{column}
% \end{columns}
% \begin{table}
%   \begin{tabular}{r|r|r|l}
%   $label\ 1$ & $label\ 2$ & $label\ 3$ & label set \\
%   \hline
%   $1$ & $0$ & 0 & $\{label\ 1\}$ \\
%   $0$ & $1$ & 1 & $\{label\ 2, label\ 3\}$ \\
%   $1$ & $1$ & 1 & $\{label\ 1, label\ 2, label\ 3\}$ \\
%   $1$ & $0$ & 1 & $\{label\ 1, label\ 3\}$ \\
%   \hline
%   \end{tabular}
% \end{table}
%
% \framebreak
%
% \textbf{Cost-sensitive Classification Task}
% \lz
% \begin{columns}[T]
%   \begin{column}{0.5\textwidth}
%     \structure{Goal}: Minimize cost of predicting the correct class label $y$
%     \begin{itemize}
%       \item A more general setting of regular classification
%       \item Costs caused by different kinds of errors are not assumed to be equal
%     \end{itemize}
%   \end{column}
%   \begin{column}{0.5\textwidth}
% <<costsens-task-plot, fig.height=5.5>>=
% data(GermanCredit, package = "caret")
% credit.task = makeClassifTask(data = GermanCredit, target = "Class")
% credit.task = removeConstantFeatures(credit.task)
% costs = matrix(c(0, 1, 5, 0), 2)
% colnames(costs) = rownames(costs) = getTaskClassLevels(credit.task)
% lrn = makeLearner("classif.multinom", predict.type = "prob", trace = FALSE)
% mod = train(lrn, credit.task)
% pred = predict(mod, task = credit.task)
% th = costs[2,1]/(costs[2,1] + costs[1,2])
% pred.th = setThreshold(pred, th)
% credit.costs = makeCostMeasure(id = "credit.costs", name = "Credit costs", costs = costs,
%   best = 0, worst = 5)
% rin = makeResampleInstance("CV", iters = 3, task = credit.task)
% lrn = makeLearner("classif.multinom", predict.type = "prob", predict.threshold = th, trace = FALSE)
% r = resample(lrn, credit.task, resampling = rin, measures = list(credit.costs, mmce), show.info = FALSE)
% d = generateThreshVsPerfData(r, measures = list(credit.costs, mmce))
% plotThreshVsPerf(d, mark.th = th, facet.wrap.ncol = 1L)
% @
%   \end{column}
% \end{columns}
% \textbf{Example}
% \begin{itemize}
%   \item \textbf{Loan Applications}\\
%   \begin{itemize}
%     \item rejecting an applicant who will not pay back $\rightarrow$ minimal costs
%     \item accepting an applicant who will pay back $\rightarrow$ gain
%     \item accepting an applicant who will not pay back $\rightarrow$ big loss
%     \item rejecting an applicant who would pay back $\rightarrow$ loss
%   \end{itemize}
% \end{itemize}
%
% \framebreak
%
% \textbf{Forecasting Task}
% \lz
% \begin{columns}[T]
%   \begin{column}{0.5\textwidth}
%     \structure{Goal}: Predict future values of a time series
%     \begin{itemize}
%       \item $y$ is metric variable depending on time $t$
%       \item $t$ could be e.g. hours, days, years, \ldots
%     \end{itemize}
%   \end{column}
%   \begin{column}{0.5\textwidth}
% <<forecasting-task-plot, fig.height=8>>=
% data(AirPassengers)
% data = AirPassengers
% model = auto.arima(data, start.P = 2, start.Q = 2, max.D = 2, max.P = 5, max.Q = 5)
% plot(forecast(model, h = 10, level = 95), main = "")
% @
%
%   \end{column}
% \end{columns}
%
% \framebreak
%
% % \begin{itemize}
% % \item \Sexpr{nrow(spambase)} Emails were classified as \enquote{Spam} or \enquote{No Spam}.
% % \item Inputs: including percentages of \Sexpr{ncol(spambase)-1} frequent words and symbols within the email
% % \end{itemize}
% % <<echo=FALSE>>=
% % round(colMeans(spambase[,c(paste("capital_run_length", c("total", "longest", "average"), sep = "_"),
% %                     paste("word_freq", c("you", "your", "george", "hp", "will", "all"), sep = "_"),
% %                     "char_freq_exclamation")]), digits = 2)
% % @
% % \newpage
%
% \textbf{Survival Task}
% \lz
% \begin{columns}[T]
%   \begin{column}{0.5\textwidth}
%     \structure{Goal}: Predict a survival function $\hat{S}(t)$, i.e.\ the probability to survive to time point $t$
%     \begin{itemize}
%       \item $y$ is survival function $S(t)$ of time $t$
%       \item Values for survival $S(t)$ are between $0$ and $1$
%     \end{itemize}
%   \end{column}
%   \begin{column}{0.5\textwidth}
% <<survial-task-plot, fig.height=8>>=
% set.seed(1)
% data("rats", package = "survival")
% sf = survfit(Surv(time, status) ~ rx, data = rats)
% survMisc:::autoplot.survfit(sf, title = "", xLab = "Time", yLab = "S(t)",
%   yScale = "frac", survLineSize = 1.5)$plot
% @
%   \end{column}
% \end{columns}
% \end{vbframe}
%



%
% \textbf{Examples}
%
% \lz
%
% \begin{itemize}
%  \item Handwritten digit recognotion
%  \item Lung cancer prediction
%  \item Email spam recognition
%  \item Recommender system (movies, books, etc.)
%  \item Word recognition from spoken language
%  \item $\ldots$
% \end{itemize}
%
% \framebreak
%
% \begin{itemize}
%  \item Classical approach for metric $y$ is to consider \textbf{linear functions} $f$. \\
%   $\to$ \textbf{Linear Regression}
%  \item Many events can't be modelled appropriately by linear functions \\
%   $\to$ In the last decades, many \textbf{\enquote{flexible} regression} models were developed. Most models do not provide a close solution for the parameters. Iterative estimation methods (\enquote{learning algorithms}) are used.
%  \item \textbf{Local Methods:} The input space gets divided, in each subspace a simple modell is estimated.
%   $\to$ (Smoothing) Splines, trees, \ldots
%  \item \textbf{Kernel Methods:} The non-linear problem is projected into a high dimensional space in which it is linearly solvable.
%   $\to$ Support Vector Machines
%  \item \textbf{Additive Models:} summation of several simple non-linear models.
%   $\to$ Neural Networks, Generalised Additive Models, Projection Pursuit, \ldots
% \end{itemize}

% Jann's summary slide for all other learning tasks
\begin{vbframe}{Additional Learning Tasks}
\lz
  \textbf{Unsupervised learning}
    \begin{itemize}
      \item Data without labels $y$
      \item Search for patterns within the inputs $x$
      \item The algorithm has no specific goal, is therefore \textit{unsupervised}
      \item Possible applications:
      \begin{itemize}
        \item Dimensionality reduction (PCA, Autoencoders ...)
        \item Clustering
        \item Outlier detection
        \item Association rules
      \end{itemize}
    \end{itemize}
\framebreak
\lz
  \textbf{Semi-Supervised learning}
  \begin{itemize}
    \item Massive amount of expensive labeled data to train reliable model
    \item Learn from (expensive) labeled \textbf{and} (cheap) unlabeled data
    \item Unlabeled data in conjunction with a small amount of labeled data improves learning accuracy
  \end{itemize}
  \vspace{0.5cm}
  \textbf{Reinforcement learning}
  \begin{itemize}
    \item Select actions in subsequent  states within a certain environment to maximize lagged future reward
    \item Example: train neural net to play mario kart (environment)
    \begin{itemize}
      \item Accelerate/ steer/ break (actions) at each time point (states) during playing
      \item Reward: ranking after finish, should be maximized
    \end{itemize}
  \end{itemize}
\end{vbframe}



% \framebreak
%
% \textbf{Clustering Task}
% \lz
% \begin{columns}[T]
%   \begin{column}{0.5\textwidth}
%     \structure{Goal}: Group data into similar clusters (or estimate fuzzy membership probabilities)\\
%     \lz
% <<cluster-task-plot2, fig.height=8>>=
% df4 = getTaskData(iris.task)
% idx = sample(1:dim(df4)[1], 40)
% irisSample = df4[idx,]
% irisSample$Species = NULL
% hc = hclust(dist(irisSample), method = "ave")
% plot(hc, hang = -1, labels = df4$Species[idx], sub = "", xlab = "")
% @
%   \end{column}
%   \begin{column}{0.5\textwidth}
% <<cluster-task-plot1, fig.height=8>>=
% # df = iris
% m = as.matrix(cbind(df4$Petal.Length, df4$Petal.Width), ncol = 2)
% cl = (kmeans(m,3))
% df4$cluster = factor(cl$cluster)
% centers = as.data.frame(cl$centers)
% ggplot(data = df4, aes(x = Petal.Length, y = Petal.Width, color = cluster )) +
%  geom_point(size = 4) +
%  geom_point(data = centers, aes(x = V1, y = V2, color = 'Center')) +
%  geom_point(data = centers, aes(x = V1,y = V2, color = 'Center'), size = 90, alpha = .3) +
%  theme(legend.position = "none")
% @
%   \end{column}
% \end{columns}
%
% \framebreak
%
% \textbf{Dimensionality reduction Task}
% \lz
% \begin{columns}[T]
%   \begin{column}{0.5\textwidth}
%     \structure{Goal}: describe data in fewer features
%     \begin{blocki}{Common methods:}
%       \item Principle Component Analysis (PCA)
%       \item Linear Discriminant Analysis (LDA)
%       \item Filter Methods
%     \end{blocki}
%   \end{column}
%   \begin{column}{0.5\textwidth}
% <<dim-red-task-plot-pca, fig.height=9>>=
% pca = prcomp(BBmisc::dropNamed(iris, "Species"), scale. = TRUE)
% ggbiplot(pca, obs.scale = 1, var.scale = 1,
%   groups = iris$Species, ellipse = TRUE, circle = TRUE) +
%   scale_color_discrete(name = '') +
%   theme(legend.direction = 'horizontal', legend.position = 'top')
% @
%   \end{column}
% \end{columns}
%
% \framebreak
%
% \textbf{Outlier detection Task}
% \lz
% \begin{columns}[T]
%   \begin{column}{0.5\textwidth}
%     \structure{Goal}: identify observations which do not conform to an expected pattern
%     \begin{itemize}
%       \item outlier detection is also referred to anomaly detection and one class classification
%       \item Several methods exist based on:
%       \begin{itemize}
%         \item density or correlation
%         \item cluster analysis
%         \item neural networks
%         \item ensemble techniques
%       \end{itemize}
%     \end{itemize}
%   \end{column}
%   \begin{column}{0.5\textwidth}
% <<doutlier-task-plot, fig.height=9>>=
% # Inject outliers into data.
% cars1 = cars[1:30, ]  # original data
% cars.outliers = data.frame(speed = c(19, 19, 20, 20, 20), dist = c(190, 186, 210, 220, 218))  # introduce outliers.
% cars2 = rbind(cars1, cars.outliers)  # data with outliers.
%
% # Plot of data with outliers.
% par(mfrow = c(2, 1))
% plot(cars2$speed, cars2$dist, xlim = c(0, 28), ylim = c(0, 230), main = "With Outliers",
%   xlab = "speed", ylab = "dist", pch = "*", col = "red", cex = 2)
% abline(lm(dist ~ speed, data = cars2), col = "blue", lwd = 3, lty = 2)
%
% # Plot of original data without outliers. Note the change in slope (angle) of best fit line.
% plot(cars1$speed, cars1$dist, xlim = c(0, 28), ylim = c(0, 230), main = "Outliers removed \n A much better fit!",
%   xlab = "speed", ylab = "dist", pch = "*", col = "red", cex = 2)
% abline(lm(dist ~ speed, data = cars1), col = "blue", lwd = 3, lty = 2)
% @
%   \end{column}
% \end{columns}
%
% \framebreak
%
% \textbf{Association rules Task}
% \lz
% \begin{columns}[T]
%   \begin{column}{0.5\textwidth}
%     \structure{Goal}: Discover relations between features
%     \begin{itemize}
%       \item Rule-based machine learning method
%       \item For two features $x_1$ and $x_2$, a \textbf{rule} is defined as an implication of the form $x_1 \Rightarrow x_2 $
%       \item \textbf{Support} is an indication of how frequently the feature appears in the data
%       \item \textbf{Confidence} is an indication of how often the rule has been found to be true
%     \end{itemize}
%   \end{column}
%   \begin{column}{0.5\textwidth}
% <<association-task-plot, fig.height=9>>=
% titanic.raw = read.csv("titanic.raw")
% rules = apriori(titanic.raw,
%   parameter = list(minlen = 2, supp = 0.005, conf = 0.8),
%   appearance = list(rhs = c("Survived=No", "Survived=Yes"),
%   default = "lhs"),
%   control = list(verbose = FALSE))
% plot(rules, method="graph", control=list(type="items"))
% @
%   \end{column}
% \end{columns}
% \end{vbframe}

% \begin{vbframe}{Semi-Supervised Learning}
% \lz
% \begin{itemize}
%   \item Learning a reliable model usually requires plenty of labeled data
%   \item Labeled data: can be expensive
%   \item Unlabeled data: abundant and free/cheap
%   \item General idea: learning from both labeled and unlabeled data
%   \item There exist few labeled training data $\Dtrain^L=\{ (x_1, y_1), \ldots, (x_l,  y_l)\}$ and many unlabeled observations $\Dtrain^U=\{ (x_{l+1}, y_{l+1}), \ldots, (x_{l+u},  y_{l+u})\}$ for training (usually $u\gg l$).
%  \item Semi-Supervised learning falls between supervised (completely labeled training data) and unsupervised (without any labeled training data) learning
%  \item Unlabeled data in conjunction with a small amount of labeled data improves learning accuracy.
%  % \begin{itemize}
%  %   \item \textbf{Semi-supervised regression/classification:} uses unlabeled data to get a better model than with the labeled data alone.
%  %   \item \textbf{Semi-supervised clustering:} uses labeled data must-/cannot-links.
%    % \item \textbf{inductive learning:} infer mapping from \enquote{input} $x$ to \enquote{output} $y$.
%    % \item \textbf{Active learning:} special case in which a learning algorithm is able to interactively query the user to obtain the desired outputs at new data points.
%  %  \end{itemize}
% \end{itemize}
%
% \framebreak
%
% \textbf{Semi-Supervised classification task}
% \lz
% \begin{columns}[T]
%   \begin{column}{0.5\textwidth}
%     \structure{Goal}: Learning a classifier $f$ better than using labeled data alone.
%     \begin{itemize}
%       \item Assumption: Examples from the same class follow a coherent distribution
%       \item Unlabeled data can give a better sense of the class separation boundary
%     \end{itemize}
%   \end{column}
%   \begin{column}{0.5\textwidth}
%   \centering
%     \includegraphics[width=.5\textwidth]{figure_man/semi1.png}\\
%     \includegraphics[width=.5\textwidth]{figure_man/semi3.png}\\
%     \lz
%     \includegraphics[width=.5\textwidth]{figure_man/semi7.png}
%   \end{column}
% \end{columns}
%
% \framebreak
%
% \textbf{Semi-Supervised Clustering task}
% \lz
% \begin{columns}[T]
%   \begin{column}{0.5\textwidth}
%     \structure{Goal}: Use labeled data to group unlabeled data.
%       \begin{itemize}
%         \item Having labeled observations means to have information about if two observations have to be in the same or different clusters
%         \item This can be expressed by constraints among observations:\\
%         \textit{must links} and \textit{cannot links}
%         \item Look only for models which maintain these constraints
%       \end{itemize}
%   \end{column}
%   \begin{column}{0.5\textwidth}
%     \centering
%       \includegraphics[width=1\textwidth]{figure_man/semi-clustering.png}
%   \end{column}
% \end{columns}
%
% \framebreak
%
% \textbf{Active Learning task}
% \lz
% \begin{columns}[T]
%   \begin{column}{0.5\textwidth}
%     \structure{Goal}: Label extra-data in a smart way to improve model most efficiently
%       \begin{itemize}
%         \item Extra labels can be requested, but expensive
%         \item We iterate:
%         \begin{itemize}
%           \item learn on labeled data
%           \item request labels for some unlabeled instances
%         \end{itemize}
%         \item Only most useful observations for learning need to be labeled
%       \end{itemize}
%   \end{column}
%   \begin{column}{0.25\textwidth}
%     1.\\
%     \includegraphics[width=.8\textwidth]{figure_man/active1.png}\\
%     \lz
%     3.\\
%     \includegraphics[width=.8\textwidth]{figure_man/active3.png}\\
%     \lz
%     5.\\
%     \includegraphics[width=.8\textwidth]{figure_man/active5.png}
%   \end{column}
%     \begin{column}{0.25\textwidth}
%     2.\\
%     \includegraphics[width=.8\textwidth]{figure_man/active2.png}\\
%     \lz
%     4.\\
%     \includegraphics[width=.8\textwidth]{figure_man/active4.png}\\
%     \lz
%     6.\\
%     \includegraphics[width=.8\textwidth]{figure_man/active6.png}\\
%   \end{column}
% \end{columns}
% \end{vbframe}
%
%
% \begin{vbframe}{Reinforcement Learning}
% \lz
% \begin{itemize}
%   \item Inputs are observations and feedback (rewards or punishments) from interacting with an environment
%   \item Output: achieve some goal
%   \item Goal: Select actions to maximize future reward $\triangleq$ return
% \end{itemize}
%
% \framebreak
%
% % \textbf{The RL Setting}
% % \lz
% % \begin{columns}[T]
% %   \begin{column}{0.5\textwidth}
% %   RL is a \textbf{general-purpose framework} for AI\\
% %   \lz
% %   At each time step $t$ an \textbf{agent} interacts with an \textbf{environment} $\mathcal{E}$ and
% % 		\begin{itemize}
% % 			\item observes \textbf{state} $s_t \in \R^d$
% % 			\item receives \textbf{reward} $r_t \in \R$
% % 			\item executes \textbf{action} $a_t \in A$
% % 		\end{itemize}
% % 		Reward signals may be sparse, noisy and delayed.\\
% % 		\lz
% % 		$\rightarrow$ \textbf{Agent-environment-loop}
% %   \end{column}
% %   \begin{column}{0.5\textwidth}
% %   \centering
% %     \includegraphics[width=1\textwidth]{figure_man/state_action_reward_diagram.png}
% %   \end{column}
% % \end{columns}
% %
% % \end{vbframe}


% \begin{vbframe}{Topics of the lecture}
% \begin{itemize}
%  \item Neuronal Networks
%  \item Decision Trees
%  \item Support Vector Machines
%  \item Ensemble Methods (Bagging, Boosting)
%  \item Benchmarking and modelselection
%  \item Parameter tuning / Algorithm configuration
%  \item Machine Learning in R
%  \item Parallel calculation in R
%  \item \ldots
% \end{itemize}
% \end{vbframe}
%
% \begin{vbframe}{Material}
% \begin{itemize}
%  \item PDF Files of all slides can be downloaded from Moodle.
%  \item There is no script available.
%  \item On the Moodle page, there will be references to literature and extra material.
%    This is supposed to be read.
% \end{itemize}
% \end{vbframe}

% \begin{vbframe}{Machine Learning - What's in a name?}
% \begin{itemize}
%   \item Many names: data mining, data science, machine learning, statistical learning,...
%   \item Subtle differences in scope, partly marketing
%   \item We'll mostly use the term 'machine learning'
%   \item Has deep roots in statistics, neurology, biology, psychology,... but has developed into a new field of study.
%   \item How is it different from statistics?
%   \begin{itemize}
%     \item Breiman. Statistical Modeling: The Two Cultures. Statistical Science, 2001
%   \end{itemize}
% \end{itemize}
% \end{vbframe}

\begin{vbframe}{To date or not to date?}
% first slide
 \begin{table}
 \small
    \begin{tabular}{cccccc}
      \hline
      Nr & Day of Week & Type of Date & Weather & TV Tonight & Date?\\
      \hline
      1 & Weekday & Dinner & Warm & Bad & Yes \\
      2 & Weekend & Club & Warm & Good & No \\
      3 & Weekend & Club & Warm & Bad & No \\
      4 & Weekend & Club & Cold & Bad & Yes \\
      Now & Weekend & Club & Cold & Good & ? \\
      \hline
    \end{tabular}
  \end{table}

 \textbf{Some terminology}
 \begin{itemize}
    \item Rows: \textit{Instances, Examples} (labelled/unlabeled)
    \item Columns: \textit{Factors, Features, Attributes}
    \item Last column: Target feature
    \item First column: Identifier (Never give this to the learner!)
    \item Other columns: Predictive features
  \end{itemize}
\end{vbframe}

\begin{vbframe}{To date or not to date?}

% second slide
  \begin{table}
  \small
    \begin{tabular}{cccccc}
      \hline
      Nr & Day of Week & Type of Date & Weather & TV Tonight & Date?\\
      \hline
      1 & Weekday & Dinner & Warm & Bad & Yes \\
      2 & Weekend & Club & Warm & Good & No \\
      3 & Weekend & Club & Warm & Bad & No \\
      4 & Weekend & Club & Cold & Bad & Yes \\
      Now & Weekend & Club & Cold & Good & ? \\
      \hline
    \end{tabular}
  \end{table}

 \textbf{Draw conclusions}
  \begin{itemize}
    \item Is there one factor that perfectly predicts the answer?
    \item What about a \textit{conjunction of factors}?
    \begin{itemize}
     \item Warm \& Weekend $\rightarrow$ No date today :(
     \item Dinner \& TV Bad $\rightarrow$ Date :)
     \item Club \& Warm $\rightarrow$ No date...
     \item Weekday \& TV Bad $\rightarrow$ Date...
    \end{itemize}
    \item There's no way to know!
  \end{itemize}
\end{vbframe}


\begin{vbframe}{How can we learn?}
\textbf{Hume's problem of induction (David Hume, 1748)} \\
\lz
\textit{How can we ever be justified in generalizing from what we've seen to what we haven't?} \\
\begin{itemize}
    \item You have no basis to pick one generalization over the other
    \item Big data (Casanova-approach) won't help: answer may depend on a factor you didn't consider
    \item What if the answer is just random?
\end{itemize}

\framebreak
\textit{What if we just assume that the future will be like the past?}
\lz
\begin{itemize}
  \item Risky assumption (e.g. inductivist turkey)
  \item Still the best we can do (and generally seems to work)
  \item Even then, this only helps if we have seen the exact same situation before
  \item The machine learning problem remains: How do we generalize from cases that we haven't seen before?
    \begin{itemize}
      \item What if somebody types a unique Google query?
      \item What if a patient comes in with slightly different symptoms?
      \item What if someone writes a new unique spam email?
    \end{itemize}
  \item Even with all the data in the world, your chances or finding the exact same case are almost zero. We need induction.
\end{itemize}
\end{vbframe}


\begin{vbframe}{Learning = Representation + Evaluation + Optimization}
All learners consist of three main components (all introducing bias):
\begin{itemize}
\item \textbf{Representation}: A model must be represented in a formal language that the computer can handle.
\begin{itemize}
\item Defines the concepts it can learn: \textit{The hypothesis space}
\end{itemize}
\item \textbf{Evaluation}: How to choose one hypothesis over the other?
\begin{itemize}
\item The evaluation function, objective function, scoring function
\item Can differ from the external evaluation function (e.g. accuracy)
\end{itemize}
\item \textbf{Optimization}: How do we search the hypothesis space?
\begin{itemize}
\item Key to the efficiency of the learner
\item Defines how many optima it finds
\item Often starts from most simple hypothesis, relaxing it if needed to explain the data
\end{itemize}
\end{itemize}
\end{vbframe}

\begin{vbframe}{A dating algorithm}
 \begin{table}
 \small
    \begin{tabular}{cccccc}
      \hline
      Nr & Day of Week & Type of Date & Weather & TV Tonight & Date?\\
      \hline
      1 & Weekday & Dinner & Warm & Bad & Yes \\
      2 & Weekend & Club & Warm & Good & No \\
      3 & Weekend & Club & Warm & Bad & No \\
      4 & Weekend & Club & Cold & Bad & Yes \\
      Now & Weekend & Club & Cold & Good & ? \\
      \hline
    \end{tabular}
  \end{table}

\begin{itemize}
\item Representation: conjunctions of factors (conjunctive concepts)
\item Optimization: start with best 1-factor concept, then add best other factor on \emph{remaining} data
\begin{itemize}
\item Don't try all combinations (combinatorial explosion)!
\end{itemize}
\item Evaluation: exclude most bad matches and fewest good ones
\item Result: Weekend $\wedge$ Warm ($\rightarrow$ No date)
\item Thoughts?
\end{itemize}
\end{vbframe}

% \begin{vbframe}{Learning sets of rules (Michalski)}

% \begin{itemize}
% \item Real concepts are disjunctive $\rightarrow$ \emph{sets} of rules
% \begin{itemize}
% \item Credit card used in 3 different continents yesterday $\rightarrow$ stolen
% \item Credit card used twice after 23:00 on weekday $\rightarrow$ stolen
% \item Credit card used to buy 1 dollar of gas $\rightarrow$ stolen
% \end{itemize}
% \item Divide and conquer approach: build rule covering as many positive examples as possible, discard all positive examples that it covers, repeat until all are covered
% \item Sets of rules can represent \emph{any} concept. How?
% \begin{itemize}
% \item Just turn each positive instance into a rule using all factors
% \item Weekend $\wedge$ Club $\wedge$ Warm $\wedge$ Bad $\rightarrow$ Yes
% \end{itemize}
% \item 100\% accurate rule? Or an illusion?
% \begin{itemize}
% \item Every new (unseen) example will be negative
% \item No free lunch: you can't learn without assuming anything
% \end{itemize}
% \end{itemize}
% \end{vbframe}


\begin{vbframe}{Different `tribes' of Machine Learning}

\small
Rival schools of thought in machine learning, each with core beliefs, and with distinct strategy to learn \textit{anything}:
  \begin{itemize}
    \item \textbf{Symbolic Learning}: Express, manipulate symbolic knowledge
      \begin{itemize} \item Rules and trees \end{itemize}
    \item \textbf{Neural Networks} (Connectionism): Mimick the Human brain
      \begin{itemize} \item Neural Nets \end{itemize}
    \item \textbf{Evolution}: Simulate the evolutionary process
      \begin{itemize} \item Genetic algorithms \end{itemize}
    \item \textbf{Probabilistic (Bayesian) Inference}: Reduce uncertainties by incorporating new evidence
      \begin{itemize} \item Graphical models, Gaussian processes \end{itemize}
    \item \textbf{Learning by Analogy}: Recognize geometric similarities between points
      \begin{itemize} \item kNN, Support Vector Machines \end{itemize}
\end{itemize}
\normalsize
\end{vbframe}



\endlecture
