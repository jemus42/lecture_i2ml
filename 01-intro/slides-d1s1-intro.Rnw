% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@
% Load all R packages and set up knitr
<<setup-r, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{1}{Introduction}
\lecture{Introduction to Machine Learning}




<<include=FALSE>>=
set.seed(19042011)
runifCirc <- function(n, radius = 1, d = 2)
  t(sapply(seq_len(n), function(i) HI::rballunif(d, radius)))
@
\sloppy

\begin{vbframe}
\centering
\includegraphics[width=0.98\textwidth]{figure_man/datascience.jpg}

\framebreak

\ \\
\vspace{1.5cm}
\centering
\includegraphics[width=1\textwidth]{figure_man/learning2.png}

\framebreak
\ \\
\vspace{1cm}
\centering
\includegraphics[width=1\textwidth]{figure_man/learning.pdf}
\end{vbframe}

\begin{frame}{Machine Learning is changing our world}
\begin{itemize}
   \item Search engines learn what you want
   \item Recommender systems learn your taste in books, music, movies,...
   \item Algorithms do automatic stock trading
   \item Elections are won by understanding voters
   \item Google Translate learns how to translate text
   \item Siri learns to understand speech
   \item DeepMind beats humans at Go
   \item Cars drive themselves
   \item Medicines are developed faster
   \item Smartwatches monitor your health
   \item Data-driven discoveries are made in Physics, Biology, Genetics, Astronomy, Chemistry, Neurology,...
\end{itemize}
\end{frame}

\begin{vbframe}{Quotes}
\begin{itemize}
  \item New York Times (August 2009): \emph{\enquote{I keep saying that the
    sexy job in the next 10 years will be statisticians,} said Hal
    Varian, chief economist at Google. \enquote{And I'm not kidding.}}\\[0.1cm]
    http://www.nytimes.com/2009/08/06/technology/06stats.html
  \item \emph{\enquote{We can say with complete confidence that in the coming century, high-dimensional data analysis will be a very significant activity, and completely new methods of high-dimensional data analysis will be developed; \ldots}}\\[0.1cm]
    David Donoho in his lecture 'Math Challenges of the 21st Century' to the American Mathematical Society (2000)
\end{itemize}

\framebreak

\begin{itemize}
  \item Barack Obama at Wired (October 2016):
  \begin{itemize}
    \item \emph{\enquote{There's a distinction [\ldots ] between generalized AI and specialized AI.
    \item In science fiction, what you hear about is generalized AI, right? Computers start getting smarter than we are [\ldots ].
    \item My impression, based on talking to my top science advisers, is that we're still a reasonably long way away from that.
    \item Specialized AI [\ldots ] is about using algorithms and computers to figure out increasingly complex tasks.
    \item We've been seeing specialized AI in every aspect of our lives, from medicine and transportation to how electricity is distributed, [\ldots ]}}\\[0.1cm]
  \end{itemize}
  https://www.wired.com/2016/10/president-obama-mit-joi-ito-interview/
\end{itemize}
\end{vbframe}

\begin{vbframe}{Motivation}
\lz
\centering
\includegraphics[width=0.98\textwidth]{figure_man/food.png}
\end{vbframe}

\begin{vbframe}{Machine Learning Workflow}
\vspace{1cm}
\centering
\includegraphics[width=1\textwidth]{figure_man/ml-workflow.png}
\end{vbframe}


\begin{vbframe}{Supervised Learning}
\lz
\begin{itemize}
  \item One tries to learn the relationship between \enquote{input} $x$ and \enquote{output} $y$.
  \item For learning, there is training data with labels available
 \item Mathematically, we face a problem of function approximation: search for an $f$, such that,
  for all points in the training data, and also all newly observed points,
$$ y \approx f(x). $$
\end{itemize}

\framebreak

\textbf{Regression Task}
\lz
\begin{columns}[T]
  \begin{column}{0.5\textwidth}
    \structure{Goal}: Predict a continuous output
    \begin{itemize}
      \item $y$ is metric variable (with values in $\R$)
      \item Regression model can be constructed by different methods, e.g. trees or splines
    \end{itemize}
  \end{column}
  \begin{column}{0.5\textwidth}
<<regression-task-plot, fig.height=4>>=
set.seed(1)
f = function(x) 0.5 * x^2 + x + sin(x)
x = runif(40, min = -3, max = 3)
y = f(x) + rnorm(40)
df = data.frame(x = x, y = y)
ggplot(df, aes(x, y)) + geom_point(size = 3) + stat_function(fun = f, color = "#FF9999", size = 2)
@
\lz
<<regression-task-plot-tree, fig.height=4>>=
tree = ctree(y ~ ., data = df,
  controls = ctree_control(maxdepth = 3))
plot(tree)
@
  \end{column}
\end{columns}

\framebreak

\textbf{Regression example data set: Boston Housing}\\
\begin{columns}[T]
  \begin{column}{0.4\textwidth}
  \small
    \begin{itemize}
      \item The Boston Housing data set has 506 observations (from 1970) and 14 variables, with \code{medv} (median value of the owner-occupied homes) being the target variable
    \end{itemize}
<<boston-plot, fig.width=5, fig.height=4.5>>=
bh = getTaskData(bh.task)
ggplot(data = bh, aes(x = lstat, y = medv)) + geom_point(size = 1) + geom_smooth(size = 1)
@
  \end{column}
  \begin{column}{0.6\textwidth}
    \begin{table}
    \scriptsize
      \begin{tabular}{ll}
      \code{crim} & per capita crime rate by town\\
      \code{zn} & proportion of residential land zoned for lots\\
                & over 25,000 sq.ft.\\
      \code{indus} & proportion of non-retail business acres per town\\
      \code{chas} & Charles River dummy variable (= 1 if tract\\
                  & bounds river; 0 otherwise)\\
      \code{nox} & nitric oxides concentration (parts per 10 million)\\
      \code{rm} & average number of rooms per dwelling \\
      \code{age} & proportion of owner-occupied units built prior\\
                 & to 1940\\
      \code{dis} & weighted distances to five Boston employment\\
                 & centers\\
      \code{rad} & index of accessibility to radial highways\\
      \code{tax} & full-value property-tax rate per \$10,000\\
      \code{ptratio} & pupil-teacher ratio by town\\
      \code{b} & $1000(b_k - 0.63)^2$ where $b_k$ is the proportion\\
               & of blacks by town\\
      \code{lstat} & $\%$ lower status of the population \\
      \code{medv} & Median value of owner-occupied homes in\\
                  & \$1000's\\
    \end{tabular}
  \end{table}
  \end{column}
\end{columns}

\framebreak

\textbf{Binary Classification Task}
\lz
\begin{columns}[T]
  \begin{column}{0.5\textwidth}
    \structure{Goal}: Predict a class (or membership probabilities)
    \begin{itemize}
      \item $y$ is a categorical variable (with two different unordered discrete values)
    \end{itemize}
  \end{column}
  \begin{column}{0.5\textwidth}
<<classification-task-plot, fig.height=6, fig.width=6>>=
set.seed(1)
df2 = data.frame(x1 = c(rnorm(10, mean = 3), rnorm(10, mean = 5)), x2 = runif(10), class = rep(c("a", "b"), each = 10))
ggplot(df2, aes(x = x1, y = x2, shape = class, color = class)) + geom_point(size = 3) + geom_vline(xintercept = 4, linetype = "longdash")
@
  \end{column}
\end{columns}

\framebreak

\textbf{Binary classification example data set: Spam Detection}\\
\begin{columns}[T]
  \begin{column}{0.5\textwidth}
    \begin{itemize}
      \item The data set contains data of 4601 emails collected at the Hewlett-Packard-Laboratories, to train a personalized spam mail detector
      \item 2788 mails were wanted and 1813 were spam
      \item There are 57 numerical predictors available measuring for example the frequency of some words and special characters, as well as length of words in capital letters
    \end{itemize}
  \end{column}
  \begin{column}{0.5\textwidth}
<<spam-plot, fig.height=8, fig.width=6>>=
data("spambase", package = "nutshell")
tree = ctree(is_spam ~ ., data = spambase,
  controls = ctree_control(maxdepth = 2))
plot(tree)
@
  \end{column}
\end{columns}

\framebreak

\textbf{Multiclass Classification Task}
\lz
\begin{columns}[T]
  \begin{column}{0.6\textwidth}
    \structure{Goal}: Predict a class (or membership probabilities)
    \begin{itemize}
      \item $y$ is a categorical variable with more than two different unordered discrete values
      \item Each instance belongs only to one class!
    \end{itemize}
  \end{column}
  \begin{column}{0.4\textwidth}
<<multi-classification-task-plot, fig.height=4, fig.width=6>>=
plotLearnerPrediction(makeLearner("classif.svm"), iris.task, c("Petal.Length", "Petal.Width")) +
  ggtitle("")
@
\lz
<<multi-classification-task-plot-tree, fig.height=4, fig.width=6>>=
tree = ctree(Species ~ ., data = iris,
  controls = ctree_control(maxdepth = 3))
plot(tree)
@
  \end{column}
\end{columns}

\framebreak

\textbf{Multiclass classification example data set: Iris}\\
\begin{columns}[T]
  \begin{column}{0.4\textwidth}
    \begin{itemize}
      \item The iris data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris (setosa, versicolor, and virginica)
    \end{itemize}
  \end{column}
  \begin{column}{0.6\textwidth}
<<iris-plot, fig.height=12, fig.width=11>>=
iris = getTaskData(iris.task)
ggpairs(iris, aes(colour = Species))
@
  \end{column}
\end{columns}

\framebreak
\lz
\textbf{Other supervised learning tasks}
\begin{itemize}
  \item Multilabel classification
  \item Forecasting
  \item Survival prediction
  \item Cost-sensitive classification
\end{itemize}
\end{vbframe}


% \textbf{Multilabel Classification Task}
% \lz
% \begin{columns}[T]
%   \begin{column}{0.5\textwidth}
%     \structure{Goal}: Predict multiple classes (or membership probabilities) for a single observation
%     \begin{itemize}
%       \item $Y$ is a matrix containing one dummy coded vector for each categorical label
%       \item Example of $Y$:
%     \end{itemize}
%   \end{column}
%   \begin{column}{0.5\textwidth}
% <<multilabel-task-plot, echo=FALSE,fig.height=6>>=
% set.seed(1)
% df3 = data.frame(x = c(rnorm(10, mean = 2, sd = 1), rnorm(10, mean = 4, sd = 1), rnorm(10, mean = 6, sd = 1)),
%   y = runif(10), class = c(rep("a", 20), rep("b", 10)), fill = c(rep("a", 10), rep("b", 20)))
% ggplot(df3, aes(x = x, y = y)) + geom_point(aes(colour = class),size = 6) + geom_point(aes(colour = fill), size = 3) +
%   geom_vline(xintercept = 3, linetype = "longdash") + geom_vline(xintercept = 5, linetype = "longdash") +
%   theme(legend.position = "none")
% @
%   \end{column}
% \end{columns}
% \begin{table}
%   \begin{tabular}{r|r|r|l}
%   $label\ 1$ & $label\ 2$ & $label\ 3$ & label set \\
%   \hline
%   $1$ & $0$ & 0 & $\{label\ 1\}$ \\
%   $0$ & $1$ & 1 & $\{label\ 2, label\ 3\}$ \\
%   $1$ & $1$ & 1 & $\{label\ 1, label\ 2, label\ 3\}$ \\
%   $1$ & $0$ & 1 & $\{label\ 1, label\ 3\}$ \\
%   \hline
%   \end{tabular}
% \end{table}
%
% \framebreak
%
% \textbf{Cost-sensitive Classification Task}
% \lz
% \begin{columns}[T]
%   \begin{column}{0.5\textwidth}
%     \structure{Goal}: Minimize cost of predicting the correct class label $y$
%     \begin{itemize}
%       \item A more general setting of regular classification
%       \item Costs caused by different kinds of errors are not assumed to be equal
%     \end{itemize}
%   \end{column}
%   \begin{column}{0.5\textwidth}
% <<costsens-task-plot, fig.height=5.5>>=
% data(GermanCredit, package = "caret")
% credit.task = makeClassifTask(data = GermanCredit, target = "Class")
% credit.task = removeConstantFeatures(credit.task)
% costs = matrix(c(0, 1, 5, 0), 2)
% colnames(costs) = rownames(costs) = getTaskClassLevels(credit.task)
% lrn = makeLearner("classif.multinom", predict.type = "prob", trace = FALSE)
% mod = train(lrn, credit.task)
% pred = predict(mod, task = credit.task)
% th = costs[2,1]/(costs[2,1] + costs[1,2])
% pred.th = setThreshold(pred, th)
% credit.costs = makeCostMeasure(id = "credit.costs", name = "Credit costs", costs = costs,
%   best = 0, worst = 5)
% rin = makeResampleInstance("CV", iters = 3, task = credit.task)
% lrn = makeLearner("classif.multinom", predict.type = "prob", predict.threshold = th, trace = FALSE)
% r = resample(lrn, credit.task, resampling = rin, measures = list(credit.costs, mmce), show.info = FALSE)
% d = generateThreshVsPerfData(r, measures = list(credit.costs, mmce))
% plotThreshVsPerf(d, mark.th = th, facet.wrap.ncol = 1L)
% @
%   \end{column}
% \end{columns}
% \textbf{Example}
% \begin{itemize}
%   \item \textbf{Loan Applications}\\
%   \begin{itemize}
%     \item rejecting an applicant who will not pay back $\rightarrow$ minimal costs
%     \item accepting an applicant who will pay back $\rightarrow$ gain
%     \item accepting an applicant who will not pay back $\rightarrow$ big loss
%     \item rejecting an applicant who would pay back $\rightarrow$ loss
%   \end{itemize}
% \end{itemize}
%
% \framebreak
%
% \textbf{Forecasting Task}
% \lz
% \begin{columns}[T]
%   \begin{column}{0.5\textwidth}
%     \structure{Goal}: Predict future values of a time series
%     \begin{itemize}
%       \item $y$ is metric variable depending on time $t$
%       \item $t$ could be e.g. hours, days, years, \ldots
%     \end{itemize}
%   \end{column}
%   \begin{column}{0.5\textwidth}
% <<forecasting-task-plot, fig.height=8>>=
% data(AirPassengers)
% data = AirPassengers
% model = auto.arima(data, start.P = 2, start.Q = 2, max.D = 2, max.P = 5, max.Q = 5)
% plot(forecast(model, h = 10, level = 95), main = "")
% @
%
%   \end{column}
% \end{columns}
%
% \framebreak
%
% % \begin{itemize}
% % \item \Sexpr{nrow(spambase)} Emails were classified as \enquote{Spam} or \enquote{No Spam}.
% % \item Inputs: including percentages of \Sexpr{ncol(spambase)-1} frequent words and symbols within the email
% % \end{itemize}
% % <<echo=FALSE>>=
% % round(colMeans(spambase[,c(paste("capital_run_length", c("total", "longest", "average"), sep = "_"),
% %                     paste("word_freq", c("you", "your", "george", "hp", "will", "all"), sep = "_"),
% %                     "char_freq_exclamation")]), digits = 2)
% % @
% % \newpage
%
% \textbf{Survival Task}
% \lz
% \begin{columns}[T]
%   \begin{column}{0.5\textwidth}
%     \structure{Goal}: Predict a survival function $\hat{S}(t)$, i.e.\ the probability to survive to time point $t$
%     \begin{itemize}
%       \item $y$ is survival function $S(t)$ of time $t$
%       \item Values for survival $S(t)$ are between $0$ and $1$
%     \end{itemize}
%   \end{column}
%   \begin{column}{0.5\textwidth}
% <<survial-task-plot, fig.height=8>>=
% set.seed(1)
% data("rats", package = "survival")
% sf = survfit(Surv(time, status) ~ rx, data = rats)
% survMisc:::autoplot.survfit(sf, title = "", xLab = "Time", yLab = "S(t)",
%   yScale = "frac", survLineSize = 1.5)$plot
% @
%   \end{column}
% \end{columns}
% \end{vbframe}
%



%
% \textbf{Examples}
%
% \lz
%
% \begin{itemize}
%  \item Handwritten digit recognotion
%  \item Lung cancer prediction
%  \item Email spam recognition
%  \item Recommender system (movies, books, etc.)
%  \item Word recognition from spoken language
%  \item $\ldots$
% \end{itemize}
%
% \framebreak
%
% \begin{itemize}
%  \item Classical approach for metric $y$ is to consider \textbf{linear functions} $f$. \\
%   $\to$ \textbf{Linear Regression}
%  \item Many events can't be modelled appropriately by linear functions \\
%   $\to$ In the last decades, many \textbf{\enquote{flexible} regression} models were developed. Most models do not provide a close solution for the parameters. Iterative estimation methods (\enquote{learning algorithms}) are used.
%  \item \textbf{Local Methods:} The input space gets divided, in each subspace a simple modell is estimated.
%   $\to$ (Smoothing) Splines, trees, \ldots
%  \item \textbf{Kernel Methods:} The non-linear problem is projected into a high dimensional space in which it is linearly solvable.
%   $\to$ Support Vector Machines
%  \item \textbf{Additive Models:} summation of several simple non-linear models.
%   $\to$ Neural Networks, Generalised Additive Models, Projection Pursuit, \ldots
% \end{itemize}

% Jann's summary slide for all other learning tasks
\begin{vbframe}{Additional Learning Tasks}
\lz
  \textbf{Unsupervised learning}
    \begin{itemize}
      \item Data without labels $y$
      \item Search for patterns within the inputs $x$
      \item The algorithm has no specific goal, is therefore \textit{unsupervised}
      \item Possible applications:
      \begin{itemize}
        \item Dimensionality reduction (PCA, Autoencoders ...)
        \item Clustering
        \item Outlier detection
        \item Association rules
      \end{itemize}
    \end{itemize}
\framebreak
\lz
  \textbf{Semi-Supervised learning}
  \begin{itemize}
    \item Massive amount of expensive labeled data to train reliable model
    \item Learn from (expensive) labeled \textbf{and} (cheap) unlabeled data
    \item Unlabeled data in conjunction with a small amount of labeled data improves learning accuracy
  \end{itemize}
  \vspace{0.5cm}
  \textbf{Reinforcement learning}
  \begin{itemize}
    \item Select actions in subsequent  states within a certain environment to maximize lagged future reward
    \item Example: train neural net to play mario kart (environment)
    \begin{itemize}
      \item Accelerate/ steer/ break (actions) at each time point (states) during playing
      \item Reward: ranking after finish, should be maximized
    \end{itemize}
  \end{itemize}
\end{vbframe}



% \framebreak
%
% \textbf{Clustering Task}
% \lz
% \begin{columns}[T]
%   \begin{column}{0.5\textwidth}
%     \structure{Goal}: Group data into similar clusters (or estimate fuzzy membership probabilities)\\
%     \lz
% <<cluster-task-plot2, fig.height=8>>=
% df4 = getTaskData(iris.task)
% idx = sample(1:dim(df4)[1], 40)
% irisSample = df4[idx,]
% irisSample$Species = NULL
% hc = hclust(dist(irisSample), method = "ave")
% plot(hc, hang = -1, labels = df4$Species[idx], sub = "", xlab = "")
% @
%   \end{column}
%   \begin{column}{0.5\textwidth}
% <<cluster-task-plot1, fig.height=8>>=
% # df = iris
% m = as.matrix(cbind(df4$Petal.Length, df4$Petal.Width), ncol = 2)
% cl = (kmeans(m,3))
% df4$cluster = factor(cl$cluster)
% centers = as.data.frame(cl$centers)
% ggplot(data = df4, aes(x = Petal.Length, y = Petal.Width, color = cluster )) +
%  geom_point(size = 4) +
%  geom_point(data = centers, aes(x = V1, y = V2, color = 'Center')) +
%  geom_point(data = centers, aes(x = V1,y = V2, color = 'Center'), size = 90, alpha = .3) +
%  theme(legend.position = "none")
% @
%   \end{column}
% \end{columns}
%
% \framebreak
%
% \textbf{Dimensionality reduction Task}
% \lz
% \begin{columns}[T]
%   \begin{column}{0.5\textwidth}
%     \structure{Goal}: describe data in fewer features
%     \begin{blocki}{Common methods:}
%       \item Principle Component Analysis (PCA)
%       \item Linear Discriminant Analysis (LDA)
%       \item Filter Methods
%     \end{blocki}
%   \end{column}
%   \begin{column}{0.5\textwidth}
% <<dim-red-task-plot-pca, fig.height=9>>=
% pca = prcomp(BBmisc::dropNamed(iris, "Species"), scale. = TRUE)
% ggbiplot(pca, obs.scale = 1, var.scale = 1,
%   groups = iris$Species, ellipse = TRUE, circle = TRUE) +
%   scale_color_discrete(name = '') +
%   theme(legend.direction = 'horizontal', legend.position = 'top')
% @
%   \end{column}
% \end{columns}
%
% \framebreak
%
% \textbf{Outlier detection Task}
% \lz
% \begin{columns}[T]
%   \begin{column}{0.5\textwidth}
%     \structure{Goal}: identify observations which do not conform to an expected pattern
%     \begin{itemize}
%       \item outlier detection is also referred to anomaly detection and one class classification
%       \item Several methods exist based on:
%       \begin{itemize}
%         \item density or correlation
%         \item cluster analysis
%         \item neural networks
%         \item ensemble techniques
%       \end{itemize}
%     \end{itemize}
%   \end{column}
%   \begin{column}{0.5\textwidth}
% <<doutlier-task-plot, fig.height=9>>=
% # Inject outliers into data.
% cars1 = cars[1:30, ]  # original data
% cars.outliers = data.frame(speed = c(19, 19, 20, 20, 20), dist = c(190, 186, 210, 220, 218))  # introduce outliers.
% cars2 = rbind(cars1, cars.outliers)  # data with outliers.
%
% # Plot of data with outliers.
% par(mfrow = c(2, 1))
% plot(cars2$speed, cars2$dist, xlim = c(0, 28), ylim = c(0, 230), main = "With Outliers",
%   xlab = "speed", ylab = "dist", pch = "*", col = "red", cex = 2)
% abline(lm(dist ~ speed, data = cars2), col = "blue", lwd = 3, lty = 2)
%
% # Plot of original data without outliers. Note the change in slope (angle) of best fit line.
% plot(cars1$speed, cars1$dist, xlim = c(0, 28), ylim = c(0, 230), main = "Outliers removed \n A much better fit!",
%   xlab = "speed", ylab = "dist", pch = "*", col = "red", cex = 2)
% abline(lm(dist ~ speed, data = cars1), col = "blue", lwd = 3, lty = 2)
% @
%   \end{column}
% \end{columns}
%
% \framebreak
%
% \textbf{Association rules Task}
% \lz
% \begin{columns}[T]
%   \begin{column}{0.5\textwidth}
%     \structure{Goal}: Discover relations between features
%     \begin{itemize}
%       \item Rule-based machine learning method
%       \item For two features $x_1$ and $x_2$, a \textbf{rule} is defined as an implication of the form $x_1 \Rightarrow x_2 $
%       \item \textbf{Support} is an indication of how frequently the feature appears in the data
%       \item \textbf{Confidence} is an indication of how often the rule has been found to be true
%     \end{itemize}
%   \end{column}
%   \begin{column}{0.5\textwidth}
% <<association-task-plot, fig.height=9>>=
% titanic.raw = read.csv("titanic.raw")
% rules = apriori(titanic.raw,
%   parameter = list(minlen = 2, supp = 0.005, conf = 0.8),
%   appearance = list(rhs = c("Survived=No", "Survived=Yes"),
%   default = "lhs"),
%   control = list(verbose = FALSE))
% plot(rules, method="graph", control=list(type="items"))
% @
%   \end{column}
% \end{columns}
% \end{vbframe}

% \begin{vbframe}{Semi-Supervised Learning}
% \lz
% \begin{itemize}
%   \item Learning a reliable model usually requires plenty of labeled data
%   \item Labeled data: can be expensive
%   \item Unlabeled data: abundant and free/cheap
%   \item General idea: learning from both labeled and unlabeled data
%   \item There exist few labeled training data $\Dtrain^L=\{ (x_1, y_1), \ldots, (x_l,  y_l)\}$ and many unlabeled observations $\Dtrain^U=\{ (x_{l+1}, y_{l+1}), \ldots, (x_{l+u},  y_{l+u})\}$ for training (usually $u\gg l$).
%  \item Semi-Supervised learning falls between supervised (completely labeled training data) and unsupervised (without any labeled training data) learning
%  \item Unlabeled data in conjunction with a small amount of labeled data improves learning accuracy.
%  % \begin{itemize}
%  %   \item \textbf{Semi-supervised regression/classification:} uses unlabeled data to get a better model than with the labeled data alone.
%  %   \item \textbf{Semi-supervised clustering:} uses labeled data must-/cannot-links.
%    % \item \textbf{inductive learning:} infer mapping from \enquote{input} $x$ to \enquote{output} $y$.
%    % \item \textbf{Active learning:} special case in which a learning algorithm is able to interactively query the user to obtain the desired outputs at new data points.
%  %  \end{itemize}
% \end{itemize}
%
% \framebreak
%
% \textbf{Semi-Supervised classification task}
% \lz
% \begin{columns}[T]
%   \begin{column}{0.5\textwidth}
%     \structure{Goal}: Learning a classifier $f$ better than using labeled data alone.
%     \begin{itemize}
%       \item Assumption: Examples from the same class follow a coherent distribution
%       \item Unlabeled data can give a better sense of the class separation boundary
%     \end{itemize}
%   \end{column}
%   \begin{column}{0.5\textwidth}
%   \centering
%     \includegraphics[width=.5\textwidth]{figure_man/semi1.png}\\
%     \includegraphics[width=.5\textwidth]{figure_man/semi3.png}\\
%     \lz
%     \includegraphics[width=.5\textwidth]{figure_man/semi7.png}
%   \end{column}
% \end{columns}
%
% \framebreak
%
% \textbf{Semi-Supervised Clustering task}
% \lz
% \begin{columns}[T]
%   \begin{column}{0.5\textwidth}
%     \structure{Goal}: Use labeled data to group unlabeled data.
%       \begin{itemize}
%         \item Having labeled observations means to have information about if two observations have to be in the same or different clusters
%         \item This can be expressed by constraints among observations:\\
%         \textit{must links} and \textit{cannot links}
%         \item Look only for models which maintain these constraints
%       \end{itemize}
%   \end{column}
%   \begin{column}{0.5\textwidth}
%     \centering
%       \includegraphics[width=1\textwidth]{figure_man/semi-clustering.png}
%   \end{column}
% \end{columns}
%
% \framebreak
%
% \textbf{Active Learning task}
% \lz
% \begin{columns}[T]
%   \begin{column}{0.5\textwidth}
%     \structure{Goal}: Label extra-data in a smart way to improve model most efficiently
%       \begin{itemize}
%         \item Extra labels can be requested, but expensive
%         \item We iterate:
%         \begin{itemize}
%           \item learn on labeled data
%           \item request labels for some unlabeled instances
%         \end{itemize}
%         \item Only most useful observations for learning need to be labeled
%       \end{itemize}
%   \end{column}
%   \begin{column}{0.25\textwidth}
%     1.\\
%     \includegraphics[width=.8\textwidth]{figure_man/active1.png}\\
%     \lz
%     3.\\
%     \includegraphics[width=.8\textwidth]{figure_man/active3.png}\\
%     \lz
%     5.\\
%     \includegraphics[width=.8\textwidth]{figure_man/active5.png}
%   \end{column}
%     \begin{column}{0.25\textwidth}
%     2.\\
%     \includegraphics[width=.8\textwidth]{figure_man/active2.png}\\
%     \lz
%     4.\\
%     \includegraphics[width=.8\textwidth]{figure_man/active4.png}\\
%     \lz
%     6.\\
%     \includegraphics[width=.8\textwidth]{figure_man/active6.png}\\
%   \end{column}
% \end{columns}
% \end{vbframe}
%
%
% \begin{vbframe}{Reinforcement Learning}
% \lz
% \begin{itemize}
%   \item Inputs are observations and feedback (rewards or punishments) from interacting with an environment
%   \item Output: achieve some goal
%   \item Goal: Select actions to maximize future reward $\triangleq$ return
% \end{itemize}
%
% \framebreak
%
% % \textbf{The RL Setting}
% % \lz
% % \begin{columns}[T]
% %   \begin{column}{0.5\textwidth}
% %   RL is a \textbf{general-purpose framework} for AI\\
% %   \lz
% %   At each time step $t$ an \textbf{agent} interacts with an \textbf{environment} $\mathcal{E}$ and
% % 		\begin{itemize}
% % 			\item observes \textbf{state} $s_t \in \R^d$
% % 			\item receives \textbf{reward} $r_t \in \R$
% % 			\item executes \textbf{action} $a_t \in A$
% % 		\end{itemize}
% % 		Reward signals may be sparse, noisy and delayed.\\
% % 		\lz
% % 		$\rightarrow$ \textbf{Agent-environment-loop}
% %   \end{column}
% %   \begin{column}{0.5\textwidth}
% %   \centering
% %     \includegraphics[width=1\textwidth]{figure_man/state_action_reward_diagram.png}
% %   \end{column}
% % \end{columns}
% %
% % \end{vbframe}


% \begin{vbframe}{Topics of the lecture}
% \begin{itemize}
%  \item Neuronal Networks
%  \item Decision Trees
%  \item Support Vector Machines
%  \item Ensemble Methods (Bagging, Boosting)
%  \item Benchmarking and modelselection
%  \item Parameter tuning / Algorithm configuration
%  \item Machine Learning in R
%  \item Parallel calculation in R
%  \item \ldots
% \end{itemize}
% \end{vbframe}
%
% \begin{vbframe}{Material}
% \begin{itemize}
%  \item PDF Files of all slides can be downloaded from Moodle.
%  \item There is no script available.
%  \item On the Moodle page, there will be references to literature and extra material.
%    This is supposed to be read.
% \end{itemize}
% \end{vbframe}

% \begin{vbframe}{Machine Learning - What's in a name?}
% \begin{itemize}
%   \item Many names: data mining, data science, machine learning, statistical learning,...
%   \item Subtle differences in scope, partly marketing
%   \item We'll mostly use the term 'machine learning'
%   \item Has deep roots in statistics, neurology, biology, psychology,... but has developed into a new field of study.
%   \item How is it different from statistics?
%   \begin{itemize}
%     \item Breiman. Statistical Modeling: The Two Cultures. Statistical Science, 2001
%   \end{itemize}
% \end{itemize}
% \end{vbframe}

\begin{vbframe}{To date or not to date?}
% first slide
 \begin{table}
 \small
    \begin{tabular}{cccccc}
      \hline
      Nr & Day of Week & Type of Date & Weather & TV Tonight & Date?\\
      \hline
      1 & Weekday & Dinner & Warm & Bad & Yes \\
      2 & Weekend & Club & Warm & Good & No \\
      3 & Weekend & Club & Warm & Bad & No \\
      4 & Weekend & Club & Cold & Bad & Yes \\
      Now & Weekend & Club & Cold & Good & ? \\
      \hline
    \end{tabular}
  \end{table}

 \textbf{Some terminology}
 \begin{itemize}
    \item Rows: \textit{Instances, Examples} (labelled/unlabeled)
    \item Columns: \textit{Factors, Features, Attributes}
    \item Last column: Target feature
    \item First column: Identifier (Never give this to the learner!)
    \item Other columns: Predictive features
  \end{itemize}
\end{vbframe}

\begin{vbframe}{To date or not to date?}

% second slide
  \begin{table}
  \small
    \begin{tabular}{cccccc}
      \hline
      Nr & Day of Week & Type of Date & Weather & TV Tonight & Date?\\
      \hline
      1 & Weekday & Dinner & Warm & Bad & Yes \\
      2 & Weekend & Club & Warm & Good & No \\
      3 & Weekend & Club & Warm & Bad & No \\
      4 & Weekend & Club & Cold & Bad & Yes \\
      Now & Weekend & Club & Cold & Good & ? \\
      \hline
    \end{tabular}
  \end{table}

 \textbf{Draw conclusions}
  \begin{itemize}
    \item Is there one factor that perfectly predicts the answer?
    \item What about a \textit{conjunction of factors}?
    \begin{itemize}
     \item Warm \& Weekend $\rightarrow$ No date today :(
     \item Dinner \& TV Bad $\rightarrow$ Date :)
     \item Club \& Warm $\rightarrow$ No date...
     \item Weekday \& TV Bad $\rightarrow$ Date...
    \end{itemize}
    \item There's no way to know!
  \end{itemize}
\end{vbframe}


\begin{vbframe}{How can we learn?}
\textbf{Hume's problem of induction (David Hume, 1748)} \\
\lz
\textit{How can we ever be justified in generalizing from what we've seen to what we haven't?} \\
\begin{itemize}
    \item You have no basis to pick one generalization over the other
    \item Big data (Casanova-approach) won't help: answer may depend on a factor you didn't consider
    \item What if the answer is just random?
\end{itemize}

\framebreak
\textit{What if we just assume that the future will be like the past?}
\lz
\begin{itemize}
  \item Risky assumption (e.g. inductivist turkey)
  \item Still the best we can do (and generally seems to work)
  \item Even then, this only helps if we have seen the exact same situation before
  \item The machine learning problem remains: How do we generalize from cases that we haven't seen before?
    \begin{itemize}
      \item What if somebody types a unique Google query?
      \item What if a patient comes in with slightly different symptoms?
      \item What if someone writes a new unique spam email?
    \end{itemize}
  \item Even with all the data in the world, your chances or finding the exact same case are almost zero. We need induction.
\end{itemize}
\end{vbframe}

\begin{vbframe}{No Free Lunch Theorem (David Wolpert, 1997)}
\textit{If all functions $f(x) = y$  are equally likely, all algorithms that aim to optimize that function have identical performance.} \\
\lz
\begin{itemize}
\item Sets a limit on how good a learner can be: no learner can be better than random guessing!
\item But then why is the world full of highly successful learners?
\item For every world where a learner does better than random guessing, we can construct an anti-world by flipping the labels of all \emph{unseen} instances: it performs worse by the same amount
\item We \emph{don't care} about all possible worlds, only the one we live in
\item We assume we know something about this world that gives us an advantage. That knowledge is fallible, but it's a risk we'll have to take
\end{itemize}
\end{vbframe}

\begin{vbframe}{The futility of bias-free learning}
Practical consequence: \textit{There's no such thing as learning without knowledge (assumptions).} Data alone is not enough.
\lz
\begin{itemize}
\item We need to provide \emph{prior knowledge} to the algorithm, or make assumptions when constructing hypotheses
  \begin{itemize}
  \item The structure of a neural net, a Bayesian prior, background knowledge as rules, the way a tree represents knowledge,...
  \end{itemize}
\lz
\item These assumptions are called a learner's \textit{bias}, i.e. \emph{bias-free learning} is impossible.
  \begin{itemize}
  \item Every new piece of knowledge is the basis for more knowledge.
  \item What assumptions can we start from that are not too strong?
  \end{itemize}
\end{itemize}
\framebreak
\begin{itemize}
\item Newton's Principle of induction: \textit{Whatever is true of everything we've seen, is true for everything in the universe}
\lz
\item We induce the most widely applicable rules we can, and reduce scope only when the data forces us to.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Learning = Representation + Evaluation + Optimization}
All learners consist of three main components (all introducing bias):
\begin{itemize}
\item \textbf{Representation}: A model must be represented in a formal language that the computer can handle.
\begin{itemize}
\item Defines the concepts it can learn: \textit{The hypothesis space}
\end{itemize}
\item \textbf{Evaluation}: How to choose one hypothesis over the other?
\begin{itemize}
\item The evaluation function, objective function, scoring function
\item Can differ from the external evaluation function (e.g. accuracy)
\end{itemize}
\item \textbf{Optimization}: How do we search the hypothesis space?
\begin{itemize}
\item Key to the efficiency of the learner
\item Defines how many optima it finds
\item Often starts from most simple hypothesis, relaxing it if needed to explain the data
\end{itemize}
\end{itemize}
\end{vbframe}

\begin{vbframe}{A dating algorithm}
 \begin{table}
 \small
    \begin{tabular}{cccccc}
      \hline
      Nr & Day of Week & Type of Date & Weather & TV Tonight & Date?\\
      \hline
      1 & Weekday & Dinner & Warm & Bad & Yes \\
      2 & Weekend & Club & Warm & Good & No \\
      3 & Weekend & Club & Warm & Bad & No \\
      4 & Weekend & Club & Cold & Bad & Yes \\
      Now & Weekend & Club & Cold & Good & ? \\
      \hline
    \end{tabular}
  \end{table}

\begin{itemize}
\item Representation: conjunctions of factors (conjunctive concepts)
\item Optimization: start with best 1-factor concept, then add best other factor on \emph{remaining} data
\begin{itemize}
\item Don't try all combinations (combinatorial explosion)!
\end{itemize}
\item Evaluation: exclude most bad matches and fewest good ones
\item Result: Weekend $\wedge$ Warm ($\rightarrow$ No date)
\item Thoughts?
\end{itemize}
\end{vbframe}

% \begin{vbframe}{Learning sets of rules (Michalski)}

% \begin{itemize}
% \item Real concepts are disjunctive $\rightarrow$ \emph{sets} of rules
% \begin{itemize}
% \item Credit card used in 3 different continents yesterday $\rightarrow$ stolen
% \item Credit card used twice after 23:00 on weekday $\rightarrow$ stolen
% \item Credit card used to buy 1 dollar of gas $\rightarrow$ stolen
% \end{itemize}
% \item Divide and conquer approach: build rule covering as many positive examples as possible, discard all positive examples that it covers, repeat until all are covered
% \item Sets of rules can represent \emph{any} concept. How?
% \begin{itemize}
% \item Just turn each positive instance into a rule using all factors
% \item Weekend $\wedge$ Club $\wedge$ Warm $\wedge$ Bad $\rightarrow$ Yes
% \end{itemize}
% \item 100\% accurate rule? Or an illusion?
% \begin{itemize}
% \item Every new (unseen) example will be negative
% \item No free lunch: you can't learn without assuming anything
% \end{itemize}
% \end{itemize}
% \end{vbframe}


\begin{vbframe}{Different `tribes' of Machine Learning}

\small
Rival schools of thought in machine learning, each with core beliefs, and with distinct strategy to learn \textit{anything}:
  \begin{itemize}
    \item \textbf{Symbolic Learning}: Express, manipulate symbolic knowledge
      \begin{itemize} \item Rules and trees \end{itemize}
    \item \textbf{Neural Networks} (Connectionism): Mimick the Human brain
      \begin{itemize} \item Neural Nets \end{itemize}
    \item \textbf{Evolution}: Simulate the evolutionary process
      \begin{itemize} \item Genetic algorithms \end{itemize}
    \item \textbf{Probabilistic (Bayesian) Inference}: Reduce uncertainties by incorporating new evidence
      \begin{itemize} \item Graphical models, Gaussian processes \end{itemize}
    \item \textbf{Learning by Analogy}: Recognize geometric similarities between points
      \begin{itemize} \item kNN, Support Vector Machines \end{itemize}
\end{itemize}
\normalsize
\end{vbframe}



\endlecture
