% Introduction to Machine Learning
% Day 1

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@
% Load all R packages and set up knitr
<<setup-r, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{Loss minimization}
\lecture{Introduction to Machine Learning}


\begin{vbframe}{Why do we care about Losses?}
  \begin{itemize}
    \item Assume we trained a model to predict flat rent based on some features
    (size, location, age, ...).
    \item The real rent of a new flat, that the model never saw before, is
    EUR 1600, our model predicts EUR 1300.
    \item How do we measure the performance of our model?
    \item We calculate the prediction error and therefore need a suitable error
    measure, aka a loss function such as:
    \begin{itemize}
      \item Absolute loss: $L(y = 1600, \hat y = 1300) = |1600 - 1300| = 300$
      \item Squared loss: $L(y = 1600, \hat y = 1300) = (1600 - 1300)^2 = 90000$\\
      (puts more emphasis on predictions that are far off the mark)
    \end{itemize}
    \item The choice of the loss has a major influence on the final model.
  \end{itemize}
\end{vbframe}


\begin{vbframe}{Loss minimization}
\begin{itemize}
    \item The \enquote{goodness} of a prediction $\fx$ is measured by a \textbf{loss function} $\Lxy$.
    \item The ability of a model $f$ to reproduce the association between $x$ and $y$ that is
    present in the data $\D$ can be measured by the average loss: the \textbf{empirical risk}
    $$
  \riske(f) = \frac{1}{n} \sumin \Lxyi.
  $$
   \item Learning then amounts to \textbf{empirical risk minimization} -- figuring out which model $f$ has the smallest average loss:
$$
\fh = \argmin_{f \in \Hspace} \riske(f).
$$
\end{itemize}
\framebreak

Since the model $f$ is usually controlled by \textbf{parameters} $\theta$ in a parameter space $\Theta$, this becomes:

\begin{eqnarray*}
\riske(\theta) & = & \frac{1}{n} \sumin \Lxyit \cr
\hat{\theta} & = & \argmin_{\theta \in \Theta} \riske(\theta)
\end{eqnarray*}

Most learners in ML try to solve the above \emph{optimization problem}, which
implies a tight connection between ML and optimization.

\framebreak

\begin{itemize}
\item For regression tasks, the loss usually only depends on residual $\Lxy = L(y - \fx) = L(\eps)$.
\item Since learning can be re-phrased as minimizing the loss, the choice of loss strongly affects the computational difficulty of learning:
\begin{itemize}
    \item smoothness of $\riske(\theta)$ in $\theta$
    \item can gradient-based methods be applied to minimize $\riske(\theta)$?
    \item uni- or multimodality $\riske(\theta)$ over $\Theta$. \\
\end{itemize}
\item The choice of loss implies which kinds of errors are important or not -- need \emph{domain knowledge}!
\item For learners that correspond to probabilistic models, the loss determines / is equivalent to distributional assumptions.
\end{itemize}
\end{vbframe}


\begin{vbframe}{Regression losses - L2 squared loss}
\begin{itemize}
\item $\Lxy = (y-\fx)^2$ or $\Lxy = 0.5 (y-\fx)^2$
\item Convex
\item Differentiable, gradient no problem in loss minimization
\item For latter: $\pd{0.5 (y-\fx)^2}{\fx} = y - \fx = \eps$, derivative is residual
\item Tries to reduce large residuals (if residual is twice as large, loss is 4 times as large), hence
  outliers in $y$ can become problematic
\item Connection to Gaussian distribution (see later)
\end{itemize}

<<echo=FALSE, results='hide', fig.height=3>>=
source("figure_man/plot_loss.R")

set.seed(31415)

x = 1:5
y = 2 + 0.5 * x + rnorm(length(x), 0, 1.5)
data = data.frame(x = x, y = y)
model = lm(y ~ x)

plotModQuadraticLoss(data = data, model = model, pt_idx = c(1,4))
@
\framebreak

What's the optimal constant prediction $c$ (i.e. the same $\hat{y}$ for all $x$)?
$$\Lxy = (y-\fx)^2 = (y - c)^2$$
We search the $c$ that minimizes the empirical risk.
$$  \hat{c} = \argmin_{c \in \R}\riske(c)  =  \argmin_{c \in \R} \frac{1}{n} \sumin  (\yi-c)^2 $$
We set the derivative of the empirical risk to zero and solve for $c$:
\begin{eqnarray*}
 \frac{1}{n}\sumin 2(\yi - c) &=& 0 \cr
\hat{c} &=& \frac{1}{n} \sumin \yi
\end{eqnarray*}
% $\frac{1}{n} \sumin \yi$ is also the maximum likelihood estimator for $\E[y]$ .

% \framebreak
% What is the optimal prediction if we allow $c$ to depend on $x$? \\
% According to the law of total expectation:
% \begin{displaymath}
%   \E_{xy} [\Lxy] = \E_x
%   \left[\E_{y|x}[(y-\fx)^2|x=x]\right]
% \end{displaymath}
% For the optimal prediction it suffices to minimize the risk pointwise:
% $$
%   \fh(x) = \mbox{argmin}_c \E_{y|x}[(y-c)^2|x=x]=\E (y | x=x)
% $$
% So for squared loss, the best prediction in every point is the conditional mean of $y$ given $x$.
%
% \lz
%
% The last step follows from:
% $$
% E[(y - c)^2] = Var[y - c] + (E[y - c])^2 = Var[y] + (E[y] - c)^2
% $$
% This is minimal for $c=E[y]$ if $c$ constant and for $c = E[y|x = x]$ if $c$ is not constant.
\end{vbframe}

\begin{vbframe}{Regression losses - L1 absolute loss}
\begin{itemize}
\item $\Lxy = |y-f(x)|$
\item Convex
\item No derivatives for $r = 0$, $y = f(x)$, optimization becomes harder
%\item More robust, outliers in $y$ are less problematic
\item $\fh(x) = \text{median of } y | x$
%\item Connection to Laplace distribution (see later)
\end{itemize}

<<echo=FALSE, results='hide', fig.height=3, fig.align='center'>>=
plotModAbsoluteLoss(data, model = model, pt_idx = c(1,4), add_quadratic = FALSE) +
  ylim(c(0, 16))
@

\framebreak

\begin{itemize}
\item $\Lxy = |y-f(x)|$
\item Convex
\item No derivatives for $r = 0$, $y = f(x)$, optimization becomes harder
\item $\fh(x) = \text{median of } y | x$
\item More robust, outliers in $y$ are less influential than for L2
%\item Connection to Laplace distribution (see later)
\end{itemize}

<<echo=FALSE, results='hide', fig.height=3, fig.align='center'>>=
plotModAbsoluteLoss(data, model = model, pt_idx = c(1,4))
@

\end{vbframe}

% \begin{vframe}{Regression losses - Huber loss}
% \begin{itemize}
% \item Huber loss $L_\delta(y, f(x)) =
%   \begin{cases}
%   \frac{1}{2}(y-f(x))^2  & \text{ if } |y-f(x)| \le \delta \\
%   \delta |y-f(x)|-\frac{1}{2}\delta^2 \quad & \text{ otherwise }
%   \end{cases}$
% \item Piecewise combination of of L1 and L2 loss
% \item Convex
% \item Combines advantages of L1 and L2 loss: differentiable, robust
% \end{itemize}
%
% <<echo=FALSE, results='hide', fig.height=3, fig.align='center'>>=
% x = seq(-2, 2, by = 0.01); y = ifelse(abs(x) <= 1, 1 / 2 * x^2, abs(x) - 1 / 2)
% qplot(x, y, geom = "line", xlab = expression(y-f(x)), ylab = expression(L(y-f(x))))
% @
% \end{vframe}


\endlecture
