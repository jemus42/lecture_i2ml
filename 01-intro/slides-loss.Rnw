% Introduction to Machine Learning
% Day 1

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@
% Load all R packages and set up knitr
<<setup-r, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{4}{Loss minimization}
\lecture{Introduction to Machine Learning}


\begin{vbframe}{Why do we care about Losses?}
  \begin{itemize}
    \item Assume we trained a model to predict flat rent based on some features
    (size, location, age, ...).
    \item The real rent of a new flat, that the model never saw before, is
    EUR 1000, our model predicts EUR 1300.
    \item How do we measure the performance of our model?
    \item We calculate the prediction error and therefore need a suitable error
    measure, aka a loss function such as:
    \begin{itemize}
      \item Absolute loss: $L(y = 1000, \hat y = 1300) = |1000 - 1300| = 300$
      \item Squared loss: $L(y = 1000, \hat y = 1300) = (1000 - 1300)^2 = 900$, weighs more heavily
      for predictions that are far off.
    \end{itemize}
    \item The choice of the loss has a major influence on the final model.
  \end{itemize}
\end{vbframe}


\begin{vbframe}{Loss minimization}

The goodness of the prediction $y=\fx$ is measured by a \emph{loss function} $\Lxy $
  and its expectation, the so-called \emph{risk},
  $$ \riskf = \E [\Lxy] = \int \Lxy d\Pxy. $$

  Obvious aim: Minimize $\riskf$ over $f$. But this is not (in general) practical:

\begin{itemize}
\item $\Pxy$ is unknown.
\item We could estimate $\Pxy$ in non-parametric fashion from the data $D$, e.g., by kernel density
  estimation, but this really does not scale to higher dimensions (see curse of dimensionality).
\item We can efficiently estimate $\Pxy$, if we place rigorous assumptions on its distributional form,
  and methods like discriminant analysis work exactly this way. ML usually studies more flexible models.
\end{itemize}


\framebreak

An alternative (without directly assuming something about $\P_{xy}$) is to approximate $\riskf$ based on
the data $\D$, by means of the \emph{empirical risk}

$$
\riske(f) = \frac{1}{n} \sumin \Lxyi
$$

Learning then amounts to \emph{empirical risk minimization}
$$
\fh = \argmin_{f \in \Hspace} \riske(f).
$$

\framebreak

When $f$ is parameterized by $\theta$, this becomes:

\begin{eqnarray*}
\riske(\theta) & = & \frac{1}{n} \sumin \Lxyit \cr
\hat{\theta} & = & \argmin_{\theta \in \Theta} \riske(\theta)
\end{eqnarray*}

Thus learning (often) means solving the above \emph{optimization problem}.
Which implies a tight connection between ML and optimization.

Note that (with a slight abuse of notation), if it is more convenient, and as there is no difference w.r.t.
the minimizer, we might also define the $\riske$ in its non-average-but-instead-summed version as:

$$
\risket = \sumin \Lxyit
$$

\framebreak

\begin{itemize}
\item For regression, the loss usually only depends on residual $\Lxy = L(y - \fx) = L(\eps)$,
  this is a \emph{translation invariant} loss
\item Choice of loss decides statistical properties of $f$: Robustness, error distribution (see later)
\item Choice of loss decides computational / optimization properties of minimization of $\risket$:
  Smoothness of objective, can gradient methods be applied, uni- or multimodality. \\
  If $\Lxy$ is convex in its second arguments, and $\fxt$ is linear in $\theta$, then $\risket$ is convex.
  Hence every local minimum of $\risket$ is a global one. If $\Lxy$ not convex,
  R might have multiple local minima (bad!).
\end{itemize}
\end{vbframe}


\begin{vbframe}{Regression losses - L2 squared loss}
\begin{itemize}
\item $\Lxy = (y-\fx)^2$ or $\Lxy = 0.5 (y-\fx)^2$
\item Convex
\item Differentiable, gradient no problem in loss minimization
\item For latter: $\pd{0.5 (y-\fx)^2}{\fx} = y - \fx = \eps$, derivative is residual
\item Tries to reduce large residuals (if residual is twice as large, loss is 4 times as large), hence
  outliers in $y$ can become problematic
\item Connection to Gaussian distribution (see later)
\end{itemize}

<<echo=FALSE, results='hide', fig.height=3>>=
x = seq(-2, 2, by = 0.01); y = x^2
qplot(x, y, geom = "line", xlab = expression(y-f(x)), ylab = expression(L(y-f(x))))
@
\framebreak
What's the optimal constant prediction $c$ (i.e. the same $\hat{y}$ for all $x$)?
$$\Lxy = (y-\fx)^2 = (y - c)^2$$
We search the $c$ that minimizes the empirical risk.
$$  \hat{c} = \argmin_{c \in \R}\riske(c)  =  \argmin_{c \in \R} \frac{1}{n} \sumin  (\yi-c)^2 $$
We set the derivative of the empirical risk to zero and solve for $c$:
\begin{eqnarray*}
 \frac{1}{n}\sumin 2(\yi - c) &=& 0 \cr
\hat{c} &=& \frac{1}{n} \sumin \yi
\end{eqnarray*}
$\frac{1}{n} \sumin \yi$ is also the maximum likelihood estimator for the mean  $E[y]$.

\framebreak
What is the optimal prediction if we allow $c$ to depend on $x$? \\
According to the law of total expectation:
\begin{displaymath}
  \E_{xy} [\Lxy] = \E_x
  \left[\E_{y|x}[(y-\fx)^2|x=x]\right]
\end{displaymath}
For the optimal prediction it suffices to minimize the risk pointwise:
$$
  \fh(x) = \mbox{argmin}_c \E_{y|x}[(y-c)^2|x=x]=\E (y | x=x)
$$
So for squared loss, the best prediction in every point is the conditional mean of $y$ given $x$.

\lz

The last step follows from:
$$
E[(y - c)^2] = Var[y - c] + (E[y - c])^2 = Var[y] + (E[y] - c)^2
$$
This is minimal for $c=E[y]$ if $c$ constant and for $c = E[y|x = x]$ if $c$ is not constant.
\end{vbframe}



\begin{vframe}{Regression losses - L1 absolute loss}
\begin{itemize}
\item $\Lxy = |y-f(x)|$
\item Convex
\item No derivatives for $r = 0$, $y = f(x)$, optimization becomes harder
\item More robust, outliers in $y$ are less problematic
\item $\fh(x) = \text{median of } y | x$
\item Connection to Laplace distribution (see later)
\end{itemize}

<<echo=FALSE, results='hide', fig.height=3, fig.align='center'>>=
x = seq(-2, 2, by = 0.01); y = abs(x)
qplot(x, y, geom = "line", xlab = expression(y-f(x)), ylab = expression(L(y-f(x))))
@
\end{vframe}

% \begin{vframe}{Regression losses - Huber loss}
% \begin{itemize}
% \item Huber loss $L_\delta(y, f(x)) =
%   \begin{cases}
%   \frac{1}{2}(y-f(x))^2  & \text{ if } |y-f(x)| \le \delta \\
%   \delta |y-f(x)|-\frac{1}{2}\delta^2 \quad & \text{ otherwise }
%   \end{cases}$
% \item Piecewise combination of of L1 and L2 loss
% \item Convex
% \item Combines advantages of L1 and L2 loss: differentiable, robust
% \end{itemize}
%
% <<echo=FALSE, results='hide', fig.height=3, fig.align='center'>>=
% x = seq(-2, 2, by = 0.01); y = ifelse(abs(x) <= 1, 1 / 2 * x^2, abs(x) - 1 / 2)
% qplot(x, y, geom = "line", xlab = expression(y-f(x)), ylab = expression(L(y-f(x))))
% @
% \end{vframe}


\endlecture
