% Introduction to Machine Learning
% Day 1

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@
% Load all R packages and set up knitr
<<setup-r, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{1}{Introduction}
\lecture{Fortgeschrittene Computerintensive Methoden}


<<include=FALSE>>=
set.seed(19042011)
runifCirc <- function(n, radius = 1, d = 2)
  t(sapply(seq_len(n), function(i) HI::rballunif(d, radius)))
@
\sloppy

\begin{vbframe}
\centering
\includegraphics[width=0.98\textwidth]{figure_man/datascience.jpg}

\framebreak

\ \\
\vspace{1.5cm}
\centering
\includegraphics[width=1\textwidth]{figure_man/learning2.png}

\framebreak
\ \\
\vspace{1cm}
\centering
\includegraphics[width=1\textwidth]{figure_man/learning.pdf}
\end{vbframe}

\begin{frame}{Machine Learning is changing our world}
\begin{itemize}
   \item Search engines learn what you want
   \item Recommender systems learn your taste in books, music, movies,...
   \item Algorithms do automatic stock trading
   \item Elections are won by understanding voters
   \item Google Translate learns how to translate text
   \item Siri learns to understand speech
   \item DeepMind beats humans at Go
   \item Cars drive themselves
   \item Medicines are developed faster
   \item Smartwatches monitor your health
   \item Data-driven discoveries are made in Physics, Biology, Genetics, Astronomy, Chemistry, Neurology,...
\end{itemize}
\end{frame}

% \begin{vbframe}{Learning from data}
% \begin{itemize}
% \item Data analytic problems increased in the last decade regarding size and complexity
%   \begin{itemize}
%    \item Data mining: storage, organization and analysis of big data sets
%    \item Bioinformatics: solving statistical and computational problems in medicine an biology
%   \end{itemize}
%   \item Role of statistics: recognition of important patterns and trends,\\
%   attempt to understand what \enquote{data reveals},\\
%   creation of predictions
%   \end{itemize}
% \end{vbframe}

\begin{vbframe}{Quotes}
\begin{itemize}
  \item New York Times (August 2009): \emph{\enquote{I keep saying that the
    sexy job in the next 10 years will be statisticians,} said Hal
    Varian, chief economist at Google. \enquote{And I'm not kidding.}}\\[0.1cm]
    http://www.nytimes.com/2009/08/06/technology/06stats.html
  \item \emph{\enquote{We can say with complete confidence that in the coming century, high-dimensional data analysis will be a very significant activity, and completely new methods of high-dimensional data analysis will be developed; \ldots}}\\[0.1cm]
    David Donoho in his lecture 'Math Challenges of the 21st Century' to the American Mathematical Society (2000)
\end{itemize}

\framebreak

\begin{itemize}
  \item Barack Obama at Wired (October 2016):
  \begin{itemize}
    \item \emph{\enquote{There's a distinction [\ldots ] between generalized AI and specialized AI.
    \item In science fiction, what you hear about is generalized AI, right? Computers start getting smarter than we are [\ldots ].
    \item My impression, based on talking to my top science advisers, is that we're still a reasonably long way away from that.
    \item Specialized AI [\ldots ] is about using algorithms and computers to figure out increasingly complex tasks.
    \item We've been seeing specialized AI in every aspect of our lives, from medicine and transportation to how electricity is distributed, [\ldots ]}}\\[0.1cm]
  \end{itemize}
  https://www.wired.com/2016/10/president-obama-mit-joi-ito-interview/
\end{itemize}
\end{vbframe}

\begin{vbframe}{Motivation}
\lz
\centering
\includegraphics[width=0.98\textwidth]{figure_man/food.png}
\end{vbframe}

\begin{vbframe}{Machine Learning Workflow}
\vspace{1cm}
\centering
\includegraphics[width=1\textwidth]{figure_man/ml-workflow.png}
\end{vbframe}


\begin{vbframe}{Supervised Learning}
\lz
\begin{itemize}
  \item One tries to learn the relationship between \enquote{input} $x$ and \enquote{output} $y$.
  \item For learning, there is training data with labels available
 \item Mathematically, we face a problem of function approximation: search for an $f$, such that,
  for all points in the training data, and also all newly observed points,
$$ y \approx f(x). $$
\end{itemize}

\framebreak

\textbf{Regression Task}
\lz
\begin{columns}[T]
  \begin{column}{0.5\textwidth}
    \structure{Goal}: Predict a continuous output
    \begin{itemize}
      \item $y$ is metric variable (with values in $\R$)
      \item Regression model can be constructed by different methods, e.g. trees or splines
    \end{itemize}
  \end{column}
  \begin{column}{0.5\textwidth}
<<regression-task-plot, fig.height=4>>=
set.seed(1)
f = function(x) 0.5 * x^2 + x + sin(x)
x = runif(40, min = -3, max = 3)
y = f(x) + rnorm(40)
df = data.frame(x = x, y = y)
ggplot(df, aes(x, y)) + geom_point(size = 3) + stat_function(fun = f, color = "#FF9999", size = 2)
@
\lz
<<regression-task-plot-tree, fig.height=4>>=
tree = ctree(y ~ ., data = df,
  controls = ctree_control(maxdepth = 3))
plot(tree)
@
  \end{column}
\end{columns}

\framebreak

\textbf{Regression example data set: Boston Housing}\\
\begin{columns}[T]
  \begin{column}{0.4\textwidth}
  \small
    \begin{itemize}
      \item The Boston Housing data set has 506 observations (from 1970) and 14 variables, with \code{medv} (median value of the owner-occupied homes) being the target variable
    \end{itemize}
<<boston-plot, fig.width=5, fig.height=4.5>>=
bh = getTaskData(bh.task)
ggplot(data = bh, aes(x = lstat, y = medv)) + geom_point(size = 1) + geom_smooth(size = 1)
@
  \end{column}
  \begin{column}{0.6\textwidth}
    \begin{table}
    \scriptsize
      \begin{tabular}{ll}
      \code{crim} & per capita crime rate by town\\
      \code{zn} & proportion of residential land zoned for lots\\
                & over 25,000 sq.ft.\\
      \code{indus} & proportion of non-retail business acres per town\\
      \code{chas} & Charles River dummy variable (= 1 if tract\\
                  & bounds river; 0 otherwise)\\
      \code{nox} & nitric oxides concentration (parts per 10 million)\\
      \code{rm} & average number of rooms per dwelling \\
      \code{age} & proportion of owner-occupied units built prior\\
                 & to 1940\\
      \code{dis} & weighted distances to five Boston employment\\
                 & centers\\
      \code{rad} & index of accessibility to radial highways\\
      \code{tax} & full-value property-tax rate per \$10,000\\
      \code{ptratio} & pupil-teacher ratio by town\\
      \code{b} & $1000(b_k - 0.63)^2$ where $b_k$ is the proportion\\
               & of blacks by town\\
      \code{lstat} & $\%$ lower status of the population \\
      \code{medv} & Median value of owner-occupied homes in\\
                  & \$1000's\\
    \end{tabular}
  \end{table}
  \end{column}
\end{columns}

\framebreak

\textbf{Binary Classification Task}
\lz
\begin{columns}[T]
  \begin{column}{0.5\textwidth}
    \structure{Goal}: Predict a class (or membership probabilities)
    \begin{itemize}
      \item $y$ is a categorical variable (with two different unordered discrete values)
    \end{itemize}
  \end{column}
  \begin{column}{0.5\textwidth}
<<classification-task-plot, fig.height=6, fig.width=6>>=
set.seed(1)
df2 = data.frame(x1 = c(rnorm(10, mean = 3), rnorm(10, mean = 5)), x2 = runif(10), class = rep(c("a", "b"), each = 10))
ggplot(df2, aes(x = x1, y = x2, shape = class, color = class)) + geom_point(size = 3) + geom_vline(xintercept = 4, linetype = "longdash")
@
  \end{column}
\end{columns}

\framebreak

\textbf{Binary classification example data set: Spam Detection}\\
\begin{columns}[T]
  \begin{column}{0.5\textwidth}
    \begin{itemize}
      \item The data set contains data of 4601 emails collected at the Hewlett-Packard-Laboratories, to train a personalized spam mail detector
      \item 2788 mails were wanted and 1813 were spam
      \item There are 57 numerical predictors available measuring for example the frequency of some words and special characters, as well as length of words in capital letters
    \end{itemize}
  \end{column}
  \begin{column}{0.5\textwidth}
<<spam-plot, fig.height=8, fig.width=6>>=
data("spambase", package = "nutshell")
tree = ctree(is_spam ~ ., data = spambase,
  controls = ctree_control(maxdepth = 2))
plot(tree)
@
  \end{column}
\end{columns}

\framebreak

\textbf{Multiclass Classification Task}
\lz
\begin{columns}[T]
  \begin{column}{0.6\textwidth}
    \structure{Goal}: Predict a class (or membership probabilities)
    \begin{itemize}
      \item $y$ is a categorical variable with more than two different unordered discrete values
      \item Each instance belongs only to one class!
    \end{itemize}
  \end{column}
  \begin{column}{0.4\textwidth}
<<multi-classification-task-plot, fig.height=4, fig.width=6>>=
plotLearnerPrediction(makeLearner("classif.svm"), iris.task, c("Petal.Length", "Petal.Width")) +
  ggtitle("")
@
\lz
<<multi-classification-task-plot-tree, fig.height=4, fig.width=6>>=
tree = ctree(Species ~ ., data = iris,
  controls = ctree_control(maxdepth = 3))
plot(tree)
@
  \end{column}
\end{columns}

\framebreak

\textbf{Multiclass classification example data set: Iris}\\
\begin{columns}[T]
  \begin{column}{0.4\textwidth}
    \begin{itemize}
      \item The iris data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris (setosa, versicolor, and virginica)
    \end{itemize}
  \end{column}
  \begin{column}{0.6\textwidth}
<<iris-plot, fig.height=12, fig.width=11>>=
iris = getTaskData(iris.task)
ggpairs(iris, aes(colour=Species))
@
  \end{column}
\end{columns}

\framebreak

\textbf{Multilabel Classification Task}
\lz
\begin{columns}[T]
  \begin{column}{0.5\textwidth}
    \structure{Goal}: Predict multiple classes (or membership probabilities) for a single observation
    \begin{itemize}
      \item $Y$ is a matrix containing one dummy coded vector for each categorical label
      \item Example of $Y$:
    \end{itemize}
  \end{column}
  \begin{column}{0.5\textwidth}
<<multilabel-task-plot, echo=FALSE,fig.height=6>>=
set.seed(1)
df3 = data.frame(x = c(rnorm(10, mean = 2, sd = 1), rnorm(10, mean = 4, sd = 1), rnorm(10, mean = 6, sd = 1)),
  y = runif(10), class = c(rep("a", 20), rep("b", 10)), fill = c(rep("a", 10), rep("b", 20)))
ggplot(df3, aes(x = x, y = y)) + geom_point(aes(colour = class),size = 6) + geom_point(aes(colour = fill), size = 3) +
  geom_vline(xintercept = 3, linetype = "longdash") + geom_vline(xintercept = 5, linetype = "longdash") +
  theme(legend.position = "none")
@
  \end{column}
\end{columns}
\begin{table}
  \begin{tabular}{r|r|r|l}
  $label\ 1$ & $label\ 2$ & $label\ 3$ & label set \\
  \hline
  $1$ & $0$ & 0 & $\{label\ 1\}$ \\
  $0$ & $1$ & 1 & $\{label\ 2, label\ 3\}$ \\
  $1$ & $1$ & 1 & $\{label\ 1, label\ 2, label\ 3\}$ \\
  $1$ & $0$ & 1 & $\{label\ 1, label\ 3\}$ \\
  \hline
  \end{tabular}
\end{table}

\framebreak

\textbf{Cost-sensitive Classification Task}
\lz
\begin{columns}[T]
  \begin{column}{0.5\textwidth}
    \structure{Goal}: Minimize cost of predicting the correct class label $y$
    \begin{itemize}
      \item A more general setting of regular classification
      \item Costs caused by different kinds of errors are not assumed to be equal
    \end{itemize}
  \end{column}
  \begin{column}{0.5\textwidth}
<<costsens-task-plot, fig.height=5.5>>=
data(GermanCredit, package = "caret")
credit.task = makeClassifTask(data = GermanCredit, target = "Class")
credit.task = removeConstantFeatures(credit.task)
costs = matrix(c(0, 1, 5, 0), 2)
colnames(costs) = rownames(costs) = getTaskClassLevels(credit.task)
lrn = makeLearner("classif.multinom", predict.type = "prob", trace = FALSE)
mod = train(lrn, credit.task)
pred = predict(mod, task = credit.task)
th = costs[2,1]/(costs[2,1] + costs[1,2])
pred.th = setThreshold(pred, th)
credit.costs = makeCostMeasure(id = "credit.costs", name = "Credit costs", costs = costs,
  best = 0, worst = 5)
rin = makeResampleInstance("CV", iters = 3, task = credit.task)
lrn = makeLearner("classif.multinom", predict.type = "prob", predict.threshold = th, trace = FALSE)
r = resample(lrn, credit.task, resampling = rin, measures = list(credit.costs, mmce), show.info = FALSE)
d = generateThreshVsPerfData(r, measures = list(credit.costs, mmce))
plotThreshVsPerf(d, mark.th = th, facet.wrap.ncol = 1L)
@
  \end{column}
\end{columns}
\textbf{Example}
\begin{itemize}
  \item \textbf{Loan Applications}\\
  \begin{itemize}
    \item rejecting an applicant who will not pay back $\rightarrow$ minimal costs
    \item accepting an applicant who will pay back $\rightarrow$ gain
    \item accepting an applicant who will not pay back $\rightarrow$ big loss
    \item rejecting an applicant who would pay back $\rightarrow$ loss
  \end{itemize}
\end{itemize}

\framebreak

\textbf{Forecasting Task}
\lz
\begin{columns}[T]
  \begin{column}{0.5\textwidth}
    \structure{Goal}: Predict future values of a time series
    \begin{itemize}
      \item $y$ is metric variable depending on time $t$
      \item $t$ could be e.g. hours, days, years, \ldots
    \end{itemize}
  \end{column}
  \begin{column}{0.5\textwidth}
<<forecasting-task-plot, fig.height=8>>=
data(AirPassengers)
data = AirPassengers
model = auto.arima(data, start.P = 2, start.Q = 2, max.D = 2, max.P = 5, max.Q = 5)
plot(forecast(model, h = 10, level = 95), main = "")
@

  \end{column}
\end{columns}

\framebreak

% \begin{itemize}
% \item \Sexpr{nrow(spambase)} Emails were classified as \enquote{Spam} or \enquote{No Spam}.
% \item Inputs: including percentages of \Sexpr{ncol(spambase)-1} frequent words and symbols within the email
% \end{itemize}
% <<echo=FALSE>>=
% round(colMeans(spambase[,c(paste("capital_run_length", c("total", "longest", "average"), sep = "_"),
%                     paste("word_freq", c("you", "your", "george", "hp", "will", "all"), sep = "_"),
%                     "char_freq_exclamation")]), digits = 2)
% @
% \newpage




\textbf{Survival Task}
\lz
\begin{columns}[T]
  \begin{column}{0.5\textwidth}
    \structure{Goal}: Predict a survival function $\hat{S}(t)$, i.e.\ the probability to survive to time point $t$
    \begin{itemize}
      \item $y$ is survival function $S(t)$ of time $t$
      \item Values for survival $S(t)$ are between $0$ and $1$
    \end{itemize}
  \end{column}
  \begin{column}{0.5\textwidth}
<<survial-task-plot, fig.height=8>>=
set.seed(1)
data("rats", package = "survival")
sf = survfit(Surv(time, status) ~ rx, data = rats)
survMisc:::autoplot.survfit(sf, title = "", xLab = "Time", yLab = "S(t)",
  yScale = "frac", survLineSize = 1.5)$plot
@
  \end{column}
\end{columns}
\end{vbframe}

%
% \textbf{Examples}
%
% \lz
%
% \begin{itemize}
%  \item Handwritten digit recognotion
%  \item Lung cancer prediction
%  \item Email spam recognition
%  \item Recommender system (movies, books, etc.)
%  \item Word recognition from spoken language
%  \item $\ldots$
% \end{itemize}
%
% \framebreak
%
% \begin{itemize}
%  \item Classical approach for metric $y$ is to consider \textbf{linear functions} $f$. \\
%   $\to$ \textbf{Linear Regression}
%  \item Many events can't be modelled appropriately by linear functions \\
%   $\to$ In the last decades, many \textbf{\enquote{flexible} regression} models were developed. Most models do not provide a close solution for the parameters. Iterative estimation methods (\enquote{learning algorithms}) are used.
%  \item \textbf{Local Methods:} The input space gets divided, in each subspace a simple modell is estimated.
%   $\to$ (Smoothing) Splines, trees, \ldots
%  \item \textbf{Kernel Methods:} The non-linear problem is projected into a high dimensional space in which it is linearly solvable.
%   $\to$ Support Vector Machines
%  \item \textbf{Additive Models:} summation of several simple non-linear models.
%   $\to$ Neural Networks, Generalised Additive Models, Projection Pursuit, \ldots
% \end{itemize}

\begin{vbframe}{Unsupervised Learning}
\lz
\begin{itemize}
  \item There exist no outputs $y$
  \item Search for patterns within the inputs $x$
  \item Focus of this lecture lies on supervised learning
\end{itemize}

\framebreak

\textbf{Clustering Task}
\lz
\begin{columns}[T]
  \begin{column}{0.5\textwidth}
    \structure{Goal}: Group data into similar clusters (or estimate fuzzy membership probabilities)\\
    \lz
<<cluster-task-plot2, fig.height=8>>=
df4 = getTaskData(iris.task)
idx = sample(1:dim(df4)[1], 40)
irisSample = df4[idx,]
irisSample$Species = NULL
hc = hclust(dist(irisSample), method = "ave")
plot(hc, hang = -1, labels = df4$Species[idx], sub = "", xlab = "")
@
  \end{column}
  \begin{column}{0.5\textwidth}
<<cluster-task-plot1, fig.height=8>>=
# df = iris
m = as.matrix(cbind(df4$Petal.Length, df4$Petal.Width), ncol = 2)
cl = (kmeans(m,3))
df4$cluster = factor(cl$cluster)
centers = as.data.frame(cl$centers)
ggplot(data = df4, aes(x = Petal.Length, y = Petal.Width, color = cluster )) +
 geom_point(size = 4) +
 geom_point(data = centers, aes(x = V1, y = V2, color = 'Center')) +
 geom_point(data = centers, aes(x = V1,y = V2, color = 'Center'), size = 90, alpha = .3) +
 theme(legend.position = "none")
@
  \end{column}
\end{columns}

\framebreak

\textbf{Dimensionality reduction Task}
\lz
\begin{columns}[T]
  \begin{column}{0.5\textwidth}
    \structure{Goal}: describe data in fewer features
    \begin{blocki}{Common methods:}
      \item Principle Component Analysis (PCA)
      \item Linear Discriminant Analysis (LDA)
      \item Filter Methods
    \end{blocki}
  \end{column}
  \begin{column}{0.5\textwidth}
<<dim-red-task-plot-pca, fig.height=9>>=
pca = prcomp(BBmisc::dropNamed(iris, "Species"), scale. = TRUE)
ggbiplot(pca, obs.scale = 1, var.scale = 1,
  groups = iris$Species, ellipse = TRUE, circle = TRUE) +
  scale_color_discrete(name = '') +
  theme(legend.direction = 'horizontal', legend.position = 'top')
@
  \end{column}
\end{columns}

\framebreak

\textbf{Outlier detection Task}
\lz
\begin{columns}[T]
  \begin{column}{0.5\textwidth}
    \structure{Goal}: identify observations which do not conform to an expected pattern
    \begin{itemize}
      \item outlier detection is also referred to anomaly detection and one class classification
      \item Several methods exist based on:
      \begin{itemize}
        \item density or correlation
        \item cluster analysis
        \item neural networks
        \item ensemble techniques
      \end{itemize}
    \end{itemize}
  \end{column}
  \begin{column}{0.5\textwidth}
<<doutlier-task-plot, fig.height=9>>=
# Inject outliers into data.
cars1 = cars[1:30, ]  # original data
cars.outliers = data.frame(speed = c(19, 19, 20, 20, 20), dist = c(190, 186, 210, 220, 218))  # introduce outliers.
cars2 = rbind(cars1, cars.outliers)  # data with outliers.

# Plot of data with outliers.
par(mfrow = c(2, 1))
plot(cars2$speed, cars2$dist, xlim = c(0, 28), ylim = c(0, 230), main = "With Outliers",
  xlab = "speed", ylab = "dist", pch = "*", col = "red", cex = 2)
abline(lm(dist ~ speed, data = cars2), col = "blue", lwd = 3, lty = 2)

# Plot of original data without outliers. Note the change in slope (angle) of best fit line.
plot(cars1$speed, cars1$dist, xlim = c(0, 28), ylim = c(0, 230), main = "Outliers removed \n A much better fit!",
  xlab = "speed", ylab = "dist", pch = "*", col = "red", cex = 2)
abline(lm(dist ~ speed, data = cars1), col = "blue", lwd = 3, lty = 2)
@
  \end{column}
\end{columns}

\framebreak

\textbf{Association rules Task}
\lz
\begin{columns}[T]
  \begin{column}{0.5\textwidth}
    \structure{Goal}: Discover relations between features
    \begin{itemize}
      \item Rule-based machine learning method
      \item For two features $x_1$ and $x_2$, a \textbf{rule} is defined as an implication of the form $x_1 \Rightarrow x_2 $
      \item \textbf{Support} is an indication of how frequently the feature appears in the data
      \item \textbf{Confidence} is an indication of how often the rule has been found to be true
    \end{itemize}
  \end{column}
  \begin{column}{0.5\textwidth}
<<association-task-plot, fig.height=9>>=
titanic.raw = read.csv("titanic.raw")
rules = apriori(titanic.raw,
  parameter = list(minlen = 2, supp = 0.005, conf = 0.8),
  appearance = list(rhs = c("Survived=No", "Survived=Yes"),
  default = "lhs"),
  control = list(verbose = FALSE))
plot(rules, method="graph", control=list(type="items"))
@
  \end{column}
\end{columns}
\end{vbframe}


\begin{vbframe}{Semi-Supervised Learning}
\lz
\begin{itemize}
  \item Learning a reliable model usually requires plenty of labeled data
  \item Labeled data: can be expensive
  \item Unlabeled data: abundant and free/cheap
  \item General idea: learning from both labeled and unlabeled data
  \item There exist few labeled training data $\Dtrain^L=\{ (x_1, y_1), \ldots, (x_l,  y_l)\}$ and many unlabeled observations $\Dtrain^U=\{ (x_{l+1}, y_{l+1}), \ldots, (x_{l+u},  y_{l+u})\}$ for training (usually $u\gg l$).
 \item Semi-Supervised learning falls between supervised (completely labeled training data) and unsupervised (without any labeled training data) learning
 \item Unlabeled data in conjunction with a small amount of labeled data improves learning accuracy.
 % \begin{itemize}
 %   \item \textbf{Semi-supervised regression/classification:} uses unlabeled data to get a better model than with the labeled data alone.
 %   \item \textbf{Semi-supervised clustering:} uses labeled data must-/cannot-links.
   % \item \textbf{inductive learning:} infer mapping from \enquote{input} $x$ to \enquote{output} $y$.
   % \item \textbf{Active learning:} special case in which a learning algorithm is able to interactively query the user to obtain the desired outputs at new data points.
 %  \end{itemize}
\end{itemize}

\framebreak

\textbf{Semi-Supervised classification task}
\lz
\begin{columns}[T]
  \begin{column}{0.5\textwidth}
    \structure{Goal}: Learning a classifier $f$ better than using labeled data alone.
    \begin{itemize}
      \item Assumption: Examples from the same class follow a coherent distribution
      \item Unlabeled data can give a better sense of the class separation boundary
    \end{itemize}
  \end{column}
  \begin{column}{0.5\textwidth}
  \centering
    \includegraphics[width=.5\textwidth]{figure_man/semi1.png}\\
    \includegraphics[width=.5\textwidth]{figure_man/semi3.png}\\
    \lz
    \includegraphics[width=.5\textwidth]{figure_man/semi7.png}
  \end{column}
\end{columns}

\framebreak

\textbf{Semi-Supervised Clustering task}
\lz
\begin{columns}[T]
  \begin{column}{0.5\textwidth}
    \structure{Goal}: Use labeled data to group unlabeled data.
      \begin{itemize}
        \item Having labeled observations means to have information about if two observations have to be in the same or different clusters
        \item This can be expressed by constraints among observations:\\
        \textit{must links} and \textit{cannot links}
        \item Look only for models which maintain these constraints
      \end{itemize}
  \end{column}
  \begin{column}{0.5\textwidth}
    \centering
      \includegraphics[width=1\textwidth]{figure_man/semi-clustering.png}
  \end{column}
\end{columns}

\framebreak

\textbf{Active Learning task}
\lz
\begin{columns}[T]
  \begin{column}{0.5\textwidth}
    \structure{Goal}: Label extra-data in a smart way to improve model most efficiently
      \begin{itemize}
        \item Extra labels can be requested, but expensive
        \item We iterate:
        \begin{itemize}
          \item learn on labeled data
          \item request labels for some unlabeled instances
        \end{itemize}
        \item Only most useful observations for learning need to be labeled
      \end{itemize}
  \end{column}
  \begin{column}{0.25\textwidth}
    1.\\
    \includegraphics[width=.8\textwidth]{figure_man/active1.png}\\
    \lz
    3.\\
    \includegraphics[width=.8\textwidth]{figure_man/active3.png}\\
    \lz
    5.\\
    \includegraphics[width=.8\textwidth]{figure_man/active5.png}
  \end{column}
    \begin{column}{0.25\textwidth}
    2.\\
    \includegraphics[width=.8\textwidth]{figure_man/active2.png}\\
    \lz
    4.\\
    \includegraphics[width=.8\textwidth]{figure_man/active4.png}\\
    \lz
    6.\\
    \includegraphics[width=.8\textwidth]{figure_man/active6.png}\\
  \end{column}
\end{columns}
\end{vbframe}


\begin{vbframe}{Reinforcement Learning}
\lz
\begin{itemize}
  \item Inputs are observations and feedback (rewards or punishments) from interacting with an environment
  \item Output: achieve some goal
  \item Goal: Select actions to maximize future reward $\triangleq$ return
\end{itemize}

\framebreak

\textbf{The RL Setting}
\lz
\begin{columns}[T]
  \begin{column}{0.5\textwidth}
  RL is a \textbf{general-purpose framework} for AI\\
  \lz
  At each time step $t$ an \textbf{agent} interacts with an \textbf{environment} $\mathcal{E}$ and
		\begin{itemize}
			\item observes \textbf{state} $s_t \in \R^d$
			\item receives \textbf{reward} $r_t \in \R$
			\item executes \textbf{action} $a_t \in A$
		\end{itemize}
		Reward signals may be sparse, noisy and delayed.\\
		\lz
		$\rightarrow$ \textbf{Agent-environment-loop}
  \end{column}
  \begin{column}{0.5\textwidth}
  \centering
    \includegraphics[width=1\textwidth]{figure_man/state_action_reward_diagram.png}
  \end{column}
\end{columns}

\end{vbframe}


% \begin{vbframe}{Topics of the lecture}
% \begin{itemize}
%  \item Neuronal Networks
%  \item Decision Trees
%  \item Support Vector Machines
%  \item Ensemble Methods (Bagging, Boosting)
%  \item Benchmarking and modelselection
%  \item Parameter tuning / Algorithm configuration
%  \item Machine Learning in R
%  \item Parallel calculation in R
%  \item \ldots
% \end{itemize}
% \end{vbframe}
%
% \begin{vbframe}{Material}
% \begin{itemize}
%  \item PDF Files of all slides can be downloaded from Moodle.
%  \item There is no script available.
%  \item On the Moodle page, there will be references to literature and extra material.
%    This is supposed to be read.
% \end{itemize}
% \end{vbframe}

% \begin{vbframe}{Machine Learning - What's in a name?}
% \begin{itemize}
%   \item Many names: data mining, data science, machine learning, statistical learning,...
%   \item Subtle differences in scope, partly marketing
%   \item We'll mostly use the term 'machine learning'
%   \item Has deep roots in statistics, neurology, biology, psychology,... but has developed into a new field of study.
%   \item How is it different from statistics?
%   \begin{itemize}
%     \item Breiman. Statistical Modeling: The Two Cultures. Statistical Science, 2001
%   \end{itemize}
% \end{itemize}
% \end{vbframe}

% \begin{vbframe}{Modeling: two cultures}
% \begin{center}
% <<fig.height=3, fig.width=12>>=
% grid.newpage()
% pushViewport(viewport(x = 0.1, y = 0.5, width = 0.14, height = 0.56, angle = 45))
% grid.rect(gp = gpar(lwd = 4))
% grid.text("x", rot = -45, gp = gpar(fontsize = 24, fontface = "bold"))
% popViewport()
% pushViewport(viewport(x = 0.5, y = 0.5, width = 0.4, height = 0.8, angle = 0))
% grid.rect(gp = gpar(lwd = 4))
% grid.text("System", gp = gpar(fontsize = 24, fontface = "bold"))
% popViewport()
% pushViewport(viewport(x = 0.9, y = 0.5, width = 0.14, height = 0.56, angle = 45))
% grid.rect(gp = gpar(lwd = 4))
% grid.text("y", rot = -45, gp = gpar(fontsize = 24, fontface = "bold"))
% popViewport()
% grid.lines(c(0.1 + sqrt(2*0.07^2), 0.3), rep(0.5, 2), arrow = arrow(length = unit(0.2, "inch"), type = "closed"),
%            gp = gpar(lwd = 4))
% grid.lines(c(0.7, 0.9 - sqrt(2*0.07^2)), rep(0.5, 2), arrow = arrow(length = unit(0.2, "inch"), type = "closed"),
%            gp = gpar(lwd = 4))
% @
% \end{center}
% System: nature, organism, chemical reaction, technical process, human behavior, \ldots
% \end{vbframe}
%
% \begin{vbframe}{two cultures: classical statistics}
% We assume that one can sufficiently describe the unknown system by the stochastic model
% $$y = f(x, \theta) + \epsilon$$
% and a \emph{parametric} function class for $f()$ is given. \\
% \lz
% \emph{Under the assumption} that the model is correct, now the usual operation of statistical inference can be built up:
% \begin{itemize}
%  \item Hypothesis tests, variance analysis, model comparison
%  \item Confidence intervals for parameters and predictions
% \end{itemize}
%
% \newpage
% Many articles in JASA, the Annals of Statistics etc. start with
% \enquote{Assume that the data are generated by the following model \ldots}.
%
% \begin{itemize}
% \item \textbf{Advantages:}
%   \begin{itemize}
%   \item Parameter can be interpreted if the model is easy enough
%   \item Good theory for model diagnostics
%   \end{itemize}
%  \item \textbf{Disadvantages (Breiman 2001):}
%    \begin{itemize}
%   \item \enquote{irrelevant theory}
%   \item \enquote{questionable scientific conclusions}
%   \end{itemize}
% \end{itemize}
% \end{vbframe}
%
% \begin{vbframe}{Two cultures: machine learning}
% \begin{center}
% <<fig.height=6, fig.width=12>>=
% grid.newpage()
% pushViewport(viewport(x = 0.1, y = 0.75, width = 0.14, height = 0.28, angle = 45))
% grid.rect(gp = gpar(lwd = 4))
% grid.text("x", rot = -45, gp = gpar(fontsize = 24, fontface = "bold"))
% popViewport()
% pushViewport(viewport(x = 0.5, y = 0.75, width = 0.4, height = 0.4, angle = 0))
% grid.rect(gp = gpar(lwd = 4))
% grid.text("System", gp = gpar(fontsize = 24, fontface = "bold"))
% popViewport()
% pushViewport(viewport(x = 0.9, y = 0.75, width = 0.14, height = 0.28, angle = 45))
% grid.rect(gp = gpar(lwd = 4))
% grid.text("y", rot = -45, gp = gpar(fontsize = 24, fontface = "bold"))
% popViewport()
% grid.lines(c(0.1 + sqrt(2*0.07^2), 0.3), rep(0.75, 2), arrow = arrow(length = unit(0.2, "inch"), type = "closed"),
%            gp = gpar(lwd = 4))
% grid.lines(c(0.7, 0.9 - sqrt(2*0.07^2)), rep(0.75, 2), arrow = arrow(length = unit(0.2, "inch"), type = "closed"),
%            gp = gpar(lwd = 4))
% grid.roundrect(0.5, 0.2, 0.7, 0.3, name = "rr", gp = gpar(lwd = 4))
% grid.text("Approximation by\nneuronal network, tree, ...", y = 0.2,
%           gp = gpar(fontsize = 24, fontface = "bold"))
% grid.curve(0.1, 0.55, 0.15, 0.2,
%    arrow = arrow(length = unit(0.2, "inch"), type = "closed"),
%    gp = gpar(lwd = 4))
% grid.curve(0.85, 0.2, 0.9, 0.55,
%    arrow = arrow(length = unit(0.2, "inch"), type = "closed"),
%    gp = gpar(lwd = 4))
% @
% \end{center}
% The system is considered as unknown, explicit modelling is not even tried.
% Essential measure of goodness is the prediction quality.
%
% \framebreak
%
% Use of methods which are able to model many systems (universal approximations), development of methods by examples of nature, intuitive behavioral or because of computational attractivity.
% \vspace{0.25cm}
% \begin{itemize}
%  \item \textbf{Advantages:}
%    \begin{itemize}
%    \item Many more model classes are available, much quicker development and implementation of new ideas.
%    \item For predictions, knowledge about the distribution of the parameters,
%    diligent task, knowledge about the error distribution is sufficient.
%    \end{itemize}
%  \item \textbf{Disadvantages:}
%    \begin{itemize}
%    \item Models are mostly hard to interpret (\enquote{black box})
%    \end{itemize}
% \end{itemize}
% \vspace{0.25cm}
% Many models which originally come from machine learning are also statistical mainstream nowadays (and have in parts been considerably improved).


\begin{vbframe}{Modeling: two cultures}
\textbf{Statistics, the Data Modeling Culture}
\begin{center}
\vspace{1cm}
  \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1cm,
      thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}]
    \node[punkt] (nature) {nature};
    \node[left=of nature] (x) {x};
    \node[right=of nature] (y) {y};
    \path[every node/.style={font=\sffamily\small}]
    (nature) edge node {} (y)
    (x) edge node  {} (nature)
    ;
  \end{tikzpicture} \\
\vspace{1cm}
\begin{itemize}
  \item In a strongly simplified world an arbitrary outcome $y$ is produced by the nature given the covariates $x$
  \item The knowledge about the natures true mechanisms range between entirely unknown and established (scientific) explanations of the mechanism
  \item Example: Outcome $y$ is the rent for apartments and covariates $x$ are size, number of bathrooms and location
\end{itemize}
\end{center}

\framebreak

\begin{itemize}
  \item Focuses on the modeling of data, which can be reduced to two targets:
  \begin{enumerate}
    \item Learn a model to predict the outcome for new covariates
    \item Get a better understanding about the relationship between covariates and outcome
  \end{enumerate}
\end{itemize}
\begin{center}
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1cm,
        thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}]
      \node[punkt] (natur) {Logistic Regression, \\Cox Model, \\GEE, \\ \ldots};
      \node[left=of natur] (x) {x};
      \node[right=of natur] (y) {y};
      \path[every node/.style={font=\sffamily\small}]
      (natur) edge node {} (y)
      (x) edge node  {} (natur)
      ;
    \end{tikzpicture}
  \end{center}
\begin{itemize}
  \item Find a stochastic model of the data-generating process:
  $$y = f(x, \text{parameters}, \text{random error})$$
\end{itemize}

\framebreak

In this \enquote{data modeling culture} a stochastic model for the data- generating process is assumed
\begin{blocki}{Typical assumptions and restrictions}
  \item Specific stochastic model that generated the data
  \item Distribution of residuals
  \item Linearity (e.g. linear predictor)
  \item Manual specification of interactions
\end{blocki}

\framebreak

\begin{blocki}{Problems}
  \item Conclusions about model, not about nature
  \item Assumptions often violated
  \item Often improper model evaluation\\
  $\Rightarrow$ can lead to irrelevant theory and questionable statistical conclusions
  \item Focus not on prediction
  \item Data models fail in areas like image and speech recognition
\end{blocki}

\framebreak

\textbf{Machine Learning, the Algorithmic Modeling Culture}
\lz
  \begin{center}
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1cm,
        thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}]
      \node[punkt] (natur) {unknown};
      \node[left=of natur] (x) {x};
      \node[right=of natur] (y) {y};
      \node[below=of natur, label=below:algorithm] (algo) { \includegraphics*[width = 3cm]{figure_man/machine.png}};
      \path[every node/.style={font=\sffamily\small}]
      (natur) edge node {} (y)
      (x) edge node  {} (natur)
      (x) edge[<->, bend right=30] node {} (algo)
      (y) edge[<->, bend left=30] node {} (algo)
      ;
    \end{tikzpicture}
  \end{center}
\lz
Find a function $\fx$ that minimizes the loss: $\Lxy$

\framebreak

\begin{itemize}
  \item In the \enquote{algorithmic modeling culture}, the true mechanism is treated as unknown
  \item It is not the target to find the true data-generating mechanism but to use an algorithm that imitates the mechanism as good as possible
  \item Modeling is reduced to a mere problem of function optimization: Given the covariates $x$, outcome $y$ and a loss function find a function $\fx$ which minimizes the loss for the prediction of the outcome
\end{itemize}
\begin{blocki}{Algorithm in Machine Learning}
  \item Boosting
  \item Support Vector Machines
  \item Artificial neural networks
  \item Random Forests
  \item Hidden Markov
  \item Bayes-Nets
  \item \ldots
\end{blocki}

\framebreak
\ \\
\lz
\structure{Summary}\\
\emph{The data modeling culture tries to find the true data-generating mechanism, the algorithmic modeling culture tries to imitate the true mechanism as good as possible.}\\
\vspace{1cm}
\structure{Rashomon Effect}\\
\emph{(Often) Many different models describe a situation equally accurate which makes it difficult to find the true mechanism in the data modeling cultures.}

\framebreak

\begin{blocki}{Dimensionality of the data}
  \item The higher the dimensionality of the data (\# covariates) the more difficult is the separation of signal and noise
  \item Common practice in data modeling: variable selection (by expert selection or data driven) and reduction of dimensionality (e.g. PCA)
  \item Common practice in algorithmic modeling: Engineering of new features (covariates) to increase predictive accuracy; algorithms robust for many covariates
\end{blocki}

\framebreak

\textbf{Prediction vs. Interpretation}\\
\vspace{1.5cm}
\begin{center}
  \begin{tikzpicture}
    \draw (0,0) -- (0,1.5) -- (9.9,0) -- (0,0);
    \draw  (0, 1.6) -- (10,1.6) -- (10, 0.1) -- (0, 1.6);
    \node at (1.5, 0.2) {Interpretability};
    \node at (8, 1.3) {Predictive accuracy};
    \node at (1.3, -0.5) {Tree};
    \node at (9, -0.5) {Random Forest};
    \node at (1.5, -1.2) {Logistic Regression};
    \node at (9, -1.2) {Neural networks};
    \node at (1.3, -1.9) {\ldots};
    \node at (9, -1.9) {\ldots};
  \end{tikzpicture}
\end{center}

\framebreak

\begin{itemize}
  \item There is a trade-off between interpretability and predictive accuracy: the models that are good in prediction are often complex and models that are easy to interpret are often bad predictors
  \item Example trees and random forests: A single decision tree is very intuitive and easy to read for non-professionals, but they are unstable and give weak predictions while a complex aggregation of decision trees (random forest) has an excellent prediction accuracy, but it is impossible to interpret the model structure
\end{itemize}

\framebreak

\begin{blocki}{Goodness of model}
  \item Data modeling culture: Goodness of fit often based on model assumptions (e.g. AIC) and calculated on training data
  \item Algorithmic modeling culture: Evaluation of predictive accuracy with an extra test set or cross validation
\end{blocki}
How good is a statistical model if the predictive accuracy is weak? Is it legit to interpret parameters and p-values?

\framebreak

Different notation for machine learning and statistics
\lz
  \begin{table}
    \begin{tabular}{rr}
      \hline
      Machine Learning & Statistics \\
      \hline
      Feature,Attribute & Covariate \\
      Minimizing loss & Maximizing likelihood \\
      Learning & Fitting/Estimation \\
      Weights & Parameter/Coefficient \\
      Bias term & Intercept \\
      Hypothesis & Model \\
      Example/Instance & Observation \\
      Supervised Learning & Regression/Classification \\
      Label & Response \\
      Data mining (good) & Data mining (bad)\\
      Log. regr. is classification & Log. regr. is regression\\
      \hline
    \end{tabular}
  \end{table}
\end{vbframe}

% \section{How can we learn?}

\begin{frame}{To date or not to date?}

 \begin{table}
 \small
    \begin{tabular}{cccccc}
      \hline
      Nr & Day of Week & Type of Date & Weather & TV Tonight & Date?\\
      \hline
      1 & Weekday & Dinner & Warm & Bad & Yes \\
      2 & Weekend & Club & Warm & Good & No \\
      3 & Weekend & Club & Warm & Bad & No \\
      4 & Weekend & Club & Cold & Bad & Yes \\
      Now & Weekend & Club & Cold & Good & ? \\
      \hline
    \end{tabular}
  \end{table}

 \only<2>{
 \lz
 \textbf{Some terminology}
 \begin{itemize}
    \item Rows: \textit{Instances, Examples} (labelled/unlabeled)
    \item Columns: \textit{Factors, Features, Attributes}
    \item Last column: Target feature
    \item First column: Identifier (Never give this to the learner!)
    \item Other columns: Predictive features
  \end{itemize}
 }
  \begin{itemize}
    \item<3-> Is there one factor that perfectly predicts the answer?
    \item<4-> What about a \textit{conjunction of factors}?
    \begin{itemize}
     \item<5-> Warm \& Weekend $\rightarrow$ No date today :(
     \item<5-> Dinner \& TV Bad $\rightarrow$ Date :)
     \item<6-> Club \& Warm $\rightarrow$ No date...
     \item<6-> Weekday \& TV Bad $\rightarrow$ Date...
    \end{itemize}
    \item<6-> There's no way to know!
  \end{itemize}

\end{frame}

\begin{vbframe}{How can we learn?}
\textbf{Hume's problem of induction (David Hume, 1748)} \\
\lz
\textit{How can we ever be justified in generalizing from what we've seen to what we haven't?} \\
\begin{itemize}
    \item You have no basis to pick one generalization over the other
    \item Big data (Casanova-approach) won't help: answer may depend on a factor you didn't consider
    \item What if the answer is just random?
\end{itemize}

\framebreak
\textit{What if we just assume that the future will be like the past?}
\lz
\begin{itemize}
  \item Risky assumption (e.g. inductivist turkey)
  \item Still the best we can do (and generally seems to work)
  \item Even then, this only helps if we have seen the exact same situation before
  \item The machine learning problem remains: How do we generalize from cases that we haven't seen before?
    \begin{itemize}
      \item What if somebody types a unique Google query?
      \item What if a patient comes in with slightly different symptoms?
      \item What if someone writes a new unique spam email?
    \end{itemize}
  \item Even with all the data in the world, your chances or finding the exact same case are almost zero. We need induction.
\end{itemize}
\end{vbframe}

\begin{vbframe}{No Free Lunch Theorem (David Wolpert, 1997)}
\textit{If all functions $f(x) = y$  are equally likely, all algorithms that aim to optimize that function have identical performance.} \\
\lz
\begin{itemize}
\item Sets a limit on how good a learner can be: no learner can be better than random guessing!
\item But then why is the world full of highly successful learners?
\item For every world where a learner does better than random guessing, we can construct an anti-world by flipping the labels of all \emph{unseen} instances: it performs worse by the same amount
\item We \emph{don't care} about all possible worlds, only the one we live in
\item We assume we know something about this world that gives us an advantage. That knowledge is fallible, but it's a risk we'll have to take
\end{itemize}
\end{vbframe}

\begin{vbframe}{The futility of bias-free learning}
Practical consequence: \textit{There's no such thing as learning without knowledge (assumptions).} Data alone is not enough.
\lz
\begin{itemize}
\item We need to provide \emph{prior knowledge} to the algorithm, or make assumptions when constructing hypotheses
  \begin{itemize}
  \item The structure of a neural net, a Bayesian prior, background knowledge as rules, the way a tree represents knowledge,...
  \end{itemize}
\lz
\item These assumptions are called a learner's \textit{bias}, i.e. \emph{bias-free learning} is impossible.
  \begin{itemize}
  \item Every new piece of knowledge is the basis for more knowledge.
  \item What assumptions can we start from that are not too strong?
  \end{itemize}
\end{itemize}
\framebreak
\begin{itemize}
\item Newton's Principle of induction: \textit{Whatever is true of everything we've seen, is true for everything in the universe}
\lz
\item We induce the most widely applicable rules we can, and reduce scope only when the data forces us to.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Learning = Representation + Evaluation + Optimization}
All learners consist of three main components (all introducing bias):
\begin{itemize}
\item \textbf{Representation}: A model must be represented in a formal language that the computer can handle.
\begin{itemize}
\item Defines the concepts it can learn: \textit{The hypothesis space}
\end{itemize}
\item \textbf{Evaluation}: How to choose one hypothesis over the other?
\begin{itemize}
\item The evaluation function, objective function, scoring function
\item Can differ from the external evaluation function (e.g. accuracy)
\end{itemize}
\item \textbf{Optimization}: How do we search the hypothesis space?
\begin{itemize}
\item Key to the efficiency of the learner
\item Defines how many optima it finds
\item Often starts from most simple hypothesis, relaxing it if needed to explain the data
\end{itemize}
\end{itemize}
\end{vbframe}

\begin{vbframe}{A dating algorithm}
 \begin{table}
 \small
    \begin{tabular}{cccccc}
      \hline
      Nr & Day of Week & Type of Date & Weather & TV Tonight & Date?\\
      \hline
      1 & Weekday & Dinner & Warm & Bad & Yes \\
      2 & Weekend & Club & Warm & Good & No \\
      3 & Weekend & Club & Warm & Bad & No \\
      4 & Weekend & Club & Cold & Bad & Yes \\
      Now & Weekend & Club & Cold & Good & ? \\
      \hline
    \end{tabular}
  \end{table}

\begin{itemize}
\item Representation: conjunctions of factors (conjunctive concepts)
\item Optimization: start with best 1-factor concept, then add best other factor on \emph{remaining} data
\begin{itemize}
\item Don't try all combinations (combinatorial explosion)!
\end{itemize}
\item Evaluation: exclude most bad matches and fewest good ones
\item Result: Weekend $\wedge$ Warm ($\rightarrow$ No date)
\item Thoughts?
\end{itemize}
\end{vbframe}

% \begin{vbframe}{Learning sets of rules (Michalski)}

% \begin{itemize}
% \item Real concepts are disjunctive $\rightarrow$ \emph{sets} of rules
% \begin{itemize}
% \item Credit card used in 3 different continents yesterday $\rightarrow$ stolen
% \item Credit card used twice after 23:00 on weekday $\rightarrow$ stolen
% \item Credit card used to buy 1 dollar of gas $\rightarrow$ stolen
% \end{itemize}
% \item Divide and conquer approach: build rule covering as many positive examples as possible, discard all positive examples that it covers, repeat until all are covered
% \item Sets of rules can represent \emph{any} concept. How?
% \begin{itemize}
% \item Just turn each positive instance into a rule using all factors
% \item Weekend $\wedge$ Club $\wedge$ Warm $\wedge$ Bad $\rightarrow$ Yes
% \end{itemize}
% \item 100\% accurate rule? Or an illusion?
% \begin{itemize}
% \item Every new (unseen) example will be negative
% \item No free lunch: you can't learn without assuming anything
% \end{itemize}
% \end{itemize}
% \end{vbframe}


\begin{vbframe}{Different `tribes' of Machine Learning}

\small
Rival schools of thought in machine learning, each with core beliefs, and with distinct strategy to learn \textit{anything}:
  \begin{itemize}
    \item \textbf{Symbolic Learning}: Express, manipulate symbolic knowledge
      \begin{itemize} \item Rules and trees \end{itemize}
    \item \textbf{Neural Networks} (Connectionism): Mimick the Human brain
      \begin{itemize} \item Neural Nets \end{itemize}
    \item \textbf{Evolution}: Simulate the evolutionary process
      \begin{itemize} \item Genetic algorithms \end{itemize}
    \item \textbf{Probabilistic (Bayesian) Inference}: Reduce uncertainties by incorporating new evidence
      \begin{itemize} \item Graphical models, Gaussian processes \end{itemize}
    \item \textbf{Learning by Analogy}: Recognize geometric similarities between points
      \begin{itemize} \item kNN, Support Vector Machines \end{itemize}
\end{itemize}
\normalsize
\end{vbframe}

\begin{vbframe}{Symbolic Learning}
\begin{itemize}
 \item All intelligence can be reduced to manipulating symbols
 \item Can incorporate preexisting knowledge (e.g. as rules)
 \item Can combine knowledge, data, to fill in gaps (like scientists)
\end{itemize}

 \begin{table}
 \small
    \begin{tabular}{l|l}
      \hline
      Representation & Rules, trees, first order logic rules\\
      \hline
      Evaluation & Accuracy, information gain\\
      \hline
      Optimization & Top-down induction, inverse deduction\\
      \hline
      Algorithms & Decision trees, Logic programs\\
      \hline
    \end{tabular}
  \end{table}

<<results='hide', include=FALSE>>=
data("iris")
tree_iris <- ctree(Species ~ ., data = iris)
@

\begin{center}
<<results='hide'>>=
plot(tree_iris)
@
\end{center}
\end{vbframe}

% \begin{vbframe}{Symbolic Learning}
% \begin{center}
%   Socrates is human + humans are mortal = ? \\
% \lz
%   Socrates is human + ? = Socrates is mortal
% \end{center}
% \lz

% \begin{itemize}
% \item Background knowledge (e.g. gene interactions, metabolic pathways) can be expressed in first-order knowledge
% \item Inverse deduction can infer new hypotheses
% \item Robot scientist: learns hypotheses, then designs and runs experiments to test hypotheses
% \end{itemize}
% \end{vbframe}

\begin{vbframe}{Neural Networks}
\begin{itemize}
 \item Learning is what the brain does: reverse-engineer it
 \item Adjust strengths of connection between neurons
 \item Can handle raw, high-dimensional data, constructs it own features
\end{itemize}

 \begin{table}
 \small
    \begin{tabular}{l|l}
      \hline
      Representation & Neural network\\
      \hline
      Evaluation & Squared error\\
      \hline
      Optimization & Gradient descent\\
      \hline
      Algorithms & Backpropagation\\
      \hline
    \end{tabular}
  \end{table}
\end{vbframe}

\begin{vbframe}{Neural Networks}
\begin{itemize}
 \item Hebbian learning: Neurons that fire together, wire together
 \item Backpropagation: assigns 'blame' for errors to neurons earlier in the network
 \item Many applications (deep learning).
\end{itemize}
  \center
  \includegraphics[width=.6\textwidth]{figure_man/backprop.png}

\end{vbframe}

\begin{vbframe}{Evolution}
\begin{itemize}
 \item Natural selection is the mother of all learning
 \item Simulate evolution on a computer
 \item Can learn \emph{structure}, e.g. the shape of a brain
\end{itemize}

 \begin{table}
 \small
    \begin{tabular}{l|l}
      \hline
      Representation & Genetic programs (often trees) \\
      \hline
      Evaluation & Fitness function\\
      \hline
      Optimization & Genetic search\\
      \hline
      Algorithms & Genetic programming (crossover, mutation)\\
      \hline
    \end{tabular}
  \end{table}
\end{vbframe}

\begin{vbframe}{Probabilistic (Bayesian) Learning}
\begin{itemize}
 \item Learning is a form of uncertain inference
 \item Uses Bayes' theorem to incorporate new evidence into our beliefs
 \item Can deal with noisy, incomplete, contradictory data
\end{itemize}

 \begin{table}
 \small
    \begin{tabular}{l|l}
      \hline
      Representation & Graphical models, Markov networks \\
      \hline
      Evaluation & Posterior probability\\
      \hline
      Optimization & Probabilistic inference\\
      \hline
      Algorithms & MCMC\\
      \hline
    \end{tabular}
  \end{table}
\end{vbframe}

\begin{vbframe}{Bayesian Learning}
\begin{itemize}
 \item Choose hypothesis space + \textbf{prior} for each hypothesis
 \item As evidence comes in, update probability of each hypothesis
 \item \textbf{Posterior}: how likely is hypotheses after seeing the data
\end{itemize}
\begin{center}
  \includegraphics[width=0.8\textwidth]{figure_man/bayes.png}
\end{center}
\end{vbframe}


\begin{vbframe}{Learning by Analogy}
\begin{itemize}
 \item Learning is recognizing similarities between situations and inferring other similarities
 \item Generalizes from similarity
 \item Transfer solution from previous situations to new situations
\end{itemize}

 \begin{table}
 \small
    \begin{tabular}{l|l}
      \hline
      Representation & Memory, support vectors \\
      \hline
      Evaluation & Margin\\
      \hline
      Optimization & Kernel machines\\
      \hline
      Algorithms & Nearest Neighbor, Support Vector Machines\\
      \hline
    \end{tabular}
  \end{table}
\end{vbframe}

\begin{vbframe}{Nearest neighbors}
\begin{itemize}
 \item Given cities belonging to 2 countries. Where is the border?
 \item Nearest neighbor: point belongs to closest cities
 \item k-Nearest neighbor: do vote over k nearest ones (smoother)
\end{itemize}
\begin{center}
  \includegraphics[width=.5\textwidth]{figure_man/nn.png}
\end{center}
\end{vbframe}
\endlecture






<<setup-child2, include = FALSE>>=
library('kknn')
n = 30
set.seed(1234L)
tar = factor(c(rep(1L, times = n), rep(2L, times = n)))
feat1 = c(rnorm(n, sd = 1.5), rnorm(n, mean = 2, sd = 1.5))
feat2 = c(rnorm(n, sd = 1.5), rnorm(n, mean = 2, sd = 1.5))
bin.data = data.frame(feat1, feat2, tar)
bin.tsk = makeClassifTask(data = bin.data, target = "tar")
@
\lecturechapter{2}{Learning Theory I - Loss minimization, Simple Models and Information Theory}
\lecture{Fortgeschrittene Computerintensive Methoden}

\begin{vbframe}{Fundamental definitions and notation}

\begin{itemize}
  \item $\Xspace$ $p$-dim. input space, usually we assume $\Xspace = \R^p$, but
    categorical features can occur, too
\item $\Yspace$ target space,  \\
  e. g. $\Yspace = \R$, $\Yspace = \lbrace 0, 1 \rbrace$, $\Yspace = \lbrace -1, 1 \rbrace$, $\Yspace = \gset$, $\Yspace = \lbrace \textrm{label}_1 \ldots \textrm{label}_g \rbrace$
\item $x = \xvec \in \Xspace$ observation
\item $y \in \Yspace$ dependent variable (target, label, output)
\item $\P_{xy}$ joint probability distribution on $\Xspace \times \Yspace$
\item $\pdfxy$ or $\pdfxyt$ joint pdf for $x$ and $y$
\end{itemize}

\remark{
This lecture is mainly developed from a frequentist perspective. If parameters appear behind the |, this is
for better reading, and does not imply that we condition on them in a Bayesian sense (but this notation
would actually make a Bayesian treatment simple).
So formally, $p(x | \theta)$ should be read as $p_\theta(x)$ or $p(x, \theta)$ or $p(x; \theta)$.
}

\framebreak

\begin{itemize}
\item $\D = \Dset$ data set with $n$ observations
\item $\xyi$ $i$-th observation
\item $\Dtrain$, $\Dtest$ data for training and testing, often $\D = \Dtrain \dot{\cup} ~ \Dtest$
\item $\fx$ or $\fxt \in \R$ prediction function learnt on data, we might suppress $\theta$ in notation
\item $\hx$ or $\hxt \in \Yspace$ discrete prediction for classification (see later)
\item $\theta \in \Theta$ model parameters\\
  (some models may traditionally use different symbols)
\item $\Hspace$ hypothesis space, $f$ lives here, restricts the functional form of $f$
\item $\eps = y - \fx$ or $\epsi = \yi - \fxi$, residual in regression
\item $\yf$ or $\yfi$, margin for binary classification with  $\Yspace = \{-1, 1\}$ (see later)
\end{itemize}

\framebreak

\begin{itemize}
\item $\pikx = \postk$, posterior probability for class $k$, given $x$, in case of binary labels we might abbreviate
  $\pix = \post$
\item $\pi_k = \P(y = k)$, prior probability for class $k$, in case of binary labels we might abbreviate
  $\pi = \P(y = 1)$
\item $\LLt$ and $\llt$, Likelihood and log-Likelihood for a parameter $\theta$, based on a statistical model
\item $\fh$, $\hh$, $\pikxh$, $\pixh$ and $\thetah$, learned functions and parameters
% \item $\phi(x | \mu, \sigma^2)$, $\Phi(x | \mu, \sigma^2)$, pdf and cdf of the univariate normal distribution,
  % with $\phi(x)$ and $\Phi(x)$ for the N(0,1) standard normal case,
  % and $\phi(x | \mu, \Sigma)$ (or $\phi_p(x)$) for the (standard) multivariate case in $p$ dimensions

\end{itemize}

\lz

\remark{
With a slight abuse of notation we write random variables, e.g., $x$ and $y$, in lowercase, as normal
variables or function arguments. The context will make clear what is meant.
}

\framebreak



In the simplest case we have i.i.d. data $\D$, where the input and output space are both real-valued and one-dimensional.

\vspace{0.5cm}

<<fig.height=4>>=
qplot(wt, mpg, data = mtcars) + xlab("x") + ylab("y")
@

\framebreak

Design matrix and intercept term:

\begin{minipage}{0.45\textwidth}
$$
X  = \mat{x_1^{(1)} & \cdots & x_p^{(1)} \\
          \vdots    & \vdots & \vdots \\
          x_1^{(n)} & \cdots & x_p^{(n)}}
$$
\end{minipage}
\begin{minipage}{0.45\textwidth}
$$
X  = \mat{1      & x_1^{(1)} & \cdots & x_p^{(1)} \\
          \vdots & \vdots    & \vdots & \vdots \\
          1      & x_1^{(n)} & \cdots & x_p^{(n)}}
$$
\end{minipage}

\begin{itemize}
  \item $\xjb = \xjvec$ j-th observed feature vector.
  \item $\ydat = \yvec$ vector of target values.
  \item The right design matrix demonstrates the trick to encode the intercept via an additional
    constant-1 feature, so the feature space will be $(p+1)$-dimensional.
    This allows to simplify notation, e.g., to write $\fx = \theta^T x$, instead
    of $\fx = \theta^T x + \theta_0$.
\end{itemize}



\end{vbframe}

\begin{vbframe}{Binary label coding}

\remark{
Notation in binary classification can be sometimes confusing because of different coding styles,
and as we have to talk about predicted scores, classes and probabilities.
}

\lz

A binary variable can take only two possible values.
For probability / likelihood-based model derivations a 0-1-coding, for geometric / loss-based models
the -1+1-coding is often preferred.

\begin{itemize}
\item $\Yspace = \{0, 1\}$. Here, the approach often models $\pix$, the posterior probability for class 1 given $x$.
  Usually, we then define $\hx = [\pix \geq 0.5] \in \Yspace$.
\item $\Yspace = \{-1, 1\}$. Here, the approach often models $\fx$, a real-valued score from $\R$ given x.
  Usually, we define $\hx = \sign(\fx) \in \Yspace$, and we interpret
  |\fx| as \enquote{confidence} for the predicted class $\hx$.
\end{itemize}

\lz


\end{vbframe}


\begin{vbframe}{Loss minimization}

The goodness of the prediction $y=\fx$ is measured by a \emph{loss function} $\Lxy $
  and its expectation, the so-called \emph{risk},
  $$ \riskf = \E [\Lxy] = \int \Lxy d\Pxy. $$

  Obvious aim: Minimize $\riskf$ over $f$. But this is not (in general) practical:

\begin{itemize}
\item $\Pxy$ is unknown.
\item We could estimate $\Pxy$ in non-parametric fashion from the data $D$, e.g., by kernel density
  estimation, but this really does not scale to higher dimensions (see curse of dimensionality).
\item We can efficiently estimate $\Pxy$, if we place rigorous assumptions on its distributional form,
  and methods like discriminant analysis work exactly this way. ML usually studies more flexible models.
\end{itemize}


\framebreak

An alternative (without directly assuming something about $\P_{xy}$) is to approximate $\riskf$ based on
the data $\D$, by means of the \emph{empirical risk}

$$
\riske(f) = \frac{1}{n} \sumin \Lxyi
$$

Learning then amounts to \emph{empirical risk minimization}
$$
\fh = \argmin_{f \in \Hspace} \riske(f).
$$

\framebreak

When $f$ is parameterized by $\theta$, this becomes:

\begin{eqnarray*}
\riske(\theta) & = & \frac{1}{n} \sumin \Lxyit \cr
\hat{\theta} & = & \argmin_{\theta \in \Theta} \riske(\theta)
\end{eqnarray*}

Thus learning (often) means solving the above \emph{optimization problem}.
Which implies a tight connection between ML and optimization.

Note that (with a slight abuse of notation), if it is more convenient, and as there is no difference w.r.t.
the minimizer, we might also define the $\riske$ in its non-average-but-instead-summed version as:

$$
\risket = \sumin \Lxyit
$$

\framebreak

\begin{itemize}
\item For regression, the loss usually only depends on residual $\Lxy = L(y - \fx) = L(\eps)$,
  this is a \emph{translation invariant} loss
\item Choice of loss decides statistical properties of $f$: Robustness, error distribution (see later)
\item Choice of loss decides computational / optimization properties of minimization of $\risket$:
  Smoothness of objective, can gradient methods be applied, uni- or multimodality. \\
  If $\Lxy$ is convex in its second arguments, and $\fxt$ is linear in $\theta$, then $\risket$ is convex.
  Hence every local minimum of $\risket$ is a global one. If $\Lxy$ not convex,
  R might have multiple local minima (bad!).
\end{itemize}
\end{vbframe}


\begin{vbframe}{Regression losses - L2 squared loss}
\begin{itemize}
\item $\Lxy = (y-\fx)^2$ or $\Lxy = 0.5 (y-\fx)^2$
\item Convex
\item Differentiable, gradient no problem in loss minimization
\item For latter: $\pd{0.5 (y-\fx)^2}{\fx} = y - \fx = \eps$, derivative is residual
\item Tries to reduce large residuals (if residual is twice as large, loss is 4 times as large), hence
  outliers in $y$ can become problematic
\item Connection to Gaussian distribution (see later)
\end{itemize}

<<echo=FALSE, results='hide', fig.height=3>>=
x = seq(-2, 2, by = 0.01); y = x^2
qplot(x, y, geom = "line", xlab = expression(y-f(x)), ylab = expression(L(y-f(x))))
@

\framebreak

According to the law of total expectation
\begin{displaymath}
  \E_{xy} [\Lxy] = \E_x
  \left[\E_{y|x}[(y-\fx)^2|x=x]\right]
\end{displaymath}
For the optimal prediction of $f$ holds
$$
  \fh(x) = \mbox{argmin}_c \E_{y|x}[(y-c)^2|x=x]=\E (y | x=x)
$$

So for squared loss the best prediction in every point is the conditional expectation of $y$ given $x$.

\lz

The last step follows from:
$$
E[(y - c)^2] = Var[y - c] + (E[y - c])^2 = Var[y] + (E[y] - c)^2
$$
This is obviously minimal for $c = E[y]$.



\end{vbframe}


\begin{vframe}{Regression losses - L1 absolute loss}
\begin{itemize}
\item $\Lxy = |y-f(x)|$
\item Convex
\item No derivatives for $r = 0$, $y = f(x)$, optimization becomes harder
\item More robust, outliers in $y$ are less problematic
\item $\fh(x) = \text{median of } y | x$
\item Connection to Laplace distribution (see later)
\end{itemize}

<<echo=FALSE, results='hide', fig.height=3, fig.align='center'>>=
x = seq(-2, 2, by = 0.01); y = abs(x)
qplot(x, y, geom = "line", xlab = expression(y-f(x)), ylab = expression(L(y-f(x))))
@
\end{vframe}

\begin{vframe}{Regression losses - Huber loss}
\begin{itemize}
\item Huber loss $L_\delta(y, f(x)) =
  \begin{cases}
  \frac{1}{2}(y-f(x))^2  & \text{ if } |y-f(x)| \le \delta \\
  \delta |y-f(x)|-\frac{1}{2}\delta^2 \quad & \text{ otherwise }
  \end{cases}$
\item Piecewise combination of of L1 and L2 loss
\item Convex
\item Combines advantages of L1 and L2 loss: differentiable, robust
\end{itemize}

<<echo=FALSE, results='hide', fig.height=3, fig.align='center'>>=
x = seq(-2, 2, by = 0.01); y = ifelse(abs(x) <= 1, 1 / 2 * x^2, abs(x) - 1 / 2)
qplot(x, y, geom = "line", xlab = expression(y-f(x)), ylab = expression(L(y-f(x))))
@
\end{vframe}

\begin{vbframe}{Classification losses}


% For classification problems, i. e. output variable $y \in \{-1, 1\}$ the risk can be written as


% For categorical output variable $y\in \gset$ we can use the \emph{0-1-loss}

% $$
% L(y, h(x)) =\I_{y \ne h(x)} =
   % \footnotesize \begin{cases} 1 \quad \text{ if } y \ne h(x) \\ 0 \quad    \text{ if } y = h(x)  \end{cases}
% $$

% by applying \emph{Bayes theorem}

% \begin{eqnarray*}
% p(x, y) = p(y|x)p(x).
% \end{eqnarray*}

% \framebreak

% This can be simplified to

% $$
% \risk(f) = \int_X [L(1, f(x)) \pix + L(-1, f(x)) (1 - \pix) p(x)] dx.
% $$

% For given $x$, one can minimize the risk pointwise for any convex loss function by differentiating the last equality with respect to $f$. Thus, minimizers for all of the loss functions are easily obtained as functions of only $f(x)$ and $\pi(x)$.

% \framebreak

We will now introduce some loss functions, mainly for binary output.

Notice that $f(x)$ outputs a score and $\sign(\fx)$ will be the corresponding label.

\lz

Most following loss functions will depend on the so-called \emph{margin}
$$
y\fx =  \begin{cases} > 0  \quad &\text{ if } y = \sign(\fx) \text{ (correct classification)} \\
                      < 0 \quad &\text{ if } y \ne \sign(\fx) \text{ (misclassification)} \end{cases}
$$


% \begin{eqnarray*}
% \risk(f) &=& \E_{xy}[\Lxy] = \int_X \int_Y \Lxy p(x, y) dy~dx \\
% &=& \int_X \int_Y \Lxy p(y|x)p(x) dy~dx
% \end{eqnarray*}

\framebreak


\end{vbframe}


\begin{vbframe}{Bin. classif. losses - 0-1 loss}
\begin{itemize}
\item $\Lxy = [y \neq f(x)] = [\yf < 0]$
\item Intuitive, often what we are interested in
\item Not even continuous, even for linear $f$ the optimization problem is NP-hard and
  close to intractable
\end{itemize}

<<echo=FALSE, results='hide', fig.height=3, fig.align='center'>>=
x = seq(-2, 2, by = 0.01); y = as.numeric(x < 0)
qplot(x, y, geom = "line", xlab = expression(yf(x)), ylab = expression(L(yf(x))))
@
\end{vbframe}

\begin{vbframe}{Multiclass 0-1 loss and Bayes classifier}

Assume $h \in \Yspace$ with $|\Yspace| = g$. We can define the 0-1-loss for multiclass:
$$L(y, \hx) = [y \neq \hx]$$.
We can in general rewrite the loss again as:
\begin{eqnarray*}
  \risk(h) & = & \E_{xy}[L(y, h)] = E_x [ E_{y|x} [ L(y, \hx) ] ] =  \\
           & = & E_x \sum_{k \in \Yspace} L(k, \hx) P(y = k| x = x) \\
           & = & E_x \sum_{k \in \Yspace} L(k, \hx) \pikx
\end{eqnarray*}

NB: This works, too, (of course) for $\Yspace = \{-1, 1\}$ and a score function $f$:
$$
\risk(f) = \mathbb{E}_x [L(1, f(x)) \pix + L(-1, f(x)) (1 - \pix)].
$$

\framebreak
We can again minimize pointwise, and for a general cost-sensitive loss $L(y, h)$ this is:

\begin{eqnarray*}
  \hxh &=& \argmin_{l \in \Yspace} \sum_{k \in \Yspace} L(k, l) \pikx \\
\end{eqnarray*}

For the 0-1 loss this becomes:

$$
\hxh = \argmin_{k \in \Yspace} (1 - \pikx) = \argmax_{k \in \Yspace} \pikx
$$

If we know $\Pxy$ perfectly (and hence $\pikx$), we have basically constructed the loss-optimal
classifier and we call it the \emph{Bayes classifier} and its expected loss the \emph{Bayes loss} or
\emph{Bayes error rate} for 0-1-loss.


% and get the risk function


% The minimizer of $\risk(f)$ for the 0-1-loss is

% \begin{eqnarray*}
% \fh(x) &=&    \footnotesize \begin{cases} 1 \quad \text{ if } \pix > 1/2 \\ -1 \quad \pix < 1/2  \end{cases}
% \end{eqnarray*}

\lz


\end{vbframe}


% \framebreak

% \textbf{Square-loss:}


% If we use instead the \emph{square-loss}

% $$
% \Lxy = (1-y\fx)^2,
% $$

% we get the risk function

% \begin{eqnarray*}
% \risk(f) &=& \mathbb{E}_x [(1-\fx)^2 \pix + (1+\fx)^2 (1-\pix)] \\
% &=& \mathbb{E}_x [1 + 2\fx + \fx^2-4\fx\pix].
% \end{eqnarray*}

% By differentiating w. r. t. $f(x)$ we get the minimizer of $\risk(f)$ for the square loss function

% \begin{eqnarray*}
% \fh(x) &=& 2\pix -1.
% \end{eqnarray*}

% \framebreak

% \vspace*{0.2cm}

% The square loss function tends to penalize outliers excessively. Functions which yield high values of $(x)$ will perform poorly with the square loss function, since high values of $yf(x)$ will be penalized severely, regardless of whether the signs of $y$ and $f(x)$ match.

% <<echo=FALSE, results='hide', fig.height= 1.8, fig.asp = 0.4>>=
% x = seq(-2, 5, by = 0.01)
% plot(x, (1-x)^2, type = "l", xlab = expression(yf(x)), ylab = expression(paste((1-yf(x))^2)), main = "Square loss")
% box()
% @

% \lz

% \end{vbframe}

\begin{vbframe}{Bin. classif. losses - Hinge loss}
\begin{itemize}
\item $\Lxy = \max\{0, 1 - \yf\}$, used in SVMs
\item Convex
\item No derivatives for $\yf = 1$, optimization becomes harder
\item More robust, outliers in $y$ are less problematic
\end{itemize}

<<echo=FALSE, results='hide', fig.height=3, fig.align='center'>>=
x = seq(-2, 2, by = 0.01); y = pmax(0, 1 - x)
qplot(x, y, geom = "line", xlab = expression(yf(x)), ylab = expression(L(yf(x))))
@

% \framebreak

% we get the risk function

% \begin{eqnarray*}
% \risk(f) &=& \mathbb{E}_x [\max\{0, 1 - \fx\} \pix + \max\{0, 1 + y\fx\} (1-\pix)].
% \end{eqnarray*}

% The minimizer of $\risk(f)$ for the hinge loss function is

% \begin{eqnarray*}
  % $fh(x) =  \footnotesize \begin{cases} 1 \quad \text{ if } \pix > 1/2 \\ -1 \quad \pix < 1/2  \end{cases}$
% \end{eqnarray*}

\end{vbframe}

\begin{vbframe}{Bin. classif. losses - Logistic loss}
\begin{itemize}
  \item $\Lxy = \ln(1+\exp(-y\fx))$, used in logistic regression
  \item Also called Bernoulli loss
\item Convex, differentiable, not robust
\end{itemize}

<<fig.height=3>>=
x = seq(-2, 2, by = 0.01); y = log(1 + exp(-x))
qplot(x, y, geom = "line", xlab = expression(yf(x)), ylab = expression(L(yf(x))))
@
% the minimizer of $\risk(f)$ for the logistic loss function is

% \begin{eqnarray*}
% \fh(x) &=&  \ln \biggl(\frac{\pix}{1-\pix}\biggr)
% \end{eqnarray*}

% The function is undefined when $\pix = 1$ or $\pix = 0$, but predicts a smooth curve which grows when $\pix$ increases and equals $0$ when $\pix = 0.5$


\end{vbframe}

\begin{vbframe}{Bin. classif. losses - Cross-entropy loss}
\begin{itemize}
  \item Using the alternative label convention $y\in \{0, 1\}$
  \item $\Lxy = -y\ln(\pix)-(1-y)\ln(1-\pix)$
  \item Basically the same as the logistic loss when we act on $\pix \in [0,1]$ instead of $\fx \in \R$
  \item  The cross entropy loss is closely related to the Kullback-Leibler divergence, which will be introduced later in the chapter.
  \item Very often used in neural networks with binary output nodes for classification.
\end{itemize}


\end{vbframe}


\begin{vbframe}{Bin. classif. losses - Exponential loss}
\begin{itemize}
  \item $\Lxy = \exp(-y\fx)$, used in AdaBoost
\item Convex, differentiable, not robust
\item Quite similar to logistic loss
\end{itemize}

<<fig.height=3>>=
x = seq(-2, 2, by = 0.01); y = exp(-x)
qplot(x, y, geom = "line", xlab = expression(yf(x)), ylab = expression(L(yf(x))))
@

% we get the risk function

% \vspace*{-.5cm}

% \begin{eqnarray*}
% \risk(f) &=& \mathbb{E}_x [\exp(-\fx) \pix + \exp(f(x)) (1-\pix)]
% \end{eqnarray*}

\end{vbframe}
%
% \begin{vbframe}{Risk minimizing functions}
%
% Overview of binary classification losses and the corresponding risk minimizing functions:
%
% \lz
%
% \begin{tabular}{p{2.5cm}|p{3.5cm}|p{4.5cm}}
%   loss name & loss formula  & minimizing function \\
%   \hline
%   0-1 & $[y \neq \hx]$ & $\hxh = \footnotesize \begin{cases} 1 \quad \text{ if } \pix > 1/2 \\ -1 \quad \pix < 1/2  \end{cases}$ \\
%   Hinge & $\max\{0, 1 - \yf\}$ & $\fh(x) =  \footnotesize \begin{cases} 1 \quad \text{ if } \pix > 1/2 \\ -1 \quad \pix < 1/2  \end{cases}$ \\
%   Logistic & $\ln(1+\exp(-y\fx))$ & $\fh(x) =  \ln \biggl(\frac{\pix}{1-\pix}\biggr)$ \\
%   Cross entropy & $-y\ln(\pix) \newline -(1-y)\ln(1-\pix)$ & \\
%   & & \\
%   Exponential & $\exp(-y\fx)$ &
%
% \end{tabular}
%
% \end{vbframe}

% \section{Selected methods for regression and classification}

\begin{vbframe}{Normal linear regression}

For $i \in \nset$ (simple case and with basis functions):
\begin{eqnarray*}
\yi & = & \fxi + \epsi = \theta_0 + \theta^T \xi + \epsi\\
\yi & = & \fxi + \epsi = \theta_0 + \theta^T \phi(\xi) + \epsi
\end{eqnarray*}

\begin{itemize}
\item basis functions $\phi(x)=(\phi_1(x), \ldots, \phi_m(x))^T$
\item assumption: $\epsi \iid N(0, \sigma^2)$
\end{itemize}

\lz

Given observed data $\D$  we want to address the questions
\begin{itemize}
\item  Given basis functions, how to find $\theta$? ({\bf Parameter estimation})
\item  How to select basis functions for my problem? ({\bf Model selection} )
\end{itemize}

\framebreak

We don't want do address the model selection problem here, but leave it at:
In ML we \enquote{dislike} doing this \enquote{manually}.

\lz

A typical ML way to estimate the parameters is to not require the assumption
$\epsi \iid N(0, \sigma^2)$, but instead assume the that prediction error is measured
by \emph{squared error} as our \emph{loss function} in \emph{risk minimization}:

$$
\riske(\theta) = SSE(\theta) = \sumin \Lxyit = \sumin (\yi - \theta^T \xi)^2
$$

NB: We assume here and from now on that $\theta_0$ is included in $\theta$.

Using matrix notation the empirical risk can be written as
$$
SSE(\theta) = (\ydat - X\theta)^T(\ydat - X\theta).
$$


Differentiating w.r.t $\theta$ yields the so-called \emph{normal equations}:
$$
X^T(\ydat - X\theta) = 0
$$
The optimal $\theta$ is
$$
\thetah = (X^T X)^{-1} X^T\ydat
$$

In statistics, we would start from a maximum-likelihood perspective
$$
\yi = \fxi + \epsi \sim N(\fxi, \sigma^2)
$$
$$
\LLt = \prod_{i=1}^n \pdf(\yi | \fxit, \sigma^2) \propto \exp(-\frac{\sumin (\fxit - \yi)^2}{2\sigma^2})
$$
If we minimize the neg. log-likelihood, we see that this is equivalent to our
loss minimization approach!
$$
\llt \propto \sumin (\fxit - \yi)^2  = min!
$$


\framebreak

<<>>=
data(mtcars)
regr.task = makeRegrTask(data = mtcars, target = "mpg")
plotLearnerPrediction("regr.lm", regr.task, features = "disp")
@
\end{vbframe}

\begin{vbframe}{Example: Linear Regr. with L1 vs L2 loss}

<<>>=
set.seed(123)

# prediction with f, based on vec x and param vec beta
f = function(x, beta) {
  crossprod(x, beta)
}

# L1 and L2 loss, based on design mat X, vec, param vec beta, computed with f
loss1 = function(X, y, beta) {
  yhat = apply(X, 1, f, beta = beta)
  sum((y - yhat)^2)
}
loss2 = function(X, y, beta) {
  yhat = apply(X, 1, f, beta = beta)
  sum(abs(y - yhat))
}

# optimize loss (1 or 2) with optim
# yes, neldermead not really the best, who cares it is 1d
optLoss = function(X, y, loss) {
  start = rep(0, ncol(X))
  res = optim(start, loss, method = "Nelder-Mead", X = X, y = y)
  res$par
}

# plot data and a couple of linear models
plotIt = function(X, y, models = list()) {
  gd = as.data.frame(cbind(X[-1, 2, drop = FALSE],  y = y[-1]))
  pl = ggplot(data = gd, aes(x = x1, y = y))
  pl = pl + geom_point()
  for (i in seq_along(models)) {
    m = models[[i]]
    pl = pl + geom_abline(intercept = m$beta[1], slope = m$beta[2], col = m$col, lty = m$lty)
  }
  return(pl)
}


# generate some data, sample from line with gaussian errors
# make the leftmost obs an outlier
n = 10
x = sort(runif(n = n, min = 0, max = 10))
y = 3 * x + 1 + rnorm(n, sd = 5)
X = cbind(x0 = 1, x1 = x)
y[1] = 100
@

We study the claim that L1 loss is less sensitive to outliers than L2 loss.
To the displayed data we add a point at (\Sexpr{c(X[1,2],y[1])}). Red = L2, blue = L1 loss.
Solid = fit with, dashed = fit without outlier.

<<fig.height=4.5>>=

# fit l1/2 models on data without then with outlier data
b1 = optLoss(X[-1,], y[-1], loss = loss1)
b2 = optLoss(X[-1,], y[-1], loss = loss2)
b3 = optLoss(X, y, loss = loss1)
b4 = optLoss(X, y, loss = loss2)

# plot all 4 models
pl = plotIt(X, y, models = list(
  list(beta = b1, col = "red", lty = "solid"),
  list(beta = b2, col = "blue", lty = "solid"),
  list(beta = b3, col = "red", lty = "dashed"),
  list(beta = b4, col = "blue", lty = "dashed")
))
print(pl)
@
\end{vbframe}





\begin{vbframe}{k-Nearest-Neighbors}

\begin{itemize}
\item k-NN is a non-parametric method for regression and classification
\item Models predictions $\yh$ for $x$ by looking at $\xyi$ closest to $x$
\item Closeness implies distance measure (usually euclidean)
\item $N_k(x)$ is called the \emph{neighborhood} of $x$, if it consists of the $k$-closest points $\xi$ to $x$
  in the training sample.
\end{itemize}


<<echo=FALSE, fig.height=3>>=
circleFun = function(center = c(0,0), diameter = 1, npoints = 100){
    r = diameter / 2
    tt = seq(0,2*pi,length.out = npoints)
    xx = center[1L] + r * cos(tt)
    yy = center[2L] + r * sin(tt)
    return(data.frame(x1 = xx, x2 = yy, class = NA))
}
set.seed(1234L)
n = 30L
x1 = rnorm(n)
x2 = rnorm(n)
mat = as.data.frame(cbind(x1, x2))
mat = rbind(mat, c(0, 0))
dists = sort(as.matrix(dist(mat))[n + 1L, ])
neighbs = as.numeric(names(dists[1:10L]))
mat$class = ifelse(1:(n+1) %in% neighbs, 1L, 0L)
mat[n + 1L, "class"] = 2L
mat$class = as.factor(mat$class)
circle.dat = circleFun(c(0, 0), 2.2, npoints = 100)
q = ggplot(mat, aes(x = x1, y = x2, color = class)) + geom_point(size = 3)
q = q + geom_polygon(data = circle.dat, alpha = 0.2, fill = "#619CFF")
q = q + theme(legend.position="none")
q
@

\framebreak

Predictions:
\begin{itemize}
\item For regression: \\
$$
\yh = \frac{1}{k} \sum_{\xi \in N_k(x)} \yi
$$
\item For classification a majority vote is used: \\
$$
\yh = \argmax_l \sum_{\xi \in N_k(x)} \I(\yi = l)
$$
And posterior probabilities can be estimated with:
$$
\hat{\pi}_l(x)= \frac{1}{k} \sum_{\xi \in N_k(x)} \I(\yi = l)
$$
\end{itemize}

\framebreak

<<echo=FALSE, warning=FALSE, message=FALSE>>=
lrn1 = makeLearner("classif.kknn", par.vals = list(k = 3L))
lrn2 = makeLearner("classif.kknn", par.vals = list(k = 15L))
lrn3 = makeLearner("classif.kknn", par.vals = list(k = 30L))
lrn4 = makeLearner("classif.kknn", par.vals = list(k = 50L))
plotLearnerPrediction(lrn1, iris.task)
plotLearnerPrediction(lrn2, iris.task)
plotLearnerPrediction(lrn3, iris.task)
plotLearnerPrediction(lrn4, iris.task)
@

\framebreak

\begin{itemize}
\item k-NN has many more parameters to estimate than the simpler LM.
\item k-NN has no training-step and is a very local model.
\item We cannot simply use least-squares loss on the training data for picking $k$,
  because we would always pick $k=1$.
\item k-NN makes no assumptions about the underlying data distribution.
\item The smaller k, the less stable, less smooth and more \enquote{wiggly} the decision
  boundary becomes.
\item Accuracy of k-NN can be severely degraded by the presence of noisy or irrelevant features,
  or if the feature scales are not consistent with their importance.
\item In binary classification, we might choose an odd k to avoid ties.
\item For $\yh$, we might inversely weigh neighbors with their distance to $x$, e.g., $w_i = 1/d(\xi, x)$
\item As the size of training data set approaches infinity, the 1-NN classifier guarantees
  an error rate of no worse than twice the Bayes error rate.
\end{itemize}
\end{vbframe}



\begin{vbframe}{Classification}

Assume we are given a \emph{classification problem}:
\begin{eqnarray*}
& x \in \Xspace \quad & \text{feature vector}\\
& y \in \Yspace = \gset \quad & \text{\emph{categorical} output variable (label)}\\
&\D = \Dset & \text{observations of $x$ and $y$}
\end{eqnarray*}


Classification usually means to construct $g$ discriminant functions $f_1(x), \ldots f_g(x)$,
so that we choose our class as
$$h(x) = \argmax_k f_k(x)$$

\lz

This divides the feature space into $g$ \emph{decision regions} $\{x \in \Xspace | h(x) = k\}$.
These regions are separated by the \emph{decision boundaries} where ties occur between these
regions.

\lz

If these functions $f_k(x)$ can be specified as linear functions (possibly through a rank-preserving,
monotone transformation), we will call the classifier a \emph{linear classifier}. We can then write a
decision boundary as $x^T\theta = 0$, which is a hyperplane separating two classes.

\lz
Note that all linear classifiers can represent non-linear decision boundaries in our original input space,
if we opt to manually construct derived features like higher order interactions or polynomial features.

\lz

If only 2 classes exist, we could also construct a single discriminant function $f(x) = f_1(x) - f_2(x)$
(note that it would be more natural here to label the classes with \{+1, -1\} or \{0, 1\}).

\lz

Two fundamental approaches exist to construct classifiers:
The \emph{generative approach} and the \emph{discriminant approach}.

\framebreak

The \emph{generative approach} employs the Bayes theorem:
$$\pikx = \postk = \frac{\P(x | y = k) \P(y = k)}{\P(x)} \propto \pdfxyk \pik $$
and models $\pdfxyk$ (usually by assuming something about the structure of this distribution)
to allow the computation of $\pikx$.

\lz

Discriminant function are now $\pikx$ or $\lpdfxyk + \lpik$

\lz
The \emph{discriminant approach} tries to model the discriminant score function directly, often by loss
minimization.

\lz
% \framebreak

Examples:
\begin{itemize}
\item Linear discriminant analysis (generative, linear)
\item Quadratic discriminant analysis (generative, not linear)
\item Logistic regression (discriminant, linear)
\end{itemize}


\end{vbframe}

\begin{vbframe}{Linear discriminant analysis (LDA)}

% Linear discriminant follows a similar idea. As before, we want to classify a categorical target $y \in \Yspace = \gset$ on basis of $x$.

% \lz

LDA follows a generative approach, each class density is modeled as a \emph{multivariate Gaussian}
with equal covariance, i. e. $\Sigma_k = \Sigma \quad \forall k$. \\
$$
\pdfxyk = \frac{1}{(2\pi)^{\frac{p}{2}}|\Sigma_k|^{\frac{1}{2}}} \exp(-\frac{1}{2}(x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k))
$$

\lz
% \framebreak

Parameters $\theta$ are estimated in a straight-forward manner by estimating
\begin{eqnarray*}
\hat{\pi}_k &=& n_k / n,\text{ where $n_k$ is the number of class $k$ observations} \\
\hat{\mu}_k &=& \sum_{i: y_i = k} \frac{x_i}{n_k} \\
\hat{\Sigma} &=& \frac{1}{n - g} \sum_{k=1}^g \sum_{i: y_i = k} (x_i - \hat{\mu}_k) (x_i - \hat{\mu}_k) ^T
\end{eqnarray*}


\framebreak

For the posterior probability of class $k$ it follows:
\begin{eqnarray*}
\pikx &\propto& \pi_k \cdot \pdfxyk \\
&=& \pi_k \exp(- \frac{1}{2} x^T\Sigma^{-1}x - \frac{1}{2} \mu_k^T \Sigma^{-1} \mu_k + x^T \Sigma^{-1} \mu_k ) \\
&=& \exp(- \frac{1}{2} x^T\Sigma^{-1}x) \exp(\log \pi_k  - \frac{1}{2} \mu_k^T \Sigma^{-1} \mu_k + x^T \Sigma^{-1} \mu_k ) \\
&=& \exp(- \frac{1}{2} x^T\Sigma^{-1}x) \exp(\theta_{0k} + x^T \theta_k)
\end{eqnarray*}

by defining
$\theta_{0k} := \log \pi_k  - \frac{1}{2} \mu_k^T \Sigma^{-1} \mu_k$ and $\theta_k := \Sigma^{-1} \mu_k$.

\framebreak

Finally, the posterior probability becomes

$$
\pikx = \frac{\pi_k \cdot \pdfxyk }{\pdfx} = \frac{\exp(\theta_{0k} + x^T \theta_k)}{\sum_j \exp(\theta_{0j} + x^T \theta_j)}
$$

(the term $\exp(- \frac{1}{2} x^T\Sigma^{-1}x)$ will cancel out in numerator and denominator).

\lz

And (simplified) discriminant functions can be defined as
$$ f_k(x) =  \theta_{0k} + x^T \theta_k $$
Hence, LDA provides a Linear classifier with linear decision boundaries.

% \lz

% \framebreak

% If we compare the likelihood of two classes $k, l \in \gset$ via the log-odds, we end up in a linear function of x

% \small
% \begin{eqnarray*}
% \log \frac{\pikx}{\pi_l(x)}&=& \log\exp(\theta_{0k} + x^T \theta_k)-\log\exp(\theta_{0l} + x^T \theta_l) \\
% &=& (\theta_{0k} - \theta_{0l}) + x^T(\theta_k-\theta_l)
% \end{eqnarray*}

% \normalsize
% This equation is linear in $x$, so the decision boundary between the classes can
% only be linear. Linear discriminant analysis provides a linear classifier.



% Finally we will predict the most probable category given $x$

% $$
% \yh = h(x) = \argmax_{k \in \gset} \pikx.
% $$

\framebreak

<<echo = FALSE, warning=FALSE, message=FALSE>>=
plotLearnerPrediction("classif.lda", bin.tsk)
@

\end{vbframe}


\begin{vbframe}{Quadratic discriminant analysis (QDA)}

QDA is a direct generalization of LDA, where the class densities are now Gaussians with unequal covariances $\Sigma_k$.
$$
\pdfxyk = \frac{1}{(2\pi)^{\frac{p}{2}}|\Sigma_k|^{\frac{1}{2}}} \exp(-\frac{1}{2}(x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k))
$$

\lz

Parameters are estimated in a straight-forward manner by:\\
\begin{eqnarray*}
\hat{\pi}_j &=& \frac{n_j}{n},\text{ where $n_j$ is the number of class $j$ observations} \\
\hat{\mu}_j &=& \sum_{i: y_i = j} \frac{x_i}{n_j} \\
\hat{\Sigma_j} &=& \frac{1}{n_j - 1} \sum_{i: y_i = j} (x_i - \hat{\mu}_j) (x_i - \hat{\mu}_j)^T \\
\end{eqnarray*}

\framebreak

\begin{eqnarray*}
\pikx &\propto& \pi_k \cdot \pdfxyk \\
&=& \pi_k |\Sigma_k|^{-\frac{1}{2}}\exp(- \frac{1}{2} x^T\Sigma_k^{-1}x - \frac{1}{2} \mu_k^T \Sigma_k^{-1} \mu_k + x^T \Sigma_k^{-1} \mu_k )
\end{eqnarray*}

Taking the log of the above, we can define a discriminant function that is quadratic in $x$.

$$
\log \pi_k - \frac{1}{2} \log |\Sigma_k| - \frac{1}{2} x^T\Sigma_k^{-1}x - \frac{1}{2} \mu_k^T \Sigma_k^{-1} \mu_k + x^T \Sigma_k^{-1} \mu_k
$$


% Let's look at the log-odds now. \\

% \begin{eqnarray*}
% \log \frac{\pikx}{\pi_g(x)}&=& \log \frac{\pi_k}{\pi_g}
% - \frac{1}{2} \log \frac{|\Sigma_k|}{|\Sigma_g|}
% + x^T(\Sigma_k^{-1}\mu_k - \Sigma_g^{-1}\mu_g) \\
% &-& \frac{1}{2} x^T (\Sigma_k^{-1} - \Sigma_g^{-1})x
% - \frac{1}{2} x^T (\mu_k^T\Sigma_k^{-1}\mu_k - \mu_g^T\Sigma_g^{-1}\mu_g)
% \end{eqnarray*}

% We see that this function is quadratic in $x$, hence we obtain a quadratic decision boundary.

% \framebreak


% Finally we will predict again the most probable category given $x$

% $$
% \yh = h(x) = \argmax_{k \in \gset} \pikx.
% $$

\framebreak

<<fig.height=5>>=
plotLearnerPrediction("classif.qda", bin.tsk)
@

\framebreak

<<fig.height=5>>=
n = 30
set.seed(1234L)
tar = factor(c(rep(1L, times = n), rep(2L, times = n)))
x1 = c(rnorm(n, sd = 2), rnorm(n))
x2 = c(rnorm(n, mean = 1,  sd = 2), rnorm(n))
qda.data = data.frame(x1, x2, tar)
tsk = makeClassifTask(data = qda.data, target = "tar")
plotLearnerPrediction("classif.qda", tsk)
@

\end{vbframe}

\begin{vbframe}{Naive Bayes classifier}

Another generative technique for categorical response $y \in \gset$ is called \emph{Naive Bayes classifier}.
Here, we make the \enquote{naive} \emph{conditional independence assumption}, that the features given the category $y$ are conditionally independent of each other, i. e.,

% The technique is based on \emph{Bayes theorem}
% $$
% \pikx = \postk = \frac{\pdfxyk \cdot \pi_k}{\pdfx} = \frac{\text{likelihood } \cdot \text{ prior}}{ \text{evidence}},
% $$
$$
\pdfxyk = p((x_1, x_2, ..., x_p)|y = k)=\prod_{j=1}^p p(x_j|y = k).
$$

\lz

Putting this together we get
$$
\pikx  \propto \pik \cdot \prod_{j=1}^p p(x_j|y = k)
$$

% The Naive Bayes classifier is then obtained by maximizing the above equation
% $$
% \yh = h(x) = \argmax_{k\in \gset}\pi_k\prod_{i=1}^p p(x_i | y = k).
% $$

\framebreak

Parameters estimation now has become simple, as we only have to estimate $\pdf(x_j | y = k)$, which is univariate (given the class k).

\lz

For numerical $x_j$, often a univariate Gaussian is assumed, and we estimate $(\mu_j, \sigma^2_j)$ in the standard manner. Note, that we now have constructed a QDA model with strictly diagonal covariance structures for each class, hence this leads to quadratic discriminant functions.

\lz

For categorical features $x_j$, we simply use a Bernoulli / categorical distribution model for $p(x_j | y = k)$ and estimate the probabilities for $(j,k)$ by simply counting of relative frequencies in the standard manner.
The resulting classifier is linear in these frequencies.

\lz
Furthermore, one can show that the Naive Bayes is a linear classifier in a particular feature space if the features are from exponential families (e. g. binomial, multinomial, normal distribution).

\framebreak

<<>>=
learner = makeLearner("classif.naiveBayes")
plotLearnerPrediction(learner, iris.task)
@


% In the general categorical case the modeled likelihood for $x_j$ with parameters $p_{kj}$ is:
% % with $x = (x_1, \ldots \x_p)$
%
% $$
% p(x_j | y = k) = \frac{(\sum x_i)!}{\prod_i x_i!} \prod_j p_{kj}^{[x_j = j]}
% $$
%
% and for the completely observed data this becomes a multinomial distribution
%
% $$
% \frac{(\sum_i x_i)!}{\prod_i x_i!} \prod_j p_{kj}^{v_{kj}},
% $$
%
% with ${v_{kj}} = \sum_{i = 1}^n [x_j^{(i)} = 1]$ the number of times $(j, k)$ occurs.


% \framebreak
%
%
% We can now prove that the decision boundaries between klasses k and l are linear:
%
% $$
% \log \frac{\pi_k(x)}{\pi_l(x)} \propto \log\frac{\pi_k}{\pi_l} + \sum_j v_{kj} \ln p_{kj} - \sum_j v_{lj} \ln p_{lj}
% $$
%
% This is a linear function in the parameter vector $v = (v_{11}, \ldots, v_{1p}, \ldots, v_{g1} \ldots v_{gp})$.
%
% \framebreak
%
% Laplace smoothing: If a given class and feature value never occur together in the training data, then the frequency-based probability estimate will be zero.
%
% \lz
%
% This is problematic because it will wipe out all information in the other probabilities when they are multiplied.
%
% \lz
%
% A simple numerical correction, especially needed for smaller sample size, is to set $p_{kj} = \epsilon > 0$ instead of $p_{kj} = 0$.


\end{vbframe}

% \begin{vbframe}{Naive Bayes as a linear classifier}

% In general, the \emph{Naive Bayes classifier} is \textbf{not} a \emph{linear} classifier.

% \lz

% However, it can be shown that the \emph{Naive Bayes classifier} is a linear classifier in a particular feature space if the features are from exponential families (e. g. binomial, multinomial, normal distribution) .

% \end{vbframe}



\begin{vbframe}{Logistic regression}

A \emph{discriminant} approach for directly modeling the posterior probabilities of the classes is \emph{Logistic regression}. For now, we will only look at the binary case $y \in \{0, 1\}$.
Note that we will supress the intercept in notation.

$$
\pix = \post = \frac{\exp(\theta^Tx)}{1+\exp(\theta^Tx)} = s(\theta^T x)
$$

We can either directly assume the above form, or arrive at it by requiring that the log-odds
are linear in $\theta$:

$$
\log \frac{\pix}{1-\pix} = \log\frac{\post}{\P(y = 0 | x)} = \theta^Tx.
$$\

\lz

The logistic function $s(t) = \frac{\exp(t)}{1 + \exp(t)}$
transforms the scores / log-odds $\theta^Tx$ into a probability.



\framebreak

Logistic regression is usually fitted by maximum likelihood.
\begin{eqnarray*}
\LLt &=& \prod_{i=1}^n \P(y = y^{(i)} | x^{(i)}, \theta) \\
% &=& \prod_{i=1}^n (\P(y = 1 | x^{(i)}, \theta)^{y^{(i)}}[1 - \P(y = 1 | x^{(i)}, \theta)]^{1 - y^{(i)}} \\
&=& \prod_{i=1}^n \pi(x^{(i)}, \theta)^{y^{(i)}} [1 - \pi(x^{(i)}, \theta)]^{1 - y^{(i)}}.
\end{eqnarray*}

% \framebreak

% Consequently, the log-likelihood is derived by

\small
\begin{eqnarray*}
\llt
&=& \sum_{i=1}^n \yi \log[\pi(\xi, \theta)] + (1-\yi) \log [1 - \pi(\xi, \theta)]\\
&=& \sum_{i=1}^n \yi \log [\exp(\theta^T \xi)] - \yi \log[1 + \exp(\theta^T \xi)] \\
&\quad& + \quad (1 - \yi) \log \biggl[1 - \frac{\exp(\theta^T \xi)}{1 + \exp(\theta^T \xi)}\biggr]\\
&=& \sum_{i=1}^n \yi \theta^T \xi - \log[1 + \exp(\theta^T \xi)]
\end{eqnarray*}

\framebreak

\normalsize
We can minimize our loss, by maximizing $\llt$.
% Therefore, we need to differentiate w.r.t. $\theta$, which gives us the score function:\\
% $$
% s(\theta) = \fp{\llt}{\theta} = \sum_{i=1}^n \xi \cdot \left( \yi - \frac{\exp(\theta^T \xi)}{1 + \exp(\theta^T \xi)}\right) = 0
% $$

% \lz

This now cannot be solved analytically, but is at least concave, so we have to refer to
numerical optimization, e.g., BFGS.

\lz

In order to minimize the loss (misclassification), we should predict $y=1$, if

$$
\pi(x, \thetah) = \P(y = 1 | x, \thetah) = \frac{\exp(\thetah^T x)}{1+\exp(\thetah^Tx)} \ge 0.5,
$$

which is equal to
$$
\hat \theta^T x \ge 0.
$$

So logistic regression gives us a \emph{linear classifier}:
$$
\yh = h(\thetah^T x) =
\begin{cases}
1 & \text{ for } x^T\thetah \ge 0 \\
0 & \text{ otherwise}
\end{cases}
$$


\framebreak

<<echo=FALSE>>=
plotLearnerPrediction("classif.logreg", bin.tsk)
@

\end{vbframe}


\begin{vbframe}{Error distributions and losses}

Let us generalize what we have observed from the comparison between the maximum-likelihood
and the risk-based approach linear regression.

The maximum-likelihood principle is to maximize
$$ \LLt = \prod_{i=1}^n \pdfyigxit $$
or to minimize the neg. log-likelihood:
$$ -\llt = -\sumin \lpdfyigxit $$
Now let us define a new loss as:
$$ \Lxyt = - \lpdfygxt $$
And consider the empirical risk
$$\risket = \sumin \Lxyt$$

Then the maximum-likelihood estimator $\thetah$, which we obtain by optimizing $\LLt$ is identical
to the loss-minimal $\thetah$ we obtain by minimizing $\risket$.
This implies that we can identify for every error distribution and its pdf $\pdfygxt$ an
equivalent loss function which leads to the same point estimator for the parameter vector $\theta$.
We can even disregard multiplicative or additive constants in the loss,
as they do not change the minimizer.

The other way around does not always work: We cannot identify for every loss function and associated
pdf for the distribution - the hinge loss is a prominent example.

\framebreak

Let us reconsider the logistic regression maximum likelihood fit. The neg. log-likelihood for the pdf is:

$$
-\lpdfygxt = - y \log[\pix] - (1-y) \log [1 - \pix]
$$

This is the cross-entropy loss. Logistic regression minimizes this, and we could use this loss
for any other model with directly models $\pix$.

\lz

Now lets assume we have a score function $\fx$ instead of $\pix$. We can transform the score to a probability
via the logistic transformation:

\begin{eqnarray*}
\pix     &=& \frac{\exp(\fx)}{1 + \exp(\fx)}   =  \frac{1}{1 + \exp(-\fx)}\\
1- \pix  &=& \frac{\exp(-\fx)}{1 + \exp(-\fx)} =  \frac{1}{1 + \exp(\fx)}\\
\end{eqnarray*}

\framebreak

The loss now becomes
\begin{eqnarray*}
-\lpdfygxt &=& -y \log[\pix] - (1-y) \log [1 - \pix] \\
           &=& y \log[1 + \exp(-\fx)] + (1-y) \log[1 + \exp(\fx] \\
\end{eqnarray*}
For y=0 and y=1 this is:
\begin{eqnarray*}
y=0 &:& \log[1 + \exp(\fx] \\
y=1 &:& \log[1 + \exp(-\fx)]
\end{eqnarray*}
If we would encode now with $\Yspace = \{-1,+1\}$, we can unify this like this:
$$\Lxy = \log[1 + \exp(-\yf] $$
So we have recovered the Bernoulli loss. LR minimizes this, and we could use this loss
for any other model with directly models $\fx$.

\end{vbframe}


% We ended up maximizing

% $$ \sumin \yi \theta^T \xi - \log[1 + \exp(\theta^T \xi)] $$

% \framebreak

% Regression

% \lz

% \begin{tabular}{ l  l | l l }
%   loss name & loss formula  & pdf name   & pdf formula \\
%   \hline
%   squared   & $(y - f)^2$      & Gaussian & $exp()$       \\
%   absolute  & $|y - f|$        & Laplace  & $...$         \\
% \end{tabular}

% \lz

% Classification

% \lz

% \begin{tabular}{ l  l | l l }
%   loss name & loss formula  & pdf name   & pdf formula \\
%   \hline
%   Bernoulli / cross entropy   & $(y - f)^2$      & Gaussian & $exp()$       \\
% \end{tabular}



\begin{vbframe}{Entropy}

In some situations we might be interested in alternative definitions of uncertainty of a random variable $x$,
as in traditional statistics. \emph{Information theory} establishes the so-called \emph{entropy}.

\small
\textbf{Thought experiment:}

Level of uncertainty of rolling a die should be influenced by three factors:
  \begin{enumerate}
  \item The more sides, the harder to predict the outcome.

  $\rightarrow$ Uncertainty grows \textit{monotonically} with number of potential outcomes.
  \item The relative likelihood of each outcome determines the uncertainty.
  The result of an \enquote{unfair} die is easier to predict than the result of a fair one.

  $\rightarrow$ The uncertainty depends on the outcome probabilities $\{p_x(1),\ldots,p_x(K)\}$
  \item The weighted uncertainty of independent events must sum.

  The total uncertainty of rolling two independent dices must equal the sum of the uncertainties for each die alone.
  For a composite event, its uncertainty should equal the sum of the single uncertainties weighted by the probabilities of their occurrence.
  \end{enumerate}

\framebreak

\normalsize
Those postulates lead to a unique mathematical expression for the entropy $H(x)$ for a RV $x$ with outcome probabilities $p_x(k):=\P(x=k)$ for $k \in \Xspace$.

$$
H(x)=-\sum_{k \in \Xspace} p_x(k) \log_2(p_x(k)) \quad \text{(in bits)}
$$
(Actually the base of the log is not uniquely determined)

\lz

More intuitively, the entropy describes how much information is received when we observe a specific value for a random variable $x$.

\lz

The term $-\log_2(p_x(k))$ is called the \emph{information} $I$ an observation of $x=k$ gives us about $x$.

\lz

Base $2$ means the information is measured in bits, but you can use any number $>1$ as base of the logarithm.

\framebreak

\textbf{Example 1:} flipping an (un-)fair coin

\lz

Let $p_x(1) = p$ and $p_x(0) = 1 - p$ .

\lz

The entropy is then given by
$$
H(x)= -p \cdot \log_2(p)-(1-p)\cdot \log_2(1-p).
$$

The entropy (uncertainty) is maximal for fair coin ($p=1/2$).

<<echo=FALSE>>=
x <- seq(0.1, 0.9, by = 0.01)
y = - x * log2(x) - (1 - x) * log2(1 - x)
plot(x, y, type = 'l', xlab = "p", ylab = "H(x)")
box()
@

\framebreak

\begin{columns}[T,onlytextwidth]
\column{0.3\textwidth}
\textbf{Example 2:} \\
\lz
\begin{center}
<<echo=FALSE, size = "footnotesize">>=
ex1 <- cbind(class=c("+","+","-","+","-","-", "-"), attr_1 = c(T,T,T,F,F,F,F), attr_2 = c(T,T,F,F,T,T,T))
kable(ex1)
@
\end{center}
\column{0.65\textwidth}
\begin{itemize}
\item How big is the uncertainty/entropy in \textit{class} (in bits)?
\small
\begin{eqnarray*}
H(\text{class}) &=& - \sum_{k=+,\, -} P(k) log_2(P(k)) \\
&=& - \frac{3}{7} log_2\left(\frac{3}{7}\right)  - \frac{4}{7} log_2\left(\frac{4}{7}\right) \\
&=& 0.985
\end{eqnarray*}
\normalsize
\item How much can it be reduced by knowing the other attributes?
\end{itemize}
\end{columns}

\end{vbframe}

\begin{vbframe}{Conditional entropy}

The \emph{conditional entropy} $H(y|x)$ quantifies the uncertainty of $y$ that remains if the outcome of $x$ is given
$$
H(y|x) = \sum_{x \in \Xspace} p_x(x) H(y|x=x) \overset{(*)}{=}H(y|x) = H(y, x) - H(x).
$$

\textbf{Remark:}
\begin{itemize}
\item $H(y|x) = 0$ if (and only if) $y$ is completely determined by $x$
\item $H(y|x) = H(y)$ if (and only if) $x$ and $y$ are independent
\end{itemize}

\framebreak

\quad

\vspace{.5cm}

$\quad ^{(*)}$ Proof: \\
\footnotesize
\begin{eqnarray*}
H(y|x) &=& \sum_{x \in \Xspace} p_x(x) H(y|x=x) = -\sum_{x \in \Xspace} p_x(x) \sum_{y \in \Yspace}p(y|x)\log_2 p(y|x) \\
&=& - \sum_{x \in \Xspace}  \sum_{y \in \Yspace} p_x(x) \frac{p_{xy}(x, y)}{p_x(x)}\log_2 \frac{p_{xy}(x, y)}{p_x(x)}\\
&=& - \sum_{x \in \Xspace, y \in \Yspace} p_{xy}(x, y)\log_2 \frac{p_{xy}(x, y)}{p_x(x)}\\
&=& - \sum_{x \in \Xspace, y \in \Yspace} p_{xy}(x, y)\log_2 p_{xy}(x, y)+\sum_{x \in \Xspace, y \in \Yspace} p_{xy}(x, y)\log_2 p_x(x)\\
&=& H(x, y) + \sum_{x\in \Xspace}p_x(x)\log_2 p_x(x) = H(x, y) - H(x)
\end{eqnarray*}

\framebreak
\begin{columns}[T,onlytextwidth]
\column{0.3\textwidth}
\textbf{Example:} \\
\lz
\begin{center}
<<echo=FALSE, size = "footnotesize">>=
kable(ex1)
@
\end{center}
\column{0.65\textwidth}
\scriptsize

\vspace*{1.5cm}

$H(\text{class}|\text{attr}_1 = T) = - \frac{2}{3} log_2(\frac{2}{3}) - \frac{1}{3} log_2(\frac{1}{3}) = 0.92$ \\
$H(\text{class}|\text{attr}_1 = F) = - \frac{1}{4} log_2(\frac{1}{4}) - \frac{3}{4} log_2(\frac{3}{4}) = 0.81$ \\
$H(\text{class}|\text{attr}_2 = T) = - \frac{2}{5} log_2(\frac{2}{5}) - \frac{3}{5} log_2(\frac{3}{5}) = 0.97$ \\
$H(\text{class}|\text{attr}_2 = F) = - \frac{1}{2} log_2(\frac{1}{2}) - \frac{1}{2} log_2(\frac{1}{2}) = 1$ \\
\lz
$H(\text{class}|\text{attr}_1) = \frac{3}{7} 0.92 + \frac{4}{7} 0.81 = 0.86$ \\
$H(\text{class}|\text{attr}_2) = \frac{5}{7} 0.97 + \frac{2}{7} 1 = 0.98$

\normalsize

\end{columns}

\end{vbframe}


\begin{vbframe}{Mutual information}

The reduction of entropy after \textit{learning} $x$ is called \emph{mutual information}

$$
I(y;x) := H(y) - H(y|x) = H(y) + H(x) - H(x, y).
$$

\textbf{Remark:}
\begin{itemize}
\item The mutual information is symmetric, i. e. $I(y;x) = I(x;y)$.
\item It describes the amount of information about one RV obtained through the other one (\emph{information gain}).
\end{itemize}

\begin{figure}
 \includegraphics{mutualinformation.pdf}
\end{figure}

\framebreak

\begin{columns}[T,onlytextwidth]
\column{0.3\textwidth}
\begin{center}
<<echo=FALSE, size = "footnotesize">>=
kable(ex1)
@
\end{center}
\column{0.65\textwidth}
\begin{itemize}
\item For our example we can compute the information gains:
\footnotesize
\begin{eqnarray*}
I(\text{class}; \text{attr}_1) &=& H(\text{class}) - H(\text{class}|\text{attr}_1) \\
&=& 0.985 - 0.86 = 0.125
\end{eqnarray*}

\begin{eqnarray*}
I(\text{class}; \text{attr}_2) &=& H(\text{class}) - H(\text{class}|\text{attr}_2) \\
&=& 0.985 - 0.98 = 0.005
\end{eqnarray*}
\normalsize
\lz
\item $\text{attr}_1$ tells us more about $\text{class}$.
\end{itemize}

\end{columns}

\end{vbframe}

\begin{vbframe}{Kullback-Leibler-Divergence}

Suppose that data is being generated from an unknown distribution $p(x)$. Suppose we modeled $p(x)$ using an approximating distribution $q(x)$. A "good" approximation $q(x)$ should minimize the difference to $p(x)$.

\lz

A measure for the difference between $p$ and $q$ is the \emph{Kullback-Leibler-Divergence}
\begin{eqnarray*}
D_{KL}(p||q) := & \int_{-\infty}^{\infty} p(z) \cdot \log \frac{p(z)}{q(z)} dz &\quad \text{continuous case} \\
D_{KL}(p||q) := & \sum_{k} p(k) \cdot \log \frac{p(k)}{q(k)} &\quad \text{discrete case}
\end{eqnarray*}

\textbf{Remark:}
\begin{itemize}
\item In general: $D_{KL}(p||q) \ne D_{KL}(p||q)$ (no symmetry)
\item $D_{KL}$ is often called the \emph{information gain} achieved if $q$ is used instead of $p$.
\end{itemize}

\end{vbframe}


\begin{vbframe}{Mutual information \& Kullback-Leibler-Divergence}

$I(x;y)$ is the \emph{information gain} achieved if the product of marginal distributions $p_x(x) p_y(y)$ is used instead of the joint distribution $p_{xy}(x,y)$:

\begin{eqnarray*}
I(x;y) &=& D_{KL}(p_{xy}||p_x p_y) = \sum_{x \in \Xspace} \sum_{y \in \Yspace} p_{xy}(x, y) \cdot \log \biggl(\frac{p_{xy}(x, y)}{p_x(x)p_y(y)}\biggr)
\end{eqnarray*}

\framebreak

\footnotesize
Proof:

\begin{eqnarray*}
I(x;y) &=& H(y) + H(x) - H(x, y)\\
&=& -\sum_{y \in \Yspace} p_y(y) \log_2(p_y(y)) -\sum_{x \in \Xspace} p_x(x) \log_2(p_x(x)) \\
&& -\sum_{x \in \Xspace, y \in \Yspace} p_{xy}(x, y) \log_2(p_{xy}(x, y))\\
&=& -\sum_{x \in \Xspace, y \in \Yspace}p_{xy}(x, y) \log_2(p_y(y)) -\sum_{x \in \Xspace, y \in \Yspace} p_{xy}(x, y) \log_2(p_x(x)) \\
&& \quad+ \sum_{x \in \Xspace, y \in \Yspace} p_{xy}(x, y) \log_2(p_{xy}(x, y)) \\
&=& \sum_{x \in \Xspace} \sum_{y \in \Yspace} p_{xy}(x, y) \cdot \log \biggl(\frac{p_{xy}(x, y)}{p_x(x)p_y(y)}\biggr) = D_{KL}(p_{xy}||p_x p_y)
\end{eqnarray*}


\end{vbframe}

% \begin{vbframe}{Cross-Entropy}

% A related measure to compare two probability measures $p$ and $q$ is the \emph{cross entropy} (deviance):
% $$
% H(p, q) = - \sum_{x} p(x) \log q(x)
% $$

% The cross entropy defines a loss function.

% Remember the log-Likelihood function derived in the example of logistic regression with binary response $y\in \{0, 1\}$.

% \small
% \begin{eqnarray*}
% -\llt &=&   -\sum_{i=1}^n y^{(i)} \log[\P(y = 1 | x^{(i)} , \theta)] + (1-y^{(i)}) \log [1 - \P(y = 1 | x^{(i)} , \theta)]\\
% &=& \sum_{i=1}^n H(p_i, q_i)
% \end{eqnarray*}

% \normalsize
% with $p_i := y^{(i)}$ are our true labels and $q_i:=\P(y = 1 | x^{(i)} , \theta)$ the posterior probabilites.

% \end{vbframe}

% old slides about entropy and mutual information

%\begin{vbframe}{Entropy}
% Entnommen aus A Light Discussion and Derivation of Entropy, Jonathon Shlens, Google Research 09.04.14 in Appendix auch Beweis enthalten
%\begin{itemize}
%\item The uncertainty $H$ of a discrete random %variable $X$ is the entropy.

%(\textit{A Light Discussion and Derivation of Entropy}, Jonathon Shlens, Google Research, \url{http://arxiv.org/pdf/1404.1998.pdf})
%\item Thought experiment: The level of uncertainty of rolling a die should intuitively be influenced by three factors:
%  \begin{enumerate}
%  \item The more sides a die has, the harder to predict the outcome, the greater the uncertainty.

%  $\rightarrow$ Uncertainty/entropy grows \textit{monotonically} with the numbers of potential outcomes.
%  \item The relative likelihood of each outcome determines the uncertainty.
%  The result of an \enquote{unfair} die is easier to predict than the result of a fair one.

%  $\rightarrow$ $H$ can be written as a function of the outcome probabilities $\{P_1,\ldots,P_K\}$
%  \item The weighted uncertainty of independent events must sum.

%  The total uncertainty of rolling two independent dice must equal the sum of the uncertainties for each die alone.
%  For a composite event, its uncertainty should equal the sum of the single uncertainties weighted by the probabilities of their occurrence.
%  \end{enumerate}
% \item The first postulate gives us that $\frac{\partial H}{\partial K} > 0$, since the uncertainty grows monotonically with the number of outcomes.
% To be positive, the derivative must exist, thus $H$ is assumed to be \textit{continuous}.
% \item Those postulates lead to a unique mathematical expression for the entropy $H$ of a discrete random variable $X$ with prob. mass distr. $P(X)$:
% $$
% H(X)=-\sum_{k=1}^g P_k log_2(P_k)
% $$
% (Shannon and Weaver, The mathematical theory of communication, 1949)

% \item The term $-log_2(P_k)$ is called \textit{information} $I$ and is the information that an observation of $x=k$ gives us about $X$.
% \item Observations of rare events give more information (sometimes also called \textit{surprise}) about the random variable $X$.
% \item In general the base of the logarithm can be any fixed real number greater than 1, for binary logarithm the unit of the information is \textit{bit}.
% \item The entropy $H(X)=-\sum_{k=1}^g P_k log_2(P_k)$ equals the average (or expected) amount of information obtained by observing $x$ (in bits) $\E(I(X))$.
% \end{itemize}

% \framebreak


% \begin{vbframe}{Conditional entropy}
% \begin{itemize}
% \item $H(Y|X)$ is the remaining entropy of $Y$ given $X$ or the expected entropy of $P(Y|X)$
% $$ H(Y|X) = -\sum_x P(X = x) H(Y|X=x) $$

% \item $H(Y|X=x)$ is the entropy of $Y$ for all observations with a specific value of (rule regarding) feature $X$
% \end{itemize}

% \end{vbframe}

% \begin{vbframe}{Mutual Information}
% \begin{itemize}
% \item Entropy of a discrete RV $H(Y)$: a measure of uncertainty about $Y$
% \item Conditional entropy $H(Y|X)$: remaining entropy of $Y$ knowing $X$
% \item After \textit{learning} $X$ it is easy to define the reduction of entropy.
% \item The \textit{mutual information} between $Y$ and $X$ is
% $$
% I(Y;X) = H(Y) - H(Y|X) \; (= H(X) - H(X|Y))
% $$
% \item Describes the amount of information obtained about one random variable, through the other.
% \item Can also be derived as the Kullback-Leibler divergence of the product of the two marginal distributions from their joint distribution.
% \item In tree learning $I(Y;X)$ is also called \textit{information gain}.
% \end{itemize}
% \framebreak

% \end{vbframe}
\endlecture
