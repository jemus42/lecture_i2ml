% Introduction to Machine Learning
% Day 1

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@
% Load all R packages and set up knitr
<<setup-r, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{5}{Linear regression models}
\lecture{Introduction to Machine Learning}


\begin{vbframe}{Normal linear regression}

\begin{itemize}
  \item Machine Learning tries to approximate the true data generating function via loss minimization
  \item Classic statistics maximizes a likelihood to reach the same goal
  \item Following example with the linear regression model:
  \begin{equation*}
    \begin{split}
      y^{i} & = \hat f(x^{i}) + \epsilon^{i} = \theta_0 + \theta x^{i} + \epsilon^{i} \\
      & \text{with } \epsilon, y|x \iid N(\mu,\sigma^2)
    \end{split}
  \end{equation*}
  \item Alternative distributions are applied in generalized linear models.
  \item Basis functions $\phi(x)=(\phi_1(x), \ldots, \phi_m(x))^T$ can also be included as an extension of linear predictors.

\end{itemize}
\end{vbframe}


\begin{vbframe}{Way 1: loss minimization}

\begin{itemize}

  \item A typical machine learning way to estimate the parameters is to not require
  a distribution assumption for the error term but to define a loss function for the
prediction.
  \item In this case we decide to measure the prediction error with a \emph{squared loss}
as \emph{loss function} for the \emph{risk minimization}:

    $$
    \riske(\theta) = SSE(\theta) = \sumin \Lxyit = \sumin (\yi - \theta^T \xi)^2
    $$

  \item NB: We assume here and from now on that $\theta_0$ is included in $\theta$.

  \item Using matrix notation the empirical risk can be written as
    $$
    SSE(\theta) = (\ydat - X\theta)^T(\ydat - X\theta)
    $$

\end{itemize}
\end{vbframe}


\begin{vbframe}{Way 1: loss minimization}
  \begin{itemize}
    \item We transform this to the minimization problem:
    $$
    \hat \theta = \arg \min_\theta (\ydat - X\theta)^T(\ydat - X\theta)
    $$
    \item Differentiating w.r.t. $\theta$ yields the so-called \emph{normal equations}:
    $$
    X^T(\ydat - X\theta) = 0
    $$
    \item The optimal $\theta^*$ therefore is
    $$
    \thetah = (X^T X)^{-1} X^T\ydat
    $$
    \item If we face loss functions that we can not solve for $\theta$ analytically,
    we approximate the solution using optimizers such as \emph{Gradient Descent}.
  \end{itemize}
\end{vbframe}

\begin{vbframe}{Way 2: maximum likelihood}
\begin{itemize}
  \item In statistics, we would start from a maximum-likelihood perspective
  $$
  \yi = \fxi + \epsi \sim N(\fxi, \sigma^2)
  $$
  $$
  \LLt = \prod_{i=1}^n \pdf(\yi | \fxit, \sigma^2) \propto \exp(-\frac{\sumin (\fxit - \yi)^2}{2\sigma^2})
  $$
  \item With the log-likelihood that we want to maximize:
  $$
  \llt \propto - \sumin (\fxit - \yi)^2
  $$
  \item and the optimal parameter
  $$
  \theta ^* = \arg \max_{\theta} \llt
  $$
\end{itemize}

\end{vbframe}
\begin{vbframe}{Way 2: maximum likelihood}
\begin{itemize}
  \item This is equivalent to minimizing:
  $$
  - \llt \propto \sumin (\fxit - \yi)^2
  $$
  \item Which is equivalent to our loss minimization approach from the machine learning perspective!
  \item We can show this relation holds not only for a Gaussian distribution and L2 loss but also for other loss-distribution-combinations (Laplacian - L1 loss, Binomial - logistic loss, ...)
\end{itemize}

\end{vbframe}






\begin{vbframe}{Normal linear regression}

<<>>=
data(mtcars)
regr.task = makeRegrTask(data = mtcars, target = "mpg")
plotLearnerPrediction("regr.lm", regr.task, features = "disp")
@
\end{vbframe}

\begin{vbframe}{Example: Linear Regr. with L1 vs L2 loss}

<<>>=
set.seed(123)

# prediction with f, based on vec x and param vec beta
f = function(x, beta) {
  crossprod(x, beta)
}

# L1 and L2 loss, based on design mat X, vec, param vec beta, computed with f
loss1 = function(X, y, beta) {
  yhat = apply(X, 1, f, beta = beta)
  sum((y - yhat)^2)
}
loss2 = function(X, y, beta) {
  yhat = apply(X, 1, f, beta = beta)
  sum(abs(y - yhat))
}

# optimize loss (1 or 2) with optim
# yes, neldermead not really the best, who cares it is 1d
optLoss = function(X, y, loss) {
  start = rep(0, ncol(X))
  res = optim(start, loss, method = "Nelder-Mead", X = X, y = y)
  res$par
}

# plot data and a couple of linear models
plotIt = function(X, y, models = list()) {
  gd = as.data.frame(cbind(X[-1, 2, drop = FALSE],  y = y[-1]))
  pl = ggplot(data = gd, aes(x = x1, y = y))
  pl = pl + geom_point()
  for (i in seq_along(models)) {
    m = models[[i]]
    pl = pl + geom_abline(intercept = m$beta[1], slope = m$beta[2], col = m$col, lty = m$lty)
  }
  return(pl)
}


# generate some data, sample from line with gaussian errors
# make the leftmost obs an outlier
n = 10
x = sort(runif(n = n, min = 0, max = 10))
y = 3 * x + 1 + rnorm(n, sd = 5)
X = cbind(x0 = 1, x1 = x)
y[1] = 100
@

We study the claim that L1 loss is less sensitive to outliers than L2 loss.
To the displayed data we add a point at (\Sexpr{c(X[1,2],y[1])}). Red = L2, blue = L1 loss.
Solid : fit w/o outlier, dashed : fit with outlier.

<<fig.height=4.5>>=

# fit l1/2 models on data without then with outlier data
b1 = optLoss(X[-1,], y[-1], loss = loss1)
b2 = optLoss(X[-1,], y[-1], loss = loss2)
b3 = optLoss(X, y, loss = loss1)
b4 = optLoss(X, y, loss = loss2)

# plot all 4 models
pl = plotIt(X, y, models = list(
  list(beta = b1, col = "red", lty = "solid"),
  list(beta = b2, col = "blue", lty = "solid"),
  list(beta = b3, col = "red", lty = "dashed"),
  list(beta = b4, col = "blue", lty = "dashed")
))
print(pl)
@
\end{vbframe}

\endlecture
