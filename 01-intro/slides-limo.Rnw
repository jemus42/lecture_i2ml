% Introduction to Machine Learning
% Day 1

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@
% Load all R packages and set up knitr
<<setup-r, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{5}{Linear regression models}
\lecture{Introduction to Machine Learning}

% slides adapted from rcourses/chapters/ml/ml_simple_models/slides_sm_lmXXX.Rmd @ fbce91e4277

\begin{frame}{Straight Line / Univariate Linear Model}

\lz

<<echo=FALSE, out.width="0.7\\textwidth", fig.width=4, fig.height=3>>=
ggplot() +
  stat_function(data = data.frame(x = c(-0.5, 4)), aes(x = x), fun = function (x) { 1 + 0.5 * x }) +
  geom_segment(mapping = aes(x = 2, y = 2, xend = 3, yend = 2), linetype = "dashed") +
  geom_segment(mapping = aes(x = 3, y = 2, xend = 3, yend = 2.5), linetype = "dashed", color = "red") +
  geom_segment(mapping = aes(x = -0.3, y = 1, xend = 0.3, yend = 1), linetype = "dashed", color = "blue") +
  geom_text(mapping = aes(x = 2.5, y = 2, label = "1 Unit"), vjust = 2) +
  geom_text(mapping = aes(x = 3, y = 2.25, label = "{theta == slope} == 0.5"), hjust = -0.05, parse = TRUE, color = "red") +
  geom_text(mapping = aes(x = 0, y = 1, label = "{theta[0] == intercept} == 1"), hjust = -0.25, parse = TRUE, color = "blue") +
  geom_vline(xintercept = 0, color = "gray", linetype = "dashed") +
  ylim(c(0, 3.5)) + xlim(c(-0.5, 4.3))
@

\[
y = \theta_0 + \theta \cdot x
\]
\end{frame}

\begin{vbframe}{Linear Regression: Representation}
\lz

We want to predict a numerical target variable by a \emph{linear transformation} of the features $x \in \mathbb{R}^p$.

\lz

So with $\theta \in \mathbb{R}^p$ this mapping can be written as:
\begin{align*}
y &= \fx = \theta_0 + \theta^T x \\
  &= \theta_0 + \theta_1 x_1 + \dots + \theta_p x_p
\end{align*}

\lz

This restricts the hypothesis space $\mathcal H$ to all linear functions in $\theta$:
\[
H = \{ \theta_0 + \theta^T x\ |\ (\theta_0, \theta) \in \mathbb{R}^{p+1} \}
\]

\framebreak

\lz

Given observed labeled data $\D$, how to find $(\theta, \theta_0$)?\\

\lz

This is \textbf{learning} or parameter estimation, the learner does exactly this by \textbf{empirical risk minimization}.

\lz
\lz

NB: We assume from now on that $\theta_0$ is included in $\theta$.

\end{vbframe}

\begin{frame}{Linear Regression: Evaluation}

We could measure training error as the sum of squared prediction errors (SSE). This is also called the \textbf{L2 loss}, or L2 risk:
\[
\risket = \operatorname{SSE}(\theta) = \sumin \Lxyit = \sumin \left(\yi - \theta^T \xi\right)^2
\]

<<echo=FALSE, out.width="0.4\\textwidth", fig.width=4, fig.height=2.5>>=
set.seed(1)
x = 1:5
y = 0.2 * x + rnorm(length(x), sd = 2)
d = data.frame(x = x, y = y)
m = lm(y ~ x)
pl = ggplot(aes(x = x, y = y), data = d)
pl = pl + geom_abline(intercept = m$coefficients[1], slope = m$coefficients[2])
pl = pl + geom_rect(aes(ymin = y[3], ymax = y[3] + (m$fit[3] - y[3]), xmin = 3, xmax = 3 + abs(y[3] - m$fit[3])), color = "black", linetype = "dotted", fill = "transparent")
pl = pl + geom_rect(aes(ymin = y[4], ymax = y[4] + (m$fit[4] - y[4]), xmin = 4, xmax = 4 + abs(y[4] - m$fit[4])), color = "black", linetype = "dotted", fill = "transparent")
pl = pl + geom_segment(aes(x = 3, y = y[3], xend = 3, yend = m$fit[3]), color = "white")
pl = pl + geom_segment(aes(x = 4, y = y[4], xend = 4, yend = m$fit[4]), color = "white")
pl = pl + geom_segment(aes(x = 3, y = y[3], xend = 3, yend = m$fit[3]), linetype = "dotted", color = "red")
pl = pl + geom_segment(aes(x = 4, y = y[4], xend = 4, yend = m$fit[4]), linetype = "dotted", color = "red")
pl = pl + geom_point()
pl = pl + coord_fixed()
print(pl)
@



Minimizing the squared error is computationally much simpler than minimizing the absolute differences (this would be called \textbf{L1 loss}).

\end{frame}

\begin{vbframe}{Linear Model: Optimization}

We want to find the parameters $\theta$ of the linear model, i.e., an element of the hypothesis space $\mathcal H$ that fits the data optimally.\\ 

So we evaluate different candidates for $\theta$.\\

A first (random) try of $\theta = (2.0, 0.5)$ yields an SSE of 20.33 (\textbf{Evaluation}).

{\centering \includegraphics{figure_man/frame1.png}}

\framebreak

We want to find the parameters of the LM / an element of the hypothesis space H that best suits the data.
So we evaluate different candidates for $\theta$.\\

Another line ($\theta = (2.0, 0.2)$)  yields an lower SSE of 18.51 (\textbf{Evaluation}). Therefore, this one is better in
terms of empirical risk.

{\centering \includegraphics{figure_man/frame2.png}}

\framebreak

Instead of guessing parameters, we use \textbf{optimization} to find the best $\theta$.\\

For L2 regression, we can find this optimal value analytically:
\[
\hat{\theta} = \argmin_{\theta} \risket = \argmin_{\theta} \|y - X\theta\|^2_2
\]
$X = \left(\begin{smallmatrix}1 & x^{(1)}_1 & \ldots & x^{(1)}_p \\
                              1 & x^{(2)}_1 & \ldots & x^{(2)}_p \\
                              \vdots & \vdots & & \vdots \\
                              1 & x^{(n)}_1 & \ldots & x^{(n)}_p \end{smallmatrix}\right)$ 
is the $n \times (p+1)$-feature-data-matrix,\\
also called \textbf{design matrix}.

\lz

This yields the so called normal equations for the LM:
\[
\frac{\delta}{\delta\theta} \risket = 0 \qquad \Rightarrow\ \qquad \hat{\theta} = \left(X^T X\right)^{-1}X^Ty
\]

\framebreak

\lz

\lz

The optimal line has an SSE of 5.88.

{\centering \includegraphics{figure_man/frame3.png}}

\end{vbframe}

\begin{vbframe}{Example: Regression With L1 vs L2 Loss}

We could also minimize the L1 loss. This changes the evaluation and optimization step:
\[
\risket = \sumin \Lxyit = \sumin \left|\yi - \theta^T \xi\right| \qquad \textsf{(Evaluation)}
\]
(Much) harder to optimize, but the model is less sensitive to outliers.

\framebreak
\lz
\lz

{\centering \includegraphics{figure_man/l2-vs-l1-1.pdf}}

\framebreak

Adding an outlier (highlighted red) pulls the line fitted with L2 into the direction of the outlier:

\lz

{\centering \includegraphics{figure_man/l2-vs-l1-2.pdf}}

\end{vbframe}

\begin{frame}{Linear Regression}
\lz

\textbf{Representation:} Design matrix $X$, coefficients $\theta$.\\

\lz

\textbf{Evaluation:} Any regression loss function. 

\lz

\textbf{Optimization:} Direct analytic solution for L2-loss, numerical optimization for L1 and others.

\end{frame}


\begin{vbframe}{Regression: Polynomials}

We can make regression models much more flexible by using \emph{polynomials} $x^d$ -- or any other functions like $\sin(x)$) of $x$ -- as additional features.\\

\lz 

The optimization and evaluation for the learner remains the same.\\
\lz

Only the representation of the learner changes: it now includes the additional, derived features as additional columns in the design matrix as well as the coefficients associated with them.

\framebreak

\textbf{Polynomial regression example}

 \lz

<<echo=FALSE, out.width="0.9\\textwidth", fig.height = 4>>=
.h = function(x) 0.5 + 0.4 * sin(2 * pi * x)
h = function(x) .h(x) + rnorm(length(x), mean = 0, sd = 0.07)

set.seed(1234)
x = seq(0, 1.5, length = 31L)
y = h(x)


line.palette = viridisLite::viridis(4)
baseplot = function() {
  # par(mar = c(2, 2, 1, 1))
  plot(x, y, pch = 19L)
}

p1 = lm(y ~ poly(x, 1, raw = TRUE))
p3 = lm(y ~ poly(x, 5, raw = TRUE))
p10 = lm(y ~ poly(x, 15, raw = TRUE))
mods = list(p1, p3, p10)
x.plot = seq(0, 1.5, length = 500L)
baseplot()
@

\framebreak

\textbf{Polynomial regression example}

Models of different \emph{complexity}, i.e. of different polynomial order $d$, are fitted to the data:

<<echo=FALSE, out.width="0.9\\textwidth", fig.height=4>>=
baseplot()
for (i in 1) {
  lines(x.plot, predict(mods[[i]], newdata = data.frame(x = x.plot)),
    col = line.palette[i], lwd = 2L)
}
legend("bottomright", paste(sprintf("f(x) for d = %s", c(1, 5, 15)), c("(linear)", "", "")),
  col = line.palette, lwd = 2L)
@

\framebreak

\textbf{Polynomial regression example}

Models of different \emph{complexity}, i.e. of different polynomial order $d$, are fitted to the data:

<<echo=FALSE, out.width="0.9\\textwidth", fig.height=4>>=
baseplot()
for (i in 1:2) {
  lines(x.plot, predict(mods[[i]], newdata = data.frame(x = x.plot)),
    col = line.palette[i], lwd = 2L)
}
legend("bottomright", paste(sprintf("f(x) for d = %s", c(1, 5, 15)), c("(linear)", "", "")),
  col = line.palette, lwd = 2L)
@

\framebreak

\textbf{Polynomial regression example}

Models of different \emph{complexity}, i.e. of different polynomial order $d$, are fitted to the data:

<<echo=FALSE, out.width="0.9\\textwidth", fig.height=4>>=
baseplot()
for (i in 1:3) {
  lines(x.plot, predict(mods[[i]], newdata = data.frame(x = x.plot)),
    col = line.palette[i], lwd = 2L)
}
legend("bottomright", paste(sprintf("f(x) with d = %s", c(1, 5, 15)), c("(linear)", "", "")),
  col = line.palette, lwd = 2L)
@

The higher $d$ is, the more \textbf{capacity} the learner has to learn complicated functions of $x$, but
this also increases the danger of \textbf{overfitting} (later).

\end{vbframe}

\endlecture
