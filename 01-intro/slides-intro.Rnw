% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@
% Load all R packages and set up knitr
<<setup-r, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{1}{Introduction}
\lecture{Introduction to Machine Learning}

<<include=FALSE>>=
set.seed(19042011)
runifCirc <- function(n, radius = 1, d = 2)
  t(sapply(seq_len(n), function(i) HI::rballunif(d, radius)))
@
\sloppy

\begin{frame}{Data Science and Machine Learning}

\begin{columns}
  \column{.3\textwidth}
    \begin{figure}
      \includegraphics[width=\textwidth]{figure_man/gears.png}
    \end{figure}
  \column{0.6\textwidth}
  \begingroup
  \centering
    \fontsize{20pt}{22pt}\selectfont
    \vspace{1cm}
    \\
 % Machine Learning is a method of teaching computers to make predictions based on some data.
 %Machine learning is the study of algorithms that can automatically learn from and make predictions on data.
Machine learning is a
branch of computer
science and applied
statistics covering
algorithms that
improves their
performance at a given
task based on sample
data or experience.
  \endgroup
\end{columns}

\end{frame}

\begin{frame}{Data Science and Machine Learning}

\scriptsize

\begin{center}\includegraphics[width=0.95\textwidth]{figure_man/learning} \end{center}

\normalsize 

\end{frame}

\begin{frame}{Machine Learning is changing our world}
\begin{itemize}
   \item Search engines learn what you want
   \item Recommender systems learn your taste in books, music, movies,...
   \item Algorithms do automatic stock trading
   \item Elections are won by understanding voters
   \item Google Translate learns how to translate text
   \item Siri learns to understand speech
   \item DeepMind beats humans at Go
   \item Cars drive themselves
   \item Medicines are developed faster
   \item Smartwatches monitor your health
   \item Data-driven discoveries are made in Physics, Biology, Genetics, Astronomy, Chemistry, Neurology,...
\end{itemize}
\end{frame}

\begin{frame}{Machine Learning as Black-Box Modeling}

\begin{itemize}

\item
  Many concepts in ML can be explained without referring to the inner
  workings of a certain algorithm or model, especially things like model
  evaluation and tuning.
\item
  ML also nowadays consists of dozens (or hundreds?) of different
  modelling techniques, where it is quite unclear which of these are
  really needed (outside of pure research) and which are really best.
\item
  Understanding model-agnostic techniques is really paramount and can be
  achieved in a limited amount of time.
\end{itemize}

\end{frame}

\begin{frame}{ML as Black-Box Modeling}

Really studying the inner workings of each and every ML model can take
years. Do we even need to do this at all for some models?
\begin{itemize}
\item
  No: They exist in software. We can simply try them out, in the best
  case with an intelligent program that iterates over them and optimizes
  them.
\item
  Yes: At least some basic knowledge helps to make right choices.
  Actually knowing what you are doing is always good, also outside of
  science. And often stuff goes wrong, then understanding helps, too.
\end{itemize}

\end{frame}

\begin{frame}{ML as Black-Box Modeling}

\begin{itemize}
\item
  In the follwoing slides we will go through really fundamental terms
  and concepts in ML, that are relevant for everything that comes next.
\item
  We will also learn a couple of extremely simple models to obtain a
  basic understanding.
\item
  More complex stuff comes later.
\end{itemize}

Imagine you want to investigate how salary and workplace conditions
affect productivity of employees. Therefore, you collect data about
worked minutes per week (productivity), how many people work in the 
same office as the employees in question and the employees' salary.

\end{frame}


\begin{frame}{Data, Target and Input Features}

\scriptsize

\begin{center}\includegraphics[width=0.8\textwidth]{figure_man/data_table} \end{center}

\normalsize 

\vspace{-0.5cm}

The whole data set is expressed by \[
\D = \Dset
\] with the \(i\)-th observation \(\xyi\) $\in \mathcal{X}x\mathcal{Y}$.

$\mathcal{X}$ is called input and $\mathcal{Y}$ output or target space.

\end{frame}


\begin{frame}{Target and Features Relationship}

\begin{itemize}

\item
  For our observed data we know which outcome is produced:
\end{itemize}

\vspace{-0.5cm}

\scriptsize

\begin{center}\includegraphics[width=0.9\textwidth]{figure_man/new_data0_web} \end{center}

\normalsize 

\end{frame}

\begin{frame}{Target and Features Relationship}

\begin{itemize}

\item
  For new employees we can just observe the features:
\end{itemize}

\vspace{-0.5cm}

\scriptsize

\begin{center}\includegraphics[width=0.9\textwidth]{figure_man/new_data1_web} \end{center}

\normalsize 

\vspace{-0.5cm}

\(\Rightarrow\) The goal is to predict the target variable for
\textbf{unseen new data} by using a \textbf{model} trained on the
already seen \textbf{train data}.

\end{frame}

\begin{frame}{Supervised Learning Task}

\begin{itemize}
\item \textbf{Regression}: Given an input $x$, predict corresponding output from $\mathcal{Y} \in \mathbb{R}^m, 1 \leq m < \infty$.
\item \textbf{Classification}: Assigning an input $x$ to one class of a finite set of classes $\mathcal{Y} = \{C_1,...,C_m\}, 2 \leq m < \infty$.
\item \textbf{Density estimation}: Given an input $x$, predict the probability distribution $p(y|x)$ on $\mathcal{Y}$.


%\item 
  %\textbf{Regression task} if we have to predict a numeric target variable, e.g., the minutes an employee works per week.
%\item
  %\textbf{Classification task} if we have to predict a categorical
 % target state, e.g., if an employee is happy with her job or not.
 
\end{itemize}

\end{frame}

\begin{frame}{Regression Task}

\begin{itemize}
\item
  \textbf{Goal}: Predict a continuous output
\item
  \(y\) is a metric variable (with values in \(\R\))
\item
  Regression model can be constructed by different methods, e.g., linear
  regression, trees or splines
\end{itemize}

<<echo=FALSE, fig.height=4>>=
library(party)
library(ggplot2)

set.seed(1)
f = function(x) 0.5 * x^2 + x + sin(x)
x = runif(40, min = -3, max = 3)
y = f(x) + rnorm(40)
df = data.frame(x = x, y = y)
ggplot(df, aes(x, y)) + geom_point(size = 3) + stat_function(fun = f, color = "#FF9999", size = 2)
@

\end{frame}


\begin{frame}{Target and Features Relationship}

\scriptsize

\begin{center}\includegraphics[width=\textwidth]{figure_man/what_is_a_model_web} \end{center}

\normalsize 

\end{frame}

\begin{frame}{Target and Features Relationship}

\begin{itemize}
\item
  In ML, we want to be \enquote{lazy}. We do not want to specify \(f\)
  manually.
\item
  We want to learn it \textbf{automatically from labeled data}.
\item
  Later we will see that we do have to specify something, like \(f\)'s
  functional form and other stuff.
\item
  Mathematically, we face a problem of function approximation: search
  for an \(f\), such that, for all points in the training data and also
  all newly observed points
\end{itemize}

\begin{center}
  \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1cm,
      thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}]
    \node[punkt] (natur) {$y \approx f(x)$};
    \node[left=of natur] (x) {x};
    \node[right=of natur] (y) {y};
    \path[every node/.style={font=\sffamily\small}]
    (natur) edge node {} (y)
    (x) edge node  {} (natur)
    ;
  \end{tikzpicture}
\end{center}

\begin{itemize}

\item
  We call this \textbf{supervised learning}.
\end{itemize}

\end{frame}

\begin{frame}{What is a Model?}

%A model takes the features of new observations and produces a prediction
%\(\hat{y}\) of our target variable \(y\):

A model (or hypothesis) $f : \mathcal{X} \rightarrow \mathcal{Y}$ maps inputs (or input features) to outputs (or targets).

A hypothesis class $\mathcal{H}$ is a set of such functions.

\scriptsize

\begin{center}\includegraphics[width=0.9\textwidth]{figure_man/the_model_web} \end{center}

\normalsize 

\end{frame}

\begin{frame}{What is an Inducer?}

The \textbf{inducer} (learner, algorithm) takes our labeled data set
(\textbf{training set}) and produces a model (which again is a
function):

Applying a learning algorithm means coming up with a hypothesis given sample data (formally, it maps from $\{((x^{(1)},y^{(1)}),...,(x^{(n)},y^{(n)}))|1 \leq i \leq n < \infty ,x^{(i)} \in \mathcal{X},y^{(i)}, \in \mathcal{Y}\} to \mathcal{H}$).

\vspace{-0.5cm}

\scriptsize

\begin{center}\includegraphics[width=0.7\textwidth]{figure_man/the_inducer_web} \end{center}

\normalsize 

\end{frame}

\begin{frame}{How to Evaluate Models}

\begin{itemize}

\item
  Simply compare predictions from model with truth:
\end{itemize}

\scriptsize

\begin{center}\includegraphics[width=0.8\textwidth]{figure_man/eval_inducer1_web} \end{center}

\normalsize 
\end{frame}

\begin{frame}{Inducer Decomposition}

Nearly all ML supervised learning training algorithms can be described
by three components:

\begin{center}
\textbf{Learning = Representation + Evaluation + Optimization}
\end{center}

\begin{itemize}
\item
  \textbf{Representation / Hypothesis Space:} Defines functional
  structures of \(f\) we can learn.
\item
  \textbf{Evaluation:} How well does a certain hypothesis score on a
  given data set? Allows us to choose better candidates over worse ones.
\item
  \textbf{Optimization:} How do we search the hypothesis space? Guided
  by the evaluation metric.
\item
  All of these components represent important choices in ML which can
  have drastic effects:
  \newline
  If we make smart choices here, we can tailor our inducer to our needs
  - but that usually requires quite a lot of experience and deeper
  insights into ML.
\end{itemize}

\end{frame}

\begin{frame}{Inducer Decomposition}

\begin{table}[]
\begin{tabular}{lllll}
 \textbf{Representation} & \textbf{Evaluation} &  \textbf{Optimization}&  &  \\
Instances / Neighbours & Squared error & Gradient descent &  \\
Linear functions & Likelihood & Stochastic gradient descent  &  \\
Decision trees & Information gain & Quadratic programming & \\
Set of rules & K-L divergence & Greedy optimization & \\
Neural networks & & Combinatorial optimization & \\
Graphical models & & \\
\end{tabular}
\end{table}

Note: What is on the same line above does not belong together!

\end{frame}





\endlecture
