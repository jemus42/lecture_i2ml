% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@
% Load all R packages and set up knitr
<<setup-r, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{1}{Introduction}
\lecture{Introduction to Machine Learning}

<<include=FALSE>>=
set.seed(19042011)
runifCirc <- function(n, radius = 1, d = 2)
  t(sapply(seq_len(n), function(i) HI::rballunif(d, radius)))
@
\sloppy

\begin{frame}{Data Science and Machine Learning}

\begin{columns}
  \column{.3\textwidth}
    \begin{figure}
      % FIGURE SOURCE: No source available
      \includegraphics[width=\textwidth]{figure_man/gears.png}
    \end{figure}
  \column{0.6\textwidth}

  \begingroup
  \centering
    \fontsize{20pt}{22pt}\selectfont
    \vspace{1cm}

 % Machine Learning is a method of teaching computers to make predictions based on some data.
 %Machine learning is the study of algorithms that can automatically learn from and make predictions on data.
Machine learning is a branch of computer science and applied statistics covering
algorithms that improve their performance at a given task based on sample data.
\endgroup

\end{columns}

\end{frame}

% \begin{frame}{Data Science, Machine Learning and AI}
%
% \scriptsize
%
% \begin{center}\includegraphics[width=0.95\textwidth]{figure_man/learning} \end{center}
%
% \normalsize
%
% \end{frame}

\begin{frame}{Machine Learning is changing our world}
\begin{itemize}
   \item Search engines learn what you want
   \item Recommender systems learn your taste in books, music, movies,...
   \item Algorithms do automatic stock trading
   \item Google Translate learns how to translate text
   \item Siri learns to understand speech
   \item DeepMind beats humans at Go
   \item Cars drive themselves
   % \item Medicines are developed faster
   \item Smart-watches monitor your health
   \item Election campaigns use algorithmically targeted ads to influence voters
   \item Data-driven discoveries are made in Physics, Biology, Genetics, Astronomy, Chemistry, Neurology,...
   \item ...
\end{itemize}
\end{frame}

\begin{frame}{Machine Learning as Black-Box Modeling}

\begin{itemize}

\item
  Many concepts in ML can be explained without referring to the inner
  workings of a certain algorithm or model, especially things like model
  evaluation and hyperparameter tuning.
\item
  ML consists of dozens (or hundreds?) of different
  modelling techniques. Not clear which of them are
  really needed (outside of pure research) and which are really best.
\item
  Understanding basic concepts and model-agnostic techniques is really paramount and can be
  achieved in a limited amount of time.
\end{itemize}

\end{frame}

\begin{frame}{ML as Black-Box Modeling}

Studying to understand the inner workings of each and every ML model can take
years. Do we even need to do this at all for some models?
\begin{itemize}
\item
  No: The useful ones are implemented in software. We can simply try them out,
  hopefully using a helpful program that iterates over them and optimizes
  them for us (spoiler alert: that's \texttt{mlr}).
\item
  Yes: Some basic knowledge is required to make sensible choices.
  Actually knowing what it is you are doing is always good, also outside of
  science. \\
  And if things go wrong \\
  -- and they usually do --\\
  then understanding things really\\
  does help a lot, too.
  % this rhymes :)
\end{itemize}

\end{frame}

\begin{frame}{ML as Black-Box Modeling}

\begin{itemize}
\item
  In the following slides we will go through the fundamental terminology
  and concepts in ML which are relevant for everything that comes next.
\item
  We will also look at a couple of fairly simple ML models to obtain a
  basic understanding and look at some concrete examples.
\item
  More complex stuff comes later.
\end{itemize}

\end{frame}

\begin{vbframe}{Data, Target and Input Features}

Imagine you want to investigate how salary and workplace conditions
affect productivity of employees. Therefore, you collect data about
their worked minutes per week (productivity), how many people work in the
same office as the employees in question and the employees' salary.

\framebreak

\scriptsize

% FIGURE SOURCE: https://drive.google.com/open?id=1qIWHJq-iZqfUsLLJD81Z9LhobGTIN3sDHTevnm5dxZ0
\begin{center}\includegraphics[width=0.8\textwidth]{figure_man/data_table} \end{center}

\normalsize

\vspace{-0.5cm}

The entire \textbf{data set} is expressed by \[
\D = \Dset
\] with the \(i\)-th \textbf{observation} \(\xyi\) $\in \mathcal{X}\, \times \,\mathcal{Y}$.

$\mathcal{X}$ is called \textbf{input space} and contains all possible values of the \textbf{features}.

$\mathcal{Y}$ is the \textbf{output space} or \textbf{target space}  and contains all possible values of the \textbf{target variable}.

\end{vbframe}


\begin{frame}{Target and Features Relationship}

\begin{itemize}

\item
  For our observed data we know which outcome is produced:
\end{itemize}

\vspace{-0.5cm}

\scriptsize

% FIGURE SOURCE: https://drive.google.com/open?id=1WLPubv9vxLL-JIlHAtsvTBBG5pbF4xgRGW_prkOAEnE Page 1
\begin{center}\includegraphics[width=0.9\textwidth]{figure_man/new_data0_web} \end{center}

\normalsize

\end{frame}

\begin{frame}{Target and Features Relationship}

\begin{itemize}
\item For new employees we can only observe the features, but not the target:
\end{itemize}

\vspace{-0.5cm}

\scriptsize

% FIGURE SOURCE: https://drive.google.com/open?id=1WLPubv9vxLL-JIlHAtsvTBBG5pbF4xgRGW_prkOAEnE Page 2
\begin{center}\includegraphics[width=0.9\textwidth]{figure_man/new_data1_web} \end{center}

\normalsize

\vspace{-0.5cm}

\(\implies\) The goal is to predict the target variable for
\textbf{unseen new data} by using a \textbf{model} trained on the
already seen \textbf{training data}.\\
This is a \textbf{supervised learning} task.
In this course, we will only deal with ML problems of this kind.

\end{frame}

\begin{frame}{Supervised Learning Tasks}

\begin{itemize}
\item \textbf{Regression}: Given features $x$, predict corresponding output from $\mathcal{Y} \in \mathbb{R}^m, 1 \leq m < \infty$.
\item \textbf{Classification}: Assigning an observation with features $x$ to one class of a finite set of classes $\mathcal{Y} = \{C_1,...,C_g\}, 2 \leq g < \infty$.
\item \textbf{Density estimation}: Given an input $x$, predict the probability distribution $p(y|x)$ on $\mathcal{Y}$.
\end{itemize}

\end{frame}

\begin{frame}{Regression Task}

\begin{itemize}
\item
  \textbf{Goal}: Predict a continuous output
\item
  \(y\) is a metric variable (with values in \(\R\))
\item
  Regression model can be constructed by different methods, e.g., linear
  regression, trees or splines
\end{itemize}

<<echo=FALSE, fig.height=4>>=
library(party)
library(ggplot2)

set.seed(1)
f = function(x) 0.5 * x^2 + x + sin(x)
x = runif(40, min = -3, max = 3)
y = f(x) + rnorm(40)
df = data.frame(x = x, y = y)
ggplot(df, aes(x, y)) + geom_point(size = 3) + stat_function(fun = f, color = "#FF9999", size = 2)
@

\end{frame}


\begin{frame}{Target and Features Relationship}

\scriptsize

% FIGURE SOURCE: https://drive.google.com/open?id=1WLPubv9vxLL-JIlHAtsvTBBG5pbF4xgRGW_prkOAEnE Page 3
\begin{center}\includegraphics[width=\textwidth]{figure_man/what_is_a_model_web} \end{center}

\normalsize

\end{frame}

\begin{frame}{Target and Features Relationship}

\begin{itemize}
\item
  In ML, we want to be \enquote{lazy}. We do not want to specify \(f\)
  manually.
\item
  We want to learn \(f\) \textbf{automatically from labeled data}.
\item
  Mathematically, we face a problem of function approximation: search
  for an \(f\), such that, for all points in the training data and also
  all newly observed points
\end{itemize}

\begin{center}
  \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1cm,
      thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}]
    \node[punkt] (natur) {$y \approx f(x)$};
    \node[left=of natur] (x) {x};
    \node[right=of natur] (y) {y};
    \path[every node/.style={font=\sffamily\small}]
    (natur) edge node {} (y)
    (x) edge node  {} (natur)
    ;
  \end{tikzpicture}
\end{center}

\begin{itemize}

\item
  We call this \textbf{supervised learning}.
  \item
  (Later we will see that we need to specify at least \(f\)'s general structure
  and how to quantify the difference between $y$ and $f(x)$ in order
  to make this problem feasible.)
\end{itemize}

\end{frame}

\begin{frame}{What is a Model?}

%A model takes the features of new observations and produces a prediction
%\(\hat{y}\) of our target variable \(y\):

A \textbf{model} (or hypothesis) $f : \mathcal{X} \rightarrow \mathcal{Y}$ maps inputs (or input features) to outputs (or targets).

A hypothesis class $\Hspace$ is a set of such functions.

\scriptsize

% FIGURE SOURCE: https://drive.google.com/open?id=1bc6EQSsHEuVnyqFGX9E8oNfwOjAwVglRaIllxnOjLBo Page 1
\begin{center}\includegraphics[width=0.9\textwidth]{figure_man/the_model_web} \end{center}

\normalsize

\end{frame}

\begin{frame}{What is a learner?}

A \textbf{learner} (inducer, algorithm) takes a data set with features and outputs
(\textbf{training set}, $\in \mathcal{X}\, \times \,\mathcal{Y}$)  and produces a \textbf{model} (which is a function $f:\, \mathcal{X} \to \mathcal{Y}$):

So: Applying a learning algorithm means coming up with a model based on
training data.\\
%(formally, it maps from $\{((x^{(1)},y^{(1)}),...,(x^{(n)},y^{(n)}))|1 \leq i \leq n < \infty ,x^{(i)} \in \mathcal{X},y^{(i)}, \in \mathcal{Y}\} to \mathcal{H}$).

\vspace{-0.5cm}

\scriptsize

% FIGURE SOURCE: https://drive.google.com/open?id=1bc6EQSsHEuVnyqFGX9E8oNfwOjAwVglRaIllxnOjLBo Page 2
\begin{center}\includegraphics[width=0.7\textwidth]{figure_man/the_inducer_web} \end{center}

\normalsize

\end{frame}

\begin{frame}{How to Evaluate Models}

\begin{itemize}

\item
  Simply compare predictions from model with truth:
\end{itemize}

\scriptsize

% FIGURE SOURCE: https://drive.google.com/open?id=1dTc5act2POjELGuD8wFIUbPxG0QRZlg1PaYdHGU-FJM
\begin{center}\includegraphics[width=0.8\textwidth]{figure_man/eval_inducer1_web} \end{center}

\normalsize
\end{frame}

\begin{frame}{Components of a Learner}

Nearly all ML supervised learning training algorithms can be described
by three components:

\begin{center}
\textbf{Learning = Representation + Evaluation + Optimization}
\end{center}

\begin{itemize}
\item
  \textbf{Representation / Hypothesis Space:} Defines which kind of
  model structure of \(f\) can be learned from the data.
\item
  \textbf{Evaluation:} A metric that quantifies how well a specific model performs on a
  given data set. Allows us to rank candidate models in order to choose the best one.
\item
  \textbf{Optimization:} Defines how to search for the best model in the hypothesis space, typically guided by the evaluation metric.
\item
  All of these components represent important choices in ML which can
  have drastic effects:
  \newline
  By making smart choices here, we can tailor learners to specific problems
  - but that usually requires quite a lot of experience and deeper
  insights into ML.
\end{itemize}

\end{frame}

% \begin{frame}{Components of a Learner}
%
% \begin{table}[]
% \begin{tabular}{lllll}
%  \textbf{Representation} & \textbf{Evaluation} &  \textbf{Optimization}&  &  \\
% Instances / Neighbors & Squared error & Gradient descent &  \\
% Linear functions & Likelihood & Stochastic gradient descent  &  \\
% Decision trees & Information gain & Quadratic programming & \\
% Set of rules & K-L divergence & Greedy optimization & \\
% Neural networks & & Combinatorial optimization & \\
% Graphical models & & \\
% \end{tabular}
% \end{table}
%
% Note: What is on the same line above does not belong together!
%
% \end{frame}

\begin{frame}[squeeze]{Components of an Inducer}
\vskip -.5em
\begin{footnotesize}

$\textbf{Representation} :\begin{cases} \text{Neighbors}\\
\text{Linear functions}\\
\text{Decision trees}\\
\text{Sets of rules}\\
\text{Neural networks}\\
\text{Graphical models}\\
\text{...}
\end{cases}$

$\phantom{Representation } \textbf{Evaluation} :\begin{cases}
\text{Squared error}\\
\text{Misclassification}\\
\text{Likelihood}\\
\text{Information gain}\\
\text{...}
\end{cases}$

$\phantom{Representation Evaluation}\textbf{Optimization} :\begin{cases}
\text{Gradient descent}\\
\text{Quadratic programming}\\
\text{Combinatorial optimization}\\
\text{Genetic algorithms}\\
\text{...}
\end{cases}$

\end{footnotesize}
\end{frame}

\endlecture
