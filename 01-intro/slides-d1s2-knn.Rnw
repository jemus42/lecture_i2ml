% Introduction to Machine Learning
% Day 1

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@
% Load all R packages and set up knitr
<<setup-r, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{2}{k-nearest neighbours}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{Nearest neighbors: intuition}
\begin{itemize}
 \item Given cities belonging to 2 countries. Where is the border?
 \item Nearest neighbor: point belongs to closest cities
 \item k-Nearest neighbor: do vote over k nearest ones (smoother)
\end{itemize}
%\begin{center}
%  \includegraphics[width=.5\textwidth]{figure_man/nn.png}
%\end{center}
\end{vbframe}



\begin{vbframe}{k-Nearest-Neighbors}

\begin{itemize}
\item k-NN is a non-parametric method for regression and classification
\item Models predictions $\yh$ for $x$ by looking at $\xyi$ closest to $x$
\item Closeness implies distance measure (usually Euclidean)
\item $N_k(x)$ is called the \emph{neighborhood} of $x$, if it consists of the $k$-closest points $\xi$ to $x$
  in the training sample.
\end{itemize}


<<echo=FALSE, out.width='7cm'>>=
circleFun = function(center = c(0,0), diameter = 1, npoints = 100){
    r = diameter / 2
    tt = seq(0,2*pi,length.out = npoints)
    xx = center[1L] + r * cos(tt)
    yy = center[2L] + r * sin(tt)
    return(data.frame(x1 = xx, x2 = yy, class = NA))
}
set.seed(1234L)
n = 30L
x1 = rnorm(n)
x2 = rnorm(n)
mat = as.data.frame(cbind(x1, x2))
mat = rbind(mat, c(0, 0))
dists = sort(as.matrix(dist(mat))[n + 1L, ])
neighbs = as.numeric(names(dists[1:10L]))
mat$class = ifelse(1:(n+1) %in% neighbs, 1L, 0L)
mat[n + 1L, "class"] = 2L
mat$class = as.factor(mat$class)
circle.dat = circleFun(c(0, 0), 2.2, npoints = 100)
q = ggplot(mat, aes(x = x1, y = x2, color = class)) + geom_point(size = 3)
q = q + geom_polygon(data = circle.dat, alpha = 0.2, fill = "#619CFF")
q = q + theme(legend.position="none")
q
@

\framebreak
% new slide on distance measures

\textbf{How to calculate distances?}
  \begin{itemize}
    \item Most popular distance measure for numerical features: \textbf{Euclidian distance}
    \item Imagine two data points $x^{(i)} = (x_1^{(i)}, ..., x_q^{(i)})$ and $x^{(k)} = (x_1^{(k)}, ..., x_q^{(k)})$ with $q$ features $\in \R$
    \item The Euclidean distance:
    \begin{equation*}
      d_{Euclidean}\left(x^{(i)}, x^{(k)}\right) = \sqrt{\sum_{j=1}^q(x^{(i)}_j- x^{(k)}_j)^2}
    \end{equation*}
    \item It is based on a special case of the $L_p$-norm: $||x||_p = (|x_1|^p + ... + |x_q|^p)^\frac{1}{p}$ with $p = 2$
\framebreak

\item Example:
\begin{itemize}
  \item Three data points with two metric features each: $x^{(1)} = (1, 3),
  x^{(2)} = (4, 5)$ and $x^{(3)} = (7, 8)$
  \item Which is the nearest neighbor of $x^{(2)}$ in terms of the Euclidean distance?
  \item $d(x^{(2)},   x^{(1)}) = \sqrt{(4 - 1)^2 + (5 - 3)^2} = 3.61$
  \item $d(x^{(2)},  x^{(3)}) = \sqrt{(4 - 7)^2 + (5 - 8)^2} = 4.24$
  \item $\Rightarrow x^{(1)}$ is the nearest neighbor for $x^{(2)}$.
\end{itemize}
    \item Alternative distance measures are:
    \begin{itemize}
      \item Manhattan distance based on the $L_1$-norm:
        \begin{equation*}
          d_{manhattan}(x^{(i)}, x^{(k)}) = \sum_{j=1}^q(|x^{(i)}_j - x^{(k)}_j|)
        \end{equation*}
      \item Mahalanobis distance
    \end{itemize}
  \end{itemize}

\framebreak

Comparison between Euclidean and Manhattan distance measures \\
\vspace{0.2cm}
<<echo=FALSE, out.width='9.8cm'>>=
getCurrentAspect = function() {
   uy <- diff(grconvertY(1:2,"user","inches"))
   ux <- diff(grconvertX(1:2,"user","inches"))
   uy/ux
}
par(mar = c(4,4,0,0)+.1)
cex = 1.5
plot(x = c(1L,5L), y = c(1L,4L), ylim = c(0,5), xlim = c(0,6), pch = 19, las=1, 
  panel.first=grid(col = "lightgray", lty = "solid"),
  xlab = "Dimension 1", ylab = "Dimension 2", cex.lab = cex, cex.axis = cex, bty='l')
lines(x = c(1L,5L), y = c(1L,4L))
text(x = c(1L,5L), y = c(1L,4L), c(expression(X[i]), expression(X[j])), adj = c(1.5, 0), cex = cex)
lines(x = c(1L, 5L, 5L, 5L), y = c(1L, 1L, 1L, 4L), col = 2)
legend(x = -0.1, y = 5.1, lty = 1, legend = c("Manhattan", "Euclidean"), col = c(2,1), cex = cex,
  box.lwd = 0, box.col = "white", bg = "white")
text(x = 5, y = 0.9, expression(d(X[i],X[j])~"= |5-1| + |4-1| = 7"), adj = c(1,1), col = 2, cex = cex)
asp = getCurrentAspect()
text(x = 3, y = 2.5, expression(d(X[i],X[j])~"="~sqrt((5-1)^2 + (4-1)^2)~"= 5"),
  adj = c(0.5,0), col = 1, srt = 180/pi*atan(3/4*asp), cex=cex)
@

\framebreak

Predictions:
\begin{itemize}
\item For regression: \\
$$
\yh = \frac{1}{k} \sum_{\xi \in N_k(x)} \yi
$$
\item For classification a majority vote is used: \\
$$
\yh = \argmax_l \sum_{\xi \in N_k(x)} \I(\yi = l)
$$
And posterior probabilities can be estimated with:
$$
\hat{\pi}_l(x)= \frac{1}{k} \sum_{\xi \in N_k(x)} \I(\yi = l)
$$
\end{itemize}

\framebreak

Example with iris data excerpt (k = 3) \\
\begin{columns}[T]
  \begin{column}{0.5\textwidth}
    <<>>=
    circleFun2 = function(center = c(0,0), diameter = 1, npoints = 100){
      r = diameter / 2
      tt = seq(0,2*pi,length.out = npoints)
      xx = center[1L] + r * cos(tt)
      yy = center[2L] + r * sin(tt)
      return(data.frame(Sepal.Length = xx, Sepal.Width = yy, Species = NA))
    }
    data(iris)
    circle.dat2 = circleFun2(c(6.4,3), 0.24, npoints = 100)
    q = ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) + geom_point(size = 10) 
    q = q + geom_polygon(data = circle.dat2, alpha = 0.2, fill = "#619CFF")
    q = q + xlim(6.2, 6.6) + ylim(2.85, 3.2) + theme(legend.position = c(0.14, 0.82), text = element_text(size = 25))
    q = q  + annotate("text", x = 6.4, y = 3, label = "x[new]", size = 10, parse = TRUE)
    q
    @
  \end{column}
  \begin{column}{0.5\textwidth}
    <<>>=
    iris_excerpt = subset(iris, Sepal.Length > 6.1 & Sepal.Length < 6.7 & Sepal.Width > 2.8 & Sepal.Width < 3.3)
    iris_excerpt = unique(subset(iris_excerpt, select = -c(Petal.Width, Petal.Length)))
    rownames(iris_excerpt) = 1:nrow(iris_excerpt)
    kable(iris_excerpt, "latex", booktabs = F) %>%
      row_spec(c(3,7,10), color = "black", background = "#DFECFF")
    @
  \end{column}
\end{columns}
\vspace{0.6cm}
$ \hat{\pi}_{setosa}(x_{new}) = \frac{0}{3} = 0\% $ \\
$ \hat{\pi}_{versicolor}(x_{new}) = \frac{1}{3} = 33\% $ \\
$ \hat{\pi}_{virginica}(x_{new}) = \frac{2}{3} = 67\% $ \\
\vspace{0.6cm}
Prediction: highest posterior probability/majority vote:  \textit{virginica}

\framebreak

<<echo=FALSE, warning=FALSE, message=FALSE>>=
lrn1 = makeLearner("classif.kknn", par.vals = list(k = 3L))
lrn2 = makeLearner("classif.kknn", par.vals = list(k = 15L))
lrn3 = makeLearner("classif.kknn", par.vals = list(k = 30L))
lrn4 = makeLearner("classif.kknn", par.vals = list(k = 50L))
plotLearnerPrediction(lrn1, iris.task)
plotLearnerPrediction(lrn2, iris.task)
plotLearnerPrediction(lrn3, iris.task)
plotLearnerPrediction(lrn4, iris.task)
@

\framebreak

\begin{itemize}
\item k-NN has no training-step and is a very local model.
\item We cannot simply use least-squares loss on the training data for picking $k$,
  because we would always pick $k=1$.
\item k-NN makes no assumptions about the underlying data distribution.
\item The smaller k, the less stable, less smooth and more \enquote{wiggly} the decision
  boundary becomes.
\item Accuracy of k-NN can be severely degraded by the presence of noisy or irrelevant features,
  or if the feature scales are not consistent with their importance.
\item In binary classification, we might choose an odd k to avoid ties.
\item For $\yh$, we might inversely weigh neighbors with their distance to $x$, e.g., $w_i = 1/d(\xi, x)$
\end{itemize}
\end{vbframe}

\endlecture
