% Introduction to Machine Learning
% Day 1

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@
% Load all R packages and set up knitr
<<setup-r, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{3}{k-nearest neighbours}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{Nearest neighbors: intuition}
\begin{itemize}
 \item Given cities belonging to 2 countries. Where is the border?
 \item Nearest neighbor: point belongs to closest cities
 \item k-Nearest neighbor: do vote over k nearest ones (smoother)
\end{itemize}
\begin{center}
  \includegraphics[width=.5\textwidth]{figure_man/nn.png}
\end{center}
\end{vbframe}



<<setup-child2, include = FALSE>>=
library('kknn')
n = 30
set.seed(1234L)
tar = factor(c(rep(1L, times = n), rep(2L, times = n)))
feat1 = c(rnorm(n, sd = 1.5), rnorm(n, mean = 2, sd = 1.5))
feat2 = c(rnorm(n, sd = 1.5), rnorm(n, mean = 2, sd = 1.5))
bin.data = data.frame(feat1, feat2, tar)
bin.tsk = makeClassifTask(data = bin.data, target = "tar")
@


\begin{vbframe}{k-Nearest-Neighbors}

\begin{itemize}
\item k-NN is a non-parametric method for regression and classification
\item Models predictions $\yh$ for $x$ by looking at $\xyi$ closest to $x$
\item Closeness implies distance measure (usually euclidean)
\item $N_k(x)$ is called the \emph{neighborhood} of $x$, if it consists of the $k$-closest points $\xi$ to $x$
  in the training sample.
\end{itemize}


<<echo=FALSE, fig.height=3>>=
circleFun = function(center = c(0,0), diameter = 1, npoints = 100){
    r = diameter / 2
    tt = seq(0,2*pi,length.out = npoints)
    xx = center[1L] + r * cos(tt)
    yy = center[2L] + r * sin(tt)
    return(data.frame(x1 = xx, x2 = yy, class = NA))
}
set.seed(1234L)
n = 30L
x1 = rnorm(n)
x2 = rnorm(n)
mat = as.data.frame(cbind(x1, x2))
mat = rbind(mat, c(0, 0))
dists = sort(as.matrix(dist(mat))[n + 1L, ])
neighbs = as.numeric(names(dists[1:10L]))
mat$class = ifelse(1:(n+1) %in% neighbs, 1L, 0L)
mat[n + 1L, "class"] = 2L
mat$class = as.factor(mat$class)
circle.dat = circleFun(c(0, 0), 2.2, npoints = 100)
q = ggplot(mat, aes(x = x1, y = x2, color = class)) + geom_point(size = 3)
q = q + geom_polygon(data = circle.dat, alpha = 0.2, fill = "#619CFF")
q = q + theme(legend.position="none")
q
@

\framebreak
% new slide on distance measures

\textbf{How to calculate distances?}
  \begin{itemize}
    \item Most popular distance measure for numerical features: \textbf{euclidian distance}
    \item Imagine two data points $x^{(i)} = (x_1^{(i)}, ..., x_q^{(i)})$ and $x^{(k)} = (x_1^{(k)}, ..., x_q^{(k)})$ with $q$ features $\in \R$
    \item The euclidean distance between the points follows the formula:
    \begin{equation*}
      d_{euclidean}\left(x^{(i)}, x^{(k)}\right) = \sqrt{\sum_{j=1}^q(x^{(i)}_j- x^{(k)}_j)^2}
    \end{equation*}
    \item It is based on a special case of the $L_p$-norm: $||x||_p = (|x_1|^p + ... + |x_q|^p)^\frac{1}{p}$ with $p = 2$
\framebreak

\item Example:
\begin{itemize}
  \item Three data points with two metric features each: $x^{(1)} = (1, 3),
  x^{(2)} = (4, 5)$ and $x^{(3)} = (7, 8)^T$
  \item Which is the nearest neighbor of $x^{(2)}$ in terms of the euclidean distance?
  \item $d(x^{(2)},   x^{(1)}) = \sqrt{(4 - 1)^2 + (5 - 3)^2} = 3.61$
  \item $d(x^{(2)},  x^{(3)}) = \sqrt{(4 - 7)^2 + (5 - 8)^2} = 4.24$
  \item $\Rightarrow x^{(1)}$ is the nearest neighbor for $x^{(2)}$.
\end{itemize}
    \item Alternative distance measures are:
    \begin{itemize}
      \item Manhattan distance based on the $L_1$-norm:
        \begin{equation*}
          d_{manhattan}(x^{(i)}, x^{(k)}) = \sum_{j=1}^q(|x^{(i)}_j - x^{(k)}_j|)
        \end{equation*}
      \item Mahalanobis distance
    \end{itemize}
  \end{itemize}

\framebreak











Predictions:
\begin{itemize}
\item For regression: \\
$$
\yh = \frac{1}{k} \sum_{\xi \in N_k(x)} \yi
$$
\item For classification a majority vote is used: \\
$$
\yh = \argmax_l \sum_{\xi \in N_k(x)} \I(\yi = l)
$$
And posterior probabilities can be estimated with:
$$
\hat{\pi}_l(x)= \frac{1}{k} \sum_{\xi \in N_k(x)} \I(\yi = l)
$$
\end{itemize}

\framebreak


\textbf{Example data set: iris}\\
\begin{columns}[T]
  \begin{column}{0.4\textwidth}
    \begin{itemize}
      \item The iris data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris (setosa, versicolor, and virginica)
    \end{itemize}
  \end{column}
  \begin{column}{0.6\textwidth}
<<iris-plot, fig.height=12, fig.width=11>>=
iris = getTaskData(iris.task)
ggpairs(iris, aes(colour = Species))
@
  \end{column}
\end{columns}

\framebreak

<<echo=FALSE, warning=FALSE, message=FALSE>>=
lrn1 = makeLearner("classif.kknn", par.vals = list(k = 3L))
lrn2 = makeLearner("classif.kknn", par.vals = list(k = 15L))
lrn3 = makeLearner("classif.kknn", par.vals = list(k = 30L))
lrn4 = makeLearner("classif.kknn", par.vals = list(k = 50L))
plotLearnerPrediction(lrn1, iris.task)
plotLearnerPrediction(lrn2, iris.task)
plotLearnerPrediction(lrn3, iris.task)
plotLearnerPrediction(lrn4, iris.task)
@

\framebreak

\begin{itemize}
\item k-NN has many more parameters to estimate than the simpler LM.
\item k-NN has no training-step and is a very local model.
\item We cannot simply use least-squares loss on the training data for picking $k$,
  because we would always pick $k=1$.
\item k-NN makes no assumptions about the underlying data distribution.
\item The smaller k, the less stable, less smooth and more \enquote{wiggly} the decision
  boundary becomes.
\item Accuracy of k-NN can be severely degraded by the presence of noisy or irrelevant features,
  or if the feature scales are not consistent with their importance.
\item In binary classification, we might choose an odd k to avoid ties.
\item For $\yh$, we might inversely weigh neighbors with their distance to $x$, e.g., $w_i = 1/d(\xi, x)$
\item As the size of training data set approaches infinity, the 1-NN classifier guarantees
  an error rate of no worse than twice the Bayes error rate.
\end{itemize}
\end{vbframe}

\endlecture
