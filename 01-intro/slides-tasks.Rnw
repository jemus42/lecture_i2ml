% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@
% Load all R packages and set up knitr
<<setup-r, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{2}{Machine Learning Tasks}
\lecture{Introduction to Machine Learning}


<<include=FALSE>>=
set.seed(19042011)
runifCirc <- function(n, radius = 1, d = 2)
  t(sapply(seq_len(n), function(i) HI::rballunif(d, radius)))
library(party)
@
\sloppy

\begin{vbframe}{Supervised Learning}
\lz
\begin{itemize}
  \item One tries to learn the relationship between \enquote{input} $x$ and \enquote{output} $y$.
  \item For learning, there is training data with labels available
 \item Mathematically, we face a problem of function approximation: search for an $f$, such that,
  for all points in the training data, and also all newly observed points,
$$ y \approx f(x). $$
\end{itemize}

\framebreak

\textbf{Regression Task}
\lz
\begin{columns}[T]
  \begin{column}{0.5\textwidth}
    \structure{Goal}: Predict a continuous output
    \begin{itemize}
      \item $y$ is metric variable (with values in $\R$)
      \item Regression model can be constructed by different methods (e.g. trees or splines), not only statistical (linear) regression!
    \end{itemize}
  \end{column}
  \begin{column}{0.5\textwidth}
<<regression-task-plot, fig.height=8>>=
set.seed(1)
f = function(x) 0.5 * x^2 + x + sin(x)
x = runif(40, min = -3, max = 3)
y = f(x) + rnorm(40)
df = data.frame(x = x, y = y)
ggplot(df, aes(x, y)) + geom_point(size = 3) + stat_function(fun = f, color = "#FF9999", size = 2)
@
  \end{column}
\end{columns}

\framebreak

\textbf{Regression Task - Examples}

\begin{itemize}
  \item \textbf{Stock Trading:} Predicting the exact stock prices on the basis of company data and insider information
  \item \textbf{Pricing:} Anticipating the willingness-to-pay of new customers on the basis of purchases of other customers
  \item \textbf{Medicine:} Calculating the life expectancy for patients with a particular disease and severity (although life time analysis is often better here due to right censoring)
  \item \textbf{Income:} Predicting future income of a person based on education and skills
\end{itemize}

\framebreak

\textbf{Regression Task - Income Prediction}

\begin{center}
  \includegraphics[width=\textwidth]{figure_man/salary_prediction.png}
\end{center}
\vspace{-0.5cm}
\begin{flushright}
  \tiny https://www.dice.com/salary-calculator
\end{flushright}

\framebreak

\textbf{Binary Classification Task}
\lz
\begin{columns}[T]
  \begin{column}{0.5\textwidth}
    \structure{Goal}: Predict a class (or membership probabilities)
    \begin{itemize}
      \item $y$ is a categorical variable with two possible values
      \item Each observation belongs to exactly one class
    \end{itemize}
  \end{column}
  \begin{column}{0.5\textwidth}
<<classification-task-plot, fig.height=6, fig.width=6>>=
set.seed(1)
df2 = data.frame(x1 = c(rnorm(10, mean = 3), rnorm(10, mean = 5)), x2 = runif(10), class = rep(c("a", "b"), each = 10))
ggplot(df2, aes(x = x1, y = x2, shape = class, color = class)) + 
  geom_point(size = 3) + geom_abline(slope = -.22, intercept = 1.7, linetype = "longdash") + 
  scale_color_viridis_d()
@
  \end{column}
\end{columns}

\framebreak

\textbf{Binary Classification Task - Examples}

\begin{itemize}
  \item \textbf{Credits:} Predicting credit fraud or default risk based on transactions
  \item \textbf{Medical Diagnosis:} Medically testing whether a patient has a specific illness or not
  \item \textbf{Software:} Detecting whether an e-mail is spam or not by using its content
  \item \textbf{Lie Detection:} Determine truthfulness of statements from physiological cues
\end{itemize}

\framebreak

\textbf{Binary Classification Task - Lie Detection}
\vspace{-0.3cm}

\begin{center}
  \includegraphics[width=0.72\textwidth]{figure_man/lie-detector-polygraph.jpg}
\end{center}
\vspace{-0.6cm}
\begin{flushright}
  \tiny https://www.bendbulletin.com/localstate/deschutescounty/3430324-151/fact-or-fiction-polygraphs-just-an-investigative-tool
\end{flushright}

\framebreak

\textbf{Multiclass Classification Task}
\lz
\begin{columns}[T]
  \begin{column}{0.5\textwidth}
    \structure{Goal}: Predict a class (or membership probabilities)
    \begin{itemize}
      \item $y$ is a categorical variable with more than two different unordered discrete values
      \item Each observation belongs to exactly one class
    \end{itemize}  
  \end{column}
  \begin{column}{0.5\textwidth}
<<multi-classification-task-plot, fig.height=6, fig.width=6>>=
plotLearnerPrediction(makeLearner("classif.svm"), iris.task, c("Petal.Length", "Petal.Width")) +
  ggtitle("") +  scale_fill_viridis_d()
@
  \end{column}
\end{columns}

\framebreak

\textbf{Multiclass Classification Task - Examples}

\begin{itemize}
  \item \textbf{Image Recognition:} Deciding what animal (for example) a picture is showing
  \item \textbf{Stock Trading:} Identifying the best strategy for a specific stock (buy, sell, or wait) based on past prices 
  \item \textbf{Biology:} Classifying plants and animals based on their exterior characteristics (e. g. iris flowers)
  \item \textbf{Medical Diagnosis:} Predicting a patients illness using the their symptoms
\end{itemize}

\framebreak

\textbf{Multiclass Classification Task - Medical Diagnosis}

\begin{center}
  \includegraphics[width=0.8\textwidth]{figure_man/webmd.png}
\end{center}
\vspace{-0.5cm}
\begin{flushright}
  \tiny https://symptoms.webmd.com
\end{flushright}

\framebreak

\lz
\textbf{Classification Models}

\begin{itemize}
  \item Most classification models yield scoring functions for each of the $g$ classes: $\fx = (f_1(x), \dots, f_g(x)) \in \R^g$.
  \item These are often called \textbf{discriminant functions}, their outputs are class scores or class probabilities.
  \item The actual classification rule is usually defined as: $h(x) = \displaystyle \argmax_{k \in \{1, \dots ,g\}} f_k(x)$
  % \item This classification rule also defines the \textbf{decision boundaries} in the feature space $\mathcal X.$
\end{itemize}

\framebreak

\lz
\textbf{Other supervised learning tasks}
\begin{itemize}
  \item Multilabel classification
  \item Forecasting
  \item Survival prediction
  \item Cost-sensitive classification
\end{itemize}
\end{vbframe}


% Jann's summary slide for all other learning tasks
\begin{vbframe}{Additional Learning Tasks}
\lz
  \textbf{Unsupervised learning}
    \begin{itemize}
      \item Data without labels $y$
      \item Search for patterns within the inputs $x$
      \item \textit{unsupervised} as there is no external criterion to optimize or \enquote{true} output
      \item Possible applications:
      \begin{itemize}
        \item Dimensionality reduction (PCA, Autoencoders ...) : Compress information in $\mathcal X$
        \item Clustering: Grouping similar observations, separating dissimilar observations
        \item Outlier detection
        \item Association rules
      \end{itemize}
    \end{itemize}
\framebreak
\lz
  \textbf{Semi-Supervised learning}
  \begin{itemize}
    \item Large amount of labeled data necessary to train reliable model
    \item Creating labeled datasets often very expensive
    \item Learn from labeled (expensive) \textbf{and} unlabeled (cheap) data
    \item Unlabeled data in conjunction with a small amount of labeled data improves learning accuracy
  \end{itemize}
  \vspace{0.5cm}
  \textbf{Reinforcement learning}
  \begin{itemize}
    \item Select actions in subsequent  states within a certain environment to maximize lagged future reward
    \item Example: train neural net to play mario kart (environment)
    \begin{itemize}
      \item Accelerate/ steer/ break (actions) at each time point (states) during playing
      \item Reward: ranking after finish, should be maximized
    \end{itemize}
  \end{itemize}
\end{vbframe}


\begin{frame}{Machine Learning Tasks}

\begin{center}
  \includegraphics[height=0.5\textheight]{figure_man/ml-types.png}
\end{center}

\begin{itemize}
    \item In this course, we will deal with \textbf{supervised learning} for regression and classification only: predicting $y$ based on $x$, using a model $f(x)$ that we learned from labeled training data.
    \item Classification models come with a slight twist: they typically learn $g$ discriminant functions, and then these are turned into discrete predictions (details later). 
\end{itemize}

\end{frame}

\endlecture
