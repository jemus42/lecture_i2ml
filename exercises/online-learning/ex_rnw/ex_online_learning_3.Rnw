\renewcommand{\l}{L}
\newcommand{\Algo}{\texttt{Algo}}
\newcommand{\Aspace}{\mathcal{A}}
\newcommand{\Zspace}{\mathcal{Z}}
\newcommand{\norm}[1]{\left|\left|#1\right|\right|_2}
\begin{enumerate}
	%	
	\item  Consider an online quadratic optimization problem with action space $\Aspace=[-1,1]^d,$ environment data space $\Zspace=[-1,1]^d$ and the loss function given by $\l(a,z)=\frac12 \norm{a -z}^2.$ Furthermore, let $T=10000$ be the considered time horizon.
	%	
	Assume that the environmental data is generated uniformly at random in each time step $t\in\{1,\ldots,T\}.$
	%	
	Compute the cumulative regret of FTL and of FTRL instantiated with the squared L2-norm regularization and the optimal choice for the regularization magnitude for any time step $t=1,\ldots,T.$
	%	
	
	Repeat this procedure $100$ times and compute the empirical average of the resulting cumulative regrets in each time step $t=1,\ldots,T.$
	%	
	Note that this results in a curve with support points $(t,\bar R^\Algo_t)_{t=1,\ldots,T},$ where $\bar R^\Algo_t$ is the average cumulative regret (over the 100 repetitions) of algorithm $\Algo\in \{\text{FTL},\text{FTRL}\}$ till time step $t.$
	%	
	Plot this mean cumulative regret curve together with the theoretical upper bound for the cumulative regret of the FTRL algorithm in this case into one chart. 
%	
	Include also the empirical standard error of the mean cumulative regret, i.e.,  include the points $(t,\bar R^\Algo_t  \pm \hat\sigma(R^\Algo_t))_{t=1,\ldots,T},$ where $\hat\sigma(R^\Algo_t)$ is the empirical standard deviation of the cumulative regret (over the 100 repetitions) of algorithm $\Algo\in \{\text{FTL},\text{FTRL}\}$ till time step $t.$
	%	
	How does the mean cumulative regret curve of FTRL vary with respect to the regularization magnitude? Illustrate this also by means of a chart.
	
	
	\emph{Hint:} For $d$ you can, of course, consider different settings. 
	%
	\item Repeat (a), but this time consider an online linear optimization problem with the same action space, the same environment data space, but with the loss function given by $\l(a,z)= a^\top z.$  Comment on your findings.
	%	
	%
\end{enumerate}