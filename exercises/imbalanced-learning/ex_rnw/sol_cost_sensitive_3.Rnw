
Implement the MetaCost algorithm and use it with some classifier of your choice on an imbalanced data set of your choice, where the cost-matrix is given by the cost-sensitive heuristic we saw in the lecture. Compare the confusion matrices of the underlying classifier and the MetaCost classifier as well as their total costs.


<<fig.width=6, fig.height=4>>=
## --------------------------------------------


library(mlr3) 

set.seed(123)


# we use the spam data in the mlr3 package (spam=positive)
task        = tsk("spam")
data        = task$data()
n_vec       = table(data[,1])
g           = length(n_vec) # number of classes (two of course)

# we make a naive training-test split
n           = round(0.8 * task$nrow)
train_set   = 1:n
train_set   = sort(sample(task$nrow, n))
#unique(data[train_set,1])

test_set    = setdiff(seq_len(task$nrow), train_set)

# we use a CART as a classifier
classifier  = "classif.rpart"

# Create the cost matrix via the heuristic

cost_mat = matrix(rep(0,g*g),ncol=2)

for (i in 1:g){
  for (j in 1:g){
    cost_mat[i,j] = max(n_vec[i]/n_vec[j],1)
  }
  cost_mat[i,i]   = 0
}


#  we use CART as the black-box classifier
classifier  = "classif.rpart"
B           = round(n/4)
L           = 10

# 1st phase: Bagging

indices     = matrix(rep(0,B*L),ncol=L)

classifiers = c()

for (l in 1:L){
  
  # bootstrapped data set (or rather the indices)
  indices[,l]         = sort(sample(train_set, size=B, replace = FALSE))
  # fit classifier on bootstrapped data set
  learner             = lrn(classifier, predict_type = "prob")
  classifiers         = c(classifiers,learner$train(task, row_ids = indices[,l]))
  
}

#    2nd phase: Relabeling

y_tilde           = task$data()[,1]

for (i in train_set){
  
  
  # compute the proxy probability vector
  p               = rep(0,g)
  
  tilde_L         = c()
  
  # get all bootstrapped data sets, where i is not included
  
  temp            = which(colSums(i==indices)==0)
  
  if (length(temp)==0){ # the data point appears in each bootstrapped sample -> use all classifiers
    tilde_L       = 1:L
  } else { # only use classifiers which haven't seen the data point during training
    tilde_L       = sort(temp)
  }
  
  
  for (l in tilde_L){
    prediction    = classifiers[[l]]$predict(task, row_ids = i  )
    p             = p + prediction$prob
  }
  # average it
  p               = p/length(tilde_L)
  
  # relabeling
  # computes the estimated expected costs for predicting class 1,..,g
  est_cost        = cost_mat %*% t(p)
  # assign y the class with lowest estimated expected costs
  y_tilde[i]      = which(est_cost==min(est_cost))[1]
  
}

# create the relabeled data set
data_rel          =  task$data()
data_rel[,1]      =  y_tilde
task_new          =  TaskClassif$new("rel_data", backend = data_rel, target = task$target_names )

print("Number of relabeled data points")
print(sum(task_new$data()[,1]!=task$data()[,1]))

# initialize the cost-sensitive classifier
meta_classifier = lrn(classifier, predict_type = "prob")

# 3rd phase: make classifier cost-sensitive
meta_classifier$train(task_new, row_ids = train_set)        

# predict with meta

meta_pred   = meta_classifier$predict(task_new, row_ids = test_set)

# train also the cost-insensitive CART
learner     = lrn("classif.rpart", predict_type = "prob")
learner$train(task, row_ids = train_set)
CART_pred   = learner$predict(task, row_ids = test_set)


# Confusion Matrices

meta_pred$confusion
CART_pred$confusion

# Costs generated

sum(meta_pred$confusion*cost_mat)
sum(CART_pred$confusion*cost_mat)






@