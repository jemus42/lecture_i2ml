\documentclass[a4paper]{article}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R



\usepackage[utf8]{inputenc}
\pagenumbering{arabic}
%\usepackage[ngerman]{babel}
\usepackage{a4wide,paralist}
\usepackage{amsmath, amssymb, xfrac, amsthm}
\usepackage{dsfont}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{framed}
\usepackage{multirow}
\usepackage{bytefield}
\usepackage{csquotes}
\usepackage[breakable, theorems, skins]{tcolorbox}
\usepackage{hyperref}
\usepackage{cancel}
\usepackage{bm}

\input{../../style/common}

\tcbset{enhanced}

%exercise numbering
\renewcommand{\theenumi}{(\alph{enumi})}
\renewcommand{\theenumii}{\roman{enumii}}
\renewcommand\labelenumi{\theenumi}

\font \sfbold=cmssbx10
\setlength{\oddsidemargin}{0cm} \setlength{\textwidth}{16cm}

\sloppy
\parindent0em
\parskip0.5em
\topmargin-2.3 cm
\textheight25cm
\textwidth17.5cm
\oddsidemargin-0.8cm
% \pagestyle{empty}

\newcommand{\kopf}[1] {
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
	{\sf \bf \huge Exercise collection -- #1}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newcommand{\exlect}
  {\color{black} \hrule \section{Lecture exercises}}
  
\newcommand{\exexams}
  {\color{black} \hrule \section{Questions from past exams}}
  
\newcommand{\exinspo}
  {\color{black} \hrule \section{Ideas \& exercises from other sources}}

\newcounter{aufg}
\newenvironment{aufgabe}[1]
	{\color{black} \refstepcounter{aufg}
	\subsection{Exercise \arabic{aufg}: #1} 
	\noindent}
	{\vspace{0.5cm}}
	
\newenvironment{aufgabeexam}[3] % semester, main or retry exam, question number
	{\color{black} \refstepcounter{aufg}
	\subsection{Exercise \arabic{aufg}: #1, #2 exam, question #3}
	\noindent}
	{\vspace{1.5cm}}

\newcounter{loes}
\newenvironment{loesung}
	{\color{gray} \refstepcounter{loes}\textbf{Solution \arabic{loes}:}
	\\ \noindent}
	{\bigskip}

\setcounter{secnumdepth}{0}



\begin{document}
% !Rnw weave = knitr



\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-trees.tex}

\kopf{CART}

\tableofcontents

% ------------------------------------------------------------------------------
% LECTURE EXERCISES
% ------------------------------------------------------------------------------

\dlz
\exlect
\lz

\aufgabe{bagging and correlation}{

Show that the variance of the bagging prediction depends on the correlation 
between trees. 

Hint: compute $\text{Var}(\frac{1}{B}\sum_{b=1}^B f_b)$ when 
$\text{Var}(f_b)=\sigma^2$ and $\text{Corr}(f_i, f_j) = \rho$, where $f_b$ is a 
single tree of the ensemble.
}

\dlz
\loesung{

$f(x) = \frac{1}{B} \sum_{b=1}^B f_b(x)$ is the bagging estimator based on $B$ bootstrap samples. 
Then we can easily calculate:
\begin{align*}
\text{Var}(f(x)) &= \frac{1}{B^2} \left( \sum_{b=1}^B \text{Var}(f_b(x)) + \sum_{i \neq j}^B \text{Cov}(f_i(x), f_j(x)) \right ) \\
          &= \frac{1}{B^2} \left( B \sigma^2  + (B^2 - B) \rho \sigma^2) \right ) \\
          &=  \frac{1}{B}\sigma^2 + \rho \sigma^2 - \frac{1}{B} \rho \sigma^2  \\
          &=  \rho \sigma^2 + \frac{\sigma^2}{B} (1 - \rho) 
\end{align*}

In the first line the rules for variance of a non-independent sum of random variables is used. All other steps are trivial.

}

\aufgabe{split computation}{

Given are the dataset

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\begin{tabular}{l|r|r|r|r|r}
\hline
x & 1 & 2 & 7.0 & 10 & 20\\
\hline
y & 1 & 1 & 0.5 & 10 & 11\\
\hline
\end{tabular}

\end{knitrout}

and the same dataset, but with the feature x log-transformed

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\begin{tabular}{l|r|r|r|r|r}
\hline
log(x) & 0 & 0.7 & 1.9 & 2.3 & 3\\
\hline
y & 1 & 1.0 & 0.5 & 10.0 & 11\\
\hline
\end{tabular}

\end{knitrout}
Either manually compute the first split point that the CART algorithm would find for each dataset or
implement your own CART split-point-finding algorithm with a few lines of code.

}

\dlz
\loesung{

\begin{itemize}
  \item 

  Proceed as follows, when solving manually:

  \begin{enumerate}
    \item Split $x$ in two groups using the following split points.
    \begin{itemize}
      \item $ (1)$, $(2,7,10,20)$ (splitpoint 1.5)
      \item $ (1,2)$, $(7,10,20)$ (splitpoint 4.5)
      \item $ (1,2,7)$, $(10,20)$ (splitpoint 8.5)
      \item $ (1,2,7,10)$, $(20)$ (splitpoint 15)
    \end{itemize}
    \item For each possible split point compute the sum of squares in both groups.
    \item Use as split point the point that splits both groups best w.r.t. minimizing the sum of squares in both groups.
  \end{enumerate}

  Here, we have only one split variable $x$. A split point $t$, leads to the following half-spaces:% (here, we will have 4 possibilities, see figure below):

  \begin{align*}
  \Nl(t) &= \{ (x,y) \in \Np: x \leq t \} \text{ and } \Nr(t) = \{ (x,y) \in \Np: x > t \}.
  \end{align*}

  Remember the minimization Problem (here only for one split variable $x$):

  \begin{align*}
  \min_{t} \left(\min_{c_1} \sum_{(x,y) \in \Nl} (y - c_1)^2 + \min_{c_2} \sum_{(x,y) \in \Nr} (y - c_2)^2 \right).
  \end{align*}
  The inner minimization is solved through:
  $\hat{c}_1 = \bar{y}_1$ and $\hat{c}_2 = \bar{y}_2$

  Which results in:

  \begin{align*}
  \min_{t} \left(\sum_{(x,y) \in \Nl} (y - \bar{y}_1)^2 + \sum_{(x,y) \in \Nr} (y - \bar{y}_2)^2 \right).
  \end{align*}

  % <<echo = -1, fig.height=5, fig.width=5, fig.align='center'>>=
  % par(mfrow = c(2,2), mar = c(4,4,1,1))
  % x = c(1,2,7,10,20)
  % y = c(1,1,0.5,10,11)
  % for (i in 1:(length(x) - 1)) plot(x, y, col = as.factor(x <= x[i]))
  % @
  The sum of squares error of the parent is:
  $$Impurity_{parent} = MSE_{parent} = \frac{1}{5} \sum_{i=1}^5 (y_i - 4.7)^2 = 22.56 $$

  Calculate the risk for each split point:

  \begin{itemize}
    \item[$x \leq 1.5$]
      \begin{align*}
        \mathcal{R}(1, 1.5) &= \frac{1}{5}\text{MSE}_{left} + \frac{4}{5}\text{MSE}_{right}  =  \\
        &= \frac{1}{5} \cdot \frac{1}{1}(1 - 1)^2 + \frac{4}{5} \cdot\frac{1}{4}((1 - 5.625)^2 + (0.5 - 5.625)^2 + (10 - 5.625)^2 + (11 - 5.625)^2) \\
        &= 19.1375
      \end{align*}

    \item[$x \leq 4.5$] $\mathcal{R}(1, 4.5) = 13.43$
    \item[$x \leq 8.5$] $\mathcal{R}(1, 8.5) = 0.13$
    \item[$x \leq 15$] $\mathcal{R}(1, 15) = 12.64$

  \end{itemize}

  Minimal empirical risk is obtained by choosing the split point $8.5$.

  Doing the same for the $\log$-transformation gives:
  \begin{itemize}
    \item[$x \leq 0.3$] $\mathcal{R}(1, 0.3) = 19.14$
    \item[$x \leq 1.3$] $\mathcal{R}(1, 1.3) = 13.43$
    \item[$x \leq 2.1$] $\mathcal{R}(1, 2.1) = 0.13$
    \item[$x \leq 2.6$] $\mathcal{R}(1, 2.6) = 12.64$
  \end{itemize}

  Minimal empirical risk is obtained by choosing the split point $2.1$.


  \item Code example:
  
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{x} \hlkwb{=} \hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,}\hlnum{2}\hlstd{,}\hlnum{7}\hlstd{,}\hlnum{10}\hlstd{,}\hlnum{20}\hlstd{)}
\hlstd{y} \hlkwb{=} \hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,}\hlnum{1}\hlstd{,}\hlnum{0.5}\hlstd{,}\hlnum{10}\hlstd{,}\hlnum{11}\hlstd{)}

\hlstd{calculate_mse} \hlkwb{<-} \hlkwa{function} \hlstd{(}\hlkwc{y}\hlstd{)} \hlkwd{mean}\hlstd{((y} \hlopt{-} \hlkwd{mean}\hlstd{(y))}\hlopt{^}\hlnum{2}\hlstd{)}
\hlstd{calculate_total_mse} \hlkwb{<-} \hlkwa{function} \hlstd{(}\hlkwc{yleft}\hlstd{,} \hlkwc{yright}\hlstd{) \{}
  \hlstd{num_left} \hlkwb{<-} \hlkwd{length}\hlstd{(yleft)}
  \hlstd{num_right} \hlkwb{<-} \hlkwd{length}\hlstd{(yright)}

  \hlstd{w_mse_left} \hlkwb{<-} \hlstd{num_left} \hlopt{/} \hlstd{(num_left} \hlopt{+} \hlstd{num_right)} \hlopt{*} \hlkwd{calculate_mse}\hlstd{(yleft)}
  \hlstd{w_mse_right} \hlkwb{<-} \hlstd{num_right} \hlopt{/} \hlstd{(num_left} \hlopt{+} \hlstd{num_right)} \hlopt{*} \hlkwd{calculate_mse}\hlstd{(yright)}

  \hlkwd{return}\hlstd{(w_mse_left} \hlopt{+} \hlstd{w_mse_right)}
\hlstd{\}}

\hlstd{split} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{,} \hlkwc{y}\hlstd{) \{}
  \hlcom{# try out all unique points as potential split points and ...}
  \hlstd{unique_sorted_x} \hlkwb{<-} \hlkwd{sort}\hlstd{(}\hlkwd{unique}\hlstd{(x))}
  \hlstd{split_points} \hlkwb{<-} \hlstd{unique_sorted_x[}\hlnum{1}\hlopt{:}\hlstd{(}\hlkwd{length}\hlstd{(unique_sorted_x)} \hlopt{-} \hlnum{1}\hlstd{)]} \hlopt{+}
    \hlnum{0.5} \hlopt{*} \hlkwd{diff}\hlstd{(unique_sorted_x)}
  \hlstd{node_mses} \hlkwb{<-} \hlkwd{lapply}\hlstd{(split_points,} \hlkwa{function}\hlstd{(}\hlkwc{i}\hlstd{) \{}
    \hlstd{y_left} \hlkwb{<-} \hlstd{y[x} \hlopt{<=} \hlstd{i]}
    \hlstd{y_right} \hlkwb{<-} \hlstd{y[x} \hlopt{>} \hlstd{i]}

    \hlcom{# ... compute SS in both groups}
    \hlstd{mse_split} \hlkwb{<-} \hlkwd{calculate_total_mse}\hlstd{(y_left, y_right)}
    \hlkwd{print}\hlstd{(}\hlkwd{sprintf}\hlstd{(}\hlstr{"Split at %.1f: empirical Risk = %.2f"}\hlstd{, i, mse_split))}

    \hlkwd{return}\hlstd{(mse_split)}
  \hlstd{\})}
  \hlcom{# select the split point yielding the maximum impurity reduction}
  \hlstd{best} \hlkwb{<-} \hlkwd{which.min}\hlstd{(node_mses)}
  \hlstd{split_points[best]}
\hlstd{\}}

\hlstd{x}
\end{alltt}
\begin{verbatim}
## [1]  1  2  7 10 20
\end{verbatim}
\begin{alltt}
\hlkwd{split}\hlstd{(x, y)} \hlcom{# the 3rd observation is the best split point}
\end{alltt}
\begin{verbatim}
## [1] "Split at 1.5: empirical Risk = 19.14"
## [1] "Split at 4.5: empirical Risk = 13.43"
## [1] "Split at 8.5: empirical Risk = 0.13"
## [1] "Split at 15.0: empirical Risk = 12.64"
## [1] 8.5
\end{verbatim}
\begin{alltt}
\hlkwd{log}\hlstd{(x)}
\end{alltt}
\begin{verbatim}
## [1] 0.0000000 0.6931472 1.9459101 2.3025851 2.9957323
\end{verbatim}
\begin{alltt}
\hlkwd{split}\hlstd{(}\hlkwd{log}\hlstd{(x), y)} \hlcom{# also here, the 3rd observation is the best split point}
\end{alltt}
\begin{verbatim}
## [1] "Split at 0.3: empirical Risk = 19.14"
## [1] "Split at 1.3: empirical Risk = 13.43"
## [1] "Split at 2.1: empirical Risk = 0.13"
## [1] "Split at 2.6: empirical Risk = 12.64"
## [1] 2.124248
\end{verbatim}
\end{kframe}
\end{knitrout}


\end{itemize}
}

\aufgabe{lymphography classification}{

Download the lymphography dataset from moodle.

\begin{enumerate}

  \item Download the file lymphography.csv from moodle and read it in using \texttt{read.csv()}
  \item Have a short look into the background and structure of the data.
  \item Delete 6 observations from the smallest class, so the resulting problem is binary classification.

\end{enumerate}

Now fit CART (from \texttt{rpart}) and a second model of your choice to the data and answer the following questions:

\begin{itemize}

  \item How ``stable'' are the resulting trees from the CART model?
  \item How do the results differ between pruned and unpruned trees?
  \item Is one of the two methods better suited for the data?
\end{itemize}
}

\dlz
\loesung{

See 
\href{https://github.com/compstat-lmu/lecture_i2ml/blob/master/exercises/trees/ex_rnw/sol_trees_2.R}{R code}
}


\newpage

\aufgabe{leaf node prediction}{

The fractions of the classes $k=1,\ldots, g$ in node $\mathcal{N}$ of a decision 
tree are $\pi^{\mathcal{(N)}}_1,\ldots,\pi^{\mathcal{(N)}}_g$.
Assume we replace the classification rule in node $\mathcal{N}$

\begin{eqnarray*}
\hat{k}|\mathcal{N}=\arg\max_k \pikN
\end{eqnarray*}
with a randomizing rule, in which we draw the classes in one node from their 
estimated probabilities.

Compute the expectation of the misclassification rate in node $\mathcal{N}$, for 
data distributed like the training data, assuming independent observations. 
What do you notice? (\textit{Hint}: The observations and the predictions using 
the randomizing rule follow the same distribution.)
}

\dlz
\loesung{

According to the lecture for a target $y$ with target space $\mathcal{Y} = \{1, \dots, g\}$ the target class proportion $\pikN$  of class $k \in \mathcal{Y}$ in a node can be computed, s.t.
$$ \pikN = \frac{1}{|\Np|} \sum\limits_{(x^{(i)},y^{(i)}) \in \Np} [y^{(i)} = k].$$
Now for any $n \in \mathbb{N}$ let $Y^{(1)},\dots,Y^{(n)},\hat{Y}^{(1)},\dots,\hat{Y}^{(n)}$ be i.i.d. random variables, where $Y^{(i)}$ and $\hat{Y}^{(i)}$ are categorically distributed with
$$\mathbb{P}(Y^{(i)} = k| \mathcal{N}) = \mathbb{P}(\hat{Y}^{(i)} = k| \mathcal{N}) = \pikN \quad \forall i \in \{1,\dots,n\},\quad k \in \mathcal{Y}.$$
The random variables $Y^{(1)},\dots,Y^{(n)}$ represent data distributed like the training data\footnote{under the independence assumption} of size $n$ and the random variables $\hat{Y}^{(1)},\dots,\hat{Y}^{(n)}$ the corresponding estimators using the randomizing rule.
With these we can define the misclassification rate $\mathsf{err}_{\mathcal{N}}$ of node $\mathcal{N}$ for data distributed like the training data, s.t
$$\mathsf{err}_{\mathcal{N}} = \frac{1}{n}\sum^n_{i=1}[Y^{(i)} \neq \hat{Y}^{(i)}].$$
We're interested in the expected misclassification rate $\mathsf{err}_{\mathcal{N}}$ of node $\mathcal{N}$ for data distributed like the training data, i.e.,
\begin{align*}
\mathbb{E}_{Y^{(1)},\dots,Y^{(n)},\hat{Y}^{(1)},\dots,\hat{Y}^{(n)}} \left(\mathsf{err}_{\mathcal{N}}\right) &= \mathbb{E}_{Y^{(1)},\dots,Y^{(n)},\hat{Y}^{(1)},\dots,\hat{Y}^{(n)}}\left(\frac{1}{n}\sum^n_{i=1}[Y^{(i)} \neq \hat{Y}^{(i)}] \right)\\
& = \frac{1}{n}\sum^n_{i=1}\mathbb{E}_{Y^{(i)},\hat{Y}^{(i)}}\left([Y^{(i)} \neq \hat{Y}^{(i)}]\right) \\
& = \frac{1}{n}\sum^n_{i=1}\mathbb{E}_{Y^{(i)}}\left(\mathbb{E}_{\hat{Y}^{(i)}}\left([Y^{(i)} \neq \hat{Y}^{(i)}]\right)\right) \\
& = \frac{1}{n}\sum^n_{i=1}\mathbb{E}_{Y^{(i)}} \left( \sum^g_{k=1} [Y^{(i)} \neq k] \pikN\right) \\
& = \frac{1}{n}\sum^n_{i=1}\mathbb{E}_{Y^{(i)}} \left( \sum_{k \in \mathcal{Y} \setminus \{Y^{(i)}\}} \pikN\right) \\
& = \frac{1}{n}\sum^n_{i=1}\mathbb{E}_{Y^{(i)}} \left( 1 - \pi^{\mathcal{(N)}}_{Y^{(i)}}\right) \\
& = \frac{1}{n}\sum^n_{i=1} \sum^g_{k=1} (1 - \pikN)\pikN \\
& = \frac{n}{n} \sum^g_{k=1} (1 - \pikN)\pikN \\
& = 1 -  \sum^g_{k=1} \left(\pikN \right)^2.
\end{align*}

This is exactly the Gini-Index which CART uses for splitting the tree.
}

% ------------------------------------------------------------------------------
% PAST EXAMS
% ------------------------------------------------------------------------------

\dlz
\exexams
\lz

\aufgabeexam{WS2020/21}{retry}{1}{}

% ------------------------------------------------------------------------------
% INSPO
% ------------------------------------------------------------------------------

\dlz
\exinspo
\end{document}
