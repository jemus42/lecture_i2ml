\documentclass[a4paper]{article}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R




\usepackage[utf8]{inputenc}
%\usepackage[ngerman]{babel}
\usepackage{a4wide,paralist}
\usepackage{amsmath, amssymb, xfrac, amsthm}
\usepackage{dsfont}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{framed}
\usepackage{multirow}
\usepackage{bytefield}
\usepackage{csquotes}
\usepackage[breakable, theorems, skins]{tcolorbox}
\usepackage{hyperref}
\usepackage{cancel}
\usepackage{bm}


\input{../../style/common}

\tcbset{enhanced}

\DeclareRobustCommand{\mybox}[2][gray!20]{%
	\iffalse
	\begin{tcolorbox}[   %% Adjust the following parameters at will.
		breakable,
		left=0pt,
		right=0pt,
		top=0pt,
		bottom=0pt,
		colback=#1,
		colframe=#1,
		width=\dimexpr\linewidth\relax,
		enlarge left by=0mm,
		boxsep=5pt,
		arc=0pt,outer arc=0pt,
		]
		#2
	\end{tcolorbox}
	\fi
}

\DeclareRobustCommand{\myboxshow}[2][gray!20]{%
%	\iffalse
	\begin{tcolorbox}[   %% Adjust the following parameters at will.
		breakable,
		left=0pt,
		right=0pt,
		top=0pt,
		bottom=0pt,
		colback=#1,
		colframe=#1,
		width=\dimexpr\linewidth\relax,
		enlarge left by=0mm,
		boxsep=5pt,
		arc=0pt,outer arc=0pt,
		]
		#2
	\end{tcolorbox}
%	\fi
}


%exercise numbering
\renewcommand{\theenumi}{(\alph{enumi})}
\renewcommand{\theenumii}{\roman{enumii}}
\renewcommand\labelenumi{\theenumi}


\font \sfbold=cmssbx10

\setlength{\oddsidemargin}{0cm} \setlength{\textwidth}{16cm}


\sloppy
\parindent0em
\parskip0.5em
\topmargin-2.3 cm
\textheight25cm
\textwidth17.5cm
\oddsidemargin-0.8cm
\pagestyle{empty}

\newcommand{\kopf}[1] {
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Introduction to Machine Learning \hfill Exercise sheet #1\\
	 \url{https://introduction-to-machine-learning.netlify.app/} \hfill WiSe 2020/2021}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newenvironment{allgemein}
	{\noindent}{\vspace{1cm}}

\newcounter{aufg}
\newenvironment{aufgabe}
	{\refstepcounter{aufg}\textbf{Exercise \arabic{aufg}:}\\ \noindent}
	{\vspace{0.5cm}}

\newcounter{loes}
\newenvironment{loesung}
	{\refstepcounter{loes}\textbf{Solution \arabic{loes}:}\\\noindent}
	{\bigskip}
	
\newenvironment{bonusaufgabe}
	{\refstepcounter{aufg}\textbf{Exercise \arabic{aufg}*\footnote{This
	is a bonus exercise.}:}\\ \noindent}
	{\vspace{0.5cm}}

\newenvironment{bonusloesung}
	{\refstepcounter{loes}\textbf{Solution \arabic{loes}*:}\\\noindent}
	{\bigskip}



\begin{document}
% !Rnw weave = knitr



\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-trees.tex}

\kopf{7}


\loesung{

\begin{itemize}
  \item 

  Proceed as follows, when solving manually:

  \begin{enumerate}
    \item Split $x$ in two groups using the following split points.
    \begin{itemize}
      \item $ (1)$, $(2,7,10,20)$ (splitpoint 1.5)
      \item $ (1,2)$, $(7,10,20)$ (splitpoint 4.5)
      \item $ (1,2,7)$, $(10,20)$ (splitpoint 8.5)
      \item $ (1,2,7,10)$, $(20)$ (splitpoint 15)
    \end{itemize}
    \item For each possible split point compute the sum of squares in both groups.
    \item Use as split point the point that splits both groups best w.r.t. minimizing the sum of squares in both groups.
  \end{enumerate}

  Here, we have only one split variable $x$. A split point $t$, leads to the following half-spaces:% (here, we will have 4 possibilities, see figure below):

  \begin{align*}
  \Nl(t) &= \{ (x,y) \in \Np: x \leq t \} \text{ and } \Nr(t) = \{ (x,y) \in \Np: x > t \}.
  \end{align*}

  Remember the minimization Problem (here only for one split variable $x$):

  \begin{align*}
  \min_{t} \left(\min_{c_1} \sum_{(x,y) \in \Nl} (y - c_1)^2 + \min_{c_2} \sum_{(x,y) \in \Nr} (y - c_2)^2 \right).
  \end{align*}
  The inner minimization is solved through:
  $\hat{c}_1 = \bar{y}_1$ and $\hat{c}_2 = \bar{y}_2$

  Which results in:

  \begin{align*}
  \min_{t} \left(\sum_{(x,y) \in \Nl} (y - \bar{y}_1)^2 + \sum_{(x,y) \in \Nr} (y - \bar{y}_2)^2 \right).
  \end{align*}

  % <<echo = -1, fig.height=5, fig.width=5, fig.align='center'>>=
  % par(mfrow = c(2,2), mar = c(4,4,1,1))
  % x = c(1,2,7,10,20)
  % y = c(1,1,0.5,10,11)
  % for (i in 1:(length(x) - 1)) plot(x, y, col = as.factor(x <= x[i]))
  % @
  The sum of squares error of the parent is:
  $$Impurity_{parent} = MSE_{parent} = \frac{1}{5} \sum_{i=1}^5 (y_i - 4.7)^2 = 22.56 $$

  Calculate the risk for each split point:

  \begin{itemize}
    \item[$x \leq 1.5$]
      \begin{align*}
        \mathcal{R}(1, 1.5) &= \frac{1}{5}\text{MSE}_{left} + \frac{4}{5}\text{MSE}_{right}  =  \\
        &= \frac{1}{5} \cdot \frac{1}{1}(1 - 1)^2 + \frac{4}{5} \cdot\frac{1}{4}((1 - 5.625)^2 + (0.5 - 5.625)^2 + (10 - 5.625)^2 + (11 - 5.625)^2) \\
        &= 19.1375
      \end{align*}

    \item[$x \leq 4.5$] $\mathcal{R}(1, 4.5) = 13.43$
    \item[$x \leq 8.5$] $\mathcal{R}(1, 8.5) = 0.13$
    \item[$x \leq 15$] $\mathcal{R}(1, 15) = 12.64$

  \end{itemize}

  Minimal empirical risk is obtained by choosing the split point $8.5$.

  Doing the same for the $\log$-transformation gives:
  \begin{itemize}
    \item[$x \leq 0.3$] $\mathcal{R}(1, 0.3) = 19.14$
    \item[$x \leq 1.3$] $\mathcal{R}(1, 1.3) = 13.43$
    \item[$x \leq 2.1$] $\mathcal{R}(1, 2.1) = 0.13$
    \item[$x \leq 2.6$] $\mathcal{R}(1, 2.6) = 12.64$
  \end{itemize}

  Minimal empirical risk is obtained by choosing the split point $2.1$.


  \item Code example:
  
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{x} \hlkwb{=} \hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,}\hlnum{2}\hlstd{,}\hlnum{7}\hlstd{,}\hlnum{10}\hlstd{,}\hlnum{20}\hlstd{)}
\hlstd{y} \hlkwb{=} \hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,}\hlnum{1}\hlstd{,}\hlnum{0.5}\hlstd{,}\hlnum{10}\hlstd{,}\hlnum{11}\hlstd{)}

\hlstd{calculate_mse} \hlkwb{<-} \hlkwa{function} \hlstd{(}\hlkwc{y}\hlstd{)} \hlkwd{mean}\hlstd{((y} \hlopt{-} \hlkwd{mean}\hlstd{(y))}\hlopt{^}\hlnum{2}\hlstd{)}
\hlstd{calculate_total_mse} \hlkwb{<-} \hlkwa{function} \hlstd{(}\hlkwc{yleft}\hlstd{,} \hlkwc{yright}\hlstd{) \{}
  \hlstd{num_left} \hlkwb{<-} \hlkwd{length}\hlstd{(yleft)}
  \hlstd{num_right} \hlkwb{<-} \hlkwd{length}\hlstd{(yright)}

  \hlstd{w_mse_left} \hlkwb{<-} \hlstd{num_left} \hlopt{/} \hlstd{(num_left} \hlopt{+} \hlstd{num_right)} \hlopt{*} \hlkwd{calculate_mse}\hlstd{(yleft)}
  \hlstd{w_mse_right} \hlkwb{<-} \hlstd{num_right} \hlopt{/} \hlstd{(num_left} \hlopt{+} \hlstd{num_right)} \hlopt{*} \hlkwd{calculate_mse}\hlstd{(yright)}

  \hlkwd{return}\hlstd{(w_mse_left} \hlopt{+} \hlstd{w_mse_right)}
\hlstd{\}}

\hlstd{split} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{,} \hlkwc{y}\hlstd{) \{}
  \hlcom{# try out all unique points as potential split points and ...}
  \hlstd{unique_sorted_x} \hlkwb{<-} \hlkwd{sort}\hlstd{(}\hlkwd{unique}\hlstd{(x))}
  \hlstd{split_points} \hlkwb{<-} \hlstd{unique_sorted_x[}\hlnum{1}\hlopt{:}\hlstd{(}\hlkwd{length}\hlstd{(unique_sorted_x)} \hlopt{-} \hlnum{1}\hlstd{)]} \hlopt{+}
    \hlnum{0.5} \hlopt{*} \hlkwd{diff}\hlstd{(unique_sorted_x)}
  \hlstd{node_mses} \hlkwb{<-} \hlkwd{lapply}\hlstd{(split_points,} \hlkwa{function}\hlstd{(}\hlkwc{i}\hlstd{) \{}
    \hlstd{y_left} \hlkwb{<-} \hlstd{y[x} \hlopt{<=} \hlstd{i]}
    \hlstd{y_right} \hlkwb{<-} \hlstd{y[x} \hlopt{>} \hlstd{i]}

    \hlcom{# ... compute SS in both groups}
    \hlstd{mse_split} \hlkwb{<-} \hlkwd{calculate_total_mse}\hlstd{(y_left, y_right)}
    \hlkwd{print}\hlstd{(}\hlkwd{sprintf}\hlstd{(}\hlstr{"Split at %.1f: empirical Risk = %.2f"}\hlstd{, i, mse_split))}

    \hlkwd{return}\hlstd{(mse_split)}
  \hlstd{\})}
  \hlcom{# select the split point yielding the maximum impurity reduction}
  \hlstd{best} \hlkwb{<-} \hlkwd{which.min}\hlstd{(node_mses)}
  \hlstd{split_points[best]}
\hlstd{\}}

\hlstd{x}
\end{alltt}
\begin{verbatim}
## [1]  1  2  7 10 20
\end{verbatim}
\begin{alltt}
\hlkwd{split}\hlstd{(x, y)} \hlcom{# the 3rd observation is the best split point}
\end{alltt}
\begin{verbatim}
## [1] "Split at 1.5: empirical Risk = 19.14"
## [1] "Split at 4.5: empirical Risk = 13.43"
## [1] "Split at 8.5: empirical Risk = 0.13"
## [1] "Split at 15.0: empirical Risk = 12.64"
## [1] 8.5
\end{verbatim}
\begin{alltt}
\hlkwd{log}\hlstd{(x)}
\end{alltt}
\begin{verbatim}
## [1] 0.0000000 0.6931472 1.9459101 2.3025851 2.9957323
\end{verbatim}
\begin{alltt}
\hlkwd{split}\hlstd{(}\hlkwd{log}\hlstd{(x), y)} \hlcom{# also here, the 3rd observation is the best split point}
\end{alltt}
\begin{verbatim}
## [1] "Split at 0.3: empirical Risk = 19.14"
## [1] "Split at 1.3: empirical Risk = 13.43"
## [1] "Split at 2.1: empirical Risk = 0.13"
## [1] "Split at 2.6: empirical Risk = 12.64"
## [1] 2.124248
\end{verbatim}
\end{kframe}
\end{knitrout}


\end{itemize}
}

\dlz

\pagebreak

\loesung{

According to the lecture for a target $y$ with target space $\mathcal{Y} = \{1, \dots, g\}$ the target class proportion $\pikN$  of class $k \in \mathcal{Y}$ in a node can be computed, s.t.
$$ \pikN = \frac{1}{|\Np|} \sum\limits_{(x^{(i)},y^{(i)}) \in \Np} [y^{(i)} = k].$$
Now for any $n \in \mathbb{N}$ let $Y^{(1)},\dots,Y^{(n)},\hat{Y}^{(1)},\dots,\hat{Y}^{(n)}$ be i.i.d. random variables, where $Y^{(i)}$ and $\hat{Y}^{(i)}$ are categorically distributed with
$$\mathbb{P}(Y^{(i)} = k| \mathcal{N}) = \mathbb{P}(\hat{Y}^{(i)} = k| \mathcal{N}) = \pikN \quad \forall i \in \{1,\dots,n\},\quad k \in \mathcal{Y}.$$
The random variables $Y^{(1)},\dots,Y^{(n)}$ represent data distributed like the training data\footnote{under the independence assumption} of size $n$ and the random variables $\hat{Y}^{(1)},\dots,\hat{Y}^{(n)}$ the corresponding estimators using the randomizing rule.
With these we can define the misclassification rate $\mathsf{err}_{\mathcal{N}}$ of node $\mathcal{N}$ for data distributed like the training data, s.t
$$\mathsf{err}_{\mathcal{N}} = \frac{1}{n}\sum^n_{i=1}[Y^{(i)} \neq \hat{Y}^{(i)}].$$
We're interested in the expected misclassification rate $\mathsf{err}_{\mathcal{N}}$ of node $\mathcal{N}$ for data distributed like the training data, i.e.,
\begin{align*}
\mathbb{E}_{Y^{(1)},\dots,Y^{(n)},\hat{Y}^{(1)},\dots,\hat{Y}^{(n)}} \left(\mathsf{err}_{\mathcal{N}}\right) &= \mathbb{E}_{Y^{(1)},\dots,Y^{(n)},\hat{Y}^{(1)},\dots,\hat{Y}^{(n)}}\left(\frac{1}{n}\sum^n_{i=1}[Y^{(i)} \neq \hat{Y}^{(i)}] \right)\\
& = \frac{1}{n}\sum^n_{i=1}\mathbb{E}_{Y^{(i)},\hat{Y}^{(i)}}\left([Y^{(i)} \neq \hat{Y}^{(i)}]\right) \\
& = \frac{1}{n}\sum^n_{i=1}\mathbb{E}_{Y^{(i)}}\left(\mathbb{E}_{\hat{Y}^{(i)}}\left([Y^{(i)} \neq \hat{Y}^{(i)}]\right)\right) \\
& = \frac{1}{n}\sum^n_{i=1}\mathbb{E}_{Y^{(i)}} \left( \sum^g_{k=1} [Y^{(i)} \neq k] \pikN\right) \\
& = \frac{1}{n}\sum^n_{i=1}\mathbb{E}_{Y^{(i)}} \left( \sum_{k \in \mathcal{Y} \setminus \{Y^{(i)}\}} \pikN\right) \\
& = \frac{1}{n}\sum^n_{i=1}\mathbb{E}_{Y^{(i)}} \left( 1 - \pi^{\mathcal{(N)}}_{Y^{(i)}}\right) \\
& = \frac{1}{n}\sum^n_{i=1} \sum^g_{k=1} (1 - \pikN)\pikN \\
& = \frac{n}{n} \sum^g_{k=1} (1 - \pikN)\pikN \\
& = 1 -  \sum^g_{k=1} \left(\pikN \right)^2.
\end{align*}

This is exactly the Gini-Index which CART uses for splitting the tree.
}

\dlz
\end{document}
