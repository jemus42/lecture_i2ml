The fractions of the classes $k=1,\ldots, g$ in node $\Np$ of a decision 
tree are $\pi^{(\Np)}_1,\ldots,\pi^{(\Np)}_g$.
Assume we replace the classification rule in node $\Np$
$$\hat{k}~|~\Np=\arg\max_k \pikN$$
with a randomizing rule
$$\hat k \sim \text{Cat} \left(\pi^{(\Np)}_1,\ldots,\pi^{(\Np)}_g \right),$$ 
in which we draw the classes in one node from the categorical distribution of 
their estimated probabilities (i.e., class $k$ is predicted with probability 
$\pi^{(\Np)}_k$).

Compute the expected MCE in node $\Np$ for 
data distributed i.i.d. like the training data.
What do you notice?

(\textit{Hint}: The observations and the predictions using 
the randomizing rule follow the same distribution.)