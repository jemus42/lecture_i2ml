Let $\Xspace = \R$ and assume the following statistical model
%
$$
y = f(x) + \eps, \qquad \eps \sim\mathcal{N}\left(0, \sigma^2\right),
$$
%
where $f(x) \in \mathcal{GP}\left(0, k(x,x') \right).$
%
Suppose the covariance function of the GP is 
%
$$  k(x,x') = \mathds{1}_{[|x-x'|<1]} \cdot (1-|x-x'|)$$
%
and we have seen the training data:
%

\begin{center}
	\begin{tabular}{c|cc}
		$i$ & $\xi$ & $\yi$  \\
		\hline
		1 & 1.6 & 3.0 \\
		\hline
		2 & 2.8 & 3.3 \\
		\hline
		3 & 0.5 & 2.0 \\
		\hline
		4 & 3.9 & 2.7 \\
	\end{tabular}
\end{center}

As a test input we observe $x_* = 1.2.$
%
Recall that the predictive distribution for $f(x_*)$ is
%
  \begin{eqnarray*}
	f(x_*) ~|~  \Xmat, \bm{y}, x_* \sim \mathcal{N}(m_{\text{post}}, k_\text{post}).
\end{eqnarray*}
%
with 
%
\begin{eqnarray*}
%	
	m_{\text{post}} &=& \bm{K}_*^{T} \left(\Kmat+ \sigma^2 \cdot \id\right)^{-1}\bm{y} \\
%	
	k_\text{post} &=& K_{**} - \bm{K}_*^T \left(\Kmat + \sigma^2 \cdot \id\right)^{-1} \bm{K}_*,
%	
\end{eqnarray*}
%
Here, $\Kmat = \left(k\left(\xi, \xv^{(j)}\right)\right)_{i,j}$, $\bm{K}_* = \left(k\left(x_*, \xi[1]\right), ..., k\left(x_*, \xi[n]\right)\right)^\top$ and $ K_{**}\ = k(x_*, x_*)$. 
%

\begin{enumerate}
%	
	\item Compute the predictive mean $	m_{\text{post}}.$
	\lz
	\lz
	\lz
	\lz
	\lz
	\lz
	\lz
	\lz
%
	\item Compute the predictive variance $k_\text{post}.$
	\lz
	\lz
	\lz
	\lz
	\lz
%
	\item Repeat the calculations from (a) and (b) by using as the test input $x_* = \xi$ for each $i=1,2,3,4,$ respectively. 
	\lz
	\lz
	\lz
	\lz
	\lz
	\item Based on your calculations so far, try to sketch the posterior Gaussian process. 
	\lz
	\lz
	\lz
	\lz
	\lz	
	\lz
	\lz
	\lz
	\lz
	\lz
	\item If the nugget $\sigma^2$ would be zero, how would the posterior Gaussian process (roughly) look like?
	\lz
	\lz
	\lz
	\lz
	\lz	
	\lz
	\lz
	\lz
	\lz
	\lz
	\item Quiz time: Log in to Particify (\url{https://partici.fi/63221686}) and try to answer the questions for \textbf{Week 13}.	
%	
%	
\end{enumerate}