
Consider the regression learning setting, i.e., $\mathcal{Y} = \R,$ and feature space $\Xspace = \R^p.$
%
Let the hypothesis space be the linear models:
%
$$  \Hspace = \{ \fx = \thetab^\top \xv ~|~  \thetab \in \R^p  \}. $$
%
Suppose your loss function of interest is the L2 loss $\Lxy= \frac12 \big(y-\fx\big)^2.$
%
Consider the $L_1$-regularized empirical risk of a model $\fxt$ (i.e., Lasso regression):
  $$
  \riskrt = \risket + \lambda \|\thetab\|_1 =  \frac12 \sum_{i=1}^n \left( \yi - \thetab^\top \xi \right)^2 + \lambda \sum_{i=1}^p  |\theta_i|.
  $$
%
Assume that ${\Xmat}^T \Xmat = \id,$ which holds if $\Xmat$ has orthonormal columns.
%
Show that the minimizer $\thetah_{\text{Lasso}} = (\thetah_{\text{Lasso},1},\ldots,\thetah_{\text{Lasso},p})^\top$ is given by
%
$$		\thetah_{\text{Lasso},i} = sgn(\thetah_{i}) \max\{|\thetah_{i}| - \lambda,0 \}, \quad i=1,\ldots,p,	$$
%
where $\thetabh = (\thetah_{1},\ldots,\thetah_{p})^\top = ({\Xmat}^T \Xmat)^{-1} \Xmat^T\yv$ is the minimizer of the unregularized empirical risk (w.r.t.\ the L2 loss).

For this purpose, use the following steps:
%
\begin{itemize}
%	
	\item [(i)] Derive that 
%	
	$$	\argmin_{\thetab} \riskrt  = 	\argmin_{\thetab} \sum_{i=1}^p	- \thetah_{i}\theta_i + \frac{\theta_i^2}{2} + \lambda|\theta_i|.$$
	
	\lz
	\lz
	\lz
	\lz
	\lz
	\lz
	\lz
	\lz
	\lz
	
	
%	
	\item [(ii)] Note that the minimization problem on the right-hand side of (i) can be written as $\sum_{i=1}^p g_i(\theta_i),$ where 
	$$ g_i(\theta) =  - \thetah_{i}\theta + \frac{\theta^2}{2} + \lambda |\theta|. $$ 
%
	What is the advantage of this representation if we seek to find the $\thetab$ with entries $\theta_1,\ldots,\theta_p$ minimizing $\riskrt?$
		
	\lz
	\lz
	\lz
	
	
%
	\item [(iii)] Consider first the case that $\thetah_i>0$ and infer that for the minimizer $\theta^*_i$ of $g_i$ it must hold that $\theta^*_i\geq 0.$ 
	
	\emph{Hint:} Compare $g_i(\theta)$ and $g_i(-\theta)$ for $\theta\geq0.$
	
	
		
	\lz
	\lz
	\lz
	\lz
	\lz
	\lz
	\lz
	\lz
	\lz
	
	\item [(iv)] Derive that $\theta^*_i = \max\{|\thetah_{i}| - \lambda,0 \},$   by using (iii) (and also still considering the case $\thetah_i > 0.$)
	
	
		
	\lz
	\lz
	\lz
	\lz
	\lz
	\lz
	\lz
	\lz
	\lz
	
%	
	\item [(v)] Consider the complementary case of (iii) and (iv), i.e., $\thetah_i \leq 0,$ and infer that for the minimizer $\theta^*_i$ of $g_i$ it must hold that $\theta^*_i \leq 0.$
	
	
		
	\lz
	\lz
	\lz
	\lz
	\lz
	\lz
	\lz
	\lz
	\lz
	
	
	%
	\item [(vi)] Derive that $\theta^*_i = \min\{ \thetah_{i} + \lambda,0 \},$   by using (v) (and also still considering the case $\thetah_i \leq 0.$)
	
		
	\lz
	\lz
	\lz
	\lz
	\lz
	\lz
	\lz
	\lz
	\lz
	
	%
	\item [(vii)] Make sure that both minimizers in the two case can indeed be written as $sgn(\thetah_{i}) \max\{|\thetah_{i}| - \lambda,0 \}.$
	
	
		
	\lz
	\lz
	\lz
	\lz
	\lz
	\lz
	\lz
	\lz
	\lz
	
%	
	\item [(viii)] Quiz time: Log in to Particify (\url{https://partici.fi/63221686}) and try to answer the questions for \textbf{Week 9}.
%	
\end{itemize}