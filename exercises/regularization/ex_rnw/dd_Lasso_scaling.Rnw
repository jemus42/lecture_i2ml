

Consider the regression learning setting, i.e., $\mathcal{Y} = \R,$ and feature space $\Xspace = \R^p.$
%
Let the hypothesis space be the linear models:
%
$$  \Hspace = \{ \fx = \thetab^\top \xv ~|~  \thetab \in \R^p  \}. $$
%
%Suppose your loss function of interest is the L2 loss $\Lxy= \frac12 \big(y-\fx\big)^2.$
%
Assume that ${\Xmat}^T \Xmat = \id,$ which holds if $\Xmat$ has orthonormal columns.
%

Now let $\mathbf{s} =(s_1,\ldots,s_p)^\top \in \R \backslash \{0\}$ be some scaling vector for the features, so that we obtain the scaled feature matrix
%
$$  \mathbf{Z} =  \Xmat  \begin{pmatrix}
	s_1 & 0 & \ldots & 0 \\
	0 & s_2 & \ldots & 0 \\
	\vdots & \vdots & \ddots & \vdots \\
	0 & 0 & \ldots & s_p
\end{pmatrix} 
 =  \begin{pmatrix}
s_1 x_1^{(1)} & s_2 x_2^{(1)} & \ldots & s_p x_p^{(1)} \\
s_1 x_1^{(2)} & s_2 x_2^{(2)} & \ldots & s_p x_p^{(2)} \\
\vdots & \vdots & \ddots & \vdots \\
s_1 x_1^{(n)} & s_2 x_2^{(n)} & \ldots & s_p x_p^{(n)}
\end{pmatrix}. $$
%
Thus, the feature $\xv_i$ is scaled by $s_i$ for any $i=1,\ldots,p.$

It holds that
%
\begin{align} \label{z_product}
%	
	\mathbf{Z}^\top \mathbf{Z} = \begin{pmatrix}
		s_1 & 0 & \ldots & 0 \\
		0 & s_2 & \ldots & 0 \\
		\vdots & \vdots & \ddots & \vdots \\
		0 & 0 & \ldots & s_p
	\end{pmatrix}  {\Xmat}^T  \Xmat  \begin{pmatrix}
	s_1 & 0 & \ldots & 0 \\
	0 & s_2 & \ldots & 0 \\
	\vdots & \vdots & \ddots & \vdots \\
	0 & 0 & \ldots & s_p
\end{pmatrix}  = \begin{pmatrix}
		s_1^2 & 0 & \ldots & 0 \\
		0 & s_2^2 & \ldots & 0 \\
		0 & 0 & \ldots & s_p^2
	\end{pmatrix} 
%	
\end{align}
%
and 
%
\begin{align} \label{y_projection}
%	
	\mathbf{Z}^\top \yv = \begin{pmatrix}
		s_1 & 0 & \ldots & 0 \\
		0 & s_2 & \ldots & 0 \\
		\vdots & \vdots & \ddots & \vdots \\
		0 & 0 & \ldots & s_p
	\end{pmatrix}  {\Xmat}^T  \yv
%	
	= \begin{pmatrix}
		s_1 & 0 & \ldots & 0 \\
		0 & s_2 & \ldots & 0 \\
		\vdots & \vdots & \ddots & \vdots \\
		0 & 0 & \ldots & s_p
	\end{pmatrix} \underbrace{( {\Xmat}^T \Xmat )^{-1} {\Xmat}^T  \yv}_{= \thetabh^X}
%
	= \begin{pmatrix}
		s_1  \thetabh^X_1\\
		s_2 \thetabh^X_2\\
		\vdots \\
		s_p \thetabh^X_p
	\end{pmatrix}.
%	
\end{align}
%
So, $\thetabh^X$ is the minimizer of the unregularized empirical risk (w.r.t.\ the L2 loss) if we wouldn't rescale the features and accordingly use the original $\xv$ data.

Let us now specify the Lasso regularization risk for the scaled data (the $\mathbf{z}$ data):
%
  $$
\mathcal{R}_{\text{reg}}^{\mathbf{Z}}(\thetab) =  \frac12 \sum_{i=1}^n \left( \yi - \thetab^\top \mathbf{z}^{(i)} \right)^2 + \lambda \sum_{i=1}^p  |\theta_i|.
$$
\clearpage
%
\begin{itemize}
%	
	\item  We can show that
%	
	$$	\argmin_{\thetab} \mathcal{R}_{\text{reg}}^{\mathbf{Z}}(\thetab) =  	\argmin_{\thetab} \sum_{i=1}^p	- s_i \thetah_{i}^X \theta_i + \frac{s_i^2 \theta_i^2}{2} + \lambda|\theta_i|$$
	%
	holds as follows:
	%
	\begin{align*}
	%	
	\argmin_{\thetab}  \mathcal{R}_{\text{reg}}^{\mathbf{Z}}(\thetab) 
	%
	&= \argmin_{\thetab} \frac12 \sum_{i=1}^n \left( \yi - \thetab^\top \mathbf{z}^{(i)} \right)^2 + \lambda \sum_{i=1}^p |\theta_i| \\
	%	
	&= \argmin_{\thetab} \frac12 \| \yv - \mathbf{Z} \thetab \|_2^2 + \lambda \sum_{i=1}^p |\theta_i|  \\
	%	
	&= \argmin_{\thetab} \frac12  (\yv - \mathbf{Z} \thetab)^\top(\yv - \mathbf{Z} \thetab) + \lambda \sum_{i=1}^p |\theta_i|  \\
	%	
	&= \argmin_{\thetab} \frac12  \yv^\top \yv -\yv^\top \mathbf{Z} \thetab +  \frac12 \thetab^\top \mathbf{Z}^\top \mathbf{Z} \thetab  + \lambda \sum_{i=1}^p |\theta_i|  \\
	%	
	&= \argmin_{\thetab}  -\yv^\top \mathbf{Z} \thetab +  \frac12 \thetab^\top \mathbf{Z}^\top \mathbf{Z} \thetab  + \lambda \sum_{i=1}^p |\theta_i|  \tag{ $ \yv^\top \yv$ does not depend on $\thetab$  }\\
	%	
	&= \argmin_{\thetab}  -\yv^\top \mathbf{Z} \thetab +  \frac12 \thetab^\top \begin{pmatrix}
		s_1^2 & 0 & \ldots & 0 \\
		0 & s_2^2 & \ldots & 0 \\
		0 & 0 & \ldots & s_p^2
	\end{pmatrix}  \thetab  + \lambda \sum_{i=1}^p |\theta_i|  \tag{By \eqref{z_product} }\\
	%	
	&= \argmin_{\thetab}  -(s_1  \thetabh^X_1,	s_2 \thetabh^X_2,
		\ldots ,
		s_p \thetabh^X_p )^\top \thetab +  \frac12 \thetab^\top \begin{pmatrix}
			s_1^2 & 0 & \ldots & 0 \\
			0 & s_2^2 & \ldots & 0 \\
			0 & 0 & \ldots & s_p^2
		\end{pmatrix} \thetab  + \lambda \sum_{i=1}^p |\theta_i|  \tag{By \eqref{y_projection} }\\	
	%	
	&= \argmin_{\thetab} \sum_{i=1}^p	- s_i \thetah_{i}^X \theta_i + \frac{s_i^2 \theta_i^2}{2} + \lambda |\theta_i|  \tag{Writing out the inner products}\\
	%	
	\end{align*}
%
%
	\item If we consider the case that $\thetah_i^X>0,$ we can infer that for the minimizer $\theta^*_i$ of 	$$ g_i(\theta) = - s_i \thetah_{i}^X  \theta + \frac{s_i^2 \theta^2}{2} + \lambda |\theta| $$  it must hold that $\theta^*_i\geq 0.$ 
%	
	Indeed, if $\theta\geq0,$ then 
%	
	$$	g_i(\theta) =  - s_i \thetah_{i}^X  \theta + \frac{s_i^2 \theta^2}{2} + \lambda |\theta|  <  s_i \thetah_{i}^X  \theta + \frac{s_i^2 \theta^2}{2} + \lambda |\theta|  =  g_i(-\theta),		$$
%	
	so the minimizer of $g_i$ has the same sign as $\thetah_i^X.$
	
	\item Now differentiating  $g_i$ (for $\theta\geq 0$) and setting it to zero implies
	%
	\begin{align*}
		%	
		&\frac{\partial g_i(\theta)}{\partial \theta} = - s_i \thetah_{i}^X + s_i^2 \theta + \lambda \stackrel{!}{=} 0 \\
		%	
		\Leftrightarrow \quad & \theta = \thetah_{i}^X/s_i - \lambda/s_i^2.
		%	
	\end{align*}
%
	Thus, the minimizer in this case is 
%	
	$$\theta^*_i = sgn(\thetah_{i}^X) \max\left\{ \frac{|\thetah_{i}^X|}{s_i}  -  \frac{\lambda}{s_i^2} ,0 \right\}.$$   
%	
	\item In the complementary case, i.e., $\thetah_i \leq 0,$ we obtain similarly that 
%	
	$$\theta^*_i = sgn(\thetah_{i}^X) \max\left\{ \frac{|\thetah_{i}^X|}{s_i}  -  \frac{\lambda}{s_i^2} ,0 \right\}.$$
%	
%	
\end{itemize}
%
Thus, the minimizer of $\mathcal{R}_{\text{reg}}^{\mathbf{Z}}(\thetab) $ is $\thetabh^Z_{\text{Lasso}} = ( \thetah^Z_{\text{Lasso},1}, \ldots, \thetah^Z_{\text{Lasso},p}    )^\top $ with 
%
$$  \thetah^Z_{\text{Lasso},i} = sgn(\thetah_{i}^X) \max\left\{ \frac{|\thetah_{i}^X|}{s_i}  -  \frac{\lambda}{s_i^2} ,0 \right\}, \quad i=1,\ldots,p, $$
%
while the minimizer of the $L_1$-regularized empirical risk for the original $\xv$ data, i.e., 
$$
\riskrt = \risket + \lambda \|\thetab\|_1 =  \frac12 \sum_{i=1}^n \left( \yi - \thetab^\top \xi \right)^2 + \lambda \sum_{i=1}^p  |\theta_i|
$$
%
is\footnote{This is what we show on in-class exercise sheet 09.} $\thetah_{\text{Lasso}} = (\thetah_{\text{Lasso},1},\ldots,\thetah_{\text{Lasso},p})^\top$ where
%
$$		\thetah_{\text{Lasso},i} = sgn(\thetah_{i}^X) \max\{|\thetah_{i}^X| - \lambda,0 \}, \quad i=1,\ldots,p.	$$
%
%
If $s_i>1$, i.e., we scale the $i$-th feature up, then its coefficient will live on a smaller scale $ \leadsto \frac{|\thetah_{i}^X|}{s_i}$ and the feature will be less penalized $\leadsto \frac{\lambda}{s_i^2}.$


