

Consider the regression learning setting, i.e., $\mathcal{Y} = \R,$ and feature space $\Xspace = \R^p.$
%
Let the hypothesis space be the linear models:
%
$$  \Hspace = \{ \fx = \thetab^\top \xv ~|~  \thetab \in \R^p  \}. $$
%
Suppose your loss function of interest is the L2 loss $\Lxy= \frac12 \big(y-\fx\big)^2.$
%
Consider the $L_0$-regularized empirical risk of a model $\fxt:$
  $$
  \riskrt = \risket + \lambda \|\thetab\|_0 =  \frac12 \sum_{i=1}^n \left( \yi - \thetab^\top \xi \right)^2 + \lambda \sum_{i=1}^p \mathds{1}_{|\theta_i| \neq 0}.
  $$
%
Assume that ${\Xmat}^T \Xmat = \id,$ which holds if $\Xmat$ has orthonormal columns.
%
Show that the minimizer $\thetah_{\text{L0}} = (\thetah_{\text{L0},1},\ldots,\thetah_{\text{L0},p})^\top$ is given by
%
$$		\thetah_{\text{L0},i} = \thetah_{i} \mathds{1}_{|\thetah_{i}| > \sqrt{2\lambda}}, \quad i=1,\ldots,p,	$$
%
where $\thetabh = (\thetah_{1},\ldots,\thetah_{p})^\top = ({\Xmat}^T \Xmat)^{-1} \Xmat^T\yv$ is the minimizer of the unregularized empirical risk (w.r.t.\ the L2 loss).

For this purpose, use the following steps:


\begin{itemize}
%	
	\item [(i)] Derive that 
%	
	$$	\argmin_{\thetab} \riskrt  = 	\argmin_{\thetab} \sum_{i=1}^p	- \thetah_{i}\theta_i + \frac{\theta_i^2}{2} + \lambda \mathds{1}_{|\theta_i| \neq 0}.$$
%	
	\item [(ii)] Note that the minimization problem on the right-hand side of (i) can be written as $\sum_{i=1}^p g_i(\theta_i),$ where 
	$$ g_i(\theta) =  - \thetah_{i}\theta + \frac{\theta^2}{2} + \lambda \mathds{1}_{|\theta| \neq 0}. $$ 
%
	What is the advantage of this representation if we seek to find the $\thetab$ with entries $\theta_1,\ldots,\theta_p$ minimizing $\riskrt?$
%
	\item [(iii)] Consider first the case that $|\thetah_i|>\sqrt{2\lambda}$ and infer that for the minimizer $\theta^*_i$ of $g_i$ it must hold that $\theta^*_i=\thetah_i.$
	
	\emph{Hint:} Show that $g_i(\thetah_i)<  0 = g_i(0)$ and argue that the minimizer must have the same sign as $\thetah_i.$
	
	\item [(iv)] Derive that $\theta^*_i = \thetah_{i} \mathds{1}_{|\thetah_{i}| > \sqrt{2\lambda}},$   by using (iii) (and also still considering the case $|\thetah_i|>\sqrt{2\lambda}$).
%	
	\item [(v)] Consider the complementary case of (iii) and (iv), i.e., $|\thetah_i|\leq \sqrt{2\lambda},$ and infer that for the minimizer $\theta^*_i$ of $g_i$ it must hold that $\theta^*_i=0.$
	
	\emph{Hint:} What is $g_i(0)?$ Consider $\tilde g_i(\theta) =  
		- \thetah_{i}\theta + \frac{\theta^2}{2} + \lambda$ which is the smooth extension of $g_i.$ What is the relationship between the minimizer of $g_i$ and the minimizer of $\tilde g_i?$
%	
\end{itemize}