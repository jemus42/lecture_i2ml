\begin{enumerate}
\item Take a look at the \texttt{spam} dataset from the package \texttt{ElemStatLearn}.
Shortly describe what kind of classification problem this is and create a task for \texttt{mlr}.

The spam data is a classification task where the aim is to classify an email as spam or no-spam.

<<>>=
library(mlr)
library(ElemStatLearn)
spam.task = makeClassifTask(data = spam, target = "spam")
spam.task
@
\item Use a decision tree to predit spam. Try refitting with different samples. How stable are the trees?

<<fig.height=5, cache = FALSE>>=
lrn = makeLearner("classif.rpart")
model = train(lrn, spam.task)
mod = getLearnerModel(model)
mod

set.seed(42)
subset1 = sample.int(nrow(spam), size = 0.8 * nrow(spam))
subset2 = sample.int(nrow(spam), size = 0.8 * nrow(spam))

model = train(lrn, spam.task, subset = subset1)
mod = getLearnerModel(model)
mod


model = train(lrn, spam.task, subset = subset2)
mod = getLearnerModel(model)
mod

@
Observation: Trees with different sample find different split points and variables, leading to different trees! 

\item Use a random forest to fit the model and plot the oob-error against the number of trees used.

Hint: You can use \texttt{getLearnerModel(model)}.

<<fig.height=5, cache = FALSE>>=
lrn = makeLearner("classif.randomForest")
model = train(lrn, spam.task)
mod = getLearnerModel(model)
mod
plot(mod)
@

\item Your boss wants to know which variables have the biggest influence on the prediction quality. Explain your approach in words as well as code.

Hint: use \texttt{mlr::getFeatureImportance} and/or \texttt{randomForest::varImpPlot}.

<<fig.height=5>>=
imp = getFeatureImportance(model)
sort(imp$res, decreasing = TRUE)

# as alternative, the randomForest package provides a plotting function
randomForest::varImpPlot(getLearnerModel(model))
@

\end{enumerate}
