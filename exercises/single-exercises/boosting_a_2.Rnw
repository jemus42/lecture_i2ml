Create your own gradient boosting algorithm implementation. 

Keep it simple:

\begin{itemize}
\item Your version only need to deal with Regression
\item Use L2-Loss
\item Use Decision stumps (trees of depth one) as base learner
\item Use linesearch to find the "step size" (optional)
\end{itemize}

Show with a simple univerate example that your implementation works. 

<<eval=FALSE>>=
n = 10
x = seq(0, 10, length.out = n)
X = data.frame(x1 = x)
y = sin(x) + rnorm(n, mean = 0, sd = 0.01)
@


Visualize the fit and the pseudo-residuals for each step in the fitting process.


