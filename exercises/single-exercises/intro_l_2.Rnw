Derive the estimator of a normal linear regression model.
Proceed as following: Write down the quadratic loss of the training data (OLS criterion) and minimize it for $\theta$. We don't need any assumption about the distribution, but define the functional form of the model and optimize it in regard to the loss.

Now, do the same for ridge regression.


\begin{eqnarray*}
y &=& f(x) = \theta^T x \\
\Lxyt&=& (y - \fxt)^2 = (y - \theta^T x)^2 \\
RSS(\theta) &=& \sum_{i=1}^n (\yi - \theta^T \xi)^2 \\
\end{eqnarray*}

To minimize $RSS(\theta)$ we have to calculate the gradient with respect to $\theta$:\\
$$
\nabla_{\theta}RSS(\theta) = \sum_{i=1}^n -2 \xi (\yi - \theta^T \xi) =
-2 X^T (y - X \theta) \quad \in \R^p
$$
Set it to 0: \\
\begin{align*}
            & \nabla_{\theta}RSS(\theta) &\overset{!}{=}& 0 \\
\Rightarrow & -2X^T(y-X\theta) &=& 0 \\
\Rightarrow & X^T(y-X\theta) &=& 0 \\
\Rightarrow & X^T y &=& X^TX\theta \\
\Rightarrow & \hat{\theta} &=& (X^TX)^{-1} X^Ty
\end{align*}

Now for ridge regression:\\

\begin{eqnarray*}
PRSS(\theta) &=& \sum_{i=1}^n (\yi - \theta^T \xi)^2 + \lambda \theta^T \theta\\
\end{eqnarray*}

$$
\nabla_{\theta}RSS(\theta) = \sum_{i=1}^n -2\xi (\yi - \theta^T \xi)  + 2 \lambda\theta =
-2X^T(y - X\theta)  + 2 \lambda\theta
$$

\begin{eqnarray*}
\nabla_{\theta}PRSS(\theta) &\overset{!}{=}& 0 \\
-2X^T(y-X\theta)  + 2 \lambda \theta &=& 0 \\
X^T(y-X\theta)  - \lambda \theta &=& 0 \\
X^Ty - X^TX\theta  - \lambda \theta &=& 0 \\
X^Ty &=& X^T X\theta  + \lambda \theta \\
X^Ty &=& (X^T X  + I \lambda) \theta \\
\hat{\theta} &=& (x^T x  + I \lambda)^{-1} x^T y\\
\end{eqnarray*}
