\begin{enumerate}
  \item[a)] What is the relationship between softmax $\pi_i(x)=\frac{e^{\theta_i^T x}}{\sum_k e^{\theta_k^T x}}$ and the logistic function $\pi(x)=\frac{1}{1+e^{\theta^{T} x}}$ for $k = 2$ (binary classification)?

  \item[b)] Derive the negative log likelihood of softmax regression. Suppose there are $n$ instances and $p$ features.

  \item[c)] Explain how the predictions of softmax regression (multiclass classification) looks like (probabilities and classes) and define the parameter space.

  % \item[c)] Calculate the derivative of the negative likelihood as a loss function for softmax regression: $\triangledown_{\theta_k}L= \sum_{i=1}^n {-x ([y[i]==k] - \pi_k)}$ (suppose there are n instances)
\end{enumerate}

