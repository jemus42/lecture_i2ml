\begin{enumerate}
  \item What is the relationship between softmax $\pi_i(x)=\frac{e^{\theta_i^T x}}{\sum_k e^{\theta_k^T x}}$ and logistic function $\pi(x)=\frac{1}{1+e^{\theta^{T} x}}$ ? 

\item Derive the negative log likelihood of softmax regression. Suppose there are $n$ instances and $p$ features.
  
\item Derive the derivative of the negative likelihood as a loss function for softmax regression. $\triangledown_{\theta_k}L= \sum_{i=1}^n {-x ([y[i]==k] - \pi_k)}$ (suppose there are n instances)

\item Implement the softmax regression. Compare your implementation on the iris datasets with the mlr \textbf{classif.multinom} learner. Hint: use lt\_softmaxTemplate.R as a template where most functions has been written for you and you only need to fill in the missing parts. For mlr, use function \textbf{holdout} and one third train test split.  
\end{enumerate}

