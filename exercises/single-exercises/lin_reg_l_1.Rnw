%x <- rnorm(6,1)
%x <- c(0.56, 0.22, 1.7, 0.63, 0.36,1.2)
%y <- c(160,150,175,185,165,170)
%data <- t(cbind(x,y))
a) We use the least squares-estimator introduced in the lecture:
$\beta = (X^TX)^{-1}X^Ty$  with \\ $ X = \begin{bmatrix}
1 & x_{1,1} & x_{1,2} & ... & x_{1,m} \\
2 & x_{2,1} & x_{2,2} & ... & x_{2,m} \\
\vdots & \vdots & \vdots & ... & \vdots \\
n & x_{n,1} & x_{n,2} & ... & x_{n,m} \\
\end{bmatrix}
$  \\
$ x =  \begin{bmatrix}
0.56  \\
0.22  \\
1.7  \\
0.63  \\
0.36  \\
1.2  \\
\end{bmatrix}, X =  \begin{bmatrix}
1 & 0.56  \\
1 & 0.22  \\
1 & 1.7  \\
1 & 0.63  \\
1 & 0.36  \\
1 & 1.2  \\
\end{bmatrix}
$ 
and
 $
y =  \begin{bmatrix}
160  \\
150  \\
175  \\
185  \\
165  \\
170  \\
\end{bmatrix}
$ 

Then \begin{align*}  
\beta &= \begin{bmatrix}
0.2740185 & 0.4419793 & -0.2856359 & 0.23661731 & 0.3728011 & -0.03978034 \\
-0.1382325 & -0.3545088 & 0.5824115 & -0.09007257 & -0.2654309 & 0.26583334 \\
\end{bmatrix}
\begin{bmatrix}
160  \\
150  \\
175  \\
185  \\
165  \\
170  \\
\end{bmatrix} \\ 
&= \begin{bmatrix}
158.67730  \\
11.36062  \\
\end{bmatrix}
\end{align*}


Hence the linear model $y_i = \beta_0 + \beta_1 x_i = 158.67730 + 11.36062 x_i$

%plot(x,y)
%abline(lm(y~x),col="red",lwd=1.5)
\includegraphics{lin_reg.jpeg}

b) 
Here  $
X =  \begin{bmatrix}
1 & 0.56 & 0.3136 \\
1 & 0.22 & 0.0484 \\
1 & 1.7 & 2.89 \\
1 & 0.63 & 0.3969 \\
1 & 0.36 & 0.1296 \\
1 & 1.2  & 1.44\\
\end{bmatrix}
$
 and $\beta =  \begin{bmatrix}
143.51682 \\
57.59155 \\
-23.96347 \\
\end{bmatrix} $

%fit <- lm(mpg ~ hp + I(hp^2))
%fit <- lm(y ~ x + I(x^2))
%newdat = data.frame(x = seq(min(x), max(x), length.out = 100))
%pred= predict(fit, newdata = newdat)
%plot(y ~ x)
%with(newdat, lines(x = x, y = pred,col="red",lwd=1.5))

\includegraphics{lin_reg_q.jpeg}