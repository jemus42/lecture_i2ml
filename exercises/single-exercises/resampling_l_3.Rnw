\begin{itemize}
\item The inner loss is the loss that is optimized directly by the machine learning model. 
The outer loss is the loss (or performance measurement) used to evaluate the model.
\item Which model is more likely to overfit the training data: 
\begin{itemize}
\item knn with 1 or with 10 neighbours?
\item logistic regression with 10 or 20 features?
\item lda or qda?
\end{itemize}
\item Which of the following methods yield an unbiased generalization error estimate? Performance estimation ...
\begin{itemize}
  \item  on training data: \textbf{Biased, too optimistic}
  \item  on test data:  \textbf{Unbiased}
  \item  on training and test data combined: \textbf{Biased, too optimistic} (But a little bit less than only using training data).
  \item  using cross validation: \textbf{Unbiased}
  \item  using subsampling: \textbf{Unbiased}
\end{itemize}
\item Resampling strategies solve the problem that comes from the randomness of the training and test data split: Error estimation using a single split has a high variance. Resampling estimates are more robust because they average over different splits.
\item Nested resampling solves the problem of simultaneously doing tuning/model selection and performance estimation. When we use the performance estimates from the same data that were used for model selection (as done in simple, not-nested resampling), we get a too optimistic final error estimate.
\end{itemize}
