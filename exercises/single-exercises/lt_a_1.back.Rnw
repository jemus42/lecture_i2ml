\begin{enumerate}
\item For bionomial distribution $p=\pi^{I}(1-\pi)^{1-I}$, the log likelihood is $logL=\sum_i I_i log \pi + (1-I_i)log(1-\pi)$

\item What is the relationship between softmax $\pi_i(x)=\frac{e^{-\theta_i^T x}}{\sum_k e^{-\theta_k^T x}}$, and logistic function $\pi(x)=\frac{1}{1+e^{-\theta^T x}}$ ? 
\item Derive the derivative of the loss function in logistic regression

$\pi = \frac{1}{1+exp(- \theta^T x)}$

$\frac{\partial logL}{\partial \theta} 
= \sum_i x^T I_i(1-\sigma)+ x^T (1-I_i){-\sigma} 
= \sum_i x^T(I_i - \sigma)$


\item Derive the derivative of the softmax function $\triangledown_{\theta_k}$.

$log \frac{e^{-\theta_i^T x}}{\sum_k e^{-\theta_k^T x}}= -z_i-log{\sum_k e^{-z_k}}$

likelihood for one instance
$L = \prod_k \pi_k^{I_k}$
where $\pi_k = \frac{e^{-\theta_i^T x}}{\sum_k e^{-\theta_k^T x}}$
s.t. 
$\sum_k \pi_k = 1$
and 
$\sum_k I_k = 1$

$logL = \sum_k {I_k} log \pi_k$

% Do not jump from one conclusion to another

$\frac{\partial logL}{\partial \theta_i} = \sum_k I_k \frac{1}{\pi_k} \frac{\partial \pi_k}{\partial \theta_i}$

$ \frac{\partial \pi_i}{\partial \theta_i} = \triangledown_{\theta_i} \frac{e^{-\theta_i^T x}}{\sum_k e^{-\theta_k^T x}}= \frac{-e^{-z_i}(\sum_k e^{-z_k})+(e^{-z_i})^2}{(\sum_k e^{-z_k})^2}= \frac{-e^{-z_i}(\sum_{k \neq i} e^{-z_k})}{(\sum_k e^{-z_k})^2}= \pi_i(1 - \pi_i)$,

$ \frac{\partial \pi_i}{\partial \theta_j} = \triangledown_{\theta_j} \frac{e^{-\theta_i^T x}}{\sum_k e^{-\theta_k^T x}}= \frac{-e^{-z_j}}{(\sum_k e^{-z_k})^2}= \pi_i\pi_j$,

$\sum_k p_k(x)=1$

\item Implement the gradient descent algorithm for softmax. Compare your implementation on the iris datasets with the mlr classif.multinomial learner. 

  $\theta_i^{(new)} =  \theta_i^{(old)} - \alpha \frac{\partial logL}{\partial \theta_i^{(old)}}$

<<>>=
source("lt_softmax.R")
@

<<<test>>=
source("lt_softmax_test.R")
@

\end{enumerate}

