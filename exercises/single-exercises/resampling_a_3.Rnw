Shortly answer the following questions:

\begin{itemize}
\item What is the difference between inner and outer loss?
\item Which model is more likely to overfit the training data: 
\begin{itemize}
\item knn with 1 or with 10 neighbours? \textbf{1 neighbour}, because it's an exact memorization of training data. 
\item logistic regression with 10 or 20 features? \textbf{20 features}, because the more features, the more coefficients the learner estimates. More coefficients mean more degrees of freedom, which make overfitting more likely.
\item lda or qda? \textbf{qda}, because it has more parameters to possibly overfit the data. lda is more likely to underfit more complex relationships.
\end{itemize}
\item Which of the following methods yield an unbiased generalization error estimate? Performance estimation ...
\begin{itemize}
  \item  on training data
  \item  on test data
  \item  on training and test data combined
  \item  using cross validation
  \item  using subsampling
\end{itemize}
\item Which problem does resampling of training and test data solve?
\item Which problem does nested resampling solve?
\end{itemize}
