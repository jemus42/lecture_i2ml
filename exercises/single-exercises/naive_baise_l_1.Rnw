\begin{enumerate}

  \item[a)]
    We need:
    \begin{align*}
    & P(\text{Banana} = \text{yes}),\ P(\text{Color} = \text{yellow} | \text{Banana} =¸\text{yes}),\ P(\text{Form} = \text{round} | \text{Banana} =¸\text{yes}), \\
    & P(\text{Origin} = \text{imported} | \text{Banana} =¸\text{yes})
    \end{align*}
    Since the features are just categorical we assume them to be Bernoulli distributed. Therefore, estimating the conditional probabilities is just calculating the relative frequencies conditioned on yes:
    \begin{align*}
    & P(\text{Banana}) = 3 / 8 \\
    & P(\text{Color} = \text{yellow} | \text{Banana} =¸\text{yes}) = 1 / 3 \\
    & P(\text{Form} = \text{round} | \text{Banana} =¸\text{yes}) = 1 / 3 \\
    & P(\text{Origin} = \text{imported} | \text{Banana} =¸\text{yes}) = 1
    \end{align*}
    With the independence assumption of naive Bayes, the final step is to calculate the product of the probabilities above:
    \begin{align*}
    P(& \text{Banana} | \text{Color} = \text{yellow},\ \text{Form} = \text{round},\ \text{Origin} = \text{imported}) \\
    & = P(\text{Banana}) \cdot P(\text{Color} = \text{yellow} | \text{Banana} =¸\text{yes}) \cdot P(\text{Form} = \text{round} | \text{Banana} =¸\text{yes}) \\
    & \cdot P(\text{Origin} = \text{imported} | \text{Banana} =¸\text{yes}) \\
    & = 3 / 8 \cdot 1 / 3 \cdot 1 / 3 \cdot 1 \\
    & = 1 / 24
    \end{align*}
  % \begin{align*}
  % P(yes), P(no), P(yellow \vert yes), P(oblong \vert yes), P(imported \vert yes) , P(yellow \vert no) , P(oblong \vert no),  P(imported \vert no)
  % \end{align*}
  % We can calculate these probabilities by counting entries in the table.


  % $P(yes) = \frac{3}{8}, \quad
  % P(yellow \vert yes) = \frac{1}{3},  \quad
  % P(oblong \vert yes) = \frac{2}{3}, \quad
  % P(imported \vert yes) = \frac{3}{3}, \quad \\
  % P(no) = \frac{5}{8},  \quad
  % P(yellow \vert no) =  \frac{2}{5}, \quad
  % P(oblong \vert no) =  \frac{2}{5}, \quad
  % P(imported \vert no) =  \frac{2}{5}$

  % $P(yes) = \frac{3}{8}$ and $P(no) = \frac{5}{8}$ are the a-priori class probabilities.

  % Using the Naive Bayes assumption, we can compute the a-posteriori probabilities:

  % $P(yes \vert yellow, oblong, imported) \propto P(yes) \cdot P(yellow  \vert  yes) \cdot P(oblong  \vert  yes) \cdot P(imported \vert yes) = \frac{3}{8} \cdot \frac{1}{3} \cdot \frac{2}{3} \cdot \frac{3}{3} \approx  0.083$

  % $P(no \vert yellow, oblong, imported) \propto P(no) \cdot P(yellow  \vert  no) \cdot P(oblong  \vert  no ) \cdot P(imported \vert no ) = \frac{5}{8} \cdot \frac{2}{5} \cdot \frac{2}{5} \cdot \frac{2}{5} \approx  0.04 $

  % Since 0.083 $\geq$ 0.04, our example gets classified as ’yes’.
  % To get the real posteriori probabilities we have to calculate:  \\   \\
  % $P(yes \vert yellow, oblong, imported) = \frac{0.083}{0.04 + 0.083} \approx 0.67$

  % $P(no \vert yellow, oblong, imported) = \frac{0.04}{0.04 + 0.083} \approx 0.33$


  \item[b)]
    For numerical features, we would use a different probability distribution, not the Bernoulli distribution.
    For example, for the information "Length" we could assume that $P(Length \vert yes)$ and $P(Length \vert no)$ both follow a normal distribution.
\end{enumerate}