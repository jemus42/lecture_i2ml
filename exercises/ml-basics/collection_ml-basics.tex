\documentclass[a4paper]{article}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R



\usepackage[utf8]{inputenc}
\pagenumbering{arabic}
%\usepackage[ngerman]{babel}
\usepackage{a4wide,paralist}
\usepackage{amsmath, amssymb, xfrac, amsthm}
\usepackage{dsfont}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{framed}
\usepackage{multirow}
\usepackage{bytefield}
\usepackage{csquotes}
\usepackage[breakable, theorems, skins]{tcolorbox}
\usepackage{hyperref}
\usepackage{cancel}
\usepackage{bm}

\input{../../style/common}

\tcbset{enhanced}

%exercise numbering
\renewcommand{\theenumi}{(\alph{enumi})}
\renewcommand{\theenumii}{\roman{enumii}}
\renewcommand\labelenumi{\theenumi}

\font \sfbold=cmssbx10
\setlength{\oddsidemargin}{0cm} \setlength{\textwidth}{16cm}

\sloppy
\parindent0em
\parskip0.5em
\topmargin-2.3 cm
\textheight25cm
\textwidth17.5cm
\oddsidemargin-0.8cm
% \pagestyle{empty}

\newcommand{\kopf}[1] {
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
	{\sf \bf \huge Exercise Collection -- #1}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newcommand{\exlect}
  {\color{black} \hrule \section{Lecture exercises}}
  
\newcommand{\exexams}
  {\color{black} \hrule \section{Questions from past exams}}
  
\newcommand{\exinspo}
  {\color{black} \hrule \section{Ideas \& exercises from other sources}}

\newcounter{aufg}
\newenvironment{aufgabe}[1]
	{\color{black} \refstepcounter{aufg}
	\subsection{Exercise \arabic{aufg}: #1} 
	\noindent}
	{\vspace{0.5cm}}
	
\newenvironment{aufgabeexam}[3] % semester, main or retry exam, question number
	{\color{black} \refstepcounter{aufg}
	\subsection{Exercise \arabic{aufg}: #1, #2 exam, question #3}
	\noindent}
	{\vspace{1.5cm}}

\newcounter{loes}
\newenvironment{loesung}
	{\color{gray} \refstepcounter{loes}\textbf{Solution \arabic{loes}:}
	\\ \noindent}
	{\bigskip}

\setcounter{secnumdepth}{0}



\begin{document}
% !Rnw weave = knitr



\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\kopf{ML Basics}

\tableofcontents

% ------------------------------------------------------------------------------
% LECTURE EXERCISES
% ------------------------------------------------------------------------------

\dlz
\exlect
\lz

\aufgabe{ML tasks}{

Identify which type of machine learning (supervised or unsupervised? What type of task?) could be used in these cases:

\begin{enumerate}

  \item[a)] When crossing the alps using the Brenner Autobahn, there is the option to pay electronically in advance. When approaching the toll station, the barrier automatically opens when the number plate was recognised. The recognition happens automatically by a digital camera system.
  \item[b)] Diagnose whether a patient suffers from cancer or not.
  \item[c)] The owner of an internet site wants to protect his system against various violations of the terms of service (bot programs, manipulation of timestamps, etc.)
  \item[d)] An online shopping portal wants to determine products that are automatically offered to registered customers upon login.
  \item[e)] We want to sort our news into different groups.
  \item[f)] We want to sort our Email into Spam/Non-Spam.
  \item[g)] In a supermarket, products that are often bought together are said to be placed side by side on a shelf to increase the sales.
  \item[h)] We want to extract a list of skills from XING.
  \item[i)] We want to know our top customers (i.\,e.\ highest sales, logistics, etc.).
\end{enumerate}
}
\newpage

\loesung{

\begin{enumerate}
  \item[a)] multiclass classification (plate digits) (supervised learning)
  \item[b)] binary classification (supervised)
  \item[c)] outlier detection ((un)supervised)
  \item[d)] frequent pattern mining (unsupervised)
  \item[e)] classification (supervised) / clustering (unsupervised)
  \item[f)] classification (supervised)
  \item[g)] clustering / assocation rules (unsupervised)
  \item[h)] not a machine learning task
  \item[i)] not a machine learning task
\end{enumerate}
}

\aufgabe{simple regression problem}{

%x <- rnorm(6,1)
%x <- c(0.56, 0.22, 1.7, 0.63, 0.36,1.2)
%y <- c(160,150,175,185,165,170)
%data <- t(cbind(x,y))

Suppose we observe 6 data pairs and want to describe the underlying relationship between target $y$ and feature $\xv$.

\begin{center}
  \begin{tabular}{ | c | c | c | c | c | c | c | }
    \hline
$\xv$ & 0.56 & 0.22 & 1.7 & 0.63 & 0.36 & 1.2 \\ \hline
y & 160 & 150 & 175 & 185 & 165 & 170 \\
    \hline
  \end{tabular}
\end{center}


\begin{itemize}
\item[a)] Assume a standard linear relationship $$\yi = \beta_0 + \beta_1 \xi + \epsi$$ with iid errors $\epsi$ and calculate the least squares estimator $\hat{\bm{\beta}}$ for $\bm{\beta}=(\beta_0, \beta_1)^\top$ manually (+ calculator).


\item[b)] Assume a non-linear relationship (polynomial degree 2) $$\yi = \beta_0 + \beta_1 \xi +\beta_2 (\xi)^2 + \epsi$$ with iid errors $\epsi$ and calculate the least squares estimator $\hat{\bm{\beta}}$ for $\bm{\beta}=(\beta_0, \beta_1, \beta_2)^\top$ with R.
\end{itemize}
}
\dlz
\loesung{




\begin{enumerate}
\item[a)] We use the least squares-estimator introduced in the lecture:
\lz
$\hat{\beta} = (X^TX)^{-1}X^Ty$  with \\ $ X = \begin{bmatrix}
1 & x_{1,1} & x_{1,2} & ... & x_{1,m} \\
1 & x_{2,1} & x_{2,2} & ... & x_{2,m} \\
\vdots & \vdots & \vdots & ... & \vdots \\
1 & x_{n,1} & x_{n,2} & ... & x_{n,m} \\
\end{bmatrix}
\lz
$  \\
$ x =  \begin{bmatrix}
0.56  \\
0.22  \\
1.7  \\
0.63  \\
0.36  \\
1.2  \\
\end{bmatrix}, X =  \begin{bmatrix}
1 & 0.56  \\
1 & 0.22  \\
1 & 1.7  \\
1 & 0.63  \\
1 & 0.36  \\
1 & 1.2  \\
\end{bmatrix}
$
and
 $
y =  \begin{bmatrix}
160  \\
150  \\
175  \\
185  \\
165  \\
170  \\
\end{bmatrix}
$

Then \begin{align*}
\hat{\beta} &= (X^TX)^{-1}X^Ty \\ &= \left(
\begin{bmatrix}
1 & 1 & 1 & ... & 1  \\
x_{1,1} & x_{2,1} & x_{3,1} & ... & x_{n,1} \\
\vdots & \vdots & \vdots & ... & \vdots \\
x_{1,m} & x_{2,m} & x_{3,m} & ... & x_{n,m} \\
\end{bmatrix}
\begin{bmatrix}
1 & x_{1,1} & x_{1,2} & ... & x_{1,m} \\
1 & x_{2,1} & x_{2,2} & ... & x_{2,m} \\
\vdots & \vdots & \vdots & ... & \vdots \\
1 & x_{n,1} & x_{n,2} & ... & x_{n,m} \\
\end{bmatrix} \right)^{-1}\begin{bmatrix}
1 & 1 & 1 & ... 1  \\
x_{1,1} & x_{2,1} & x_{3,1} & ... & x_{n,1} \\
\vdots & \vdots & \vdots & ... & \vdots \\
x_{1,m} & x_{2,m} & x_{3,m} & ... & x_{n,m} \\
\end{bmatrix}
\begin{bmatrix}
160  \\
150  \\
175  \\
185  \\
165  \\
170  \\
\end{bmatrix} \\ &= \left(
\begin{bmatrix}
1 & 1 & 1 & 1 & 1 & 1 \\
0.56 & 0.22 & 1.7 & 0.63 & 0.36 &  1.2   \\
\end{bmatrix}
\begin{bmatrix}
1 & 0.56  \\
1 & 0.22  \\
1 & 1.7  \\
1 & 0.63  \\
1 & 0.36  \\
1 & 1.2  \\
\end{bmatrix} \right)^{-1}
\begin{bmatrix}
1 & 1 & 1 & 1 & 1 & 1 \\
0.56 & 0.22 & 1.7 & 0.63 & 0.36 &  1.2   \\
\end{bmatrix}
\begin{bmatrix}
160  \\
150  \\
175  \\
185  \\
165  \\
170  \\
\end{bmatrix} \\ &=
\begin{bmatrix}
6 & 4.67  \\
4.67 & 5.2185   \\
\end{bmatrix}^{-1}
\begin{bmatrix}
1 & 1 & 1 & 1 & 1 & 1 \\
0.56 & 0.22 & 1.7 & 0.63 & 0.36 &  1.2   \\
\end{bmatrix}
\begin{bmatrix}
160  \\
150  \\
175  \\
185  \\
165  \\
170  \\
\end{bmatrix} \\ &=
 \begin{bmatrix}
0.5491944 & -0.4914703  \\
-0.4914703  & 0.6314394   \\
\end{bmatrix}
\begin{bmatrix}
1 & 1 & 1 & 1 & 1 & 1 \\
0.56 & 0.22 & 1.7 & 0.63 & 0.36 &  1.2   \\
\end{bmatrix}
\begin{bmatrix}
160  \\
150  \\
175  \\
185  \\
165  \\
170  \\
\end{bmatrix} \\
&= \begin{bmatrix}
0.2739710 & 0.4410709 & -0.2863051  & 0.23956809 & 0.3722651 & -0.04056998 \\
-0.1378643 & -0.3525536 & 0.5819766 & -0.09366351 & -0.2641521 & 0.26625693 \\
\end{bmatrix}
\begin{bmatrix}
160  \\
150  \\
175  \\
185  \\
165  \\
170  \\
\end{bmatrix} \\
&= \begin{bmatrix}
158.73954  \\
11.25541  \\
\end{bmatrix}
\end{align*}


Hence the linear model $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x = 158.73954 + 11.25541 x$

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{x} \hlkwb{=} \hlkwd{c}\hlstd{(}\hlnum{0.56}\hlstd{,} \hlnum{0.22}\hlstd{,} \hlnum{1.7}\hlstd{,} \hlnum{0.63}\hlstd{,} \hlnum{0.36}\hlstd{,}\hlnum{1.2}\hlstd{)}
\hlstd{y} \hlkwb{=} \hlkwd{c}\hlstd{(}\hlnum{160}\hlstd{,}\hlnum{150}\hlstd{,}\hlnum{175}\hlstd{,}\hlnum{185}\hlstd{,}\hlnum{165}\hlstd{,}\hlnum{170}\hlstd{)}

\hlstd{X} \hlkwb{<-} \hlkwd{sapply}\hlstd{(}\hlnum{0}\hlopt{:}\hlnum{1}\hlstd{,} \hlkwa{function}\hlstd{(}\hlkwc{k}\hlstd{) x}\hlopt{^}\hlstd{k)}
\hlkwd{solve}\hlstd{(}\hlkwd{t}\hlstd{(X)} \hlopt{%*%} \hlstd{X)} \hlopt{%*%} \hlkwd{t}\hlstd{(X)} \hlopt{%*%} \hlstd{y}
\end{alltt}
\begin{verbatim}
##           [,1]
## [1,] 158.73954
## [2,]  11.25541
\end{verbatim}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/unnamed-chunk-12-1} 

}


\end{knitrout}


\item[b)]
Here  $
X =  \begin{bmatrix}
1 & 0.56 & 0.3136 \\
1 & 0.22 & 0.0484 \\
1 & 1.7 & 2.89 \\
1 & 0.63 & 0.3969 \\
1 & 0.36 & 0.1296 \\
1 & 1.2  & 1.44\\
\end{bmatrix}
$
 and $\hat{\beta} =  \begin{bmatrix}
143.51682 \\
57.59155 \\
-23.96347 \\
\end{bmatrix} $

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{x} \hlkwb{=} \hlkwd{c}\hlstd{(}\hlnum{0.56}\hlstd{,} \hlnum{0.22}\hlstd{,} \hlnum{1.7}\hlstd{,} \hlnum{0.63}\hlstd{,} \hlnum{0.36}\hlstd{,}\hlnum{1.2}\hlstd{)}
\hlstd{y} \hlkwb{=} \hlkwd{c}\hlstd{(}\hlnum{160}\hlstd{,}\hlnum{150}\hlstd{,}\hlnum{175}\hlstd{,}\hlnum{185}\hlstd{,}\hlnum{165}\hlstd{,}\hlnum{170}\hlstd{)}

\hlstd{X} \hlkwb{<-} \hlkwd{sapply}\hlstd{(}\hlnum{0}\hlopt{:}\hlnum{2}\hlstd{,} \hlkwa{function}\hlstd{(}\hlkwc{k}\hlstd{) x}\hlopt{^}\hlstd{k)}
\hlkwd{solve}\hlstd{(}\hlkwd{t}\hlstd{(X)} \hlopt{%*%} \hlstd{X)} \hlopt{%*%} \hlkwd{t}\hlstd{(X)} \hlopt{%*%} \hlstd{y}
\end{alltt}
\begin{verbatim}
##           [,1]
## [1,] 143.51681
## [2,]  57.59155
## [3,] -23.96347
\end{verbatim}
\end{kframe}
\end{knitrout}


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/unnamed-chunk-14-1} 

}


\end{knitrout}


\end{enumerate}
}

\newpage

\aufgabe{credit scoring project}{

Imagine you work at a bank and have the job to develop a credit scoring model. This means, your model should predict whether a customer applying for a credit will be able to pay it back in the end.

\begin{enumerate}[a)]

\item Is this a supervised or unsupervised learning problem? Justify your answer.
\item How would you set up your data? Which is the target variable, what feature variables could you think of? Do you need labeled or unlabeled data? Justify all answers.
\item Is this a regression or classification task? Justify your answer.
\item Is this "learning to predict" or "learning to explain"? Justify your answer.
\item In classical statistics, you could use e.g. the logit model for this task. This means we assume that the targets are conditionally independent given the features, so $\yi|\xi \ind \yi[j]|\xi[j]$ for all $i,j = 1, \dots, n, i \ne j$, where $n$ is the sample size. We further assume that $\yi|\xi \distas{} Bin(\pi^{(i)})$, where $\pi^{(i)} = \frac{\exp(\thetab^\top\xi)}{1+\exp(\thetab^\top\xi)}$.
Looking at this from a Machine Learning perspective, write down the hypothesis space for this model. State explicitly which parameters have to be learned.
\item In classical statistics, you would estimate the parameters via Maximum Likelihood estimation. (The log-Likelihood of the Logit-Model is: $\sumin\yi\log(\pi^{(i)}) + (1-\yi)(\log(1-\pi^{(i)}))$). How could you use the model assumptions to define a reasonable loss function? Write it down explicitly.
\item Now you have to optimize this risk function to find the best parameters and hence the best model. Describe with a few sentences, how you could do this.

\end{enumerate}

Congratulations, you just designed your first Machine Learning project!

}
\dlz
\loesung{


\begin{enumerate}[a)]
  \item Supervised learning problem - the model will be learned from historical
  credit data for which payment history has been observed (knowing the ground
  truth is vital here since we need to evaluate our model's accuracy)
  \item Target variable: classes (default y/n), continuous credit
  scores, or class probabilities). Potential features: monthly income, current 
  level of indebtedness, past credit behavior, profession, residential 
  environment, age, number of kids etc. Labels: yes, since we have a supervised 
  learning problem.
  \item This is a classification problem - we want to assign our customers to
  classes \emph{default} and \emph{non-default}.
  \item (Primarily) learning to predict - we want to score future borrowers.
  \item $\Hspace = \{\pi: \Xspace \mapsto [0,1] ~|~ \pixt = s(\thetab^T \xv), \thetab \in \R^d\}$,
  where $s(z) = 1 / (1 + exp(-z))$ is the sigmoid function. Parameters to be
  learned: $\thetab$.
  \item We know that, in the optimum, (log-)likelihood is maximal. We can
  directly translate this into risk minimization by using the \emph{negative}
  log-likelihood as our empirical risk. We will just use the pointwise negative
  log-likelihood as our loss function: $$\Lpixyit = - \left(\yi \log \left(
  \pixit \right) + \left(1 - \yi \right) \left(\log \left(1 - \pixit \right) 
  \right) \right)$$ (the so-called \emph{Bernoulli loss}). The
  empirical risk is then the sum of point-wise losses: $$\risket = -\sumin \yi
  \log \left(\pixit \right) + \left(1 - \yi \right) \left(\log \left(1 - \pixit
  \right) \right)$$
  \item We can now solve this optimization problem via empirical risk
  minimization, which, in this case, is perfectly equivalent to ML estimation.
  Therefore, we set the first derivative of $\risket$ wrt $\thetab$ to 0 and
  solve for $\thetab$. However -- unlike linear regression -- this has no
  closed-form solution, so a numerical optimization procedure such as gradient
  descent is required.
\end{enumerate}
}

\newpage

% ------------------------------------------------------------------------------
% PAST EXAMS
% ------------------------------------------------------------------------------

\dlz
\exexams
\lz

\aufgabeexam{WS2020/21}{retry}{1}{

\begin{tabular}{ | c | c | c |}
\hline
ID  &  $\xv$  &  $y$  \\  \hline
1   &  0.0    &  0.0  \\
2   &  1.0    &  4.0  \\
3   &  1.5    &  5.5  \\
4   &  2.5    &  7.0  \\
5   &  3.5    &  6.0  \\
6   &  5.0    &  3.0  \\
7   &  6.0    &  2.0  \\
8   &  7.0    &  3.0  \\
9   &  8.0    &  8.0  \\
\hline
\end{tabular}

\begin{centering}



\end{centering}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-8-1} 
\end{knitrout}

Now we want to train a cubic polynomial, i.e., a polynomial regression 
model with degree $d = 3$ on the data used in a). 
\begin{enumerate}
  \item[(i)] Define the hypothesis space of this model and state explicitly 
  how many parameters have to be estimated for training the model.
  \item[(ii)] Define the minimization problem that we have to optimize in 
  order to train the polynomial regression model. Use L2 loss and be as 
  explicit as possible - without plugging in the data.  
  \item[(iii)] In order to estimate the parameters of the model, it is 
  convenient to describe the model as a linear model. Compute the respective 
  design matrix using the concrete values of $\xv$ given above. Additionally, 
  state a formula for estimating the parameters using this design matrix. 
  (You do not have to derive this formula.)
\end{enumerate}
}

\newpage

\loesung{
\begin{enumerate}
  \item[(i)] $$\Hspace = \{f: \fx = \theta_0 + \theta_1 \xv + \theta_2 \xv^2 + 
  \theta_3 \xv^3 ~|~ \theta_0, \theta_1, \theta_2, \theta_3 \in \R\}$$
  The four parameters $\theta_0, \theta_1, \theta_2, \theta_3$ have to be 
  estimated
  \item[(ii)] $$\thetabh \in \argmin \limits_{\thetab \in \Theta} \risket$$ 
  This means we have to optimize the following minimization problem wrt 
  $\thetab = (\theta_0, \theta_1, \theta_2, \theta_3)$:
  $$\min \limits_{\thetab \in \Theta} \risket = \min \limits_{\thetab \in 
  \Theta} \sum_{i=1}^9(\yi - (\theta_0 + \theta_1 \xi + \theta_2 (\xi)^2 + 
  \theta_3 (\xi)^3))^2$$ 
  \item[(iii)]   $$\thetah = (X^\top X )^{-1}X^\top y$$
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\begin{tabular}{r|r|r|r}
\hline
X1 & x & x.2 & x.3\\
\hline
1 & 0.0 & 0.00 & 0.000\\
\hline
1 & 1.0 & 1.00 & 1.000\\
\hline
1 & 1.5 & 2.25 & 3.375\\
\hline
1 & 2.5 & 6.25 & 15.625\\
\hline
1 & 3.5 & 12.25 & 42.875\\
\hline
1 & 5.0 & 25.00 & 125.000\\
\hline
1 & 6.0 & 36.00 & 216.000\\
\hline
1 & 7.0 & 49.00 & 343.000\\
\hline
1 & 8.0 & 64.00 & 512.000\\
\hline
\end{tabular}

\end{knitrout}
\end{enumerate}
}

% ------------------------------------------------------------------------------

\dlz
\aufgabeexam{WS2020/21}{retry}{6}{

Describe a real-life application in which classification might be useful and 
where we want to “learn to explain”. Describe the response, as well as the 
predictors. Explain your answer thoroughly.
}

\dlz
\loesung{

No model solution
}

% ------------------------------------------------------------------------------
% INSPO
% ------------------------------------------------------------------------------

\dlz
\exinspo
\end{document}
