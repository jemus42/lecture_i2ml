Assume the following (noisy) data-generating process: 
$y = 0.5 + 0.4 \cdot \sin(2 \pi x) + \bm{\epsilon}$ with $\bm{\epsilon} 
\sim N(0, 0.1).$

<<echo=FALSE, message=FALSE, fig.align="center", fig.height = 2, fig.width=5>>=
set.seed(1L)
x <- seq(0L, 1L, length.out = 30L)
eps <- rnorm(length(x), 0L, 0.1)
y <- 0.5 * sin(2 * pi * x) + eps
ggplot2::ggplot(data.frame(x = x, y = y), ggplot2::aes(x, y)) + 
  ggplot2::geom_point() +
  ggplot2::theme_bw()
@

\begin{enumerate}[a)]
  \item We decide to model the data with a cubic polynomial (including intercept 
  term). State the corresponding hypothesis space.
  \item Demonstrate that this hypothesis space is simply a parameterized family 
  of curves by plotting in R curves for 3 different models belonging to the 
  considered model class.
  % \item State the corresponding parameter space.
  \item State the empirical risk w.r.t. $\thetab$ for a member of our hypothesis 
  space. Use $L2$ loss and be as explicit as possible.
  \item We can minimize this risk using gradient descent. In order to make this 
  somewhat easier, we will denote our transformed feature matrix 
  % (i.e., 
  % $\begin{bmatrix} x^0 & x^1 & x^2 & x^3 \end{bmatrix}$) 
  % by $\tilde \Xmat$, 
  enabling us to express our model by $\tilde \Xmat \thetab$ (note that our 
  model is still linear in its parameters, even if $\Xmat$ has been transformed 
  in a non-linear manner). 
  Derive the gradient of the empirical risk w.r.t. $\thetab$, which is 
  multiplied by $- \alpha$ to update the current parameter vector 
  $\thetab^{[t]}$. \\
  \textcolor{orange}{ggf. mit hints}
  \item How would the optimization procedure change if we had used a polynomial 
  of different degree (e.g., 1 or 5)?
  \item You will not be able to fit the data perfectly with a cubic polynomial. 
  Should you opt for a more flexible model class (i.e., a hypothesis space with 
  higher capacity)? What might be disadvantageous about this?
\end{enumerate}