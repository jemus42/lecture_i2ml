Suppose we observe 6 data pairs and want to describe the underlying relationship between target $y$ and feature $\xv$.

\begin{center}
  \begin{tabular}{ | c | c | c | c | c | c | c | }
    \hline
$\xv$ & 0.56 & 0.22 & 1.7 & 0.63 & 0.36 & 1.2 \\ \hline
y & 160 & 150 & 175 & 185 & 165 & 170 \\
    \hline
  \end{tabular}
\end{center}

\begin{itemize}
    \item[a)] For the linear model $$\fxi = \theta_0 + \theta_1 \xi$$ with L2 loss, starting from $\thetab^{[0]} = (0,0)$ calculate one step of gradient descent with a stepsize of $\alpha = 0.1$.
    \item[b)] Implement a function \texttt{grad\_desc(x, y, iterations, alpha = 0.1)} that computes \texttt{iterations} steps of gradient descent with a learning rate of \texttt{alpha} for a linear regression with L2 loss. You can initialize all model parameters at $0$.
    \item[c)] How do the parameters estimated by gradient descent differ from the parameters estimated by the analytical solution (see ex. sheet 1) for different values of \texttt{iterations}.
\end{itemize}
