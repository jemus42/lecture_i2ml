\begin{enumerate}[a)]

  \item We face a \textbf{supervised regression} task: we definitely need 
  labeled training data to infer a relationship between cars' attributes and 
  their prices, and price in EUR is a continuous target (or quasi-continuous, 
  to be exact -- as with all other quantities, we can only measure it with 
  finite precision, but the scale is sufficiently fine-grained to assume 
  continuity). \textbf{Prediction} is definitely the goal here, however, it 
  might also be interesting to examine the explanatory contribution of each 
  feature.
  
  \item Target variable and potential features: \\
  
  \begin{tabular}{l|l|l}
    \textbf{Variable} & \textbf{Role} & \textbf{Data type} \\ \hline
    Price in EUR & Target & Numeric \\ \hline
    Age in days & Feature & Numeric \\ \hline
    Mileage in km & Feature & Numeric \\ \hline
    Brand & Feature & Categorical \\ \hline
    Accident-free y/n & Feature & Binary \\ \hline
    \dots & \dots & \dots
  \end{tabular}
  
  \item Let $x_1$ and $x_2$ measure age and mileage, respectively. 
  Both features and target are numeric and (quasi-)continuous. It is also 
  reasonable to assume non-negativity for the features, such that we 
  obtain $\Xspace = (\R_{0}^{+})^2$, with $\xi = (x_1, x_2)^{(i)} \in \Xspace$ 
  for $i = 1, 2, \dots, n$ observations. 
  As the standard LM does not impose any 
  restrictions on the target, we have $\Yspace = \R$, though we would probably 
  discard negative predictions in practice.
  
  \item We can write the hypothesis space as:
  
  \begin{flalign*}
    \Hspace = \{\thetab^T \xv ~|~ \thetab \in \R^3 \}
    =  \{\theta_0 + \theta_1 x_1 + \theta_2 x_2 ~|~ 
    (\theta_0, \theta_1, \theta_2) \in \R^3 \}.
    % \Hspace &= \{ f: (\R_{0}^{+})^2 \rightarrow \R ~|~ 
    % \fx = \theta_0 + \thetab^T \xv, ~ (\theta_0, \thetab) \in \R^3 \} \\
    % &=  \{ f: (\R_{0}^{+})^2 \rightarrow \R ~|~ 
    % \fx = \theta_0 + \theta_{\text{age}} x_{\text{age}} + 
    % \theta_{\text{mileage}} x_{\text{mileage}}, ~ (\theta_0, 
    % \theta_{\text{age}}, \theta_{\text{mileage}}) \in \R^3 \},
  \end{flalign*}
  
  Note the slight abuse of notation here: in the lecture, we first define 
  $\thetab$ to only consist of the feature coefficients, with $\xv$ likewise 
  being the plain feature vector. For the sake of simplicity, however, it is 
  more convenient to append the intercept coefficient to the vector of feature 
  coefficients. This does not change our model formulation, but we have to 
  keep in mind that it implicitly entails adding an element 1 at the first 
  position of each feature vector, constituting the familiar column of ones 
  in the design matrix $\Xmat$.
  
  \item The parameter space is included in the definition of the hypothesis 
  space and in this case given by $\Theta = \R^3$.
  
  \item Loss function for the $i$-th observation: $\Lxyi = \left( \yi - 
  \thetab^T \xi \right)^2$
  
\end{enumerate}