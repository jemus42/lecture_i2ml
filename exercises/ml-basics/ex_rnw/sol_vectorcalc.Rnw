\begin{enumerate}[a)]
    \item In computing $\Amat \xv$ we multiply each of the $m$ rows in $\Amat$ with the sole length-$n$ column in $\xv$, leaving us with a column vector $\fx \in \R^{m \times 1}$. Thus, we have $f: \R^{n (\times 1)} \rightarrow \R^{m (\times 1)}$.
     
    The $i$-th function component $f_i(\xv)$ corresponds to multiplying the $i$-th row of $\Amat$ with $\xv$, amounting to $$f_i(\xv) = \sum_{j = 1}^n a_{ij} x_j,$$ with $a_{ij}$ the element in the $i$-row, $j$-th column of $\Amat$.
    
    \includegraphics[width=0.5\textwidth]{figure/illustration_matmult}
    
    \scriptsize{\url{https://mbernste.github.io/posts/matrix_vector_mult/}} \normalsize
    \item \phantom{foo}
    \begin{enumerate}[i)]
         \item The gradient is the row vector\footnote{
         Pertaining to one of two conventions; we use the \textit{numerator layout} here (the transposed version is called \textit{denominator layout}).
         } of partial derivatives, i.e., the derivatives of $f$ w.r.t. each dimension of $\xv$:
         $$\frac{\mathop{}\!\mathrm{d} \fx}{\mathop{}\!\mathrm{d} \xv} 
         = \mat{\pd{\fx}{x_1} & \dots & \pd{\fx}{x_n}}.
         $$
         Now, since $f$ is a vector-valued function, each component is itself a vector of length $m$.
         Therefore, we have $\frac{\mathop{}\!\mathrm{d} \fx}{\mathop{}\!\mathrm{d} \xv} \in \R^{m \times n}$, given by the collection of all partial derivatives of each function component:
         $$
         \frac{\mathop{}\!\mathrm{d} \fx}{\mathop{}\!\mathrm{d} \xv} = \mat{
         \pd{f_1(\xv)}{x_1} & \cdots & \pd{f_1(\xv)}{x_n} \\
         \vdots & & \vdots \\
         \pd{f_m(\xv)}{x_1} & \cdots & \pd{f_m(\xv)}{x_n}
         }
         $$
         This matrix is also called the \emph{Jacobian} of $f$.
         \item We have $$\pd{f_i(\xv)}{x_j} = \pd{\left(\sum_{j = 1}^n a_{ij} x_j \right)}{x_j} = a_{ij}.$$
         Doing this for every element yields
         $$
         \mat{a_{11} & \cdots & a_{1n}  \\ \vdots & & \vdots \\ a_{m1} & \cdots & a_{mn}},
         $$
         and we have $\frac{\mathop{}\!\mathrm{d} \fx}{\mathop{}\!\mathrm{d} \xv} = \frac{\mathop{}\!\mathrm{d} \Amat \xv}{\mathop{}\!\mathrm{d} \xv} = \Amat$.
    \end{enumerate}
\end{enumerate}

For more explanations and exercises, including a useful collection of rules for calculus, we recommend the book "Mathematics for Machine Learning" (\url{https://mml-book.github.io/book/mml-book.pdf}).
