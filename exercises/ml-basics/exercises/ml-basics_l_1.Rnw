\begin{enumerate}[a)]
  \item Supervised learning problem - the model will be learned from historical
  credit data for which payment history has been observed (knowing the ground
  truth is vital here since we need to evaluate our model's accuracy)
  \item Target variable: either classes (default y/n) or continuous credit
  score (e.g., in [0, 1], corresponding to probability of default). Potential
  features: monthly income, current level of indebtedness, past credit behavior,
  profession, residential environment, age, number of kids etc. Labels: yes
  \item Could be both, depending on nature of target (class/continuous 
  score).
  \item (Primarily) learning to predict - we want to score future borrowers.
  \item $\Hspace = \{\pi: \Xspace \mapsto \R | \pix = s(\thetab^T \xv) \}$,
  where $s(\thetab^T \xv) = 1 / (1 + exp(-(\thetab^T \xv))$ is the sigmoid
  function. Parameters to be learned: $\thetab$.
  \item We know that, in the optimum, (log-)likelihood is maximal. We can
  directly translate this into risk minimization by using the negative
  log-likelihood as our loss function, so: $\Lpixyt = -\sumin \yi 
  \log(\pi^{(i)}) + (1 - \yi)(\log(1 - \pi^{(i)}))$.
  \item We can solve this optimization problem via empirical risk minimization,
  which, in this case, is perfectly equivalent to ML estimation. Therefore, we 
  set the first derivative of $\Lpixyt$ wrt $\theta$ to 0 and solve for 
  $\theta$. However -- unlike linear regression -- this has no closed-form 
  solution, so a numerical optimization procedure such as gradient descent is 
  required.
\end{enumerate}