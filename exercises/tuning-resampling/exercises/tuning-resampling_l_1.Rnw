\begin{enumerate}

  \item


    \begin{enumerate}
      \item

        <<results="asis", echo=FALSE>>=
        cat("

          Important parts:
          \\begin{itemize}
            \\item
              Correct number of models for tuning
            \\item
              Correctly multiplying tuning models times the two learners that need tuning
            \\item
              Correctly adding $4 \\cdot 10$ for learner comparison ()
          \\end{itemize}

          $$
          \\# \\text{models} = \\underbrace{4 \\cdot 10}_{\\text{\\# models outer resampling}} + \\underbrace{2 \\cdot \\underbrace{10 \\cdot \\underbrace{5 \\cdot 200}_{\\text{\\# models for one tuning}}}_{\\text{\\# models for all outer folds for one tuning}}}_{\\text{\\# models for both tunings}} = 20040
          $$


        ")
        @

      \item

        <<results="asis", echo=FALSE>>=
        cat("

          We would select the k-NN (k-Nearest Neighbors) learner since it achieves the best values for the AUC.

        ")
        @



    \end{enumerate}

  \item
        <<results="asis", echo=FALSE>>=
        cat("


          \\begin{itemize}
            \\item
              Less data for training leads to higher bias
            \\item
              More data for training and less data for evaluation lead to higher variance
          \\end{itemize}

        ")
        @

  \item
    Are the following statements true or not, explain your answer in one sentence.
    \begin{enumerate}

      \item
          True, using 3-fold cross-validation leads to smaller train sets and therefore we are not able to learn as much as for, e.g., 10-fold cross-validation.

      \item
          False, the outer loss doesn't has as much restrictions as the inner loss, e.g. the outer loss doesn't has to be differentiable.

    \end{enumerate}
\end{enumerate}