\begin{enumerate}[a)]
  \item Benchmark result:
  \begin{enumerate}
    \item Total number of models trained:
    $$\underbrace{4 \cdot 10}_{\text{outer resampling}} + ~~ \underbrace{2 \cdot 
    \underbrace{10 \cdot \underbrace{5 \cdot 200}_{\text{one tuning iteration
    }}}_{\text{all outer folds in one tuning procedure}}}_{\text{all tuning 
    procedures}} = 20,040.$$
    \item Since we evaluate on AUC, we select $k$-NN with the best average 
    result in that respect.
  \end{enumerate}
  \item Less data for training leads to higher bias, less data for evaluation 
  leads to higher variance.
  \item Statements:
  \begin{enumerate}[i)]
    \item True -- 3-CV leads to smaller train sets, therefore we are not able to 
    learn as well as in, e.g., 10-CV.
    \item False -- we are relatively flexible in choosing the outer loss, but 
    the inner loss needs to be suitable for empirical risk minimization, 
    which encompasses differentiability in most cases (i.e., whenever 
    optimization employs derivatives).
  \end{enumerate}
\end{enumerate}