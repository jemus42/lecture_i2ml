\begin{enumerate}
\item[1)] \begin{itemize}
  \item Hyperparameter tuning means finding the hyperparameter vector $\bm{\lambda}$ s.t. the model trained by learner $\inducer$ with hyperparameter vector $\bm{\lambda}$ on $\Dtrain$ and evaluated on $\Dtest$, results in the lowest estimated generalization error.
  \item Tuning is a bi-level optimization problem because the \textbf{empirical risk minimization problem} on the level of the learner $\inducer$ depends on the hyperparameters provided to $\inducer$, and is therefore nested within the \textbf{hyperparameter optimization problem}.
\end{itemize}

\item[2)] An example tuning problem
\begin{itemize}
  \item Data set $\D$ (given in the task),
  \item A random forest learner $\inducer$ (given in the task),
  \item $d=2$ hyperparameters of the random forest learner: $\{mtry, \; ntrees\}$ and the configuration space $\bm{\Lambda}=\{1,2\} \times \{n \in \N \mid 10 \leq n \leq 1000\}$,
  \item accuracy $\rho_{ACC}$ (it must be a measure suited for classification, though)
  \item 3-fold cross-validation (CV).
\end{itemize}

\item[3)] This depends on two factors:
\begin{itemize}
  \item Since the performance was estimated wrt a model trained on $|\Jtrain| < n$, the estimate is pessimistically biased.
  \item If the performance estimate used to evaluate the hyperparameter configurations is used as an estimate of the generalization error, an optimistic bias is introduced.
  This is because the \textit{untouched test set principle} has been violated.
  By repeatedly evaluating the learner on the same CV splits, information about the CV splits “leaks” into the evaluation.
\end{itemize}
Therefore, the answer depends on which of the two effects is larger in magnitude.
To eliminate overfitting to the resampling splits / overtuning, one could apply \textbf{nested resampling}, which leaves the performance estimate with only the pessimistic bias due to $|\Jtrain| < n$.
\end{enumerate}