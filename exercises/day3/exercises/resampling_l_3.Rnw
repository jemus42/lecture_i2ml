\begin{itemize}
\item The inner loss is the loss that is optimized directly by the machine learning model. 
The outer loss is the loss (or performance measurement) used to evaluate the model.
\item Which model is more likely to overfit the training data: 
\begin{itemize}
\item k-NN with 1 or with 10 neighbors? \textbf{1 neighbor}, because it's an exact memorization of training data. 
\item Logistic regression with 10 or 20 features? \textbf{20 features}, because the more features, the more coefficients the learner estimates. More coefficients mean more degrees of freedom, which make overfitting more likely.
\item LDA or QDA? \textbf{QDA}, because it has more parameters to possibly overfit the data. LDA is more likely to underfit more complex relationships.
\end{itemize}
\item Which of the following methods yield an unbiased generalization error estimate? Performance estimation ...
\begin{itemize}
  \item  on training data: \textbf{Biased, too optimistic}
  \item  on test data:  \textbf{Unbiased / Biased, too pessimistic} (Test data is \textit{not included} / \textit{included} in the final model)
  \item  on training and test data combined: \textbf{Biased, too optimistic} (But a little bit less than only using training data).
  \item  using cross validation: \textbf{Biased, too pessimistic} (The higher the ratio of folds / number of observation, the smaller the pessimistic bias)
  \item  using subsampling: \textbf{Biased, too pessimistic} (The smaller the subsampling rate, the larger the pessimistic bias)
\end{itemize}
\item Resampling strategies solve the problem that comes from the randomness of the training and test data split: Error estimation using a single split has a high variance. Resampling estimates are more robust because they average over different splits.
\item Nested resampling solves the problem of simultaneously conducting tuning/model selection and performance estimation. When we use the performance estimates from the same data that were used for model selection (as done in simple, not-nested resampling), the final error estimate is too optimistic.
\end{itemize}
