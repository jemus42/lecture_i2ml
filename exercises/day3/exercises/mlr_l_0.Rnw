<<echo=FALSE, message=FALSE, results='hide', warning=FALSE>>=
library(mlr3)
library(mlr3learners)
library(mlr3measures)

training_set = seq(1, nrow(iris), by = 2)
test_set <- seq(2, nrow(iris), by = 2)

task <- tsk("iris")
learner <- lrn("classif.lda")
learner$train(task, row_ids = training_set)
pred <- learner$predict(task, row_ids = test_set)
pred$score()
@

\begin{enumerate}
  \item[a)]

  Each loss function we have learned so far to fit the model (inner loss) can also be used as performance measure (outer loss).

  For classification:

  \begin{itemize}
    \item 0-1 loss (= mean misclassification error),
    \item Logistic loss (bernoulli loss), ...
  \end{itemize}

  For regression:

  \begin{itemize}
    \item $L_2$-loss (= mean squared error),
    \item $L_1$-loss (= mean absolute error), ...
  \end{itemize}

  To get a list of all measures you can use \texttt{mlr\_measures}.

  \item[b)]

  <<message=FALSE>>=
  # look at the task
  task <- tsk("boston_housing")
  task
  n <- task$nrow

  # select index vectors to subset the data randomly
  set.seed(123)
  train_ind <- sample(seq_len(n), 0.5*n)
  test_ind <- setdiff(seq_len(n), train_ind)

  # specify learner
  learner <- lrn("regr.kknn", k = 3)

  # train model to the training set
  learner$train(task, row_ids = train_ind)

  # predict on the test set
  pred <- learner$predict(task, row_ids = test_ind)
  pred
  @
  \item[c)]
  <<message=FALSE>>=
  # predict on the train set
  pred_train <- learner$predict(task, row_ids = train_ind)
  pred_train$score(list(msr("regr.mse"), msr("regr.mae")))
  
  # predict on the test set
  pred_test <- learner$predict(task, row_ids = test_ind)
  pred_test$score(list(msr("regr.mse"), msr("regr.mae")))
  @
  Unsurprisingly the model performs better on the training data (smaller loss) then on the
  test data.

  \item[d)]

  <<message=FALSE>>=
  # select different index vectors to subset the data randomly
  set.seed(321)
  train_ind <- sample(seq_len(n), 0.5*n)
  test_ind <- setdiff(seq_len(n), train_ind)

  # specify learner
  learner <- lrn("regr.kknn", k = 3)

  # train model to the training set
  learner$train(task, row_ids = train_ind)

  # predict on the test set
  pred_test <- learner$predict(task, row_ids = test_ind)
  pred_test
  pred_test$score(list(msr("regr.mse"), msr("regr.mae")))
  @

  Effect: We will predict different observations since the test set is different. The same observations get a slightly different prediction (e.g. observation with id 2). This affects the final error estimation.

  \item[e)]
  <<message=FALSE>>=
  rdesc <- rsmp("cv", folds = 10)
  r <- resample(task, learner, rdesc)
  r$aggregate(list(msr("regr.mse"), msr("regr.mae")))
  @

  % \item[f)]
  % <<message=FALSE>>=
  % ## Tuning in inner resampling loop
  % ps = makeParamSet(makeDiscreteParam("k", values = 1:10))
  % ctrl = makeTuneControlGrid()
  % inner = makeResampleDesc("CV", iters = 10)
  % lrn = makeTuneWrapper("regr.kknn", resampling = inner, par.set = ps, control = ctrl, show.info = FALSE)

  % ## Outer resampling loop
  % outer = makeResampleDesc("CV", iters = 5)
  % r = resample(lrn, bh.task, resampling = outer, extract = getTuneResult, show.info = FALSE)
  % r$measures.test
  % r$aggr
  % r$extract
  % @
\end{enumerate}
