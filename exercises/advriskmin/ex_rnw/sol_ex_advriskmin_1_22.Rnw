
\begin{enumerate}

  \item 
  
  For the optimal constant model $\fx = \bm{\theta}$ for the loss  $\Lxy=  \big(m(y)-m(\fx)\big)^2,$ we first apply the following substitution $z^{(i)} = m(\yi)$ for each $i=1,\ldots,n,$ and introduce $\bm{\theta}_m = m(\bm{\theta}) \in m(\R).$ 
%  
 Note that the inverse of $m$ is continuous and strictly monotone as well, so that the minimizer of the initial optimization problem, i.e.,
%
$$
\min_{f \in \Hspace} \riskef =  \min_{\bm{\theta} \in \R} \sumin ( m(\yi) - m(\bm{\theta}))^2. 
$$
%
is the same as for the ``substituted'' optimization problem, i.e.,
%
%
$$
m^{-1} \left( \min_{\bm{\theta}_m \in  m(\R)} \sumin (z^{(i)} - \bm{\theta}_m )^2\right) .
$$
%
For the term in the brackets we have seen in the lecture (optimizer of the empirical L2 risk) that
%
$$  \argmin_{\bm{\theta}_m \in  m(\R)} \sumin (z^{(i)} - \bm{\theta}_m )^2 =   \frac1n  \sumin  z^{(i)} =   \frac1n  \sumin m(\yi)  . $$
%
Consequently, 
%
$$\fxh = m^{-1}\left(  \frac1n \sum_{i=1}^n m(\yi)  \right)$$
%
is the optimal constant model for $L$.
 %
 
\item First, note that
%
\begin{align*}
%
  \risk_L\left(\fh \right) 
  %
  &=  \E_{xy} [\Lxy]      \\
  %
  &=   \E_{xy} [ \big(m(y)-m(\fxh)\big)^2 ]      \\
%  
  &=   \E_{xy} \left[ \left(   m(y) -  \frac1n \sum_{i=1}^n m(\yi)    \right)^2 \right]  \\
  %  
  &=   \E_{xy} \left[    m(y)^2  \right]  - 2 \E_{xy} \left[    m(y)   \frac1n \sum_{i=1}^n m(\yi)  \right]  +    \E_{xy} \left[ \left(\frac1n \sum_{i=1}^n m(\yi)\right) \left(\frac1n \sum_{i=1}^n m(\yi)\right)   \right] . \\
  % 
%
\end{align*}
%
Now, because $y^{(1)},\ldots,y^{(n)}$ are i.i.d.\ with $\E_{xy} \left[ m(\yi) \right] = \E_{xy} \left[  m(y) \right],$ we get
%
\begin{align*}
%	
	\E_{xy} \left[    m(y)   \frac1n \sum_{i=1}^n m(\yi)  \right] 
%	
	&= \frac1n \E_{xy} \left[    m(y)    \sum_{i=1}^n m(\yi)  \right] \\
%	
	&= \frac1n \E_{xy} \left[    m(y)   \right]  \E_{xy} \left[   \sum_{i=1}^n m(\yi)  \right] \\
	%	
	&= \frac1n \E_{xy} \left[    m(y)   \right]  n \E_{xy} \left[  m(y) \right] = \E_{xy} \left[  m(y) \right]^2.
%	
\end{align*}
%
Similarly,
%
\begin{align*}
%	
	\E_{xy} \left[ \left(\frac1n \sum_{i=1}^n m(\yi)\right) \left(\frac1n \sum_{i=1}^n m(\yi)\right)   \right]
%	
	&= \frac{1}{n^2} \left(	\sum_{i=1}^n	\E_{xy} \left[   m(\yi)  \left( \sum_{i=1}^n m(\yi)\right)   \right]		\right) \\
%	
	&=  \frac{1}{n^2} \left(	\sum_{i=1}^n	\E_{xy} \left[   m(\yi)^2 + \sum_{j \neq i}  m(\yi) m(y^{(j)})   \right]		\right) \\
	%	
	&=  \frac{1}{n^2} \left(\sum_{i=1}^n	\E_{xy} \left[   m(\yi)^2  \right] +  	\sum_{i=1}^n \sum_{j \neq i} \E_{xy} \left[ m(\yi) m(y^{(j)})   \right]		\right) \\
%	
	&=  \frac{1}{n^2} \left( n	\E_{xy} \left[   m(y)^2  \right] +  	\sum_{i=1}^n \sum_{j \neq i} \E_{xy} \left[ m(\yi)  \right]	 \E_{xy} \left[ m(y^{(j)})   \right]		\right) \\
%	
	&=  \frac{1}{n^2} \left( n	\E_{xy} \left[   m(y)^2  \right] +  n(n-1) \E_{xy} \left[  m(y)   \right]^2		\right) \\
	%	
	&=  \frac1n	\E_{xy} \left[   m(y)^2  \right] +   (1-\frac1n) \E_{xy} \left[  m(y)   \right]^2	. \\
%	
\end{align*}

%
So, combining the three later math displays, we obtain
%
\begin{align*}
	%
	\risk_L\left(\fh \right) 
	% 
	&=   \E_{xy} \left[    m(y)^2  \right]  - 2 \E_{xy} \left[    m(y)   \frac1n \sum_{i=1}^n m(\yi)  \right]  +    \E_{xy} \left[ \frac1n \sum_{i=1}^n m(\yi)    \right]  \\
	%
	&=  \E_{xy} \left[    m(y)^2  \right]  - 2 \E_{xy} \left[  m(y) \right]^2 + \frac1n	\E_{xy} \left[   m(y)^2  \right] +   (1-\frac1n) \E_{xy} \left[  m(y)   \right]^2 \\
%	
	&= \left( 1 + \frac1n \right) \left(   \E_{xy} \left[    m(y)^2  \right]  -  \E_{xy} \left[    m(y)  \right] ^2			\right) \\
%	
	&= \left( 1 + \frac1n \right) \var(m(y)).
	%
\end{align*}
% 
\item In order to derive the risk minimizer, we consider the unrestricted hypothesis space $\Hspace = \{f: \Xspace \to \R\}$. 
%
	By the law of total expectation
		\begin{eqnarray*}
			\risk_L\left(f \right)  &=& \E_{xy} \left[\Lxy\right] 
			\\ &=& \E_x \left[\E_{y|x}\left[\Lxy~|~\xv\right]\right] \\
			&=& \E_x
			\left[\E_{y|x}\left[(m(y)- m(\fx))^2~|\xv\right]\right]. 
		\end{eqnarray*} 
	%	
	Since $\Hspace$ is unrestricted we can choose $f$ as we wish: At any point $\xv = \xv$ we can predict any value $c$ we want. The best point-wise prediction is 
	$$
	\fxbayes = \mbox{argmin}_c \E_{y|x}\left[( m(y) - m(c) )^2 ~|~ \xv  \right]\overset{(*)}{=} m^{-1} \left(  \E_{y|x}\left[ m(y) ~|~ \xv \right] \right),
	$$
%	
	where $(*)$ is due to 
%	
	\begin{align*}
%		
		\mbox{argmin}_c \E\left[(m(y) - m(c))^2\right] 
%		
		&= \mbox{argmin}_c \underbrace{\E\left[( m(y) - m(c) )^2\right] - \left(\E[ m(y)] - m(c) \right)^2}_{= \var[ m(y) - m(c)] = \var[m(y)]} + \left(\E[m(y)] - m(c)\right)^2 \\
%		
		&=   \mbox{argmin}_c \var[m(y)] + \left(\E[m(y)] - m(c)\right)^2 
%		
		= m^{-1} \left( \E[m(y)]\right) , 
%		
	\end{align*}
	%
	because $\var[m(y)] $ does not depend on $c.$
% 
Note that we could have used a similar substitution as in (a) here to derive $\fbayes.$
%
Furthermore, if we use $m(x)=x$ such that the considered loss coincides with the L2 loss, we get (quite naturally) the same best point-wise prediction as for the L2 loss. 
%
Using an $m$ corresponding to another notion of mean (e.g., harmonic or geometric mean), the best point-wise prediction for that other mean is obtained in each case.
%
\item The optimal constant model in terms of the (theoretical) risk can be obtained from the previous by forgetting the conditioning on point $\xv = \xv,$ which leads to 
%
	$$
	\bar{f}(\xv) =   m^{-1} \left(  \E_{y }\left[ m(y)  \right] \right).
	$$
% 
	The risk of the latter is $\var(m(y)):$
%	
	\begin{align*}
%		
		\risk_L\left(\bar{f} \right) 
		%
		=  \E_{xy} [ \big(m(y)-m( \bar{f}(\xv) )\big)^2 ]  
		%
		= \E_{y} [ \big(m(y)-  \E_{y }\left[ m(y)  \right] \big)^2 ] = \var(m(y)).  
%		
	\end{align*}
%	
%
\item The Bayes regret can be decomposed as follows: 
%
\begin{align*}
%	
	\risk_L\left(\hat f\right) - \riskbayes_{L} 
%	
	&= \underbrace{\left[\risk_L\left(\hat f\right) - \inf_{f \in \Hspace} \risk_L(f)\right]}_{\text{estimation error}} + \underbrace{\left[\inf_{f \in \Hspace} \risk_L(f) - \riskbayes_{L}\right]}_{\text{approximation error}}.
%	
\end{align*}
%
If we consider as the hypothesis space
%
$\Hspace = \{ f:\Xspace \to \R \, | \,  \fx = \bm{\theta}  \ \forall \bm{x} \in \Xspace  \},$ i.e.,  the set of constant models, then the estimation error is
%
\begin{align*}
	%	 
	\risk_L\left(\hat f\right) - \inf_{f \in \Hspace} \risk_L(f)
	%	 
	&= \underbrace{\risk_L\left(\hat f\right)}_{ \overset{(b)}{=} \left( 1 + \frac1n \right) \var(m(y))} -  \underbrace{\risk_L(\bar{f})}_{\overset{(d)}{=} \var(m(y))}
%	
	&= \left( 1 + \frac1n \right) \var(m(y)) - \var(m(y)) = \frac1n  \var(m(y)),
	%	
\end{align*}
%
while the approximation error is
%
%
\begin{align*}
	%	 
	\inf_{f \in \Hspace} \risk_L(f) - \riskbayes_{L}
	%	 
	&=  \underbrace{\risk_L(\bar{f})}_{\overset{(d)}{=} \var(m(y))} - \risk_L(\fbayes) \\
	%	
	&=   \var(m(y))  -  \E_x
	\left[\E_{y|x}\left[(m(y)- m(  \fxbayes ))^2~|~ \xv ~ \right]\right]  \\
%	
	&= \var(m(y))  -  \E_x
	\left[\E_{y|x}\left[ \left(m(y)- m( m^{-1} (  \E_{y|x}\left[ m(y) ~|~ \xv \right] ) )\right)^2~|~ \xv~ \right]\right]  \\
%	
	&= \var(m(y))  -  \E_x \Big[
	\underbrace{\E_{y|x} \left[(m(y)-  \E_{y|x}\left[ m(y) ~|~ \xv \right]  )^2~|~ \xv ~ \right]}_{ = \var\left[ m(y) ~|~ \xv ~ \right]  } \Big]  \\
	%	
	&= \var(m(y))  -  \E_x
	\left[  \var\left[ m(y) ~|~ \xv ~ \right] \right]    \\
%	
	&= \var\Big(  \E_{y|x}\left[ m(y) ~|~ \xv ~ \right] \Big).
	%	
\end{align*}
%
Note that the larger the sample size $n$ the lower the estimation error, while the approximation error remains constant.
%

\end{enumerate}