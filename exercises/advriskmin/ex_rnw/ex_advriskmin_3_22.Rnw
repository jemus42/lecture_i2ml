
Suppose we are facing a regression task, i.e., $\mathcal{Y} = \R,$ and the feature space is $\Xspace \subseteq \R^p.$ 
%	
Let us assume that the relationship between the features and labels is specified by  
%
\begin{align} \label{eq_relationship}
%	
	y = m^{-1} \left( m(\ftrue(\xv)) + \eps \right),
%	
\end{align}
%
where $m:\R \to \R$ is a continuous strictly monotone function with $m^{-1}$ being its inverse function, and the errors are Gaussian, i.e. $\eps \sim \mathcal{N}(0, \sigma^2)$. 
%
In particular, for the data points $(\xv^{(1)},y^{(1)}),\ldots,(\xv^{(n)},y^{(n)})$ it holds that
%
\begin{align} \label{eq_relationship_data}
	%	
	\yi = m^{-1} \left( m(\ftrue(\xi)) + \epsi \right),
	%	
\end{align}
%
where $\eps^{(1)},\ldots,\eps^{(n)}$ are iid with distribution $\mathcal{N}(0, \sigma^2)$. 
%Then $$m(y)~|~\xv \sim N\left( m(\ftrue(\xv)), \sigma^2\right).$$

\textbf{Disclaimer:} We assume in the following that $m(y)$ and $m(f(\xv))$ is well-defined for any $y\in \Yspace,$ $f\in \Hspace$ and $\xv \in \Xspace.$

\begin{enumerate}
%	
	\item  How can we transform the labels $y^{(1)},\ldots,y^{(n)}$ to ``new'' labels $z^{(1)},\ldots,z^{(n)}$ such that $z^{(i)}~|~\xv$ is normally distributed? What are the parameters of this normal distribution?
%  
	\item Assume that the hypothesis space is 
	%  
	\begin{equation*}
    \begin{split}
      \Hspace = \{f(\cdot~|~ \thetab): \Xspace \to \R \ ~|~   & f(\cdot~|~ \thetab) \text{ belongs to a certain
       functional family parameterized by } \thetab \in \Theta \},
    \end{split}
  \end{equation*}
  %
  where $\thetab = (\theta_1, \theta_2, \ldots, \theta_d)$ is a parameter vector, which is an element of a \textbf{parameter space} 
  $\Theta$.
	%
	Based on your findings in (a), establish a relationship between minimizing the negative log-likelihood for  $(\xv^{(1)},z^{(1)}),\ldots,(\xv^{(n)},z^{(n)})$ and empirical loss minimization over $\Hspace$ of the generalized L2-loss function of Exercise sheet 1, i.e., $\Lxy= \big(m(y)-m(\fx)\big)^2.$ 
%   
	\item In many practical applications such as biology, medicine, physics or social sciences one often observed statistical property is that the label $y$ given a feature $\xv$ follows a \emph{log-normal distribution}\footnote{The Wikipedia article on the log-normal distribution has quite a large part about the occurrence of the log-normal distribution.}.
%	
	Note that we can obtain such a relationship by using $m(x)=\log(x)$ above.
%  
	In the following we want to consider the conjecture of the Scottish physician James D.\ Forbes, who conjectured in the year 1857 that the relationship between the air pressure (in inches of mercury) $y$ and the boiling point of water $x$ (in degrees Farenheit) is given by
%	
	$$  y = \theta_1 \exp(\theta_2 x + \eps),$$
%	
	for some specific values $\theta_1 \in \R_+,\theta_2\in \R$ and some error term $\eps$ (of course, we assume that this error term is stochastic and normally distributed).
%	
	\begin{itemize}
%		
		\item What would be a suitable hypothesis space $\Hspace$ if this conjecture holds?
%		
		\item The dataset \texttt{forbes} in the R-package \texttt{MASS} contains 17 different observations of $y$ and $x$ at different locations in the Alps and Scotland, i.e., the data set is $(x^{(i)},\yi)_{i=1}^{17}.$
%		
  Analyze whether his conjecture was reasonable by using the following code snippet:
		<<eval=FALSE>>=
#' @param X the feature input matrix X
#' @param y the outcome vector y
#' @param theta parameter vector for the model (2-dimensional)
		
# Load MASS and data set forbes
library(MASS)
data(forbes)
attach(forbes)

# initialize the data set
X = cbind(rep(1,17),bp)
y = pres

#' function to represent your models via the parameter vector theta = c(theta_1, theta_2)
#' @return a predicted label y_hat for x
f <- function(x, theta){
 
  # >>> do something <<<
  
  return(y_hat)
  
}
		
#' @return a vector consisting of the optimal  parameter vector 
optim_coeff <- function(X,y){
 
  # >>> do something <<<
  
  return(theta)
  
}
 
  # >>>  Do something here to check Forbes' conjecture <<<


@
%
	\emph{Hint:} As a sanity check whether your function to find the optimal coefficients work, it should hold that $\hat\theta_1 \approx 0.3787548$ 	and $\hat\theta_2 \approx 0.02062236.$
	%		
	\end{itemize}
%
\end{enumerate}
