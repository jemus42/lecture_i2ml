This exercise builds upon the previous exercise sheet.

\begin{enumerate}
  \item Write a function implementing a gradient descent routine for the optimization of the linear model defined in the previous exercise sheet. Start with:
<<eval=FALSE>>=
#' @param step_size the step_size in each iteration
#' @param X the feature input matrix X
#' @param y the outcome vector y
#' @param beta a starting value for the coefficients
#' @param eps a small constant measuring the changes in each update step. 
#' Stop the algorithm if the estimated model parameters do not change
#' more than \code{eps}.

#' @return a set of optimal coefficients beta
gradient_descent <- function(step_size, X, y, beta, eps = 1e-8){
 
  # >>> do something <<<
  
  return(beta)
  
}
@
  \item Run a small simulation study by creating 20 data sets as indicated below and test different step sizes $\alpha$ (fixed across iterations) against each other and against the state-of-the-art routine for linear models (in R, using the function \texttt{lm}, in Python, e.g., \texttt{sklearn.linear\_model.LinearRegression}).
  \begin{itemize}
    \item Compare the difference in estimated coefficients $\beta_j, j=1,\ldots,p$ using the mean squared error, i.e. $$p^{-1} \sum_{j=1}^p (\beta^{truth}_j-\hat{\beta}_j)^2$$ and summarize the difference over all 20 simulation repetitions.
    \item Compare the run times of your implementation and the one given by the state-of-the-art method by wrapping the function calls into a timer (e.g., \texttt{system.time()} in R).
    \end{itemize}
<<eval=FALSE>>=
# settings
n <- 10000
p <- 100
nr_sims <- 20

# create data (only once)
X <- matrix(rnorm(n*p), ncol=p)
beta_truth <- runif(p, -2, 2)
f_truth <- X%*%beta_truth 

# create result object
result_list <- vector("list", nr_sims)

for(sim_nr in nr_sims)
{
  
  # create response
  y <- f_truth + rnorm(n, sd = 2) 
  
  # >>> do something <<<
  
  
  # save results in list (performance, time)
  result_list[[sim_nr]] <- add_something_meaningful_here
  
}
@
 \item Why is gradient descent maybe not the best option for optimization of a linear model with $L2$ loss? What other options exist? Name at least one and describe how you would apply this for the above problem. 
 \item Can we say something about the algorithm's consistency w.r.t. $\Pxy$, if $f^\ast \notin \Hspace$?
\end{enumerate}