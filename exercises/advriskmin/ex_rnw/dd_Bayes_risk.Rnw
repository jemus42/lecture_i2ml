

The point-wise optimizer for the 0-1-loss over all possible discrete classifiers $\hx$  is
%
\begin{eqnarray*}  
	\hxbayes &=& \argmax_{l \in \Yspace} \P(y = l~|~ \xv = \xv).
\end{eqnarray*}
%
%
Recall that we can write the 0-1-loss as follows:
%
\begin{align} \label{eq_alt_01_loss_repr}
%	
	\Lhxy 
%	
	= \mathds{1}_{\{y \ne \hx\}} 
%	
	= \sum_{k\in \Yspace} \mathds{1}_{\{y = k\}} \mathds{1}_{\{y \ne \hx\}}
%	
	= \sum_{k\in \Yspace} \mathds{1}_{\{y = k\}} \mathds{1}_{\{k \ne \hx\}}
%	
	= \sum_{k\in \Yspace} \mathds{1}_{\{y = k\}} L(k, h(\xv) ).
%	
\end{align}
%
With this, the risk of $\hbayes$ is
%
\begin{align*}
%	
	\risk_L(\hbayes)  
%	
	&=  \Exy\left[  L(y,\hxbayes) \right] \\
%	
	&=  \E_x \left[ \E_{y|x} [ L(y, \hxbayes) ~|~ \xv = \xv ] \right] \tag{Law of total expectation} \\
%	
	&=  \E_x \left[ \E_{y|x} \left[ \sum_{k \in \Yspace} \mathds{1}_{\{y  = k \}} L(k, \hxbayes) ~|~ \xv = \xv \right] \right] \tag{By \eqref{eq_alt_01_loss_repr}} \\
%	
	&=  \E_x \left[ \sum_{k \in \Yspace} L(k, \hxbayes) \E_{y|x}  \left[  \mathds{1}_{\{y  = k \}} ~|~ \xv = \xv  \right] \right] \tag{Linearity of cond. expectation} \\
%	
	&= \E_x \left[\sum_{k \in \Yspace} L(k, \hxbayes) \P(y = k~|~ \xv = \xv)\right] \tag{Expectation of an indicator random variable} \\ 
%	
%	&= \sum_{k \in \Yspace} \E_x \left[ L(k, \hxbayes) \P(y = k~|~ \xv = \xv)\right] \tag{Linearity of  expectation} \\
	%	
	&= \E_x \left[ \sum_{k \in \Yspace} \mathds{1}_{\{y \ne \hxbayes\}}  \P(y = k~|~ \xv = \xv)\right] \tag{Definition of 0-1-Loss} \\
%	
	&= \E_x \left[ \sum_{k \in \Yspace} \mathds{1}_{\{k \ne   \argmax_{l \in \Yspace} \P(y = l | \xv ) \}}  \P(y = k~|~ \xv = \xv)\right] \tag{Definition of  $\hbayes$} \\
%	
	&= \E_x \left[ 1 - \max_{l \in \Yspace} \P(y = l ~|~ \xv = \xv)  \right] \tag{Probabilities sum up to one} \\
	%	
	&= 1 -  \E_x \left[ \max_{l \in \Yspace} \P(y = l ~|~ \xv = \xv)  \right]. \tag{Linearity of expectation} 
%	
\end{align*}

