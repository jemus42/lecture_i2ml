You want to estimate the relationship between a continuous response variable $\bm{y} \in \mathbb{R}^n$ and some feature $\bm{X} \in \mathbb{R}^{n \times p}$ using the linear model with an appropriate loss function $L$.

\begin{enumerate}

  \item Describe the model $f$ used in this case, its hypothesis space $\mathcal{H}$ and the theoretical risk function.
  \item Given $f \in \mathcal{H}$, explain the different parts of the Bayes regret if (i) $f^\ast \in \mathcal{H}$; if (ii) $f^\ast \notin \mathcal{H}$.
  \item Define the empirical risk and derive the gradients of the empirical risk.
  \item Show that the empirical risk is convex in the model coefficients. Why is convexity a desirable property? Hint: Compute the Hessian matrix $\bm{H} \in \mathbb{R}^{p \times p}$ and show that $\bm{z}^\top \bm{H} \bm{z} \geq 0 \, \forall \bm{z} \in \mathbb{R}^p$, i.e., show that the Hessian is positive semi-definite (psd).  
  \item Write a function implementing a gradient descent routine for the optimization of this linear model. Start with:
<<eval=FALSE>>=
#' @param step_size the step_size in each iteration
#' @param X the feature input matrix X
#' @param y the outcome vector y
#' @param beta a starting value for the coefficients
#' @param eps a small constant measuring the changes in each update step. 
#' Stop the algorithm if the estimated model parameters do not change
#' more than \code{eps}.

#' @return a set of optimal coefficients beta
gradient_descent <- function(step_size, X, y, beta, eps = 1e-8){
 
  # >>> do something <<<
  
  return(beta)
  
}
@
  \item Run a small simulation study by creating 20 data sets as indicated below and test different step sizes $\alpha$ (fixed across iterations) against each other and against the state-of-the-art routine for linear models (in R using the function \texttt{lm}, in Python, e.g., \texttt{sklearn.linear\_model.LinearRegression}.
  \begin{itemize}
    \item Compare the difference in estimated coefficients $\beta_j, j=1,\ldots,p$ using the mean squared error, i.e. $$p^{-1} \sum_{j=1}^p (\beta^{truth}_j-\hat{\beta}_j)^2$$ and summarize the difference over all 100 simulation repetitions.
    \item Compare the run-times of your implementation and the one given by the state-of-the-art method by wrapping the funciton calls into a timer (e.g., \texttt{system.time()} in R).
    \end{itemize}
<<eval=FALSE>>=
# settings
n <- 10000
p <- 100
nr_sims <- 20

# create data (only once)
X <- matrix(rnorm(n*p), ncol=p)
beta_truth <- runif(p, -2, 2)
f_truth <- X%*%beta_truth 

# create result object
result_list <- vector("list", nr_sims)

for(sim_nr in nr_sims)
{
  
  # create response
  y <- f_truth + rnorm(n, sd = 2) 
  
  # >>> do something <<<
  
  
  # save results in list (performance, time)
  result_list[[sim_nr]] <- add_something_meaningful_here
  
}
@
 \item Why is gradient descent maybe not the best option for optimization of a linear model with $L_2$-loss? What other options exist? Name at least one and describe how you would apply this for the above problem. 
 \item Can we say something about the algorithm's consistency w.r.t. $\mathbb{P}_{xy}$, if $f^\ast \notin \mathcal{H}$?
\end{enumerate}