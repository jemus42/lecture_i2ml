


Consider the regression learning setting, i.e., $\mathcal{Y} = \R,$
and assume that your loss function of interest is $\Lxy= \big(m(y)-m(\fx)\big)^2,$ where $m:\R \to \R$ is a continuous strictly monotone function.
%

\textbf{Disclaimer:} In the following we always assume that $ \var(m(Y))$ exists.
%
\begin{enumerate}
	%
	\item Consider the hypothesis space of constant models 
	%
	$\Hspace = \{ f:\Xspace \to \R \, | \,  \fx = \bm{\theta}  \ \forall \xv  \in \Xspace  \},$
	%
	where $\Xspace$ is the feature space.
	%
	Show that 
	%
	$$\fxh = m^{-1}\left(  \frac1n \sum_{i=1}^n m(\yi)  \right)$$
	%
	is the optimal constant model for the loss function above, where $m^{-1}$ is the inverse function of $m.$
	
	\emph{Hint:} We can obtain several different notions of a mean value by using a specific function $m,$ e.g., the arithmetic mean by $m(x)=x,$ the harmonic mean by $m(x)=1/x$ (if $x>0$) or the geometric mean by $m(x)=\log(x)$ (if $x>0$).
	
	%
	\item Verify that the risk of the optimal constant model is $\risk_L\left(\fh \right) = \left( 1 + \frac1n \right) \var(m(y)).$
	%
	\item Derive that the risk minimizer (Bayes optimal model) $\fbayes$ is given by $\fxbayes=m^{-1} \left(  \E_{y|x}\left[ m(y) ~|~ \xv \right] \right).$
	%
	\item What is the optimal constant model in terms of the (theoretical) risk for the loss above and what is its risk?
	%
	\item Recall the decomposition of the Bayes regret into the estimation and the approximation error.
	%
	Show that the former is $\frac1n \var(m(y)),$ while the latter is $\var\Big(  \E_{y|x}\left[ m(y) ~|~ \xv \right] \Big)$ for the optimal constant model $\fxh$ if the hypothesis space $\Hspace$ consists of the constant models. 
	
	\emph{Hint:} Use the law of total variance, which states that $ \var(Y) = \E_X\left[\var(Y~|~X)\right] + \var(  \E_{Y|X} \left[ Y ~|~ X \right] ),$ where the conditional variance is defined as $\var(Y~|~X) = \E_X \left[  \big(Y - \E_{Y|X}(Y ~|~X)\Big)^2 ~|~X \right].$
  
\end{enumerate}
  