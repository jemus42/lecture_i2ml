
\begin{enumerate}

  \item 
  \begin{itemize}
  \item Hypothesis space $\Hspace$ is defined as:
        $$
        \Hspace = \{\fx = \bm{x}^\top \bm{\beta} \ |\ \bm{\beta} \in \mathbb{R}^p\}
        $$
  \item We fit a linear model, ergo using the $L2$ loss makes sense (e.g., because of the link to Gaussian MLE):
  $$
  L\left(y^{(i)},f\left(\bm{x}^{(i)}|\bm{\beta}\right)\right) = L\left(y^{(i)}, {\bm{x}^{(i)}}^\top\bm{\beta}\right) = 0.5 \left({y}^{(i)}-{\bm{x}^{(i)}}^\top\bm{\beta}\right)^2
  $$
  and the theoretical risk is
  $$
  \mathcal{R}(f) = \mathcal{R}(\bm{\beta}) = \int \Lxy\mbox{d}\mathbb{P}_{xy} = 0.5 \int (y-\fx^2) \,\mbox{d}\mathbb{P}_{xy} =  0.5 \int (y-\bm{x}^\top \bm{\beta})^2 \,\mbox{d}\mathbb{P}_{xy} .
  $$
  \end{itemize}
  \item The Bayes regret is $\mathcal{R}_L(\hat{f}) - \mathcal{R}^\ast_L$ and can be decomposed into an estimation error $\left[ \mathcal{R}_L(\hat{f}) - \inf_{f\in\Hspace} \mathcal{R}_L({f})   \right]$ and an approximation error $\left[ \inf_{f\in\Hspace} \mathcal{R}_L({f}) - \mathcal{R}^\ast_L \right]$.
  \begin{itemize}
  \item[(i)] If $f^\ast \in \Hspace$, $\mathcal{R}^\ast_L = \inf_{f\in\Hspace} \mathcal{R}_L({f})$, i.e., the approximation error is $0$ and for $n\to \infty$ the Bayes regret $\to 0$.  
  \item[(ii)] If $f^\ast \notin \Hspace$, the Bayes regret typically consists of both parts, but as $n\to\infty$, we are left with the approximation error.
  \end{itemize}
  \item 
  \begin{itemize}
  \item The empirical risk is $$
  \mathcal{R}_{emp}(\bm{\beta}) = 0.5 \sum_{i=1}^n \left({y}^{(i)}-{\bm{x}^{(i)}}^\top\bm{\beta}\right)^2 = 0.5 ||\bm{y} - \bm{X\beta}||^2.
  $$
  \item Optimization = minimization of the empirical risk can either be done analytically (the preferred solution in this case!) or using, e.g., gradient descent.
  $$
  \nabla_{\bm{\beta}} \riske(\bm{\beta}) = 0.5 \nabla_{\bm{\beta}} (\bm{y}-\bm{X\beta})^\top(\bm{y}-\bm{X\beta}) = -\bm{X}^\top (\bm{y} - \bm{X}\bm{\beta})
  $$
  \end{itemize}
  \item For convex objectives, every local minimum corresponds to a global minimum. To show convexity, calculate the second derivatives:
  $$
  \nabla_{\bm{\beta}\bm{\beta}^\top}  \riske(\bm{\beta}) = \bm{X}^\top\bm{X}.
  $$
  Since $\bm{z}^\top \bm{X}^\top \bm{X} \bm{z}$ is the inner product of a vector $\tilde{\bm{z}} = \bm{X} \bm{z}$ with itself, i.e. 
  $$
  \bm{z}^\top \bm{X}^\top \bm{X} \bm{z} = \tilde{\bm{z}}^\top \tilde{\bm{z}} = \sum_{j=1}^p \tilde{z}_j^2
  $$
  it is $\geq 0$ and hence $\bm{X}^\top \bm{X}$ psd and therefore $\riske(\bm{\beta})$ convex.
\end{enumerate}