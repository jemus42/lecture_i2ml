\documentclass[a4paper]{article}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R




\usepackage[utf8]{inputenc}
%\usepackage[ngerman]{babel}
\usepackage{a4wide,paralist}
\usepackage{amsmath, amssymb, xfrac, amsthm}
\usepackage{dsfont}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{framed}
\usepackage{multirow}
\usepackage{bytefield}
\usepackage{csquotes}
\usepackage[breakable, theorems, skins]{tcolorbox}
\usepackage{hyperref}
\usepackage{cancel}
\usepackage{bm}


\input{../../style/common}

\tcbset{enhanced}

\DeclareRobustCommand{\mybox}[2][gray!20]{%
	\iffalse
	\begin{tcolorbox}[   %% Adjust the following parameters at will.
		breakable,
		left=0pt,
		right=0pt,
		top=0pt,
		bottom=0pt,
		colback=#1,
		colframe=#1,
		width=\dimexpr\linewidth\relax,
		enlarge left by=0mm,
		boxsep=5pt,
		arc=0pt,outer arc=0pt,
		]
		#2
	\end{tcolorbox}
	\fi
}

\DeclareRobustCommand{\myboxshow}[2][gray!20]{%
%	\iffalse
	\begin{tcolorbox}[   %% Adjust the following parameters at will.
		breakable,
		left=0pt,
		right=0pt,
		top=0pt,
		bottom=0pt,
		colback=#1,
		colframe=#1,
		width=\dimexpr\linewidth\relax,
		enlarge left by=0mm,
		boxsep=5pt,
		arc=0pt,outer arc=0pt,
		]
		#2
	\end{tcolorbox}
%	\fi
}


%exercise numbering
\renewcommand{\theenumi}{(\alph{enumi})}
\renewcommand{\theenumii}{\roman{enumii}}
\renewcommand\labelenumi{\theenumi}


\font \sfbold=cmssbx10

\setlength{\oddsidemargin}{0cm} \setlength{\textwidth}{16cm}


\sloppy
\parindent0em
\parskip0.5em
\topmargin-2.3 cm
\textheight25cm
\textwidth17.5cm
\oddsidemargin-0.8cm
\pagestyle{empty}

\newcommand{\kopf}[1] {
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Introduction to Machine Learning \hfill Exercise sheet #1\\
	 \url{https://introduction-to-machine-learning.netlify.app/} \hfill WiSe 2020/2021}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newenvironment{allgemein}
	{\noindent}{\vspace{1cm}}

\newcounter{aufg}
\newenvironment{aufgabe}
	{\refstepcounter{aufg}\textbf{Exercise \arabic{aufg}:}\\ \noindent}
	{\vspace{0.5cm}}

\newcounter{loes}
\newenvironment{loesung}
	{\refstepcounter{loes}\textbf{Solution \arabic{loes}:}\\\noindent}
	{\bigskip}
	
\newenvironment{bonusaufgabe}
	{\refstepcounter{aufg}\textbf{Exercise \arabic{aufg}*\footnote{This
	is a bonus exercise.}:}\\ \noindent}
	{\vspace{0.5cm}}

\newenvironment{bonusloesung}
	{\refstepcounter{loes}\textbf{Solution \arabic{loes}*:}\\\noindent}
	{\bigskip}



\begin{document}
% !Rnw weave = knitr




\kopf{6}

\aufgabe{

Shortly answer the following questions:

\begin{enumerate}
  \item What is the difference between inner and outer loss?
  \item Which model is more likely to overfit the training data: 
  \begin{itemize}
    \item k-NN with 1 or with 10 neighbours?
    \item Logistic regression with 10 or 20 features?
    \item LDA or QDA?
  \end{itemize}
  \item Which of the following methods yield an unbiased generalization error estimate? \\
  Performance estimation ...
  \begin{itemize}
    \item  on training data
    \item  on test data
    \item  on training and test data combined
    \item  using cross validation
    \item  using subsampling
  \end{itemize}
  \item Which problem does resampling of training and test data solve?
  \item Which problem does nested resampling solve?
\end{enumerate}
}

\dlz

\aufgabe{

The Satellite dataset consists of pixels in 3x3 neighbourhoods in a satellite image, where each pixel is described by 4 spectral values, and the classification label of the central pixel. (for further information see  \texttt{?Satellite}) We fit a k-NN model to predict the class of the middle pixel. The performance is evaluated with the mmce. \\
Look at the following R code and output: The performance is estimated in different ways: using training data, test data and then with cross validation. How do the estimates differ and why? Which one should be used?
  
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(mlr3)}
\hlkwd{library}\hlstd{(mlr3learners)}
\hlkwd{library}\hlstd{(mlbench)}

\hlkwd{data}\hlstd{(Satellite)}
\hlstd{satellite_task} \hlkwb{<-}
  \hlstd{TaskClassif}\hlopt{$}\hlkwd{new}\hlstd{(}\hlkwc{id} \hlstd{=} \hlstr{"satellite_task"}\hlstd{,}
                  \hlkwc{backend} \hlstd{= Satellite,}
                  \hlkwc{target} \hlstd{=} \hlstr{"classes"}\hlstd{)}
\hlstd{knn_learner} \hlkwb{<-} \hlkwd{lrn}\hlstd{(}\hlstr{"classif.kknn"}\hlstd{,} \hlkwc{k} \hlstd{=} \hlnum{3}\hlstd{)}

\hlcom{# Train and test subsets:}
\hlkwd{set.seed}\hlstd{(}\hlnum{42}\hlstd{)}
\hlstd{train_indices} \hlkwb{<-}
  \hlkwd{sample.int}\hlstd{(}\hlkwd{nrow}\hlstd{(Satellite),} \hlkwc{size} \hlstd{=} \hlnum{0.8} \hlopt{*} \hlkwd{nrow}\hlstd{(Satellite))}
\hlstd{test_indices} \hlkwb{<-} \hlkwd{setdiff}\hlstd{(}\hlnum{1}\hlopt{:}\hlkwd{nrow}\hlstd{(Satellite), train_indices)}

\hlcom{# Training data performance estimate}
\hlstd{knn_learner}\hlopt{$}\hlkwd{train}\hlstd{(}\hlkwc{task} \hlstd{= satellite_task,} \hlkwc{row_ids} \hlstd{= train_indices)}
\hlstd{pred} \hlkwb{<-}
  \hlstd{knn_learner}\hlopt{$}\hlkwd{predict}\hlstd{(}\hlkwc{task} \hlstd{= satellite_task,} \hlkwc{row_ids} \hlstd{= train_indices)}
\hlstd{pred}\hlopt{$}\hlkwd{score}\hlstd{()}
\end{alltt}
\begin{verbatim}
## classif.ce 
##          0
\end{verbatim}
\begin{alltt}
\hlcom{# Test data performance estimate}
\hlstd{pred} \hlkwb{<-}
  \hlstd{knn_learner}\hlopt{$}\hlkwd{predict}\hlstd{(}\hlkwc{task} \hlstd{= satellite_task,} \hlkwc{row_ids} \hlstd{= test_indices)}
\hlstd{pred}\hlopt{$}\hlkwd{score}\hlstd{()}
\end{alltt}
\begin{verbatim}
## classif.ce 
## 0.09246309
\end{verbatim}
\begin{alltt}
\hlcom{# CV performance estimate}
\hlstd{rdesc} \hlkwb{<-} \hlkwd{rsmp}\hlstd{(}\hlstr{"cv"}\hlstd{,} \hlkwc{folds} \hlstd{=} \hlnum{10}\hlstd{)}
\hlstd{res} \hlkwb{<-} \hlkwd{resample}\hlstd{(satellite_task, knn_learner, rdesc)}
\end{alltt}
\begin{verbatim}
## INFO  [17:26:39.413] [mlr3]  Applying learner 'classif.kknn' on task 'satellite_task' (iter 1/10) 
## INFO  [17:26:39.605] [mlr3]  Applying learner 'classif.kknn' on task 'satellite_task' (iter 4/10) 
## INFO  [17:26:39.755] [mlr3]  Applying learner 'classif.kknn' on task 'satellite_task' (iter 7/10) 
## INFO  [17:26:39.890] [mlr3]  Applying learner 'classif.kknn' on task 'satellite_task' (iter 5/10) 
## INFO  [17:26:40.031] [mlr3]  Applying learner 'classif.kknn' on task 'satellite_task' (iter 3/10) 
## INFO  [17:26:40.167] [mlr3]  Applying learner 'classif.kknn' on task 'satellite_task' (iter 9/10) 
## INFO  [17:26:40.415] [mlr3]  Applying learner 'classif.kknn' on task 'satellite_task' (iter 6/10) 
## INFO  [17:26:40.540] [mlr3]  Applying learner 'classif.kknn' on task 'satellite_task' (iter 10/10) 
## INFO  [17:26:40.671] [mlr3]  Applying learner 'classif.kknn' on task 'satellite_task' (iter 2/10) 
## INFO  [17:26:40.801] [mlr3]  Applying learner 'classif.kknn' on task 'satellite_task' (iter 8/10)
\end{verbatim}
\begin{alltt}
\hlstd{res}\hlopt{$}\hlkwd{score}\hlstd{()}
\end{alltt}
\begin{verbatim}
##                  task        task_id                  learner   learner_id
##  1: <TaskClassif[46]> satellite_task <LearnerClassifKKNN[32]> classif.kknn
##  2: <TaskClassif[46]> satellite_task <LearnerClassifKKNN[32]> classif.kknn
##  3: <TaskClassif[46]> satellite_task <LearnerClassifKKNN[32]> classif.kknn
##  4: <TaskClassif[46]> satellite_task <LearnerClassifKKNN[32]> classif.kknn
##  5: <TaskClassif[46]> satellite_task <LearnerClassifKKNN[32]> classif.kknn
##  6: <TaskClassif[46]> satellite_task <LearnerClassifKKNN[32]> classif.kknn
##  7: <TaskClassif[46]> satellite_task <LearnerClassifKKNN[32]> classif.kknn
##  8: <TaskClassif[46]> satellite_task <LearnerClassifKKNN[32]> classif.kknn
##  9: <TaskClassif[46]> satellite_task <LearnerClassifKKNN[32]> classif.kknn
## 10: <TaskClassif[46]> satellite_task <LearnerClassifKKNN[32]> classif.kknn
##             resampling resampling_id iteration              prediction
##  1: <ResamplingCV[19]>            cv         1 <PredictionClassif[19]>
##  2: <ResamplingCV[19]>            cv         2 <PredictionClassif[19]>
##  3: <ResamplingCV[19]>            cv         3 <PredictionClassif[19]>
##  4: <ResamplingCV[19]>            cv         4 <PredictionClassif[19]>
##  5: <ResamplingCV[19]>            cv         5 <PredictionClassif[19]>
##  6: <ResamplingCV[19]>            cv         6 <PredictionClassif[19]>
##  7: <ResamplingCV[19]>            cv         7 <PredictionClassif[19]>
##  8: <ResamplingCV[19]>            cv         8 <PredictionClassif[19]>
##  9: <ResamplingCV[19]>            cv         9 <PredictionClassif[19]>
## 10: <ResamplingCV[19]>            cv        10 <PredictionClassif[19]>
##     classif.ce
##  1: 0.09627329
##  2: 0.07919255
##  3: 0.08540373
##  4: 0.09472050
##  5: 0.10559006
##  6: 0.08553655
##  7: 0.08864697
##  8: 0.10108865
##  9: 0.08864697
## 10: 0.10419907
\end{verbatim}
\begin{alltt}
\hlstd{res}\hlopt{$}\hlkwd{aggregate}\hlstd{()}
\end{alltt}
\begin{verbatim}
## classif.ce 
## 0.09292983
\end{verbatim}
\end{kframe}
\end{knitrout}
    
    
    
}

\dlz



\aufgabe{

In preparing this course you already learned about \texttt{mlr3}. If you need to refresh your knowledge you can find help at \url{https://mlr3book.mlr-org.com/} under 'Basics'.
\begin{enumerate}
\item[a)] How many performance measures do you already know? Try to explain some of them. How can you see which of them are available in \texttt{mlr3}?
\item[b)] Use the \texttt{boston\_housing} regression task from \texttt{mlr3} and split the data into $50\,\%$ training data and $50\,\%$ test data while training and predicting (i.\,e., use the \texttt{row\_ids} argument of the \texttt{train} and \texttt{predict} function).
Fit a prediction model (e.\,g.\ k-NN) to the training set and make predictions for the test set.
\item[c)] Compare the performance on training and test data. Use the \texttt{score} function.
\item[d)] Now use different observations (but still $50\,\%$ of them) for the training set. How does this affect the predictions and the error estimates of the test data?
\item[e)] Use 10 fold cross-validation to estimate the performance. Hint: Use the mlr functions \texttt{rsmp} and \texttt{resample}.
% \item[f)] Now use nested resampling to fit the best knn model (try k from 1 to 10) on the \texttt{bh.task} and estimate the generalization error. Use the functions \texttt{makeParamSet}, \texttt{makeDiscreteParam}, \texttt{makeTuneControlGrid}, \texttt{makeResampleDesc}, \texttt{makeTuneWrapper} and \texttt{resample}. See also: \url{https://mlr.mlr-org.com/articles/tutorial/nested_resampling.html}.
\end{enumerate}
}


\end{document}
