% !Rnw weave = knitr

<<setup-child, include = FALSE, echo=FALSE>>=
library('knitr')
knitr::set_parent("../../style/preamble_ueb_coll.Rnw")
@

\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-trees.tex}
\input{../../latex-math/ml-eval.tex}

\kopf{Performance Evaluation}

\tableofcontents

% ------------------------------------------------------------------------------
% LECTURE EXERCISES
% ------------------------------------------------------------------------------

\dlz
\exlect
\lz

\aufgabe{Performance Evaluation in \texttt{mlr3}}{
<<child="ex_rnw/ex_mlr.Rnw">>=
@
}

\dlz
\loesung{
<<child="ex_rnw/sol_mlr.Rnw">>=
@
}

\dlz
\aufgabe{Evaluation for Regression}{
<<child="ex_rnw/ex_regr_metrics.Rnw">>=
@
}

\dlz
\loesung{
<<child="ex_rnw/sol_regr_metrics.Rnw">>=
@
}

\dlz
\aufgabe{Evaluation for $k$-NN}{
<<child="ex_rnw/ex_eval_knn.Rnw">>=
@
}

\dlz
\loesung{
<<child="ex_rnw/sol_eval_knn.Rnw">>=
@
}

\dlz
\aufgabe{Overfitting \& Underfitting}{
<<child="ex_rnw/ex_of_uf.Rnw">>=
@
}

\dlz
\loesung{
<<child="ex_rnw/sol_of_uf.Rnw">>=
@
}

\dlz
\aufgabe{Train-Test Split}{
<<child="ex_rnw/ex_train_test_split.Rnw">>=
@
}

\dlz
\loesung{
<<child="ex_rnw/sol_train_test_split.Rnw">>=
@
}

\dlz
\aufgabe{Resampling Strategies}{
<<child="ex_rnw/ex_resa_mlr3.Rnw">>=
@
}

\dlz
\loesung{
<<child="ex_rnw/sol_resa_mlr3.Rnw">>=
@
}

\dlz
\aufgabe{Train vs Test Error}{
<<child="ex_rnw/ex_resampling_1.Rnw">>=
@
}

\dlz
\loesung{
<<child="ex_rnw/sol_resampling_1.Rnw">>=
@
}

\dlz

\aufgabe{Generalization Error}{
<<child="ex_rnw/ex_resampling_2.Rnw">>=
@
}

\dlz
\loesung{
<<child="ex_rnw/sol_resampling_2.Rnw">>=
@
}

\dlz

\aufgabe{Benchmark}{
<<child="ex_rnw/ex_benchmark.Rnw">>=
@
}

\dlz
\loesung{
See \texttt{R} file
}

\dlz

\aufgabe{confusion matrix, ROC \& AUC}{
<<child="ex_rnw/ex_roc_2.Rnw">>=
@
}

\dlz
\loesung{
<<child="ex_rnw/sol_roc_2.Rnw">>=
@
}

\dlz
\aufgabe{ROC precision}{
<<child="ex_rnw/ex_roc_1.Rnw">>=
@
}

\dlz
\loesung{
<<child="ex_rnw/sol_roc_1.Rnw">>=
@
}

\dlz
\aufgabe{LOO estimator}{
<<child="ex_rnw/ex_loo_estimator.Rnw">>=
@
}

\dlz
\loesung{
<<child="ex_rnw/sol_loo_estimator.Rnw">>=
@
}

% ------------------------------------------------------------------------------
% PAST EXAMS
% ------------------------------------------------------------------------------

\dlz
\exexams
\lz

% ------------------------------------------------------------------------------

\aufgabeexam{WS2020/21}{first}{3}{

Consider a binary classification algorithm that yielded the following results 
on 8 observations. The table shows  true classes and  predicted probabilities
for class 1:

\begin{tabular}{ | c | c | c | }
  \hline
  ID & True Class & Prediction  \\ \hline
  1 & 1 & 0.50  \\
  2 & 0 & 0.01  \\
  3 & 0 & 0.90  \\
  4 & 0 & 0.55  \\
  5 & 1 & 0.10  \\
  6 & 1 & 0.72  \\
  7 & 1 & 0.70  \\
  8 & 1 & 0.99  \\
  \hline
\end{tabular}

\begin{enumerate}
  \item Draw the ROC curve of the classifier manually. Explain every step 
  thoroughly, and make sure to annotate the axes of your plot.
  \item Calculate the AUC of the classifier. Explain every step of your 
  computation thoroughly.
  \item Calculate the partial AUC for FPR $\leq 1/3$. Explain every step of your 
  computation thoroughly.
  \item Create a confusion matrix assuming a threshold of 0.75. Point out which 
  values correspond to true positives (TP), true negatives (TN), false positives 
  (FP), and false negatives (FN).
  \item Compute sensitivity, specificity, negative predictive value, positive 
  predictive value, accuracy and F1-score. State the respective formulas first.
  \item In the following plot, you see the ROC curves of two different 
  classifiers with similar AUC's. Describe a practical situation where you would 
  prefer classifier 1 over classifier 2 and explain why. (Note: The data in this 
  question is not related to the data of the above questions.)
  <<include=TRUE, results="asis", echo=FALSE, fig.width=5, fig.height=3.5>>=
  library(reshape)
  roc_data1 <- data.frame(TPR = c(0, 0.2, 0.4, 0.6, 0.6, 0.6, 0.6, 0.8, 0.8, 1),
                          FPR = c(0, 0.0, 0.1, 0.2, 0.4, 0.6, 0.6, 0.8, 0.8, 1))
  roc_data2 <- data.frame(TPR = c(0, 0.4, 0.5, 0.65, 0.90, 0.95, 1),
                          FPR = c(0, 0.4, 0.4, 0.4, 0.5, 0.6, 1))
  melted <- melt(list(
    classifier1 = roc_data1,
    classifier2 = roc_data2),
    id.vars = c("TPR","FPR"))
  names(melted)[3] <- "Classifier"
  cols = c("blue", "orange")
  ggplot(melted, aes(x = FPR, y = TPR, colour = Classifier, group=Classifier)) +
    geom_line(aes(linetype = Classifier)) +
    scale_colour_manual(values = cols) +
    geom_abline(slope = 1, intercept = 0, linetype = 'dashed')
  @
\end{enumerate}

}

\dlz
\loesung{

\begin{enumerate}

  \item Scores: \\
   <<echo=FALSE, message=FALSE, warning = FALSE>>=
  library(pROC)  
  set.seed(1414)
  labels <- sample(c(0,1), 8, replace = TRUE)
  preds <- c(0.5, .01, .9, .55, .1, .72, .7, .99)
  cdata <- data.frame(
    true_labels = labels,
    scores = preds)
  cdata <- cdata[order(cdata$scores, decreasing = T),]
  knitr::kable(cdata)
  @
  Here we see that $\frac{1}{n_+} = \frac{1}{5} = 0.2$ and $\frac{1}{n_-} = 
  \frac{1}{3}$. Now we follow the algorithm as described in the lecture slides:
  \begin{itemize}
    \item Set  $\tau = 1$, so we start in $(0,0)$; we predict everything as 1.
    \item Set  $\tau = 0.95$ yields TPR $0 + \frac{1}{n_+} = 0.2$ and FPR  0. 
    (Obs. 8 is '1')
    \item Set  $\tau = 0.8$ yields TPR $0.2$ and FPR  $0 + \frac{1}{n_-} = 1/3$. 
    (Obs. 3 is '0')
    \item Set  $\tau = 0.71$ yields TPR $0.2 + \frac{1}{n_+} = 0.4$ and FPR  
    $1/3$. (Obs. 6 is '1')
    \item Set  $\tau = 0.6$ yields TPR $0.4 + \frac{1}{n_+} = 0.6$ and FPR  
    $1/3$. (Obs. 7 is '1')
    \item Set  $\tau = 0.52$ yields TPR $0.6$ and FPR  $1/3 + \frac{1}{n_-} = 
    2/3$. (Obs. 4 is '0')
    \item Set  $\tau = 0.3$ yields TPR $0.6+ \frac{1}{n_+} = 0.8$ and FPR  
    $2/3$. (Obs. 1 is '1')
    \item Set  $\tau = 0.05$ yields TPR $0.8 + \frac{1}{n_+} = 1$ and FPR  
    $2/3$. (Obs. 5 is '1')
    \item Set  $\tau = 0$ yields TPR $1$ and FPR  $2/3 + \frac{1}{n_-} = 1$. 
    (Obs. 2 is '0')
  \end{itemize}
  Therefore we get the polygonal path consisting of the ordered list of vertices 
  \[(0,0), (0,0.2), (1/3,0.2), (1/3,0.4), (1/3,0.6), (2/3,0.6), (2/3,0.8), 
  (2/3, 1), (1, 1).\]
  <<echo=FALSE, message=FALSE, warning = FALSE, fig.height=3, fig.width=3>>=
  library(pROC)  
  set.seed(1414)
  labels <- sample(c(0,1), 8, replace = TRUE)
  preds <- c(0.5, .01, .9, .55, .1, .72, .7, .99)
  ROC <- roc(labels, preds)
  plot(ROC, legacy.axes=TRUE)
  @   
  
  \item The AUC is the sum of three rectangles: $0.2 \cdot 1/3 + 0.6 \cdot 1/3 + 
  1\cdot 1/3 = 0.6$
  
  \item The partial AUC is the area under the curve that is left from 
  FPR $= 1/3$, so it is just the first rectangle: $0.2 \cdot 1/3 = 1/15$
  
  \item \phantom{foo}
  \begin{tabular}{ | c | c | c | } \hline
    & Actual Class - 0 & Actual Class - 1  \\
    Prediction - 0 & 2 & 4  \\
    Prediction - 1 & 1 & 1  \\ \hline
  \end{tabular}
  so we get
  \begin{tabular}{ | c | c | c | c | }
    \hline
    FN & FP & TN & TP   \\ \hline
    4 & 1 & 2 & 1 \\ \hline
  \end{tabular}
    
  \item $$\text{Sensitivity} = \frac{\text{TP}}{\text{TP} + \text{FN}} =
  \frac{1}{5} $$
  $$\text{Specificity}  = \frac{\text{TN}}{\text{TN} + \text{FP}} =\frac{2}{3}$$
  $$\text{Negative Predictive Value} = \frac{\text{TN}}{\text{TN} + \text{FN}} =
  \frac{1}{3} $$
  $$\text{Positive Predictive Value} = \frac{\text{TP}}{\text{TP} + \text{FP}} =
  \frac{1}{2} $$
  $$\text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + 
  \text{FP} + \text{FN}} =\frac{3}{8} $$
  $$\text{F1-score} = \frac{2\cdot\text{PPV}\cdot\text{Sensitivity}}{\text{PPV}
  +\text{Sensitivity}} = (0.2) / (0.7) = 2/7  $$
  
  \item Important point is to understand that classifier 1 does better in the
  'high scores'. Or: For some threshold below 0.5 the precision is far better
  for classifier 1 than for classifier 2. For example if you want to output a
  certain number of 'most promising customers' this could be a good idea.

\end{enumerate}
}

%-------------------------------------------------------------------------------

\aufgabeexam{WS2020/21}{first}{6}{

Assume a polynomial regression model with a continuous target variable $y$ and 
a continuous, $p$-dimensional feature vector $\xv$ and polynomials of degree 
$d$, i.e., 

$$\fxi = \sum_{j=1}^p \sum_{k=0}^d \theta_{j,k} (\xi_j)^k,$$ 

and 
$\yi = \fxi + \epsi$ where the $\epsi$ are iid with $\var(\epsi)=\sigma^2 \ 
\forall i\in \{1, \dots, n\}$. 

\begin{enumerate}
  \item For each of the following situations, indicate whether we would 
  generally expect the performance of a flexible polynomial learner (high $d$) 
  to be better or worse than an inflexible polynomial learner (low $d$). 
  Justify your answer.
  \begin{enumerate}
    \item[(i)] The sample size $n$ is extremely large, and the number of 
    features $p$ is small.
    \item[(ii)] The number of features $p$ is extremely large, and the number of 
    observations $n$ is small.
    \item[(iii)] The true relationship between the features and the response is 
    highly non-linear.
    \item[(iv)] The variance of the error terms, $\sigma^2$, is extremely high.
  \end{enumerate}
\end{enumerate}

}

\dlz
\loesung{

\begin{enumerate}
  \item 
  \begin{itemize}
    \item (i): flexible: because it covers a larger Hypothesis space and we 
    have enough data
    \item (ii): inflexible: to prevent overfitting
    \item (iii): flexible: because it covers a larger Hypothesis space and we 
    have enough data, an inflexible approach would be too restrictive
    \item (iv): inflexible: to prevent overfitting
  \end{itemize}
\end{enumerate}

}


%-------------------------------------------------------------------------------

\aufgabeexam{WS2020/21}{second}{3}{

Consider a binary classification algorithm that yielded the following results on 
8 observations. The table shows  true classes and  predicted probabilities for 
class 1:

\begin{tabular}{ | c | c | c | }
  \hline
  ID & True Class & Prediction  \\ \hline
  1 & 1 & 0.30  \\
  2 & 0 & 0.91  \\
  3 & 0 & 0.03  \\
  4 & 0 & 0.55  \\
  5 & 0 & 0.45  \\
  6 & 0 & 0.65  \\
  7 & 1 & 0.71  \\
  8 & 1 & 0.98  \\
  \hline
\end{tabular}

\begin{enumerate}
  \item Draw the ROC curve of the classifier manually. Compute all relevant 
  numbers explicitly, state the respective formulas first, and make sure to 
  annotate the axes of your plot.
  \item Calculate the AUC of the classifier. Explain every step of your 
  computation thoroughly.
  \item Calculate the partial AUC for TPR $\geq 2/3$. Explain every step of your 
  computation thoroughly.
  \item Create a confusion matrix assuming a threshold of 0.7. Point out which 
  values correspond to true positives (TP), true negatives (TN), false positives 
  (FP), and false negatives (FN).
  \item Compute sensitivity, specificity, negative predictive value, positive 
  predictive value, accuracy and F1-score. State the respective formulas first.  
  \item Now look at the following plot. Explain thoroughly why the solid line 
  cannot be the ROC curve of a classifier.
  <<include=TRUE, results="asis", echo=FALSE, fig.width=3, fig.height=2.5>>=
  library(reshape)
  roc_data1 <- data.frame(TPR = c(0, 0.2, 0.4, 0.6, 0.45, 0.4, 0.7, 0.8, 0.8, 1),
                          FPR = c(0, 0.0, 0.1, 0.2, 0.4, 0.6, 0.6, 0.8, 0.8, 1))
  ggplot(roc_data1, aes(x = FPR, y = TPR)) + 
    geom_line(aes(x = FPR, y = TPR)) + 
    geom_abline(slope = 1, intercept = 0, linetype = 'dashed')
  @   
  \item Imagine you are working in the marketing department of a large company. 
  Your colleagues are developing a marketing campaign where they will call 
  selected customers by phone in order to advertise a new product. Your company 
  has 1,000,000 customers in the database and the budget of the marketing 
  campaign allows to call 1,000 of these customers. Now it is your job to select 
  the most promising 1,000 customers, i.e., those who will most likely buy the 
  new product after having received the advertising phone call. Luckily, you 
  have a large amount of similar data from older marketing campaigns that are 
  representative for this campaign and can be used to train a supervised 
  classification model with the target variable indicating if the customer did 
  buy the product (1) or not (0). You train a random forest on the 1,000,000 
  observations and get the following cross-validated ROC curve. Assuming a 
  balanced target variable: Do you think this model is fairly good for your 
  purpose? Explain why and describe, how you would proceed from here to provide 
  your colleagues with the 1,000 most promising customers.
  <<include=TRUE, results="asis", echo=FALSE, fig.width=3, fig.height=2.5>>=
    library(reshape)
    roc_data1 <- data.frame(TPR = c(0, 0.2, 0.4, 0.4, .4, .6, 0.8, 0.9, 0.95, 1),
                            FPR = c(0, 0.0, 0.05, 0.2, .4, .6, 0.7, 0.8, 0.85, 1))
    ggplot(roc_data1, aes(x = FPR, y = TPR)) + 
      geom_line(aes(x = FPR, y = TPR)) + 
      geom_abline(slope = 1, intercept = 0, linetype = 'dashed')
  @   
\end{enumerate}

}

\dlz
\loesung{

\begin{enumerate}

  \item First we sort the results by score: \\
   <<echo=FALSE, message=FALSE, warning = FALSE>>=
  library(pROC)  
  set.seed(1414)
  labels <- c(1, 0, 0, 0, 0, 0, 1, 1)
  preds <- c(0.3, .91, .03, .55, .45, .65, .71, .98)
  cdata <- data.frame(true_labels = labels, scores = preds)
  cdata <- cdata[order(cdata$scores, decreasing = T),]
  knitr::kable(cdata)
  @
  Here we see that $\frac{1}{n_-} = \frac{1}{5} = 0.2$ and $\frac{1}{n_+} = 
  \frac{1}{3}$. Now we follow the algorithm as described in the lecture slides:
  \begin{itemize}
    \item Set  $\tau = 1$, so we start in $(0,0)$; we predict everything as 1.
    \item Set  $\tau = 0.95$ yields TPR $0 + \frac{1}{n_+} = 1/3$ and FPR  0. 
    (Obs. 8 is '1')
    \item Set  $\tau = 0.9$ yields TPR $1/3$ and FPR  $0 + \frac{1}{n_-} = 0.2$. 
    (Obs. 2 is '0')
    \item Set  $\tau = 0.70$ yields TPR $1/3 + \frac{1}{n_+} = 2/3$ and FPR  
    $0.2$. (Obs. 7 is '1')
    \item Set  $\tau = 0.6$ yields TPR $2/3$ and FPR  $0.2 + \frac{1}{n_-} = 
    0.4$. (Obs. 6 is '0')
    \item Set  $\tau = 0.5$ yi´elds TPR $2/3$ and FPR  $0.4 + \frac{1}{n_-} = 
    0.6$. (Obs. 4 is '0')
    \item Set  $\tau = 0.4$ yields TPR $2/3$ and FPR  $0.6 + \frac{1}{n_-} = 
    0.8$. (Obs. 5 is '0')
    \item Set  $\tau = 0.1$ yields TPR $2/3 + \frac{1}{n_+} = 1$ and FPR  $0.8$. 
    (Obs. 1 is '1')
    \item Set  $\tau = 0$ yields TPR $1$ and FPR  $0.8 + \frac{1}{n_-} = 1$. 
    (Obs. 3 is '0')
  \end{itemize}
    <<echo=FALSE, message=FALSE, warning = FALSE, fig.height=3, fig.width=3>>=
  library(pROC)  
  set.seed(1414)
  labels <- c(1, 0, 0, 0, 0, 0, 1, 1)
  preds <- c(0.3, .91, .03, .55, .45, .65, .71, .98)
  ROC <- roc(labels, preds)
  plot(ROC, legacy.axes=TRUE)
  @   

  \item The AUC is the sum of three rectangles: 
  $0.2 \cdot 1/3 + 0.6 \cdot 2/3 + 1\cdot 0.2 = 2/3$
  
  \item The partial AUC is the area under the curve that is above TPR $= 2/3$, 
  so it is just the small rectangle in the upper right corner: 
  $0.2 \cdot 1/3 = 1/15$ 
  
  \item   
  \begin{tabular}{ | c | c | c | } \hline
     & Actual Class - 0 & Actual Class - 1  \\
    Prediction - 0 & 4 & 1  \\
    Prediction - 1 & 1 & 2  \\ \hline
  \end{tabular}
  so we get
  \begin{tabular}{ | c | c | c | c | } \hline
    FN & FP & TN & TP   \\ \hline
    1 & 1 & 4 & 2 \\ \hline
  \end{tabular}
  
  \item   
  $$\text{Sensitivity} = \frac{\text{TP}}{\text{TP} + \text{FN}} =\frac{2}{3}$$
  $$\text{Specificity}  = \frac{\text{TN}}{\text{TN} + \text{FP}} =\frac{4}{5}$$
  $$\text{Negative Predictive Value} = \frac{\text{TN}}{\text{TN} + \text{FN}} 
  =\frac{4}{5} $$
  $$\text{Positive Predictive Value} = \frac{\text{TP}}{\text{TP} + \text{FP}} 
  =\frac{2}{3} $$
  $$\text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + 
  \text{FP} + \text{FN}} =\frac{6}{8} $$
  $$\text{F1-score} = \frac{2\cdot\text{PPV}\cdot\text{Sensitivity}}{\text{PPV}
  +\text{Sensitivity}} = (8/ 9) / (4/3) = 2/3  $$
  
  \item TPR and FPR are both metrics that increase monotonically as the 
  threshold $c$ traverses from 1 to 0. The plot shows two instances of 
  diminishing TPR, which would mean that, at the corresponding threshold, an 
  observation that had previously been correctly classified as positive was not 
  detected anymore. This is not possible with a binarization threshold.
  
  \item Yes. I would order the customers wrt the scores, then roughly the first 
  200,000 customers would be true positives (since the ROC is based on 1,000,000 
  customers and true class is balanced) and I would just select the top 1,000 
  customers. It doesn't matter that the classifiers gets bad later.
  
\end{enumerate}

}

% ------------------------------------------------------------------------------
% INSPO
% ------------------------------------------------------------------------------

\dlz
\exinspo