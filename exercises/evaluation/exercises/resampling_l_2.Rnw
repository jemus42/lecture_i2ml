\begin{enumerate}

\item Let $Y_1, \ldots, Y_n, Y$ all be i.i.d and Bernloulli-($\frac{1}{2}$) distributed. $Y_1, \ldots, Y_n$ is the training data, $Y$ is a new test observation and $\hat{Y}$ the prediction of our data independent rule for $Y$.

It follows:

\[
P(\hat{Y} \neq Y ) = P(\hat{Y} = 1) (1 - 0.5) + (1 - P(\hat{Y} = 1)) 0.5
\]

and (this is not really required but will be useful later)

\[
P(\hat{Y} = 1) = \frac{1}{2}.
\]

This is the case because in the product space for $Y_1, \ldots, Y_n$ all events $Y_1=y_1, \ldots, Y_n=y_n$ have the same probability ($0.5^n$) and exactly half of the events have an even number of 1s, the other half an odd number. 

So it follows that:
\[
P(\hat{Y} \neq Y ) = 0.5 
\]


\item Let $\hat{L}$ be the leave-one-out (LOO) estimator of $Y_1, \ldots, Y_n$ and $\hat{Y_i}$ the prediction of our data independent rule for $Y_i$ with LOO. This means the estimation is based on $Y_1, \ldots, Y_n$ without $Y_i$.

It follows:

\[
\hat{L} = \frac{1}{n} \sum_{i=1}^n I(\hat{Y}_i \neq Y_i). 
\]

$\hat{L}$ can only have the value 1 or 0. Which can be explained like this:

Assume that $Y_1(\omega), \ldots, Y_n(\omega)$ have an even number of 1s. Now drop one $Y_i(\omega)$. If it was a 0, the remaining observations still have an even number of 1s, a 0 will be predicted and $\hat{Y}_j(\omega)=0=Y_j(\omega)$.
If it is a 1, it results in an odd number of 1s, a 1 will be predicted and $\hat{Y}_j(\omega)=1=Y_j(\omega)$.
So, $\hat{L}(\omega) = 0$.

Now assume that $Y_1(\omega), \ldots, Y_n(\omega)$ have an odd number of 1s. Again, drop one $Y_i(\omega)$. If it was a 0, the remaining observations still have an odd number of 1s, a 1 is predicted and $\hat{Y}_j(\omega) \neq Y_j(\omega)$.
If it is a 1, it results in an even number of 1s, a 0 will be predicted and $\hat{Y}_j(\omega) \neq Y_j(\omega)$.
So, $\hat{L}(\omega) = 1$.

This means that $\hat{L}$ is a Bernoulli distributed random variable. An even number of 1s in $Y_1, \ldots, Y_n$ has the same probability as an odd number, so $\hat{L} \sim$ Bernoulli-(0.5) and consequently $E(\hat{L}) = 1/2$ and $Var(\hat{L}) = 1/4$.

The expected value is also correct like in a). But the variance is very high. The LOO can only be calculated once and it behaves like a fair coin toss. It will produce an error of $0\%$ or an error of $100\%$. The example is rather theoretical but shows the disadvantageous property of a high variance in the LOO estimator. 

\end{enumerate}
