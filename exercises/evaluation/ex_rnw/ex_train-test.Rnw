In supervised learning, we typically assume that the data set $\D = \Dset$ originates from a data generating process $\Pxy$ in an i.i.d manner, i.e., $\D \sim \left(\Pxy\right)^n$.
One could split data set $\D$ with $n$ observations into subsets $\Dtrain$ and $\Dtest$ of sizes $\ntrain$ and $\ntest$ with $\ntrain + \ntest = n$.
Both subsets can be represented with index vectors $\Jtrain \in \JtrainSpace$ and $\Jtest \in \JtestSpace$, respectively.
For such an index vector $J$ of length $m$, one can define a corresponding vector of labels $\yJ = \yJDef \in \Yspace^m$ and a corresponding matrix of prediction scores $\FJf = \FJfDef \in \R^{m\times g}$ for a model $f$.
For regression tasks, $g = 1$ and $\FJf$ is a vector.

For a learner $\inducer$, $\ntrain$ training observations and a performance measure $\rho$, the \textbf{generalization error} can be formally expressed as:
\begin{align}
\GE(\inducer, \ntrain, \rho) = \lim_{\ntest\rightarrow\infty} \E_{\Dtrain,\Dtest \sim \Pxy} \left[ \rho\left(\yv_{\Jtest}, {\F_{\Jtest,\inducer(\Dtrain)}}\right)\right],
\end{align}
where $\Dtrain$ and $\Dtest$ are independently sampled from $\Pxy$.
\begin{enumerate}\bfseries
  \item[1)] What is the generalization error? Describe the formula above in your own words.
\end{enumerate}
In practice, the data generating process $\Pxy$ is usually unknown. However, assume we can sample as many times as we like from $\Pxy$.
\begin{enumerate}\bfseries
  \item[2)] Explain how you could empirically estimate the generalization error $\GE(\inducer, \ntrain = 100, \rho)$ of a learner $\inducer$ trained on $\ntrain = 100$ observations and evaluated on performance measure $\rho$, given that you can sample from $\Pxy$ as often as you like.
\end{enumerate}
In addition to an unknown data-generating process $\Pxy$, supervised learning is often restricted to a data set $\D$ of fixed size $n$.
Therefore, the true generalization error $\GE(\inducer, \ntrain, \rho)$ remains unknown.
In this case, hold-out splitting is a simple  procedure that can be used to estimate the generalization error:
\begin{align}
{\GEh_{\Jtrain, \Jtest}(\inducer, |\Jtrain|, \rho)} = \rho\left(\yv_{\Jtest}, {\F_{\Jtest,\inducer(\Dtrain)}}\right),
\end{align}
where $\Jtrain \in \JtrainSpace$ specifies the subset of $\D$ the learner $\inducer$ is trained on, with $|\Jtrain| = \ntrain < n$.
\begin{enumerate}\bfseries
  \item[3)] Explain how the choice of $|\Jtrain|$ may influence the bias of ${\GEh_{\Jtrain, \Jtest}(\inducer, |\Jtrain|, \rho)}$.
  \item[4)] Explain how the choice of $|\Jtrain|$ may influence the variance of ${\GEh_{\Jtrain, \Jtest}(\inducer, |\Jtrain|, \rho)}$.
\end{enumerate}
%Assume we know the true generalization error $\GE(\inducer, \ntrain = 100, \rho)$ of a learner $\inducer$ that is evaluated on performance measure $\rho$ and can sample as many times as we like from $\Pxy$.
%\begin{enumerate}\bfseries
%  \item[4)] How could you empirically estimate the bias of ${\GEh_{\Jtrain, \Jtest}(\inducer, |\Jtrain|, \rho)}$ for a given $10 < |\Jtrain| < 90$?
%\end{enumerate}