Assume a polynomial regression model with a continuous target variable $y$ and 
a continuous, $p$-dimensional feature vector $\xv$ and polynomials of degree 
$d$, i.e., 
$$\fxi = \sum_{j=1}^p \sum_{k=0}^d \theta_{j,k} (\xi_j)^k,$$ and 
$\yi = \fxi + \epsi$ where the $\epsi$ are iid with $\var(\epsi)=\sigma^2 \ 
\forall i\in \{1, \dots, n\}$. 

\begin{enumerate}[a)]
  \item For each of the following situations, indicate whether we would 
  generally expect the performance of a flexible polynomial learner (high $d$) 
  to be better or worse than an inflexible one (low $d$). 
  Justify your answer.
  \begin{enumerate}
    \item[(i)] The sample size $n$ is extremely large, and the number of 
    features $p$ is small.
    \item[(ii)] The number of features $p$ is extremely large, and the number of 
    observations $n$ is small.
    \item[(iii)] The true relationship between the features and the response is 
    highly non-linear.
    \item[(iv)] The variance of the error terms, $\sigma^2$, is extremely high.
  \end{enumerate}
  \item Are overfitting and underfitting properties of a learner or of a
  fixed model? Explain your answer.
  \item Should we aim to completely avoid both overfitting and underfitting?
\end{enumerate}
