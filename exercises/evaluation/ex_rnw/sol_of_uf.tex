\begin{enumerate}[a)]
  \item NB: these are very general scenarios and we can only state tendencies --
  there might be additional factors at play that encourage under- or overfitting 
  in the specific situation.
  \begin{enumerate}[i)]
    \item Large training set \& linear regression learner: we would rather 
    expect \textit{underfitting} behavior here, as the model class is fairly 
    simple and the model will not easily fit all training points closely if 
    there are many.
    \item Noisy training data \& high-degree polynomial regression learner: 
    this situation risks \textit{overfitting}. The learner is quite flexible and 
    the regression model might interpolate between the training samples, 
    creating a wiggly curve that generalizes poorly.
    \item Few observations of many features \& linear regression learner: even 
    though the model class is simple, we might run into \textit{overfitting} 
    here. The data situation, which we frequently encounter in biostatistics 
    (e.g., genomics data), creates a high-dimensional and sparsely populated 
    input space. In particular, some combinations of features will be 
    represented by only few or even no observations at all. 
    This means we have a large number of hypotheses, i.e., linear models with 
    many covariates (possibly even interactions if we allow for it), and 
    only few data to discern between them.
    \item Data with class overlap \& LDA learner: here we might expect 
    \textit{underfitting.} Clearly, the data are not linearly separable if the 
    classes overlap in the input space, so LDA will not be able to learn the 
    shape of the underlying function perfectly.
  \end{enumerate}
  \item Overfitting and underfitting are always connected to a 
  particular fixed \textit{model}, even though attributes of the underlying 
  hypothesis space typically influence the tendency toward one or the other 
  behavior, as we have seen in the previous question.
  In order to understand this, think of a classification problem with linearly 
  separable data.
  Applying a QDA learner, which is able to learn more complex decision 
  boundaries, poses a risk of overfitting with the chosen model, but the degree 
  of overfitting depends on the model itself.
  In theory, the QDA learner is set to choose equal covariances for the 
  Gaussian class densities, amounting to LDA and \textit{not} overfitting the 
  data.
  A QDA model with distinctly different covariances, on the other hand, will 
  probably invoke overfitting.
  Under- and overfitting are therefore properties of a specific model and not 
  of an entire learner, though flexibility of the latter impacts the 
  propensity toward either behavior.
  
  A common strategy is to choose a rather flexible model class and encourage 
  simplicity in the actual model by \textit{regularization} (e.g., take a 
  higher-degree polynomial but drive as many coefficients a possible toward 
  zero).
  
  \item That will be hardly possible.
  Recall how we defined the two variants of ill fit:
  $$UF(\fh, L) = GE(\fh, L) - GE(f^\ast, L)$$
  $$OF(\fh, L) = GE(\fh, L) - \riske(\fh, L)$$
  In order to avoid underfitting we would need to always find the universally 
  loss-optimal model across arbitrary hypothesis spaces (the so-called 
  \textit{Bayes-optimal} model), which is obviously not something we can hope 
  to achieve in general.
  Zero overfitting would mean to exactly balance theoretical generalization 
  error and empirical risk, but the way empirical risk minimization is designed, 
  our model will likely fare a bit worse on unseen test data.
  
  In practice we will always experience these phenomena to some degree and 
  finding a model that trades them off well is the holy grail in machine 
  learning.
\end{enumerate}
