\begin{enumerate}
%
\item We want to show that 
%
$$ L_{0-1}(y,\hx) = \mathds{1}_{\{ y \ne \hx \}} \leq L(y,\fx) = \max_{k} \left( f_k(\xv) - f_y(\xv) + \mathds{1}_{\{ y \ne k \}} \right), $$ 
%
where 
%
$$ \Hspace = \{  f=(f_1,\ldots,f_g)^\top: \Xspace \to \R^g ~|~ f_i:\Xspace \to \R, \ \forall i \in \Yspace  \}. $$
%
and
%
\begin{align} \label{def_predictor}
	%	
	h(\xv) = \argmax_{k \in \gset} \fkx[k]. 
	%	
\end{align}
%
We distinguish two cases for any arbitrary data point pair $(\xv,y)\in \Xspace\times \Yspace.$


\textbf{Case 1:} $h(\xv)\neq y$

Thus, $L_{0-1}(y,\hx) = \mathds{1}_{\{ y \ne \hx \}}  = 1$ and in light of \eqref{def_predictor} this means that $y \neq \argmax_{k \in \gset} \fkx[k],$ so that there exists some $\tilde k \neq y$ with $\fkx[\tilde k]\geq \fkx[y].$
%
\begin{align}
%	
	L(y,\fx) 
%	
	&= \max_{k} \left(  f_k(\xv) - f_y(\xv)  + \mathds{1}_{\{ y \ne k \}} \right) \tag{Definition} \\
	%
	&\geq \left(  \underbrace{\fkx[\tilde k] - \fkx[y] }_{\geq 0} + \mathds{1}_{\{ y \ne \tilde k \}} \right) \tag{$\max$ always greater than one single component} \\
%	
	&\geq \mathds{1}_{\{ y \ne \tilde k \}} \tag{$\fkx[\tilde k] \geq \fkx[y]$} \\
%	
	&= 1. \tag{Indicator function is true}
%	
\end{align}


\textbf{Case 2:} $h(\xv)=y$

Thus, $L_{0-1}(y,\hx) = \mathds{1}_{\{ y \ne \hx \}}  = 0.$
%
But it always holds that
%
\begin{align}
	%	
	L(y,\fx) 
	%	
	&= \max_{k} \left(  f_k(\xv) - f_y(\xv)  + \mathds{1}_{\{ y \ne k \}} \right) \tag{Definition} \\
	%
	&\geq \left(  \fkx[y] - \fkx[y]   + \mathds{1}_{\{ y \ne y \}} \right) \tag{$\max$ always greater than one single component} \\
	%	
	&= \mathds{1}_{\{ y \ne y \}} \notag \\
	%	
	&= 0. \tag{Indicator function is not true}
	%	
\end{align}
%
%
\item For sake of convenience, define $g_{k,y}(\xv) =  f_k(\xv) - f_y(\xv)  + \mathds{1}_{\{ y \ne k \}}$ for any $k,y\in \Yspace$ and $\xv \in \Xspace.$
%
If $k=y,$ then of course $g_{k,y}(\xv) = 0,$ so that in order to find the maximum of the multiclass hinge loss, we need to check for each $k\neq y$ whether $g_{k,y}(\xv) > 0$ holds. If this doesn't hold for any $k\neq y,$ then the maximum is $0 = g_{y,y}(\xv).$
%
Thus,
%
\begin{align}
	%	
	L(y,\fx) 
	%	
	&= \max_{k} \left(  f_k(\xv) - f_y(\xv)  + \mathds{1}_{\{ y \ne k \}} \right) \tag{Definition} \\
%	
	&= \max_{k} g_{k,y}(\xv) \tag{Definition of $g_{k,y}(\xv)$} \\
	%
	&= \max_{k\neq y} ~ \left(  \max\{g_{y,y}(\xv), g_{k,y}(\xv)\} \right) \tag{Idea above} \\
	%	
	&= \max_{k\neq y} ~ \left(  \max\{0 , f_k(\xv) - f_y(\xv)  + \mathds{1}_{\{ y \ne k \}}  \} \right) \tag{Definition of the $g$'s} \\
%	
	&= \max_{k\neq y} ~ \left(  \max\{0 , f_k(\xv) - f_y(\xv)  + 1  \} \right) \tag{Indicator function is true} \\
	%	
	&\leq \sum_{k\neq y} \max \{  0,   f_k(\xv) - f_y(\xv)  + 1\}. \tag{$\max$ is at most the sum over all the \textbf{non-negative }components}
	%	
\end{align}
%
%
\item In the case of binary classification, i.e., $g=2$ and $\Yspace=\{-1,+1\},$ we use a single discriminant model $\fx = f_{1}(\xv) - f_{-1}(\xv)$  based on two scoring functions $f_{1},f_{-1}:\Xspace \to \R$ for the prediction by means of
%
$\hx = \text{sgn}(\fx).$
%
Here, $f_{1}$ is the score for the positive class and $f_{-1}$ is the score for the negative class.
%
Show that the upper bound in (b) coincides with the binary hinge loss $L(y,\fx)=\max\{0,1- y\fx \}.$
%

We distinguish two cases for $y\in \Yspace=\{-1,+1\}.$

\textbf{Case 1:} $y=+1$
%

Then,
%
\begin{align*}
%	
	\sum_{k\neq y} \max \{  0,   f_k(\xv) - f_y(\xv)  + 1\} 
%	
	&= \sum_{k\neq +1} \max \{  0,   f_k(\xv) - f_{1}(\xv)  + 1\}  \tag{Case $y=+1$} \\
%	
	&=  \max \{  0,   f_{-1}(\xv) - f_{1}(\xv)  + 1\}  \tag{Binary classification, i.e., $k$ can only be $-1$} \\
%	
	&= \max \{  0,   1 - f(\xv)  \} \tag{Definition of $f$} \\
%	
	&= \max \{  0,   1 - yf(\xv)\}. \tag{Case $y=+1$} 
%	
\end{align*}

\textbf{Case 2:} $y=-1$
%

Then,
%
\begin{align*}
	%	
	\sum_{k\neq y} \max \{  0,   f_k(\xv) - f_y(\xv)  + 1\} 
	%	
	&= \sum_{k\neq -1} \max \{  0,   f_k(\xv) - f_{-1}(\xv)  + 1\}  \tag{Case $y=-1$} \\
	%	
	&=  \max \{  0,   f_{1}(\xv) - f_{-1}(\xv)  + 1\}  \tag{Binary classification, i.e., $k$ can only be $+1$} \\
	%	
	&= \max \{  0,   1 + f(\xv)  \} \tag{Definition of $f$} \\
	%	
	&= \max \{  0,   1 - yf(\xv)\}. \tag{Case $y=-1$} 
	%	
\end{align*}
%
%
\item Yes, we can state say something similar for the alternative multiclass hinge loss, namely that it is only zero if all the $g-1$ margin\textbf{s} are greater or equal $1,$ where the margins are $m_{y,k}(\xv) = f_y(\xv) - f_k(\xv)$ ($y$ is the true class and $k \in \Yspace\backslash\{y\}$).
%
Indeed, it holds that
%
\begin{align*}
%	
	m_{y,k}(\xv) \geq 1 \quad \forall k\neq y \quad 
%	
	&\Leftrightarrow \quad f_y(\xv) - f_k(\xv) \geq 1 \quad \forall k\neq y \\
%	
	&\Leftrightarrow \quad  f_k(\xv) - f_y(\xv) \leq - 1 \quad \forall k\neq y \\
%	
	&\Leftrightarrow \quad  \max \{ 0 , 1 + f_k(\xv) - f_y(\xv)   \} = 0 \quad \forall k\neq y \\
%	
	&\Leftrightarrow \quad  \sum_{k\neq y}  \max \{ 0 , 1 + f_k(\xv) - f_y(\xv)   \} = 0   \\
%	
\end{align*}


\item An empirical loss minimization approach for the (alternative) multiclass hinge loss will try to minimize the $g-1$ margins \emph{simultaneously}, while the empirical loss minimization approach with the one-vs-rest technique will try to minimize the (binary) margins of the individual binary hinge losses \emph{separately}.
%
In particular, a model obtained by the former will in general not coincide with a model obtained by the latter approach.
%
As an example we can consider the \texttt{iris} dataset:

<<eval=TRUE, fig.height=4, warning=FALSE, message=FALSE>>=



# we use the iris dataset 
Z         = data.matrix(iris)
# we add an additional column with all ones for the intercept
Z         = cbind(rep(1,nrow(Z)),Z)
# p is the dimension of the features
p         = ncol(Z)-1
# classes are in the last column of Z
classes   = unique(Z[,p+1])
# g is the number of classes
g         = length(classes)


# one-vs-rest codebook
codebook <-function(y,i){
  if(y==i){
    return (1)
    }
  else{
    return (-1)
    }
}

# binary hinge loss for linear score function (characterized by theta)
bin_hinge <- function(z,theta){
  x = z[1:p]
  y = z[p+1]
  return(max(0,1-y*x%*%t(t(theta))))
}


# empirical risk for binary hinge loss for linear score function (characterized by theta)
emp_risk_bin_hinge <- function(Z,theta){
  sum(apply(Z,1,bin_hinge,theta=theta)) 
}



# fitting a linear score function in a one-vs-all manner 
one_vs_all_bin_theta <-function(Z){
  theta_mat       = matrix(rep(0,p*g),nrow=p)
  for(i in 1:g){
    # recode the last column of the data matrix according to the codebook
    Z_coded       = cbind(Z[,1:p], unlist(sapply(Z[,p+1],codebook,i) ) ) 
    # finding the best theta 
    theta_mat[,i] = optim(rep(0,p),fn=emp_risk_bin_hinge,Z=Z_coded)$par 
  }
  return ( theta_mat )
}


# predication with linear score functions in a one-vs-all manner
# hat_theta_mat stores the parameters of the binary classifiers
one_vs_all_bin_predict <- function(hat_theta_mat,x){
  scores = x %*% hat_theta_mat
  return (which(scores==max(scores)))
}

# multiclass hinge loss for linear score functions (characterized by the matrix Theta)
multiclass_hinge<-function(z,Theta){
  Theta  = matrix(Theta,nrow=p,byrow=T)
  x      = z[1:p]
  y      = z[p+1]
  temp   = -1 # we start with -1, as in the for loop below we do not leave out the case k=y
  for(i in 1:g){
    temp = temp + max(0, 1 + Theta[,i]%*%t(t(x)) - Theta[,y]%*%t(t(x))  )
  }
  return(temp)
  # alternative if you do not like for-loops
  #return (sum(pmax(rep(0,g), 1+  t(x)%*%Theta - as.numeric(Theta[,y]%*%t(t(x))) ))-1) 
}

# empirical risk for multiclass hinge loss for linear score functions 
# characterized by the matrix Theta
emp_risk_mutliclass_hinge <- function(Z,Theta){
  sum(apply(Z,1,multiclass_hinge,Theta=Theta)) 
}


# fitting linear score functions for multiclass hinge loss
mutliclass_hinge_theta <-function(Z){
  return ( optim(par=rep(0,p*g),fn=emp_risk_mutliclass_hinge,Z=Z)$par)
}


# predication with linear score functions for multiclass hinge loss
# hat_Theta is the fitted parameter matrix
mutliclass_hinge_predict <-function(hat_Theta,x){
  scores = x %*% hat_Theta
  return (which(scores==max(scores))) 
}


# split the iris data into training and test data sets
set.seed(5)
train_ind <- sort(sample(1:nrow(Z),size=50,replace=F))
test_ind <-(1:150)[-train_ind]

# fit the parameters
hat_theta_mat <- one_vs_all_bin_theta(Z[train_ind,]) 
hat_Theta     <- matrix(mutliclass_hinge_theta(Z[train_ind,]),nrow=p,byrow=T)
hat_Theta

# counting how many times the predictions coincide
count = 0

for(j in test_ind){
  # if predictions differ, print the different predictions and the true label
  if(one_vs_all_bin_predict(hat_theta_mat,Z[j,1:p])!=
     mutliclass_hinge_predict(hat_Theta,Z[j,1:p])){
    print(paste("One_vs_all predicts: ",
                one_vs_all_bin_predict(hat_theta_mat,Z[j,1:p])))
    print(paste("Multiclass hinge loss predicts: ",
                mutliclass_hinge_predict(hat_Theta,Z[j,1:p])))
    print(paste("True label: ",Z[j,p+1]))
    print("")
  }
  else{
    count <- count +1
  }
}

# how many times did the predictions coincide?
count

@

\end{enumerate}