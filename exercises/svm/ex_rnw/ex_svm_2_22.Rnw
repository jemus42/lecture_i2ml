	\newcommand{\betab}{\bm{\beta}}
	\renewcommand{\fxt}{f(\xv ~|~ \theta_0,\thetab)}
	%
	For the data set $\D = \Dset$ with $\yi \in \Yspace=\{-1,1\},$ assume we are provided with a suitable feature map $\phi:\Xspace \to \Phi,$ where $\Phi \subset \R^d.$
%	
	In the featurized SVM learning problem we are facing the following optimization problem:
	
	\vspace*{-0.5cm}
	
	\begin{eqnarray*}
		& \min\limits_{\thetab, \theta_0,\sli} & \frac{1}{2} \thetab^\top \thetab + C   \sum_{i=1}^n \sli \\
		& \text{s.t.} & \,\, \yi  \left( \scp{\thetab}{\phi\left(\xi\right)} + \theta_0 \right) \geq 1 - \sli \quad \forall\, i \in \nset,\\
		& \text{and} & \,\, \sli \geq 0 \quad \forall\, i \in \nset,
	\end{eqnarray*}
%
	where $C\geq 0$ is some constant.
	%
	\begin{enumerate}
		%
		\item Argue that this is equivalent to the following ERM problem:
%		
		$$ \risket = \frac{1}{2} \|\thetab\|^2 + C \sumin \max(1-\yi (\thetab^\top \phi(\xi) + \theta_0), 0),$$
		%  
		i.e., the regularized ERM problem for the hinge loss for the hypothesis space
%		
		$$  \Hspace = \{  f:\Phi \to \R ~|~ f(\mathbf{z}) = \thetab^\top \mathbf{z} + \theta_0 \quad \thetab \in \R^d,\theta_0\in\R  \}. $$
		%
		\item 	Now  assume we deal with a multiclass classification problem with a data set $\D = \Dset$ such that $\yi \in \Yspace=\{1,\ldots,g\}$ for each $i \in \nset.$ In this case, we can derive a similar regularized ERM problem by using the multiclass hinge loss (see Exercise Sheet 4 (b)):
		%  
		$$ \risket = \frac{1}{2} \|\thetab\|^2 + C \sumin \sum_{y\neq \yi} \max(1 + \thetab^\top\psi(\xi,y) - \thetab^\top\psi(\xi,\yi)    , 0),$$
		%
		where $\psi: \Xspace \times \Yspace \to \R^d$ is suitable (multiclass) feature map.	
%		
		Specify a $\psi$ such that this regularized multiclass ERM problem coincides with the regularized binary ERM problem in (a).
		%	
	 	\item Show that the regularized multiclass ERM problem in (b) can be written in the following kernelized form:
%	 	
		$$		\frac12 \betab^\top  \bm{K} \betab + C  \sumin \sum_{y\neq \yi} \max\left(1 + (\bm{K}\betab)_{(i-1)g+y} - (\bm{K}\betab)_{(i-1)g+\yi} ) ~ , ~ 0\right),		$$
%		
		where $\betab \in \R^{ng}$ and $\bm{K} = \Xmat \Xmat^\top$ for
%		
		$\Xmat \in \R^{ng \times d}$ with row entries $\psi(\xi,y)^\top$ for $i=1,\ldots,n,$ $y=1,\ldots,g,$ i.e.,
%		
		$$	\Xmat = \begin{pmatrix}
			\psi(\xv^{(1)},1)^\top \\
			\psi(\xv^{(1)},2)^\top \\
			\vdots \\
			\psi(\xv^{(1)},g)^\top \\
			\psi(\xv^{(2)},1)^\top \\
			\vdots \\
			\psi(\xv^{(n)},g)^\top
		\end{pmatrix}.	$$
		%	
		Here, $(\bm{K}\betab)_{(i-1)g+y}$ denotes the  $\big((i-1)g+y\big)$-th entry of the vector $\bm{K}\betab.$
		
		\emph{Hint:} The representer theorem tells us that for the solution $\thetab^*$ (if it exists) of $\risket$ it holds that  $\thetab^* \in \spn\{ (\psi(\xv^{(i)},y))_{i=1,\ldots,n,y=1,\ldots,g} \}.$
\end{enumerate}