
	\renewcommand{\fxt}{f(\xv ~|~ \theta_0,\thetab)}
	%
	For the data set $\D = \Dset$ with $\yi \in \R,$ assume that for a fixed $\eps>0$ all observations are within the $\eps$-tube around $\fxt = \thetab^\top \xv + \theta_0$ for any $(\theta_0,\thetab)^\top \in \tilde \Theta$, i.e.,
	%
	$$   \yi \in \left[f(\xi ~|~ \theta_0,\thetab)-\eps,~ f(\xi ~|~ \theta_0,\thetab)+\eps\right], \quad \forall i\in\{1,\ldots,n\},~ \forall (\theta_0,\thetab)^\top \in \tilde \Theta, $$
	%
	where $\tilde \Theta \subset \R^{p+1}$ is some non-empty parameter subset.
	%	
	Let 
	%	
	$$
	d_\eps \left(f(\cdot~|~\theta_0,\thetab), \xi \right) := \eps - |\yi - f(\xi ~|~ \theta_0,\thetab)| =   \eps - | \yi - \thetab^\top \xi - \theta_0|
	$$
	%	
	be the (signed) $\eps$-distance of the prediction error.
	%	
	The maximal $\eps$-distance of the prediction error of $f$ to the whole data set $\D$
	is  
	$$
	\gamma_\eps = \max\limits_{i=1,\ldots,n} \Big\{ d_\eps \left(f(\cdot~|~\theta_0,\thetab), \xi \right) \Big\}.
	$$
	%
	\begin{enumerate}
		%
		\item Let $(\theta_0,\thetab)^\top \in \R^{p+1}$ be arbitrary. Which (type of) values does $d_\eps \left(f(\cdot~|~\theta_0,\thetab),\xi \right)$ have if $\yi$ is 
		%  
		\begin{itemize}
		%  	
		 	\item within the $\eps$-tube around $f(\xi ~|~ \theta_0,\thetab)?$
		%  	
		 	\item not within the $\eps$-tube around $f(\xi ~|~ \theta_0,\thetab)?$
		%
		\end{itemize} 
		%
		What would be a desirable choice of the parameters $(\theta_0,\thetab)^\top$ with respect to $\gamma_\eps?$
%		
		Is the choice of the parameters unique in general?
		%
		\item Argue that 
		%  
		\begin{eqnarray*}
%			
		& \min\limits_{\thetab, \theta_0} \quad & \frac{1}{2} \|\thetab\|^2 \\
%		
		& \text{s.t.} & \,\, \eps -\yi + \thetab^\top\xi + \theta_0  \geq 0 \quad \forall\, i \in \nset \\
%		
		& \text{and} & \,\, \eps  + \yi - \thetab^\top\xi - \theta_0  \geq 0 \quad \forall\, i \in \nset.
%		
		\end{eqnarray*}
		%
		is a suitable optimization problem for the desired choice in (a).
		%	
		 	\item Derive the Lagrange function $L(\thetab, \theta_0, \alphav)$ of the optimization problem as well as its dual form.
		%
		 	\item Find the stationary points of $L.$ What can be inferred from the solution of the dual problem?
		%
		\item Derive the ``softened'' version of the optimization problem in (b).
%		
		\item Rewrite the ``softened'' version of the optimization problem in (b) as a regularized empirical risk minimization problem for a suitable loss function for regression.
		%	
\end{enumerate}