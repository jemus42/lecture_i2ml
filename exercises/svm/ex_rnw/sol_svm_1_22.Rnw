
	\renewcommand{\fxt}{f(\xv ~|~ \theta_0,\thetab)}
	%
	\begin{enumerate}
		%
		\item Regarding the values of $d_\eps \left(f(\cdot~|~\theta_0,\thetab),\xi \right)$ for an outcome $\yi$ we have that:
		%  
		\begin{itemize}
		%  	
		 	\item  If $\yi$ is within the $\eps$-tube around $f(\xi ~|~ \theta_0,\thetab),$ then $d_\eps \left(f(\cdot~|~\theta_0,\thetab),\xi \right)\geq 0.$ The largest possible value of $d_\eps \left(f(\cdot~|~\theta_0,\thetab),\xi \right)$ is $\eps,$ which corresponds to a perfect prediction for that point, i.e., $\yi = f(\xi ~|~ \theta_0,\thetab).$
		%  	
		 	\item If $\yi$ is not within the $\eps$-tube around $f(\xi ~|~ \theta_0,\thetab),$ then $d_\eps \left(f(\cdot~|~\theta_0,\thetab),\xi \right)<0.$ 
		%
		\end{itemize} 
		%
		A desirable choice of the parameters $(\theta_0,\thetab)^\top$ with respect to $\gamma_\eps$ would be such that $\gamma_\eps$ is maximized, as this would make sure that the prediction errors are as far away as possible from the $\eps$-boundaries, but still within the $\eps$-tube.
		
				
		\begin{minipage}{0.45\textwidth}
%			
		The choice of the parameters $(\theta_0,\thetab)^\top$ is not unique, as the plot on the right shows for $\eps=0.5.$
%			
		Both the black and the green model have $\gamma_\eps = \eps,$  since $d_\eps \left(f(\cdot~|~\theta_0,\thetab), -0.2 \right) = \eps = {\color{green} d_\eps \left(f(\cdot~|~\theta_0,\thetab), 0 \right)}$ 
		 and we cannot find another model such that its $\eps$-tube covers the outcomes.
%
		\end{minipage}
%	
		\begin{minipage}{0.45\textwidth}
%			
			\centering
			\includegraphics[width=0.5\linewidth]{figure/eps_tubes}
%			
		\end{minipage}

		 
		%
		\item   We formulate the desired property of a maximal $\gamma_\eps$ as an optimization problem:
%		
		\begin{eqnarray*}
%		
			& \max\limits_{\thetab, \theta_0} & \gamma_\eps \\
%		
			& \text{s.t.} & \,\, d_\eps \left(f(\cdot~|~\theta_0,\thetab), \xi \right) \geq 0 \quad \forall\, i \in \nset.
%		
		\end{eqnarray*}
%		
		The constraints mean that we require that any instance $i$ should have a positive $\eps$-distance of the prediction error for $f(\xi ~|~ \theta_0,\thetab).$		
%		
		In other words, the differences between the predictions and the outcomes should be at most $\eps$ and within the $\eps$-tube of the predictions.
%		
		The latter optimization problem can be rewritten as
		%		
		\begin{eqnarray*}
			%		
			& \max\limits_{\thetab, \theta_0} & \gamma_\eps \\
			%		
			& \text{s.t.} & \,\, \eps - | \yi - \thetab^\top \xi - \theta_0| \geq 0  \quad \forall\, i \in \nset.
			%		
		\end{eqnarray*}
%	
		And further to  
%		
		\begin{eqnarray*}
			%		
			& \max\limits_{\thetab, \theta_0} & \gamma_\eps \\
			%		
			%		
			& \text{s.t.} & \,\, \eps -\yi + \thetab^\top\xi + \theta_0  \geq 0 \quad \forall\, i \in \nset \\
			%		
			& \text{and} & \,\, \eps  + \yi - \thetab^\top\xi - \theta_0  \geq 0 \quad \forall\, i \in \nset.
			%		
			%		
		\end{eqnarray*}
%		
		As we have seen before the solution might not be unique, so that we make the reference choice $\gamma_\eps = C/\|\thetab\|$ for some constant $C>0,$  leading to
		%  
		\begin{eqnarray*}
%			
		& \min\limits_{\thetab, \theta_0} \quad & C \|\thetab\|^2 \\
%		
		& \text{s.t.} & \,\, \eps -\yi + \thetab^\top\xi + \theta_0  \geq 0 \quad \forall\, i \in \nset \\
%		
		& \text{and} & \,\, \eps  + \yi - \thetab^\top\xi - \theta_0  \geq 0 \quad \forall\, i \in \nset.
%		
		\end{eqnarray*}
%	
		For sake of convenience, we set the constant to $\frac{1}{2}.$
		%
		%	
		 	\item The Lagrange function of the SVM optimization problem is
		 	
		 	\vspace*{-.5cm}
		 	
		 	\small
		 	\begin{eqnarray*}
		 		&L(\thetab, \theta_0, \alphav,\tilde \alphav) = & \frac{1}{2}\|\thetab\|^2  -  \sum_{i=1}^n \alpha_i \left[ \eps - \yi + \left( \thetab^\top\xi + \theta_0  \right) \right] -  \sum_{i=1}^n \tilde\alpha_i \left[\eps -  \left( \thetab^\top\xi + \theta_0  \right) + \yi \right]\\
		 		& \text{s.t.} & \,\, \alpha_i,\tilde \alpha_i \ge 0 \quad \forall\, i \in \nset.
		 	\end{eqnarray*}
		 	\normalsize
		 	The \textbf{dual} form of this problem is
		 	$$\max\limits_{\alphav,\tilde\alphav} \min\limits_{\thetab, \theta_0}  L(\thetab, \theta_0,\alphav,\tilde \alphav).$$
		 	
		%
		 	\item The stationary points of $L$ can be derived by setting the derivative of the Lagrangian function to 0 and solve with respect to the corresponding term of interest, i.e., for $\thetab:$
%		 	
			\begin{align*}
%				
				\nabla_{\thetab} L(\thetab, \theta_0, \alphav,\tilde \alphav)   = \thetab - \sum_{i=1}^n \alpha_i \xi  +  \sum_{i=1}^n \tilde\alpha_i &\xi \stackrel{!}{=} 0 \\
%				
				&\Leftrightarrow \thetab = \sum_{i=1}^n (\alpha_i - \tilde \alpha_i) \xi.
%				
			\end{align*}
%		
			and for $\theta_0:$
%		
			\begin{align*}
				%				
				\nabla_{\theta_0} L(\thetab, \theta_0, \alphav,\tilde \alphav)   =  - \sum_{i=1}^n \alpha_i  +  \sum_{i=1}^n \tilde\alpha_i & \stackrel{!}{=} 0 \\
				%				
				&\Leftrightarrow 0 = \sum_{i=1}^n (\alpha_i - \tilde \alpha_i) .
				%				
			\end{align*}
%		
		If $(\thetab, \theta_0, \alphav, \tilde{\alphav})$ fulfills the KKT conditions (stationarity, primal/dual feasibility, complementary slackness), it solves both the primal and dual problem (strong duality). 
%		
		Under these conditions, and if we solve the dual problem and obtain $\alphavh$ or $\widetilde{\alphavh}$,  we know that $\thetab$ is a linear combination of our data points:
%		
		$$
		\thetah = \sumin ( \alphah_i - \widetilde{\alphah_i})  \xi 
		$$
%		
		Complementary slackness means:
%		
		\begin{align*}
%			
			&\alphah_i \left[ \eps - \yi + \left( \thetab^\top\xi + \theta_0  \right) \right] = 0 \quad \forall ~ i \in \{1, ..., n \}, \\
%			
			& \widetilde{\alphah_i} \left[\eps -  \left( \thetab^\top\xi + \theta_0  \right) + \yi \right] = 0 \quad \forall ~ i \in \{1, ..., n \}.
%			
		\end{align*}
		So either $\alphah_i = 0$, 	or $\alphah_i > 0$, then $\eps = \yi - \left( \thetab^\top\xi + \theta_0  \right) $, and $(\xi, \yi)$ is exactly on the boundary of the $\eps$-tube of the prediction and $ \thetab^\top\xi + \theta_0 $ underestimates $\yi$ (by exactly $\eps$).
%		
		Similarly, it holds either $\widetilde{\alphah_i} = 0$, 	or $\widetilde{\alphah_i} > 0$, then $\eps =  \left( \thetab^\top\xi + \theta_0  \right) - \yi $, and $(\xi, \yi)$ is exactly on the boundary of the $\eps$-tube of the prediction and $ \thetab^\top\xi + \theta_0 $ overestimates $\yi$ (by exactly $\eps$).
%		
		For the bias term $\theta_0$ we infer that
%		
		$$  \theta_0 =   \yi -  \thetab^\top\xi  -\eps$$
		%
		in the case $\alphah_i > 0$ and 
		%		
		$$  \theta_0 =   \yi -  \thetab^\top\xi  + \eps$$
		%
		in the case $\widetilde{\alphah_i} > 0.$
%		
		\item The ``softened'' version of the optimization problem is obtained by introducing slack variables $\sli,\widetilde{\sli} \geq 0$ in the constraints:
%		
		\begin{eqnarray*}
			%			
			%		
			& \,\, \eps -\yi + \thetab^\top\xi + \theta_0  \geq \sli \quad \forall\, i \in \nset \\
			%		
			& \text{and} \,\, \eps  + \yi - \thetab^\top\xi - \theta_0  \geq \widetilde{\sli} \quad \forall\, i \in \nset.
			%		
		\end{eqnarray*}
%		
		We minimize then a weighted sum of $\|\thetab\|^2$ and the sum of the slack variables:
		
		\begin{eqnarray*}
			%			
			& \min\limits_{\thetab, \theta_0} \quad & \frac{1}{2} \|\thetab\|^2 + C \sum_{i=1}^n \sli + \widetilde{\sli} \\
			%		
			& \text{s.t.} & \,\, \eps -\yi + \thetab^\top\xi + \theta_0  \geq - \sli\quad \forall\, i \in \nset \\
			%		
			& \text{and} & \,\, \eps  + \yi - \thetab^\top\xi - \theta_0  \geq - \widetilde{\sli} \quad \forall\, i \in \nset, \\
%			
			& &\sli,\widetilde{\sli} \geq 0  \quad \forall\, i \in \nset.
			%		
		\end{eqnarray*}
%
		\item   In the optimum, the inequalities will hold with equality (as we minimize the slacks), so $\sli = \eps -\yi + \thetab^\top\xi + \theta_0$ and $ \widetilde{\sli} = \eps  + \yi - \thetab^\top\xi - \theta_0$, but the lowest value $\sli$ and $\widetilde{\sli}$ can take is 0.
%		
		So we can rewrite the above: 
%		
		\begin{align*} 
			\frac{1}{2} \|\thetab\|^2 + C \sumin \Lxyi ;\; \Lxy = 
			\begin{cases} 
				0,       & \text{ if } |y - \fx| \leq \eps, \\
				|y - \fx| -\eps, & \text{ else. }  
			\end{cases}
		\end{align*} 
%	
		 This loss function is the $\eps$-insensitive loss.
		%	
\end{enumerate}