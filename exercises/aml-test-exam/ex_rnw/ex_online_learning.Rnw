\renewcommand{\l}{L}
\newcommand{\Algo}{\texttt{Algo}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\Zspace}{\mathcal{Z}}
\newcommand{\norm}[1]{\left|\left|#1\right|\right|_2}


Consider an online learning problem specified by an action space $\A,$ environmental data space $\Zspace$ and a loss function $\l:\A\times\Zspace\to \R.$
%
%Furthermore, let $\psi(a):\A \to \R$ be some regularization function.
%
The action selection of the one-step lookahead cheater version of the FTL algorithm (we refer to it as \texttt{FTLC}) is
%
$$	a_{t}^{\texttt{FTLC}} = \argmin_{a\in \A}  \sum_{s=1}^{t} \l(a,z_s),	$$
%
while FTL chooses 
%
$$	a_{t}^{\texttt{FTL}} = \argmin_{a\in \A}  \sum_{s=1}^{t-1} \l(a,z_s).	$$
%
\begin{enumerate}
	%	
	\item Show that $\l(a_{t}^{\texttt{FTLC}},z_t)  \leq \l(a_{t}^{\texttt{FTL}},z_t) $ holds for any time step $t$.
	%
	
	\emph{Hint:} Define the function $\l_t(a) =  \sum_{s=1}^{t} \l(a,z_s).$ 
	%	
	\item Now assume that the loss function is the linear loss function and $\A = \Zspace = \R^d$. Derive a closed-form for the action at time step $t+1$ of the Follow the	regularized leader (FTRL) algorithm used with squared L2-norm regularization.

	%	
	%
\end{enumerate}