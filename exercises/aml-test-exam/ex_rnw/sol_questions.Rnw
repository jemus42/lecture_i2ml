
Which of the following statements are correct and which are not? In each case, give also a brief explanation (1--2 sentences).
%
\begin{enumerate}
	%	
	\item Removing a sensitive attribute/feature at prediction (or decision) time might not ensure that fair decisions will be made.
	
	\item[] {\color{blue} \textbf{Solution:} True, as there might be a (strong) correlation between a non-sensitive and the sensitive feature, which the underlying machine learning model might exploit to make essentially the same predictions/decisions.}
	
	%
	\item Separation is a posthoc (fairness) criterion.
	
	\item[] {\color{blue} \textbf{Solution:} True, as we can only check whether the FNR and FPR are the same for the sensitive features after we have made the decisions/predictions.}
	
	
	%
	\item MetaCost can only be used for probabilistic classifiers.
	
	\item[] {\color{blue} \textbf{Solution:} False, as MetaCost can be used with hard-label classifiers as well, since it internally transforms the hard-labels into probabilities over the labels via a one-hot encoding.}
	
	
	
	%
	\item  The appealing property of SMOTE is that it takes the entire data space $\Xspace \times \Yspace$ into account for creating a new synthetic example of a minority class.
	
	\item[] {\color{blue} \textbf{Solution:} False, SMOTE takes only the feature space $\Xspace$ (and the part of $\Yspace$ belonging to the minority classes) into account.}
	
	
%	
	\item  The maximum-likelihood estimate for the mean of random observations following a multivariate Gaussian distribution is always optimal in terms of the mean-squared error.
	
	\item[] {\color{blue} \textbf{Solution:} False, as the James-Stein estimator has a smaller MSE than the MLE in such a case if the dimension of the Gaussian is larger 2.}
	
	
%	
	\item The key difference between batch and online learning is that in the former the training and testing phase are decoupled, while in the latter the two phases are intertwined.
	
		
	\item[] {\color{blue} \textbf{Solution:} True, in batch learning one first trains a machine learning model on a given batch of data and then applies (tests) this model on new data, while in online learning training and testing goes hand in hand, since data is obtained incrementally.}
	
%	
	\item The regret of the FTL algorithm is linearly growing with respect to the time horizon $T$ on every online linear optimization problem. 
	
	\item[] {\color{blue} \textbf{Solution:} False, as the FTL algorithm can also have sublinear regret on online linear optimization problems, e.g., on data sequences which are stochastic and stationary.}
	
	
	%
\end{enumerate}