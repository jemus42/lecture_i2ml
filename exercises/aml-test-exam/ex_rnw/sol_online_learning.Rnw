\renewcommand{\l}{L}
\newcommand{\Algo}{\texttt{Algo}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\Zspace}{\mathcal{Z}}
\newcommand{\norm}[1]{\left|\left|#1\right|\right|_2}


Consider an online learning problem specified by an action space $\A,$ environmental data space $\Zspace$ and a loss function $\l:\A\times\Zspace\to \R.$
%
%Furthermore, let $\psi(a):\A \to \R$ be some regularization function.
%
The action selection of the one-step lookahead cheater version of the FTL algorithm (we refer to it as \texttt{FTLC}) is
%
$$	a_{t}^{\texttt{FTLC}} = \argmin_{a\in \A}  \sum_{s=1}^{t} \l(a,z_s),	$$
%
while FTL chooses 
%
$$	a_{t}^{\texttt{FTL}} = \argmin_{a\in \A}  \sum_{s=1}^{t-1} \l(a,z_s).	$$
%
\begin{enumerate}
	%	
	\item Show that $\l(a_{t}^{\texttt{FTLC}},z_t)  \leq \l(a_{t}^{\texttt{FTL}},z_t) $ holds for any time step $t$.
	%
	
	\emph{Hint:} Define the function $\l_t(a) =  \sum_{s=1}^{t} \l(a,z_s).$ 
	
	
 	\item[]	{\color{blue} \textbf{Solution:}   
		%	
		Note that for any $a \in \A$
%		
		\begin{itemize}
%			
			\item [(i)] $\l(a,z_t) = \l_t(a) - \l_{t-1}(a);$
%			
			\item [(ii)] $\l_t(a_{t}^{\texttt{FTLC}}) \leq \l_t(a);$
%			
			\item [(iii)] $\l_{t-1}(a_{t}^{\texttt{FTL}}) \leq \l_{t-1}(a);$
%			
		\end{itemize}
		%	
		With this,
%			
		\begin{align*}
			%			
			\l(a_{t}^{\texttt{FTLC}},z_t) 
			%			
			&= \l_t(a_{t}^{\texttt{FTLC}}) - \l_{t-1}(a_{t}^{\texttt{FTLC}}) \tag{(i)}\\
			%			
			&\leq \l_t(a_{t}^{\texttt{FTL}}) - \l_{t-1}(a_{t}^{\texttt{FTL}})  \tag{(ii) and (iii)}\\
			%			
			&= \l(a_{t}^{\texttt{FTL}},z_t). \tag{(i)}
			%			
		\end{align*}
		%		
	} 
	
	%	
	\item Now assume that the loss function is the linear loss function and $\A = \Zspace = \R^d$. Derive a closed-form for the action at time step $t+1$ of the Follow the	regularized leader (FTRL) algorithm used with squared L2-norm regularization.

 	\item[]	{\color{blue} \textbf{Solution:}   
		%	
		Define $f(a) = \frac{1}{2 \eta}\norm{a}^2 + a^\top \sum_{s=1}^t z_s$ and note that 
%		
		$$	a_{t+1}^{\texttt{FTRL}}	= \argmin_{a\in \A} f(a)	$$
%		
		in this case.
		%	
		Thus, we find the minimum of $f$ in the usual way (setting the gradient to zero, solving w.r.t\ to $a$ and checking psd of the Hessian):
		%			
		\begin{align*}
			%			
			\nabla_a f(a)
			%			
			&=  \frac{a}{\eta} + \sum_{s=1}^t z_s = 0 \\
			%			
			&\quad \Leftrightarrow  a = -\eta \sum_{s=1}^t z_s.
			%			
		\end{align*}
		%		
		\begin{align*}
			%			
			\nabla_a \nabla_a^\top f(a)
			%			
			&=  \frac{1}{\eta} \id_{d \times d}.
			%			
		\end{align*}
%	
	The Hessian is psd, as for any $\xv \in \R^d$ it holds that $ \xv^\top  \left( \frac{1}{\eta} \id_{d \times d} \right) \xv = \frac{1}{\eta} \norm{\xv}^2 \geq 0. $
	} 
	%	
	%
\end{enumerate}