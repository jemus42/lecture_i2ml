
\begin{enumerate}
\item 
\begin{itemize}
\item The loss is calculated by the negative log-likelihood by: $L(y,f) = -\ell(f) = -(const - (\log_2(y) - f)^2 / 2)$ (1P)
\item The pseudo residuals are then calculated by: $\tilde r(f) = - \partial L(y,f) / \partial f = (\log_2(y) - f)$ (1P)
\end{itemize}
\item Use $\tilde{y} = \log_2(y) = (0,1,2)$ (1P)
\begin{itemize}
\item[(i)] $\hat{f}^{[0]}(\bm{x}) = \bar{\tilde{y}} = 1$ as this is the optimal constant model for squared error. (1P)
\item[(ii)] $\tilde{r}^{[1]} = \log_2(y) - \hat{f}^{[0]}(\bm{x}) = (-1,0,1)$ (1P)
\item[(iii)] $R_t^{[1]}, t=1,2$ will split using $\bm{x}_1$, as $\bm{x}_2$ carries no information. Since $x^{(1)}_{1}=x^{(2)}_{1}$, 
$$R_1 = -0.5 I(x_1 \geq 0.5)$$ and $$R_2 = 1 I(x_1 \leq 0.5).$$ (2P) 
\item[(iv)] $\hat{f}^{[1]}(\bm{x}) = \hat{f}^{[0]}(\bm{x}) + 1(-0.5, -0.5, 1) = (0.5,0.5,2)$ (1P)
\item[(v)] $\tilde{r}^{[2]} = \log_2(y) - \hat{f}^{[1]}(\bm{x}) = (-0.5,0.5,0)$ (1P)
\end{itemize}
\item Nothing, because there is no information that can be used to further improve the model. (1P)
\item 
\begin{itemize}
\item[(i)] $M$ grows: capacity will increase and the algorithm may eventually overfit (1P)
\item[(ii)] $n$ grows: capacity will stay the same and the algorithm may underfit (1P)
\end{itemize}
\end{enumerate}


