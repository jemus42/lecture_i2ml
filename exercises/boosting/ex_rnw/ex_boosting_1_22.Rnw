
Let $\fxh = \sum_{m=1}^{M} \betamh \blh(\xv)$ be the scoring function after running AdaBoost for $M\in \N$ iterations. Show that the average empirical risk (on $\Dtrain$) of the corresponding classifier $\hat{h}(\xv) = \sign(\fxh)$ is bounded as follows
%
\begin{align}\label{adaboost_emp_risk_bound}
%	
	\frac{\riske(\hat{h})}{n} = \frac{  \sum_{i=1}^n  \mathds{1}_{[\hat{h}(\xi) \neq \yi ]} }{n} \leq \prod_{m=1}^M \sqrt{1- 4 (\gamma^{[m]})^2},
%	
\end{align}
%
where $\gamma^{[m]} = \frac12 - \errm.$ For this purpose, proceed as follows:
%    
\begin{enumerate}
%	
  \item Give an interpretation of $\gamma^{[m]}.$
%  
  \item For any $m=1,\ldots,M$ let $W^{[m]} = \sum_{i=1}^n w^{[m](i)}$ be the total weigh in iteration \textbf{before} normalizing the weights. Show that $W^{[m]} = \sqrt{1- 4 (\gamma^{[m]})^2}.$
%  
  \item Show that 
%  
	\begin{align*}
%		
		w^{[M+1](i)} = \frac{w^{[1](i)}  \exp(-\yi \fxih)  }{\prod_{m=1}^M W^{[m]}},
%		
	\end{align*}
%  
	where $w^{[M+1](i)}$ is the \textbf{normalized} weight if we would run AdaBoost for $M+1$ iterations.
%	
	\item Argue that $ \mathds{1}_{[\hat{h}(\xv) \neq y ]} \leq \exp(-y \fxh )$ for any $(\xv,y) \in \Xspace \times \Yspace.$
%
	\item Combine everything to conclude \eqref{adaboost_emp_risk_bound}.
%	
\end{enumerate}

\emph{Hint:} Since for any $x$ it holds that $1+x \leq \exp(x),$ we can infer from \eqref{adaboost_emp_risk_bound} that
%
\begin{align*}
	%		
	\frac{\riske(\hat{h})}{n} \leq \exp\left(	-2 \sum_{m=1}^M (\gamma^{[m]})^2		\right) =  \exp\left(	-2 \sum_{m=1}^M (\frac12 - \errm)^2		\right),
	%		
\end{align*}
%
i.e., the average empirical risk is decreasing exponentially in the number of used iterations (provided $\errm >1/2$).
