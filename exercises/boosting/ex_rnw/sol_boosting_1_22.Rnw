
Write $\tilde w^{[m](i)}$ for the unnormalized weight and  $w^{[m](i)}$ for the normalized weight of instance $i=1,\ldots,n$ in iteration step $m=1,\ldots,M.$
%
Thus, 
%
\begin{align} \label{unnorm_weights}
%	
	\tilde w^{[m](i)} = w^{[m](i)} \cdot \exp\left(  - \betam \yi \blh(\xi) \right)
%	
\end{align}
%
and
%
\begin{align} \label{norm_weights}
%	
	 w^{[m+1](i)} = \frac{\tilde w^{[m](i)}}{\sum_{i=1}^n \tilde w^{[m](i)}} = \frac{w^{[m](i)}  \cdot \exp\left(  - \betam \yi \blh(\xi) \right)}{\sum_{i=1}^n w^{[m](i)}  \cdot \exp\left(  - \betam \yi \blh(\xi) \right)}.
%	
\end{align}
%    
\begin{enumerate}
%	
  \item  Recall that      
%  
   $$
  \errm = \sumin \wmi \cdot \mathds{1}_{\{\yi \,\neq\, \blh(\xi)\}}
  $$
%  
	is the weighted error of $\blh.$ Random guessing has an error of approximately\footnote{If the data set is balanced.} $\frac12,$ so that  $\gamma^{[m]}= \frac12 - \errm$ tells us how much better $\blh$ (in terms of the error) is compared to random guessing.
%  
  \item By means of \eqref{unnorm_weights} it holds that $W^{[m]} = \sum_{i=1}^m \tilde w^{[m](i)}$ is the total weigh in iteration $m$ \textbf{before} normalizing the weights for any $m=1,\ldots,M$.
%  
	With this,
%	
	\begin{align*}
%		
		W^{[m]} 
%		
		&= \sum_{i=1}^m \tilde w^{[m](i)} \\
%		
		&= \sum_{i=1}^m   w^{[m](i)} \cdot \exp\left(  - \betam \yi \blh(\xi) \right) \tag{Using \eqref{unnorm_weights}} \\
%		
		&= \sum_{i : \yi \neq \blh(\xi) }  w^{[m](i)} \cdot \exp\left(  - \betam \underbrace{\yi \blh(\xi)}_{=-1} \right)  + \sum_{i : \yi = \blh(\xi) }  w^{[m](i)} \cdot \exp\left(  - \betam \underbrace{\yi \blh(\xi)}_{=1} \right) \\
%		
		&= \sum_{i : \yi \neq \blh(\xi) }  w^{[m](i)} \cdot \exp\left(   \betam \right)  + \sum_{i : \yi = \blh(\xi) }  w^{[m](i)} \cdot \exp\left(  - \betam \right) \\
%		
		&= \exp\left(   \betam \right) \underbrace{\sum_{i : \yi \neq \blh(\xi) }  w^{[m](i)} }_{=\errm}   + \exp\left(  - \betam \right) \underbrace{\sum_{i : \yi = \blh(\xi) }  w^{[m](i)} }_{=(1-\errm)} \\
%		
		&= \exp\left(   \betam \right) \errm   + \exp\left(  - \betam \right) (1-\errm).
%		
	\end{align*} 
%  
	Recall that $\betam =  \frac{1}{2} \log \left( \frac{1 - \errm}{\errm}\right),$ so that
%	
	$$	 \exp\left(   \betam \right) = \sqrt{\frac{1-\errm}{\errm}}, \qquad \mbox{and} \qquad  \exp\left(  - \betam \right) = \sqrt{\frac{\errm}{1-\errm}}.		$$
%  
	Using this for our representation of $W^{[m]}$ we obtain
%	
		\begin{align*}
		%		
		W^{[m]} 
		%				
		&= \exp\left(   \betam \right) \errm   + \exp\left(  - \betam \right) (1-\errm) \\
%		
		&= 2 \sqrt{ (1-\errm)\errm } \\
%		
		&= 2 \sqrt{ \left(\frac12 + \gamma^{[m]}\right) \left(\frac12 - \gamma^{[m]}\right) } \\
%		
		&= 2 \sqrt{ 1/4 - (\gamma^{[m]})^2  }\\
%		
		&=  \sqrt{ 1- 4(\gamma^{[m]})^2  }.
		%		
	\end{align*} 

	As a side note: $ \betam$ is chosen such that $\exp\left(   \betam \right) \errm   + \exp\left(  - \betam \right) (1-\errm)$ is minimal. This is due to (we will see this below): 
%	
	$$\frac{\riske(\hat{h})}{n} \leq \prod_{m=1}^M W^{[m]} = \prod_{m=1}^M \exp\left(   \betam \right) \errm   + \exp\left(  - \betam \right) (1-\errm).$$

%
  \item Using \eqref{norm_weights} repeatedly, we obtain
%  
	\begin{align*}
%		
		w^{[M+1](i)} 
%		
		&=  w^{[M](i)} \cdot\frac{   \exp\left(  - \betam[M] \yi \blh[M](\xi) \right)}{\sum_{i=1}^n w^{[M](i)}  \cdot \exp\left(  - \betam[M] \yi \blh[M](\xi) \right)}  \tag{Using \eqref{norm_weights}}\\
%		
		&=  w^{[M](i)} \cdot\frac{   \exp\left(  - \betam[M] \yi \blh[M](\xi) \right)}{ W^{[M]} } \tag{Definition of $W^{[M]}$}\\
%		
		&=  w^{[M-1](i)}   \cdot \frac{\exp\left(  - \betam[M-1] \yi \blh[M-1](\xi) \right)}{ W^{[M-1]} }   \cdot \frac{\exp\left(  - \betam[M] \yi \blh[M](\xi) \right)}{ W^{[M]} }  \tag{Using \eqref{norm_weights} again}\\
%		
		&=  w^{[1](i)}   \cdot \frac{ \prod_{m=1}^M \exp\left(  - \betam[m] \yi \blh[m](\xi) \right)}{\prod_{m=1}^M W^{[m]}}  \tag{Using \eqref{norm_weights} again and again }\\
%		
		&=  w^{[1](i)}   \cdot \frac{  \exp\left(  - \yi  \sum_{m=1}^M \betam[m]  \blh[m](\xi)   \right)}{\prod_{m=1}^M W^{[m]}}   \\
%		
		&= \frac{w^{[1](i)}  \exp(-\yi \fxih)  }{\prod_{m=1}^M W^{[m]}}. \tag{Since $\sum_{m=1}^M \betam[m]  \blh[m](\xi) =\fxih$}
%		
	\end{align*}
%  
%	
	\item For any $(\xv,y) \in \Xspace \times \Yspace$ it holds that 
%	
	\begin{align*}
%		
			\hat{h}(\xv) \neq y \quad  &\Leftrightarrow \quad \sign(\fxh) \neq y  \\
%			
			&\Leftrightarrow \quad \fxh y < 0 \\
%			
			&\Leftrightarrow \quad - \fxh y > 0 \\
%			
			&\Leftrightarrow \quad \exp(- \fxh y) > \exp(0) =  1 =  \mathds{1}_{[\hat{h}(\xv) \neq y ]}.
%		
	\end{align*}
%
	\item We show the desired result by using (b), (c) and (d):
%	
	\begin{align*} 
		%	
		\frac{\riske(\hat{h})}{n} 
%		
		&= \frac{  \sum_{i=1}^n  \mathds{1}_{[\hat{h}(\xi) \neq \yi ]} }{n} \\
%		
		&=  \sum_{i=1}^n  \frac{1}{n} \mathds{1}_{[\hat{h}(\xi) \neq \yi ]}  \\
%		
		&\leq   \sum_{i=1}^n  \frac{1}{n} \exp\left(-\yi \fxih\right) \tag{Using (d)}  \\
%		
		&=   \sum_{i=1}^n  w^{[1](i)} \exp\left(-\yi \fxih\right) \tag{Definition of $w^{[1]}$}  \\
%		
		&=   \sum_{i=1}^n  	w^{[M+1](i)} \prod_{m=1}^M W^{[m]}  \tag{Using (c)}  \\
%		
		&=   \prod_{m=1}^M W^{[m]} \underbrace{ \sum_{i=1}^n  	w^{[M+1](i)} }_{=1}   \\
%		
		&\leq \prod_{m=1}^M \sqrt{1- 4 (\gamma^{[m]})^2}. \tag{Using (b)}
		%	
	\end{align*}
%	
\end{enumerate}
%