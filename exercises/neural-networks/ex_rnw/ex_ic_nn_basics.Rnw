This week our research group is still busy with the task of deciding whether a patient should be admitted to hospital. However, based on the findings of the last weeks, it is questioned whether a linear classifier can actually adequately separate the patients into two groups.

\begin{enumerate}\bfseries
  \item[1)] Explain why neural networks (NNs) can be useful in such a situation and how they relate to the concept of representation learning.
\end{enumerate}

Now that we know why NNs can be useful, researcher Holger wonders how such an NN actually works. Researcher Stefanie states that an \textit{input layer}, \textit{neurons, hidden layers} and an \textit{output layer} are the central components of a NN.
\\
\textit{(Tip: A small sketch of an NN might be helpful for the next tasks)}

\begin{enumerate}\bfseries
  \item[2)] Let’s start with the input layer. Explain it’s role in the network and how it interacts with subsequent components.
\end{enumerate}

After the input layer, an arbitrary number of so-called hidden layers follows. Each of these layers consists of an arbitrary number of neurons.

\begin{enumerate}\bfseries
  \item[3)] Explain why these layers are considered to be “hidden”.
  \item[4)] Each neuron in a hidden layer performs a 2-step computation. Explain what these two steps are and where the so-called \textit{weights} come into play here.
\end{enumerate}

The last component of an NN is the so called output layer, which again consists of a number of neurons.

\begin{enumerate}\bfseries
  \item[5)]Explain how the number of neurons and the activation function, which is used in these/this neuron(s) of the output layer, relate to the respective ML problem. For example, consider how the assumed distribution of the target variable in a classification/regression problem affects the choice of function and the number of neurons.
\end{enumerate}
