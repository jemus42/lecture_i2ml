<<echo=FALSE>>=
library(ggplot2)
make_plot = function(dt, nudge_x = 0) {
  ggplot(dt, aes(x, y, col = class)) +
  geom_point() +
  geom_text(
    aes(label = label), nudge_y = 0.25, nudge_x = nudge_x, show.legend = FALSE
  ) +
  labs(x = expression(x[1]), y = expression(x[2])) +
  theme_bw()
}
@

\begin{enumerate}[a)]
  \item We need to find a linear hyperplane -- i.e.,  a line in 2D -- that 
  separates our classes.
  There are infinitely many such lines, but the sketch suggests
  that the line with intercept 0.5 and slope 1 has maximal "safety margin": it 
  has the broadest corridor in which we could move the line without incurring a
  misclassification error (indicated by dotted lines).
  Intuitively, this leads to better generalization than any line with a smaller 
  such margin (the mathematical reasoning is treated in support vector machines 
  $\rightsquigarrow$ supervised learning lecture).
  
  Now, we need to translate this into logistic regression coefficients.
  Recall that the linear hyperplane for binary logistic regression is defined 
  via 
  \begin{equation} \label{eqhypplane}
    \alpha_0 + \alpha_1 x_1 + \alpha_2 x_2 = - \log(\tfrac{1}{c} - 1) ~
    \Leftrightarrow  ~
    \alpha_0 + \alpha_1 x_1 + \alpha_2 x_2 = 0,
  \end{equation}
  where $c \in [0, 1]$ is the classification threshold. 
  
  <<echo=FALSE, fig.height=2, fig.width=3>>=
  x = c(0, 1, 0, -1, 0)
  y = c(0, 0, -1, 0, 1)
  data_1 = data.frame(
    x = x,
    y = y,
    class = as.factor(c(rep(0, 3), rep(1, 2))),
    label = sprintf("x%i", 1:5)
  )
  make_plot(data_1) +
    geom_abline(intercept = 0.5, slope = 1) +
    geom_abline(intercept = 0, slope = 1, linetype = 3) +
    geom_abline(intercept = 1, slope = 1, linetype = 3)
  @
  
  \begin{itemize}
    \item \textbf{Quick solution.} Logistic regression is not the focus of this 
    exercise, so TL;DR: pick values (not unique) for which points on the line 
    fulfill (\ref{eqhypplane}).
    For example: $\bm{\alpha}^\prime = (-0.5, -1, 1)^\top$.
    
    \item \textbf{Detailed solution.} (\ref{eqhypplane}) has 3 unknown 
    quantities -- using a system of linear equations, derived from points we 
    know our line must cross, 3 equations should suffice if there is a unique 
    solution.
    It turns out that the coefficients of the linear hyperplane in logistic 
    regression are \textit{not} unique: multiplying (\ref{eqhypplane}) with a 
    constant leaves the equality unchanged.
    In ERM, we can usually still find an optimal parameter vector by striking 
    the balance between misclassified observations (larger $\bm{\alpha}$ drives 
    the loss up more strongly) and correctly classified ones (larger 
    $\bm{\alpha}$ drives the loss down more strongly).
    
    However, in this special case of linear separability, where a linear 
    classifier produces no 
    misclassification, there is no such optimum (or rather, it is loss-optimal 
    to push the parameter vector to infinity).
    We will therefore, in this highly unrealistic and simplistic example, use 
    two points the line crosses (which is enough to define any line) and just 
    choose any values for $\bm{\alpha}$ that fulfill (\ref{eqhypplane}).
    
    Let's pick the points 
    $(0, 0.5)^\top$ and $(-0.5, 0)^\top$ (0's are always convenient):
    \begin{equation} \label{eqone}
      \alpha_0 + \alpha_1 \cdot 0 + \alpha_2 \cdot 0.5 = 0
    \end{equation}
     \begin{equation} \label{eqtwo}
      \alpha_0 - \alpha_1 \cdot 0.5 + \alpha_2 \cdot 0 = 0
    \end{equation}
    Computing (\ref{eqone})-(\ref{eqtwo}) yields
    \begin{equation} \label{eqthree}
      \alpha_1 \cdot 0.5 + \alpha_2 \cdot 0.5 = 0 ~ \Leftrightarrow ~ 
      \alpha_1 = - \alpha_2,
    \end{equation}
    so set, e.g., $\alpha_1^\prime = -1$ and $\alpha_2^\prime = 1$.
    Plugging this back into either \ref{eqone} or \ref{eqtwo} 
    yields $\alpha_0^\prime = -0.5$, such that 
    $\bm{\alpha}^\prime = (-0.5, -1, 1)^\top$ 
    (note that $\bm{\alpha}^{\prime\prime} = (-5, -10, 10)^\top$, and 
    any other multiple, would work just as well.)
  \end{itemize}
  
    \begin{tabular}{|r|r|r|}
    \hline
    $i$ & $\yi$ & $\sigma\left(\alpha_0^\prime + \alpha_1^\prime x_1^{(i)} + 
    \alpha_2^\prime x_2^{(i)}\right)$\\ \hline
    1 & 0 & 0.38 \\ \hline
    2 & 0 & 0.18 \\ \hline
    3 & 0 & 0.18 \\ \hline
    4 & 1 & 0.62 \\ \hline
    5 & 1 & 0.62 \\ \hline
  \end{tabular}
  
  \item By the same principle as above, we can derive a possible solution
  $\bm{\beta} = (-0.5, 1, -1)^\top$ (you can easily verify this with two 
  points).
  
    <<echo=FALSE, fig.height=2, fig.width=3>>=
  data_2 = data_1
  data_2$class = as.factor(c(0, rep(1, 2), rep(0, 2)))
  make_plot(data_2) +
    geom_abline(intercept = -0.5, slope = 1) +
    geom_abline(intercept = 0, slope = 1, linetype = 3) +
    geom_abline(intercept = -1, slope = 1, linetype = 3)
  @
  
      \begin{tabular}{|r|r|r|}
    \hline
    $i$ & $\yi$ & $\sigma\left(\beta_0^\prime + \beta_1^\prime x_1^{(i)} + 
    \beta_2^\prime x_2^{(i)}\right)$\\ \hline
    1 & 0 & 0.38 \\ \hline
    2 & 1 & 0.62 \\ \hline
    3 & 1 & 0.62 \\ \hline
    4 & 0 & 0.18 \\ \hline
    5 & 0 & 0.18 \\ \hline
  \end{tabular}

  \item \phantom{foo}
  \begin{enumerate}[i)]
    \item Intuitively, we can see that a condition like $z_1 + z_2 > 0$ suffices 
  to separate $\xi[1]$ from the other observations, which looks very much like 
  a linear hyperplane.
  
    \begin{tabular}{|r|r|r|r|}
    \hline
    $i$ & $z_1^{(i)}$ & $z_2^{(i)}$ & $\yi$ \\ \hline
    1 & 0 & 0 & 1 \\ \hline
    2 & 0 & 1 & 0 \\ \hline
    3 & 0 & 1 & 0 \\ \hline
    4 & 1 & 0 & 0 \\ \hline
    5 & 1 & 0 & 0 \\ \hline
  \end{tabular}
  
  Graphically, we can visualize our points in the new $z_1, z_2$ coordinates and
  see that this transformed dataset is, again, linearly separable:
  
  <<echo=FALSE, fig.height=2, fig.width=3>>=
  data_3 = data.frame(
    x = c(0, 0, 1),
    y = c(0, 1, 0),
    class = as.factor(c(1, 0, 0)),
    label = c("x1", "x2 = x3", "x4 = x5")
  )
  make_plot(data_3) +
    xlim(c(-0.5, 1.5)) +
    ylim(c(-0.5, 1.5)) +
    geom_abline(intercept = 0.5, slope = -1) +
    labs(x = expression(z[1]), y = expression(z[2]))
  @
  
  \item Looking at $z_1$, we find that it serves to separate 
  observations $\{1, 2, 3\}$ from $\{4, 5\}$.
  Likewise, $z_2$ separates $\{1, 4, 5\}$ from
  $\{2, 3\}$.
  Predicting $z_1$ thus corresponds to the first,
  and $z_2$ to the second exercise, so we can use 
  $\bm{\gamma} = \bm{\alpha}^\prime = (-0.5, -1, 1)^\top$ and
  $\bm{\phi} = \bm{\beta}^\prime = (-0.5, 1, -1)^\top$.
  
  \item We can simply stack the logistic regression components as follows:
  \begin{align*}
    z_1 &= \sigma\left(\alpha_0 + \alpha_1 x_1 + \alpha_2 x_2\right), \\
    z_2 &= \sigma\left(\beta_0 + \beta_1 x_1 + \beta_1 x_2\right), \\
    \yh &= \sigma\left( \theta_0 + \theta_1 z_1 + \theta_2 z_2\right).
  \end{align*}
  Choosing parameters as before yields, e.g., 
  $\thetab^\prime = (0.5, -1, -1)^\top$.
  \end{enumerate}
  
\end{enumerate}