\begin{enumerate}
\item Logistic regression is a classification model, that estimates posterior probabilities $\pi(x)$ by linear functions in $x$.
For a binary classification problem the model can be written as:\\

$$
\hat{y} = 1 \ \ \Leftrightarrow \ \ \pix = \frac{1}{1 + \exp(-x^T\theta)} \geq a
$$

This can be reformulated, s.t. for $a \in (0,1)$

\begin{align*}
&\frac{1}{1 + \exp(-x^T\theta)} \geq a \\
\Leftrightarrow\ & 1 + \exp(-x^T\theta) \leq a^{-1} \\
\Leftrightarrow\ & \exp(-x^T\theta) \leq a^{-1} - 1 \\
\Leftrightarrow\ & -x^T \theta \leq \log(a^{-1} - 1) \\
\Leftrightarrow\ & x^T \theta \geq -\log(a^{-1} - 1) \\
\end{align*}



For $a = 0.5$ we get:

$$
\hat{y} = 1 \ \ \Leftrightarrow \ \ x^T \theta \geq -\log(0.5^{-1} - 1) = -\log(2 - 1) = -\log(1) = 0
$$
This means the linear decision boundary is defined by a hyperplane equation, i.e.,  
$x^T \theta = 0$
and it divides the input space in the "1"-space ($x^T \theta \geq 0$) and in the "0"-space ($x^T \theta < 0$).
\item  When the threshold $a = 0.5$ is chosen the Bernoulli loss, which penalizes the missclassifications $L(\hat{y}=0|y=1)$ and  $L(\hat{y}=1|y=0)$ equally, is minimized. This means $a = 0.5$ is a sensible threshold if one does not need to avoid one type of missclassification more than the other.\\
Intuitively it makes sense to cut off at $0.5$ because, if the probability for 1 is closer to 1 than to 0, we would intuitively choose 1 rather than 0.
\end{enumerate}