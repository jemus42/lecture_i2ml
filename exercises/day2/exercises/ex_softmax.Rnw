\begin{enumerate}
  \item[a)] What is the relationship between softmax $\pi_i(x)=\frac{\exp(\theta_i^T x)}{\sum_k \exp(\theta_k^T x)}$ and the logistic function $\pix =\frac{1}{1+\exp(\theta^{T} x)}$ for $k = 2$ (binary classification)?

  \item[b)] Derive the negative log likelihood of softmax regression. Suppose there are $n$ instances and $p$ features.

  \item[c)] Explain how the predictions of softmax regression (multiclass classification) looks like (probabilities and classes) and define the parameter space.

  % \item[c)] Calculate the derivative of the negative likelihood as a loss function for softmax regression: $\triangledown_{\theta_k}L= \sum_{i=1}^n {-x ([y[i]==k] - \pi_k)}$ (suppose there are n instances)
\end{enumerate}

