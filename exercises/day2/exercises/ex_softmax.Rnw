\begin{enumerate}
  \item[a)] What is the relationship between softmax $\pi_k(x)=\frac{\exp(\theta_k^T x)}{\sum^g_{j=1} \exp(\theta_j^T x)}$ and the logistic function $\pix =\frac{1}{1+\exp(\theta^{T} x)}$ for $g = 2$ (binary classification)?

  \item[b)] Derive the loss function of the softmax regression. Suppose there are $n$ instances and $g$ features. (Hint: Apply the same steps as we used in the lecture for the logistic regression)
How does this loss function relate to the negative log likelihood?

  \item[c)] Explain how the predictions of softmax regression (multiclass classification) looks like (probabilities and classes) and define the parameter space.

  % \item[c)] Calculate the derivative of the negative likelihood as a loss function for softmax regression: $\triangledown_{\theta_k}L= \sum_{i=1}^n {-x ([y[i]==k] - \pi_k)}$ (suppose there are n instances)
\end{enumerate}

