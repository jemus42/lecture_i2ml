\begin{enumerate}
  \item[a)] What is the relationship between softmax $\pi_k(x)=\frac{\exp(\theta_k^T x)}{\sum^g_{j=1} \exp(\theta_j^T x)}$ and the logistic function $\pix =\frac{1}{1+\exp(\theta^{T} x)}$ for $g = 2$ (binary classification)?

  \item[b)] Derive the likelihood of $n$ multinomially distributed ($g$ features) target variables when the posterior class probabilities are modeled with softmax regression. How can you transform this likelihood function to an empirical risk function? State the associated loss function. 

  \item[c)] Explain how the predictions of softmax regression (multiclass classification) looks like (probabilities and classes) and define the parameter space.

  % \item[c)] Calculate the derivative of the negative likelihood as a loss function for softmax regression: $\triangledown_{\theta_k}L= \sum_{i=1}^n {-x ([y[i]==k] - \pi_k)}$ (suppose there are n instances)
\end{enumerate}

