\begin{enumerate}

  \item[a)]

    To predict a new fruit as banana or not we have to calculate the posterior probabilities for \enquote{yes} and \enquote{no} regarding the Bayes' theorem :
    \begin{align*}
      P(& \text{Banana} = \text{yes}\ |\ \text{Color} = \text{yellow},\ \text{Form} = \text{round},\ \text{Origin} = \text{imported}) \\
      & = \frac{P(\text{Color} = \text{yellow},\ \text{Form} = \text{round},\ \text{Origin} = \text{imported}\ |\ \text{Banana} = \text{yes})P(\text{Banana} = \text{yes})}{P(\text{Color} = \text{yellow},\ \text{Form} = \text{round},\ \text{Origin} = \text{imported})}
    \end{align*}
    \begin{align*}
      P(& \text{Banana} = \text{no}\ |\ \text{Color} = \text{yellow},\ \text{Form} = \text{round},\ \text{Origin} = \text{imported}) \\
      & = \frac{P(\text{Color} = \text{yellow},\ \text{Form} = \text{round},\ \text{Origin} = \text{imported}\ |\ \text{Banana} = \text{no})P(\text{Banana} = \text{no})}{P(\text{Color} = \text{yellow},\ \text{Form} = \text{round},\ \text{Origin} = \text{imported})}
    \end{align*}

    \begin{itemize}

      \item
        Due to the independence assumption we make for naive Bayes, we can rewrite the numerator as product of the individual probabilities (just done for \enquote{yes}):
        \begin{align*}
          P(& \text{Color} = \text{yellow},\ \text{Form} = \text{round},\ \text{Origin} = \text{imported}\ |\ \text{Banana} = \text{yes}) \\
          & = P(\text{Color} = \text{yellow}\ |\ \text{Banana} = \text{yes}) P(\text{Form} = \text{round}\ |\ \text{Banana} = \text{yes}) P(\text{Origin} = \text{imported}\ |\ \text{Banana} = \text{yes})
        \end{align*}

      \item
        Since both posterior probabilities shares the same denominator it is sufficient to calculate just the numerator and standardize them afterwards (just done for \enquote{yes}):
        \begin{align*}
          P(& \text{Banana} = \text{yes}\ |\ \text{Color} = \text{yellow},\ \text{Form} = \text{round},\ \text{Origin} = \text{imported}) \\
          & \propto P(\text{Color} = \text{yellow},\ \text{Form} = \text{round},\ \text{Origin} = \text{imported}\ |\ \text{Banana} = \text{yes})P(\text{Banana} = \text{yes})
        \end{align*}

    \end{itemize}


    To calculate the posterior probabilities we need:
    \begin{align*}
    & P(\text{Banana} = \text{yes}),\ P(\text{Color} = \text{yellow} | \text{Banana} = \text{yes}),\ P(\text{Form} = \text{round} | \text{Banana} = \text{yes}), \\
    & P(\text{Origin} = \text{imported} | \text{Banana} = \text{yes})
    \end{align*}
    and
    \begin{align*}
    & P(\text{Banana} = \text{no}),\ P(\text{Color} = \text{yellow} | \text{Banana} = \text{no}),\ P(\text{Form} = \text{round} | \text{Banana} = \text{no}), \\
    & P(\text{Origin} = \text{imported} | \text{Banana} = \text{no})
    \end{align*}
    Since the features are categorical we use as estimator the relative frequency:
    \begin{align*}
    & P(\text{Banana} = \text{yes}) = 3 / 8 \\
    & P(\text{Color} = \text{yellow} | \text{Banana} = \text{yes}) = 1 / 3 \\
    & P(\text{Form} = \text{round} | \text{Banana} = \text{yes}) = 1 / 3 \\
    & P(\text{Origin} = \text{imported} | \text{Banana} = \text{yes}) = 1
    \end{align*}
    \begin{align*}
    & P(\text{Banana} = \text{no}) = 5 / 8 \\
    & P(\text{Color} = \text{yellow} | \text{Banana} = \text{no}) = 2 / 5 \\
    & P(\text{Form} = \text{round} | \text{Banana} = \text{no}) = 3 / 5 \\
    & P(\text{Origin} = \text{imported} | \text{Banana} = \text{no}) = 2/5
    \end{align*}
    With the independence assumption of naive Bayes, the final step is to calculate the product of the probabilities above:
    \begin{align*}
    P(& \text{Banana} = \text{yes} | \text{Color} = \text{yellow},\ \text{Form} = \text{round},\ \text{Origin} = \text{imported}) \\
    & \propto P(\text{Banana}) \cdot P(\text{Color} = \text{yellow} | \text{Banana} = \text{yes}) \cdot P(\text{Form} = \text{round} | \text{Banana} = \text{yes}) \\
    & \cdot P(\text{Origin} = \text{imported} | \text{Banana} = \text{yes}) \\
    & = 3 / 8 \cdot 1 / 3 \cdot 1 / 3 \cdot 1 \\
    & = 1 / 24 = 0.04167
    \end{align*}
    \begin{align*}
    P(& \text{Banana} = \text{no} | \text{Color} = \text{yellow},\ \text{Form} = \text{round},\ \text{Origin} = \text{imported}) \\
    & \propto P(\text{Banana}) \cdot P(\text{Color} = \text{yellow} | \text{Banana} = \text{no}) \cdot P(\text{Form} = \text{round} | \text{Banana} = \text{no}) \\
    & \cdot P(\text{Origin} = \text{imported} | \text{Banana} = \text{no}) \\
    & = 5 / 8 \cdot 2 / 5 \cdot 3 / 5 \cdot 2 / 5 \\
    & = 3 / 50 = 0.06
    \end{align*}

    At this stage we can already see that the predicted label is \enquote{no} $0.06 > 0.04167$. To get the final posterior probabilities we have to calculate the constant $c$ for standardization:
    $$
    c = 0.04167 + 0.06 = 0.10167
    $$
    This gives us the posterior probabilities:
    \begin{align*}
    P(& \text{Banana} = \text{yes} | \text{Color} = \text{yellow},\ \text{Form} = \text{round},\ \text{Origin} = \text{imported}) \\
    & = 0.04167 / c \\
    & = 0.04167 / 0.10167 \\
    & = 0.4109
    \end{align*}
    \begin{align*}
    P(& \text{Banana} = \text{no} | \text{Color} = \text{yellow},\ \text{Form} = \text{round},\ \text{Origin} = \text{imported}) \\
    & = 0.06 / 0.10167 \\
    & = 0.5901
    \end{align*}

    Corresponding \texttt{R}-Code:

    <<>>=
    df_banana = data.frame(
      Color = c("yellow", "yellow", "yellow", "brown", "brown", "green", "green", "red"),
      Form = c("oblong", "round", "oblong", "oblong", "round", "round", "oblong", "round"),
      Origin = c("imported", "domestic", "imported", "imported", "domestic", "imported",
        "domestic", "imported"),
      Banana = c("yes", "no", "no", "yes", "no", "yes", "no", "no")
    )

    library(mlr)

    nbayes_learner = makeLearner("classif.naiveBayes", predict.type = "prob")
    banana_task = makeClassifTask(data = df_banana, target = "Banana")
    model = train(learner = nbayes_learner, task = banana_task)

    predict(model, newdata = data.frame(Color = "yellow", Form = "round", Origin = "imported"))
    @

  \item[b)]
    For numerical features, we would use a different probability distribution, not the Bernoulli distribution.
    For example, for the information "Length" we could assume that $P(Length \vert yes)$ and $P(Length \vert no)$ both follow a normal distribution.
\end{enumerate}