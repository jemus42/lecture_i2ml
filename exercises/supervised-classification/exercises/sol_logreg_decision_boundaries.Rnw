\begin{enumerate}
\item %Logistic regression is a classification model, that estimates posterior probabilities $\pi(x)$ by linear functions in $x$.
For a binary classification problem the model can be written as:\\

$$
\hat{y} = 1 \ \ \Leftrightarrow \ \ \pix = \frac{1}{1 + \exp(-\thetab^T \xv)} \geq a
$$

This can be reformulated, s.t. for $a \in (0,1)$

\begin{align*}
&\frac{1}{1 + \exp(-\thetab^T \xv)} \geq a \\
\Leftrightarrow\ & 1 + \exp(-\thetab^T \xv) \leq a^{-1} \\
\Leftrightarrow\ & \exp(-\thetab^T \xv) \leq a^{-1} - 1 \\
\Leftrightarrow\ & -\thetab^T \xv \leq \log(a^{-1} - 1) \\
\Leftrightarrow\ & \thetab^T \xv \geq -\log(a^{-1} - 1) \\
\end{align*}



For $a = 0.5$ we get:

$$
\hat{y} = 1 \ \ \Leftrightarrow \ \ \thetab^T \xv \geq -\log(0.5^{-1} - 1) = -\log(2 - 1) = -\log(1) = 0
$$
This means the linear decision boundary is defined by a hyperplane equation, i.e.,  
$\thetab^T \xv = 0$
and it divides the input space in the "1"-space ($\thetab^T \xv \geq 0$) and in the "0"-space ($\thetab^T \xv < 0$).
\item  When the threshold $a = 0.5$ is chosen, the losses of missclassified observations, i.e., $L(\hat{y}=0|y=1)$ and  $L(\hat{y}=1|y=0)$ ,  are weighted equally. This means $a = 0.5$ is a sensible threshold if one does not need to avoid one type of misclassification more than the other.\\
Intuitively it makes sense to cut off at $0.5$ because, if the probability for 1 is closer to 1 than to 0, we would intuitively choose 1 rather than 0.
\end{enumerate}