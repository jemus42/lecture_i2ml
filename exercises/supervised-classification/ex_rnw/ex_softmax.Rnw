\begin{enumerate}[a)]

  \item What is the relationship between softmax 
  $$\pikx=\frac{\exp(\thetab_k^T \xv)}{\sumjg \exp(\thetab_j^T \xv)}
  ~~ \text{for } k = 1, 2$$
  and the logistic function $$\pix = \frac{1}{1 + \exp(-\thetab^{T} \xv)}$$
  for $g = 2$ (binary classification)?
  
  \item The likelihood function for a multinomially distributed target variable 
  with $g$ target classes is given by
  $$\LL_i(\thetab) = \P(\yi | \xi, \thetab_1, \thetab_2, \dots, \thetab_g) = 
  \prod_{j = 1}^g \pi_j(\xi)^{\Ind{[\yi = j]}}$$
  where the posterior class probablities $\pi_1(\xv), \pi_2(\xv), \dots, 
  \pi_g(\xv)$ are modeled with softmax regression.
  Derive the likelihood function for $n$ such independent target variables. 
  
  \item We have already addressed the connection that holds between maximum 
  likelihood estimation and empirical risik minimization. 
  How can you transform the joint likelihood function into an empirical risk 
  function?
  
  Hints: 
  \begin{itemize}
    \item By following the maximum likelihood principle, we should look for 
    parameters $\thetab_1, \thetab_2, \dots, \thetab_g$ that maximize the 
    likelihood function.
    \item The expressions $\prod \LL_i$ and $\log \prod \LL_i$, if defined,
    are maximized by the same parameters.
    % \item The empirical risk is a \emph{sum} of loss function values, not a 
    % \emph{product}.
    \item Minimizing a scalar function multiplied with -1 is equivalent to 
    maximizing the original function.
  \end{itemize}
  State the associated loss function. 

  \item Write down the discriminant functions of multiclass logistic regression 
  resulting from this minimization objective.
  How do we arrive at the final prediction? 
  
  \item State the hypothesis space $\Hspace$ and corresponding parameter space 
  $\Theta$ for the multiclass case.
  
\end{enumerate}

