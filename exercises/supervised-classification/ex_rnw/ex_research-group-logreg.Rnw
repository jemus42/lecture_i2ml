Chapter 3.4 of the lecture introduces \textbf{logistic regression}. The medical research group from last week is eager to find out if it can be used to predict whether a patient admitted to the hospital will require intensive care. This is a binary classification task with target space $\Yspace = \setzo$, with $y=1$ if the patient requires intensive care and $y=0$ if not. The feature space is the same as before:
$\Xspace = (\R_{0}^{+})^3$, with $\xi = (x_{age},\;x_{blood\;pressure},\;x_{weight})^{(i)} \in \Xspace$ for $i = 1, 2, \dots, n$ observations. 

\medbreak
Before the group trains a logistic regression model, researcher Holger remarks they could just as well fit a linear model (LM), as in the case of a binary classification task, both models would make identical predictions.

\begin{enumerate}\bfseries
  \item[1)] Are the predictions of a logistic regression model and a LM identical for a binary classification task? If not, explain why they could differ.
\end{enumerate}

Researcher Lisa knows that logistic regression follows a discriminant approach, meaning the discriminant functions are optimized directly via empirical risk minimization (ERM). She remembers the general form of ERM:
\begin{align}
\fh = \argmin_{f \in \Hspace} \riskef = \argmin_{f \in \Hspace} \sumin \Lxyi
\end{align}
Additionally, she recalls the Bernoulli loss function of the logistic regression model in statistics:
\begin{align}
\Lpixy = -y\ln(\pix)-(1-y)\ln(1-\pix)
\end{align}
Lastly, she recollects how logistic regression models the posterior probabilities $\pixt$ of the labels $\text{--}$ the estimated linear scores are "squashed" through the logistic function $s$:
\begin{align}
\pixt = \frac{\exp\left( \thx \right)}{1+\exp\left(\thx \right)} = \frac{1}{1+\exp\left( -\thx \right)} = s\left( \thx \right)
\end{align}
Given (1)$\;-\;$(3), she figures one could formulate the explicit ERM problem, but leaves the task for the data scientist in the group.

\begin{enumerate}\bfseries
  \item[2)] Write down the explicit form of the ERM problem. Hint: Start with equation (1).
\end{enumerate}

Later, the research group trains the logistic regression model and receives a corresponding parameter estimate $\thetabh = (\thetah_0,\; \thetah_{age},\; \thetah_{blood\;pressure},\; \thetah_{weight})$. Researcher Son, who has worked all night on the research problem, finds a function scribbled on his personal notes. He remembers it was useful in the context of a logistic regression model, but does not recall how.
\begin{align}
f\left( \xv^{(i)} ~|~ \thetabh, \; \alpha \right) = \scalebox{1.2}{$\I_{[\alpha, 1]}$} \left( \frac{1}{1+\exp (- \thetabh^T \xi ) } \right), ~~~ \alpha \in \; ]0,1[
\end{align}

\begin{enumerate}\bfseries
  \item[3)] What purpose does the function serve in the case of a trained logistic regression model with estimated parameters $\thetabh$? Explain the role of the parameter $\alpha$.
\end{enumerate}

Researcher Son finds out that when he derives the log-likelihood function $\logl$ of a single Bernoulli distributed random variable $Y$, his result looks somewhat similar to the Bernoulli loss function in $(2)$:
\begin{align}
\LL & = \P(Y=y) = \pi^y (1-\pi)^{1-y} \\
\logl & = ln(\LL) \\
& = y\ln{(\pi)} + (1-y)\ln{(1-\pi)}
\end{align}

\begin{enumerate}\bfseries
  \item[4)] How is the loss function used for ERM in $(2)$ related to the log-likelihood function $\logl$ of a Bernoulli distributed random variable in $(7)$?
\end{enumerate}

