We will now visualize how well different learners classify the three-class 
\texttt{mlbench::mlbench.cassini} data set.
Generate 1000 points from \texttt{cassini}, perturb the \texttt{x.2} dimension 
with Gaussian noise (mean 0, standard deviation 0.5), 
and consider the classifiers already introduced in the lecture: 

\begin{itemize}
  \item LDA,
  \item QDA, and
  \item Naive Bayes.
\end{itemize}

Plot the learners' decision boundaries.
Can you spot differences in separation ability?

(Note that logistic regression cannot handle more than two classes and is 
therefore not listed here.)