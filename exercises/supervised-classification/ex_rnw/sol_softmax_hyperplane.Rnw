Logistic regression is a classification model, that estimates posterior probabilities $\pi(x)$ by linear functions in $x$.
For a binary classification problem the model can be written as:\\

$$
\hat{y} = 1 \ \ \Leftrightarrow \ \ \pi(x) = \frac{1}{1 + \exp(-x^T\theta)} \geq a
$$

For the decision boundary we have to set $\pi(x) = a$. Solving this equation for $x$ yields:

\begin{align*}
&\frac{1}{1 + \exp(-x^T\theta)} = a \\
\Leftrightarrow\ & 1 + \exp(-x^T\theta) = a^{-1} \\
\Leftrightarrow\ & \exp(-x^T\theta) = a^{-1} - 1 \\
\Leftrightarrow\ & \exp(-x^T\theta) = a^{-1} - 1 \\
\Leftrightarrow\ & -x^T \theta = \log(a^{-1} - 1) \\
\Rightarrow\ & x^T \theta = -\log(a^{-1} - 1) \\
\end{align*}

For $a = 0.5$ we get:

$$
x^T \theta = -\log(0.5^{-1} - 1) = -\log(2 - 1) = -\log(1) = 0
$$

% $$
% \log\frac{\P(y = 1 | x)}{\P(y = 0 | x)} = \beta^T x
% $$
% We want to model the probability for class 1:
% $$
% \P(y = 1 | x) = \frac{\exp(\beta^T x)}{1 + \exp(\beta^T x)} = \frac{\exp(f(x))}{1 + \exp(f(x))}
% $$
% Let $\P(y = 1 | x) = \frac{1}{2}$, it follows:
% \begin{eqnarray*}
% \frac{1}{2}&=&  \frac{\exp(f(x))}{1 + \exp(f(x))}\\
% \frac{1}{2} + \frac{1}{2} \cdot \exp(f(x)) &=& \exp(f(x)) \\
% \frac{1}{2}&=& \exp(f(x)) - \frac{1}{2}\exp(f(x)) \\
% 1&=& \exp(f(x))\\
% \Longleftrightarrow f(x) &=& \log(1)\\
% \Longleftrightarrow \beta^T X &=& 0\\
% \end{eqnarray*}
