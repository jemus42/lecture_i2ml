<<echo=FALSE, fig.align="center", fig.width=3, fig.height=2>>=
library(ggplot2)
set.seed(31415)

x1 <- runif(49, 0.1, 1.9)
x2 <- runif(98, 2.1, 5.9)
x3 <- runif(49, 6.1, 7.9)
x4 <- c(1,3,5,7)
x <- c(x1, x2, x3, x4)

y1 <- 2 + rnorm(49, 0, 0.02)
y2 <- 4 + rnorm(98, 0, 0.02)
y3 <- 3 + rnorm(49, 0, 0.02)
y4 <- c(4, 2, 3, 4)
y <- c(y1, y2, y3, y4)

d = data.frame(x, y)

ggplot(data = d, aes(x = x, y = y)) +
  geom_point(alpha = 0.25)

@

The above plot shows $\D = \Dset $, a data set with %$n$ observations
%data $\xv = (x^{(1)}, \dots, x^{(n)})^\top$ and $\ydat = \yvec$ for 
$n$ = 200 observations of a continuous target variable $y$ and a continuous, 
1-dimensional feature variable $\xv$. In the following, we aim at predicting 
$y$ with a machine learning model that takes $\xv$ as input.

\begin{enumerate}
  \item Since the data seem to fall in 3 quite well-separable classes, we now 
  want to apply a classification model instead of the regression model in a). 
  To prepare the data for classification, we categorize the target variable $y$ 
  in 3 classes and call the transformed target variable $z$, as follows:
  \[
    z^{(i)} = \left\{\begin{array}{lr}
        1, \text{ if }&  -\infty < \yi \leq 2.5 \\
        2, \text{ if }&  2.5\ < \yi \leq 3.5 \\
        3, \text{ if }&  3.5 < \yi < \infty
        \end{array} \right\}
  \]
  Now we can apply quadratic discriminant analysis (QDA):
  
  \begin{enumerate}[i)]
  
    \item Estimate the class means $\mu_k = \E(\xv|z = k)$ for each of the 
    three classes $k \in \{1, 2, 3\}$ visually from the plot. Do not 
    overcomplicate this, a rough estimate is sufficient here.
    
    \item Make a hand-drawn plot that visualizes the different estimated 
    densities per class.
    
    \item How would your drawing from (ii) change if we used linear 
    discriminant analysis (LDA) instead of QDA? Explain your answer.
    \item Why is QDA for this data preferable over LDA?
    
  \end{enumerate} 
  
  \item Given are two new observations $\xv_{*1} = -10$ and $\xv_{*2} = 7$. 
  State the prediction for QDA and explain how you derived the predictions.
  
  \item We will now derive the LDA decision boundary for a binary problem in 
  two dimensions. For this, we use the alternative description of the LDA 
  decision rule we (implicitly) used to demonstrate LDAâ€™s linearity:
  $$x$$
  Hint: first think about the relation of the two discriminant functions on the 
  decision boundary.

  \item Plot the derived boundary and compare the result to the mlr3 plot from 
  the previous exercise sheet.
  
  \item Implement you own version of QDA:
<<eval=FALSE>>=
train_qda <- function(target, data) {...}
predict_qda <- function(target, data) {...}
@
  The first function should return the fitted model (use an adequate structure 
  for the model!). The second function should return a factor of classes for 
  \texttt{type = 'class'} and a matrix of predicted probabilities for 
  \texttt{type = 'prob'}. Your method does only need to work for numeric 
  features. Check your implementation on the \texttt{iris} dataset and compare 
  your results of both types with the \texttt{qda()} function from the 
  package \texttt{MASS}.
  
  \item Turn your QDA classifier into a Naive Bayes classifier. 
  Can you see why NB is more naive than general QDA?
  
\end{enumerate}
