<<echo=FALSE, fig.align="center", fig.width=3, fig.height=2>>=
library(ggplot2)
set.seed(31415)

x1 <- runif(49, 0.1, 1.9)
x2 <- runif(98, 2.1, 5.9)
x3 <- runif(49, 6.1, 7.9)
x4 <- c(1,3,5,7)
x <- c(x1, x2, x3, x4)

y1 <- 2 + rnorm(49, 0, 0.02)
y2 <- 4 + rnorm(98, 0, 0.02)
y3 <- 3 + rnorm(49, 0, 0.02)
y4 <- c(4, 2, 3, 4)
y <- c(y1, y2, y3, y4)

d = data.frame(x, y)

ggplot(data = d, aes(x = x, y = y)) +
  geom_point(alpha = 0.25)

@

The above plot shows $\D = \Dset $, a data set with %$n$ observations
%data $\xv = (x^{(1)}, \dots, x^{(n)})^\top$ and $\ydat = \yvec$ for 
$n$ = 200 observations of a continuous target variable $y$ and a continuous, 
1-dimensional feature variable $\xv$. In the following, we aim at predicting 
$y$ with a machine learning model that takes $\xv$ as input.

\begin{enumerate}[a)]
  \item To prepare the data for classification, we categorize the target 
  variable $y$ in 3 classes and call the transformed target variable $z$, as 
  follows:
  \[
    z^{(i)} = 
    \begin{cases}
      1, &  \yi \in (-\infty, 2.5] \\
      2, &  \yi \in (2.5, 3.5] \\
      3, &  \yi \in (3.5, \infty)
    \end{cases}
    % z^{(i)} = \left\{\begin{array}{lr}
    %     1, \text{ if }&  -\infty < \yi \leq 2.5 \\
    %     2, \text{ if }&  2.5\ < \yi \leq 3.5 \\
    %     3, \text{ if }&  3.5 < \yi < \infty
    %     \end{array}
  \]
  Now we can apply quadratic discriminant analysis (QDA):
  
  \begin{enumerate}[i)]
  
    \item Estimate the class means $\mu_k = \E(\xv|z = k)$ for each of the 
    three classes $k \in \{1, 2, 3\}$ visually from the plot. Do not 
    overcomplicate this, a rough estimate is sufficient here.
    
    \item Make a plot that visualizes the different estimated 
    densities per class.
    
    \item How would your plot from ii) change if we used linear 
    discriminant analysis (LDA) instead of QDA? Explain your answer.
    \item Why is QDA preferable over LDA for this data?
    
  \end{enumerate} 
  
  \item Given are two new observations $\xv_{*1} = -10$ and $\xv_{*2} = 7$. 
  State the prediction for QDA and explain how you arrive there.
  
  % \item We will now derive the LDA decision boundary for a binary problem in 
  % two dimensions. For this, we employ the alternative description of the LDA 
  % decision rule we (implicitly) used to demonstrate LDAâ€™s linearity:
  % \[
  %   \delta_k(\xv) = \log \pik  - \frac{1}{2} \muk^\top \Sigma^{-1} \muk 
  %   + \xv^\top \Sigma^{-1} \muk, ~~ k \in \{1, 2\}.
  % \]
  % Explicitly state the equation for the boundary between the two classes. \\
  % (Hint: first think about the relation of the two discriminant functions on the 
  % decision boundary.)
  
%   \item  Implement you own version of QDA:
% <<eval=FALSE>>=
% train_qda <- function(target, data) {...}
% predict_qda <- function(target, data) {...}
% @
%   The first function should return the fitted model (use an adequate structure 
%   for the model!). The second function should return a factor of classes for 
%   \texttt{type = 'class'} and a matrix of predicted probabilities for 
%   \texttt{type = 'prob'} (similar to the standard \texttt{mlr3} output options). 
%   Your method only needs to work for numeric features. 
%   Check your implementation on the \texttt{iris} data and compare 
%   your results of both types with the \texttt{qda()} function from  
%   package \texttt{MASS}.
%   
%   \item Turn your QDA classifier into a Naive Bayes classifier by appropriate 
%   modification.
%   Can you see why NB is more \enquote{naive} than general QDA?
  
\end{enumerate}
