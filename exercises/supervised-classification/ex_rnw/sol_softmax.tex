\begin{enumerate}[a)]

  \item As we would expect, the two formulations are equivalent (up to 
  reparameterization):
  \begin{flalign*}
    \pikxt[1] &= \frac{\exp (\thetab_1^\top \xv)}{\exp (\thetab_1^\top \xv) +
    \exp (\thetab_2^\top \xv)} \\
    \pikxt[2] &= \frac{\exp (\thetab_1^\top \xv)}{\exp (\thetab_1^\top \xv) +
    \exp (\thetab_2^\top \xv)},
  \end{flalign*}
  where $\pikxt[1] + \pikxt[2] = 1$.
  \begin{flalign*}
    \Longrightarrow \pikxt[1] &= \dfrac{1}{\dfrac{\exp (\thetab_1^\top \xv) +
    \exp (\thetab_2^\top \xv)}{\exp (\thetab_1^\top \xv)}} \\
    &= \frac{1}{1 + \exp (\thetab_2^\top \xv - \thetab_1^\top \xv)} \\
    &= \frac{1}{1 + \exp (- \thx)} \\
    &= \pixt,
  \end{flalign*}
  if we set $\thetab := \thetab_1 - \thetab_2$ and 
  $\pikxt[2] = 1 - \pikxt[1]$.

  \item The joint likelihood is easy to compute with the \textit{iid} assumption
  we are willing to make.
  It is simply given by the product over all individual likelihoods:
  $$\LLt = \prodin \prod_{j = 1}^g \pi_j(\xi)^{\I{(\yi = j)}}.$$
  
  \item Right now, $\LLt$ does not look anything like an empirical risk 
  function.
  However, we will arrive there by some simple transformations you might 
  recall from the first exercise sheet:
  \begin{itemize}
    \item First we convert our \textit{maximum} likelihood problem into an 
    empirical risk \textit{minimization} problem:
    $$\argmax_{\thetab \in \Theta} \LLt = \argmint - \LLt$$
    \item Then we get rid of the (outer) product over all 
    observations, which we would like to turn into a sum.
    This is achieved by taking the log, a strictly monotonic 
    transformation that has no effect on the optimizer:
    $$\argmint \prodin - \LL_i(\thetab) = \argmint \sumin - \logl_i(\thetab)$$
    The inner product over all classes also becomes a sum in this new 
    formulation (before, we wanted all probability functions but the one 
    corresponding to the true class to become 1 factors, now we want them to 
    become 0 summands):
    $$\argmint \sumin - \logl_i(\thetab) = \argmint \sumin - \left( \sumjg 
    \I(y = j) \log \pikx[j] \right)$$
    \item And we have already found an expression that is conformal with the 
    empirical risk minimization principle:
    $$\thetabh_{\text{MLE}} = \thetabh_{\text{ERM}} = 
    \argmint \underbrace{\sumin \underbrace{- \left( \sumjg \I(y = j) \log 
    \pikx[j] \right)}_{\Lxyit}}_{\risket}$$
  \end{itemize}
  As the above transformations are universally applicable, we can always use 
  the negative log-likelihood (NLL) as a loss function in empirical risk 
  minimization (not every loss function, however, corresponds to a likelihood 
  formulation).
  
  \item The $k$-th discriminant function has the following form:
  $$ \hat{\pi_k}(\xv ~|~ \thetab) = \frac{\exp(\thetabh_k^\top \xv)}{\sumjg 
  \exp(\thetabh_j^\top \xv)} 
  ~~ \in [0, 1],$$
  and $\sum_{k = 1}^g \hat{\pi_k}(\xv ~|~ \thetab) = 1$.
  This sum-one constraint means that one set of parameters is actually 
  redundant: if we know the first $g - 1$ discriminant functions, the $g$-th one 
  is fully determined.
  Therefore, we set $\thetb_g = \zero$ and compute $\postk[g] = 
  1 - \sum_{k = 1}^{g - 1} \hat{\pi_k}(\xv ~|~ \thetab)$.
  
  % \item
  % Since the subtraction of any fixed vector from all $\theta_k$ does not change the prediction, one set of parameters is "redundant". Thus we set $\theta_g = (0,\dots,0).$
  %   Hence for $g$ classes we get $g - 1$ discriminant functions from the softmax $\hat{\pi}_1(x), \dots, \hat{\pi}_{g-1}(x)$ which can be interpreted as probability. The probability for class $g$ can be calculated by using $\hat{\pi}_g = 1 - \sum_{k = 1}^{g-1} \hat{\pi}_k(x)$. To estimate the class we are using majority vote:
  %   $$
  %   \hat{y} = \argmax_k \hat{\pi}_k(x)
  %   $$
  %   The parameter of the softmax regression is defined as parameter matrix where each class has its own parameter vector $\theta_k$, $k \in \{1, \dots, g-1\}$:
  %   $$
  %   \theta = [\theta_1, \dots, \theta_{g-1}]
  %   $$
  % 
  % 
  % 
  % % $\frac{\partial (-\log L)}{\partial \theta_i} = - \sum_k I_k \frac{1}{\pi_k} \frac{\partial \pi_k}{\partial \theta_i}$
  % 
  % % since
  % 
  % % $ \frac{\partial \pi_i}{\partial \theta_i} = \triangledown_{\theta_i} \frac{e^{\theta_i^T x}}{\sum_k e^{\theta_k^T x}}= \frac{e^{z_i}(\sum_k e^{z_k})-(e^{z_i})^2}{(\sum_k e^{z_k})^2}x = \frac{e^{z_i}(\sum_{k \neq i} e^{z_k})}{(\sum_k e^{z_k})^2}x= \pi_i(1 - \pi_i)x$,
  % 
  % % $ \frac{\partial \pi_i}{\partial \theta_j} = \triangledown_{\theta_j} \frac{e^{\theta_i^T x}}{\sum_k e^{\theta_k^T x}}= \frac{-e^{z_i}e^{z_j}}{(\sum_k e^{z_k})^2}x= - \pi_i\pi_j x$, ($i \neq j$)
  % 
  % % Conclusion:
  % 
  % % $\frac{- \partial \log L}{\partial \theta_i} = - \sum_k I_k \frac{1}{\pi_k} \frac{\partial \pi_k}{\partial \theta_i} = -(1-\pi_i)x$ ($i = y$)
  % % and
  % 
  % % $\frac{- \partial \log L}{\partial \theta_j} = - \sum_k I_k \frac{1}{\pi_k} \frac{\partial \pi_k}{\partial \theta_j} = \pi_j x$
  % % ($j \neq y$)
  % 
  % % Writing the two cases in one formula, we have
  % 
  % % $\frac{- \partial \log L}{\partial \theta_k} = -(I_k -\pi_k )x$ where $I_k = [y = k]$
  % 
  % % summing over all instances, we have
  % 
  % % $\triangledown_{\theta_k}\mathcal{L} = \sum_{i=1}^n-([y_i = k] -\pi_k )x$
\end{enumerate}

