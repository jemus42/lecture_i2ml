We will now visualize how well different learners classify the three-class \texttt{mlbench::mlbench.cassini} data set.\\
Generate 1000 points from \texttt{cassini} using \texttt{R} or import \href{https://github.com/slds-lmu/lecture_i2ml/blob/master/exercises/data/cassini_data.csv}{\texttt{cassini\_data.csv}} in \texttt{Python}.\\
Then, perturb the \texttt{x.2} dimension 
with Gaussian noise (mean 0, standard deviation 0.5), 
and consider the classifiers already introduced in the lecture: 

\begin{itemize}
  \item LDA,
  \item QDA, and
  \item Naive Bayes.
\end{itemize}

Plot the learners' decision boundaries.
Can you spot differences in separation ability?

(Note that logistic regression cannot handle more than two classes and is 
therefore not listed here.)
