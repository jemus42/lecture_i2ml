\begin{enumerate}[a)]

  \item When using the naive Bayes classifier, the features $\xv := 
  (x_\text{Color},x_\text{Form},x_\text{Origin})$ are assumed to be 
  conditionally independent of each other, given the category $y = k \in 
  \{\text{yes}, \text{no}\}$, s.t.

  $$ \P(\xv ~|~ y = k) =
  \P((x_\text{Color}, x_\text{Form}, x_\text{Origin}) ~|~ y = k) = 
  \P(x_\text{Color} ~|~ y = k) \cdot \P(x_\text{Form} ~|~ y = k) \cdot 
  \P(x_\text{Origin} ~|~ y = k).$$
  
  Recall Bayes' theorem:
  
  $$\pikx = \postk = \bayesrulek.$$

  As the denominator is constant across all classes, the following holds for the 
  posterior probabilities:
  \begin{align*} 
    \pikx \propto & ~ \underbrace{\pik \cdot \P(x_\text{Color} ~|~ y = k)
    \cdot \P(x_\text{Form} ~|~ y = k) \cdot \P(x_\text{Origin} ~|~ y = k)}_{=: 
    \alpha_k(x)} \\
    \iff & \exists c \in \mathbb{R}: \pikx = c \cdot \alpha_k(\xv),
  \end{align*}
  where $\pik = \P(y = k)$ is the prior probability of class $k$ and $c$ is 
  the normalizing constant.
  
  From this and since the posterior probabilities need to sum up to 1, 
  we know that 
  \begin{align*}
    1 = ~ c \cdot \alpha_\text{yes}(\xv) +  c \cdot \alpha_\text{no}(\xv)
    ~\iff c = \frac{1}{\alpha_\text{yes}(\xv) + \alpha_\text{no}(\xv)}.
  \end{align*}
  This means that, in order to compute $\pi_\text{yes}(\xv)$, the scores 
  $\alpha_\text{yes}(\xv)$ and $\alpha_\text{no}(\xv)$ are needed.
  \\
  
  Now we want to estimate for a new fruit the posterior probability 
  $\hat{\pi}_{yes}((\text{yellow}, \text{round}, \text{imported}))$.
  
  Obviously, we do not know the \emph{true} prior probability and the 
  \emph{true} conditional densities. 
  Here -- since the target and the features are 
  categorical -- we use a categorical distribution, i.e., the 
  simplest distribution over a $g$-way event that is fully specified by the 
  individual probabilities for each class (which must of course sum to 1).
  This is a generalization of the Bernoulli distribution to the multi-class 
  case.
  We can estimate the distribution parameters via the relative frequencies 
  encountered in the data:
  \begin{align*}
    \hat{\alpha}_\text{yes}(\xv_{\ast}) = 
    & \;  \hat{\pi}_{yes} \cdot 
    % \hat{\P}(\text{yellow} ~|~ y = \text{yes})\cdot \hat{\P}(\text{round} ~|~ y = 
    % \text{yes}) \cdot \hat{\P}(\text{imported} ~|~ y = \text{yes}) \\
    \hat{\P}(x_\text{Color} = \text{yellow} ~|~ y = \text{yes}) \cdot 
    \hat{\P}(x_\text{Form} = \text{round} ~|~ y = \text{yes}) \cdot 
    \hat{\P}(x_\text{Origin} = \text{imported} ~|~ y = \text{yes}) \\
    = & \; \frac{3}{8} \cdot \frac{1}{3} \cdot \frac{1}{3} \cdot 1 = \frac{1}{24} 
    \approx 0.042, \\
    \hat{\alpha}_\text{no}(\xv_{\ast}) = & \;  \hat{\pi}_{no} \cdot 
    % \hat{p}(\text{yellow}|y = \text{no})\cdot \hat{p}(\text{round}|y = \text{no}) 
    % \cdot \hat{p}(\text{imported}|y = \text{no}) \\
    \hat{\P}(x_\text{Color} = \text{yellow} ~|~ y = \text{no})\cdot 
    \hat{\P}(x_\text{Form} = \text{round} ~|~ y = \text{no}) \cdot 
    \hat{\P}(x_\text{Origin} = \text{imported} ~|~ y = \text{no}) \\
    = & \; \frac{5}{8} \cdot \frac{2}{5} \cdot \frac{3}{5} \cdot \frac{2}{5} = 
    \frac{3}{50} = 0.060.
  \end{align*}
  At this stage we can already see that the predicted label is "no", since 
  $\hat{\alpha}_\text{no}(\xv_{\ast}) = 0.060 > \frac{1}{24} = 
  \hat{\alpha}_\text{yes}(\xv_{\ast})$ -- that is, if we threshold at 0.5 for 
  predicting \enquote{yes}.
  
  With the above we can compute the posterior probability
  $$\hat{\pi}_\text{yes}(\xv_{\ast}) = \frac{\hat{\alpha}_\text{yes}(
  \xv_{\ast})}{\hat{\alpha}_\text{yes}(\xv_{\ast}) + 
  \hat{\alpha}_\text{no}(\xv_{\ast})} \approx 0.410 < 0.5,$$
  and check our calculations against the corresponding \texttt{R} results:

    <<>>=
    df_banana <- data.frame(
      color = as.factor(
        c("yellow", "yellow", "yellow", "brown", "brown", "green", "green", "red")),
      form = as.factor(
        c("oblong", "round", "oblong", "oblong", "round", "round", "oblong", "round")),
      origin = as.factor(
        c("imported", "domestic", "imported", "imported", "domestic", "imported",
        "domestic", "imported")),
      banana = as.factor(c("yes", "no", "no", "yes", "no", "yes", "no", "no")))
    
    new_fruit <- data.frame(color = "yellow", form = "round", origin = "imported")
    
    library(mlr3)
    library(mlr3learners)
    
    nb_learner <- lrn("classif.naive_bayes", predict_type = "prob")
    
    banana_task <- TaskClassif$new(
      id = "banana",
      backend = df_banana,
      target = "banana")
    
    nb_learner$train(banana_task)
    nb_learner$predict_newdata(new_fruit)
    
    @

  \item Before, we only had categorical features and could use the empirical 
  frequencies as our parameters in a categorical distribution.
  For the distribution of a numerical feature, given the the category, we need 
  to define a probability distribution with continuous support.
  A popular choice is to use Gaussian distributions.
  For example, for the information $x_\text{Length}$ we could assume that 
  $$\P(x_\text{Length} ~|~ y = \text{yes}) \sim \normal(\mu_\text{yes}, 
  \sigma^2_\text{yes})$$ and $$\P(x_\text{Length} ~|~ y = \text{no}) \sim 
  \normal(\mu_\text{no}, \sigma^2_\text{no}).$$ 
  In order to fully specify these normal distributions we need to estimate 
  their parameters $\mu_\text{yes}, \mu_\text{no}, \sigma^2_\text{yes}, 
  \sigma^2_\text{no}$ from the data via the usual estimators 
  (empirical mean and empirical variance with bias correction).
\end{enumerate}