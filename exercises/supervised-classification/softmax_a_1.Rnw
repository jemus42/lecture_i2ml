\begin{enumerate}
  \item[a)] What is the relationship between softmax $\pi_k(x)=\frac{\exp(\theta_k^T x)}{\sum^g_{j=1} \exp(\theta_j^T x)}$ and the logistic function $\pix =\frac{1}{1+\exp(-\theta^{T} x)}$ for $g = 2$ (binary classification)?

  \item[b)] The likelihood function of a multinomially distributed target variable with $g$ target classes is given by
 $$\mathcal{L}_i = \P(Y^{(i)} = y^{(i)}|x^{(i)},\theta_1, \dots, \theta_g) = 
\prod^g_{j=1} \pi_j(x^{(i)})^{\mathds{1}_{\{y^{(i)} = j\}}}$$
where the posterior class probablities $\pi_1(x), \dots, \pi_g(x)$ are modeled with softmax regression.
  Derive the likelihood function of $n$ such independent target variables. How can you transform this likelihood function into an empirical risk function?
  \\ Hints: 
  \begin{itemize}
  \item By following the maximum likelihood principle, we should look for parameters $\theta_1, \dots, \theta_g$, which maximize the likelihood function.
  \item The expressions $\prod \mathcal{L}_i$ and $\log\prod\mathcal{L}_i$ (if this expression is defined) are maximized by the same parameters.
  \item The empirical risk is a \emph{sum} of loss function values, not a \emph{product}.
  \item Minimizing a scalar function multiplied with -1 is equivalent to maximizing the original function.
  \end{itemize}
  State the associated loss function. 

  \item[c)] Explain how the predictions of softmax regression (multiclass classification) look like (probabilities and classes) and define the parameter space.

  % \item[c)] Calculate the derivative of the negative likelihood as a loss function for softmax regression: $\triangledown_{\theta_k}L= \sum_{i=1}^n {-x ([y[i]==k] - \pi_k)}$ (suppose there are n instances)
\end{enumerate}

