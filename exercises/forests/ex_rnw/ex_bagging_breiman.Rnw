In this exercise, we briefly revisit why bagging is a useful technique to 
stabilize predictions.

For a fixed observation $(\xv, y)$, show that the quadratic loss of the ensemble prediction $\fM$ is less than or equal to the expected loss over base learner predictions $\blx$.
You can assume an infinite theoretical ensemble and use $\E_{\mathcal{M}}$ to denote the expectation over its members.