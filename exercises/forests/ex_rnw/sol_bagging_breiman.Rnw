Let's take a look at the average loss of individual base learner predictions. 
Since we are in the (theoretical) case of an infinitely large ensemble, the 
average contains an infinite sum, which can equally be viewed as the expectation 
over $\mathcal{M}$:
\begin{align*}
\E_{\mathcal{M}} \left( \left(y - \blx \right)^2\right) 
&= \E_{\mathcal{M}} \left(y^2 - 2y \blx + \left(\blx\right)^2 \right) \\
&= y^2 - 2y \E_{\mathcal{M}} \left(\blx \right) + \E_{\mathcal{M}} \left( \left( 
\blx \right)^2 \right),
\end{align*}
where we use the linearity of the expectation.

Note that the average base learner prediction is simply the prediction of 
the ensemble: $\E_{\mathcal{M}} (\blx) = \fM$.
Plugging this into the above equation and using the definition of the variance 
of a random variable $Z$ ("Verschiebungssatz"\footnote{
$\var(Z) = \E(Z^2) - (\E(Z))^2 ~ \Longleftrightarrow ~ \E(Z^2) = \var(Z) + 
(\E(Z))^2$, where $\var(Z) \geq 0$ by definition.
}), which tells us that $\E(Z^2) \geq (\E(Z))^2$, we obtain: 

\begin{align*}
\E_{\mathcal{M}} \left( \left(y - \blx \right)^2\right) &\geq 
y^2 - 2y \E_{\mathcal{M}} \left(\blx \right)  + \left( \E_{\mathcal{M}} 
\left(\blx \right) \right)^2 \\
&= \left( y - \E_{\mathcal{M}} \left(\blx \right) \right)^2 \\
&= \left( y - \fM \right)^2.
\end{align*}

The ensemble loss is thus less than or equal to the average loss of individual 
base learners.
How "sharp" this inequality is depends on how unequal both sides of 
$$\E_{\mathcal{M}} \left( \left( \blx \right)^2 \right)  \geq 
\left( \E_{\mathcal{M}} \left(\blx \right) \right)^2
$$ 
are, determined via the definition of variance by $\var \left( \blx \right)$.
In other words: the more instable the base learners (high variance), the more 
beneficial the ensembling procedure.
