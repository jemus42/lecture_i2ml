\begin{enumerate}
\item You want to approximate the binomial distribution with $n$ number of trials and probability $p$ with a Gaussian distribution with mean $\mu$ and variance $\sigma^2$. To find a suitable distribution you investigate the Kullback-Leibler divergence (KLD) in terms of the parameters $\boldsymbol{\theta} = (\mu, \sigma^2)^\top$.
\begin{enumerate}
\item Write down the KLD for the given setup.
\item Derive the gradients with respect to $\boldsymbol{\theta}$.
\item Is there an analytic solution for the optimal parameter setting? If yes, derive the corresponding solution. If no, give a short reasoning.
\item Independent of the previous exercise, state a numerical procedure to minimize the KLD.
\end{enumerate}
\item Sample points according to the true distribution and visualize the KLD for different parameter settings of the Gaussian distribution (including the optimal one if available).
\item Create a surface plot with axes $n$ and $p$ and colour value equal to the KLD for the optimal normal distribution.
\item Based on the previous result,
\begin{enumerate}
\item how can the behaviour for varying $p$ be explained?
\item how can the behaviour for varying $n$ be explained?
\end{enumerate}
\end{enumerate}
