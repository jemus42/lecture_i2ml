% !Rnw weave = knitr

<<setup-child, include = FALSE, echo=FALSE>>=
library('knitr')
knitr::set_parent("../../style/preamble_ueb_coll.Rnw")
@

\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\kopf{Supervised Regression}

\tableofcontents

% ------------------------------------------------------------------------------
% LECTURE EXERCISES
% ------------------------------------------------------------------------------

\dlz
\exlect
\lz

\aufgabe{HRO in \texttt{mlr3}}{
<<child="ex_rnw/ex_lrn.Rnw">>=
@
}
\dlz
\loesung{
<<child="ex_rnw/sol_lrn.Rnw">>=
@
}

\newpage

\aufgabe{Loss Functions}{
<<child="ex_rnw/ex_regr_losses.Rnw">>=
@
}
\dlz
\loesung{
<<child="ex_rnw/sol_regr_losses.Rnw">>=
@
}

\aufgabe{Polynomial Regression}{
<<child="ex_rnw/ex_polynomial.Rnw">>=
@
}
\dlz
\loesung{
<<child="ex_rnw/sol_polynomial.Rnw">>=
@
}

\aufgabe{Predicting \texttt{abalone}}{
<<child="ex_rnw/ex_lm_abalone.Rnw">>=
@
}
\dlz
\loesung{

See 
\href{https://github.com/slds-lmu/lecture_i2ml/blob/master/exercises/supervised-regression/ex_rnw/sol_abalone.R}{R code}
}

% ------------------------------------------------------------------------------
% PAST EXAMS
% ------------------------------------------------------------------------------

\dlz
\exexams
\lz

\aufgabeexam{WS2020/21}{second}{1}{

\begin{tabular}{ | c | c | c |}
\hline
ID  &  $\xv$  &  $y$  \\  \hline
1   &  0.0    &  0.0  \\
2   &  1.0    &  4.0  \\
3   &  1.5    &  5.5  \\
4   &  2.5    &  7.0  \\
5   &  3.5    &  6.0  \\
6   &  5.0    &  3.0  \\
7   &  6.0    &  2.0  \\
8   &  7.0    &  3.0  \\
9   &  8.0    &  8.0  \\
\hline
\end{tabular}

\begin{centering}

<<echo=FALSE, message=FALSE, warning = FALSE>>=
x <- c(0,1,1.5,2.5,3.5,5,6,7,8)
y <- c(0,4,5.5,7,6,3,2,3,8)
df = data.frame(x = x, y = y)
#knitr::kable(t(df))
@

\end{centering}

<<echo=FALSE, message=FALSE, warning = FALSE, fig.width=5, fig.height=3>>=
require(ggplot2)
knn_plot = ggplot(df)+
  geom_point(aes(x = x, y = y)) +
  scale_x_continuous(
    minor_breaks = seq(0, 8, by = 1), 
    breaks = seq(0, 8, by = 1),
    limits=c(0,8)) +
  scale_y_continuous(
    minor_breaks = seq(0, 8, by = 1),
    breaks = seq(0, 8, by = 1),
    limits=c(0,8)) +
  #geom_label(aes(x=x-0.5, y=y, label=y)) +
  theme_bw()

knn5 = rbind(df, df, df, df)
knn5$k = paste("k = ", rep(1:4, each = nrow(df)), sep = "")
ggplot(knn5)+
  geom_point(aes(x = x, y = y)) +
  scale_x_continuous(
    minor_breaks = seq(0, 8, by = 1),
    breaks = seq(0, 8, by = 1),
    limits=c(0,8)) +
  scale_y_continuous(
    minor_breaks = seq(0, 8, by = 1),
    breaks = seq(0, 8, by = 1),
    limits=c(0,8)) +
  facet_wrap("k", ncol = 2, scales = "free") +
  #geom_label(aes(x=x-0.5, y=y, label=y)) +
  theme_bw()
@

\begin{enumerate}
  \item Assume we use k-nearest neighbours regression with the L1 norm 
  (Manhattan distance) as a distance function. For every $k \in \{1, 2, 3, 4\}$, 
  do the following: 
  \begin{enumerate}
    \item[(i)] Mark the k-nearest neighbours of a new observation $\xv=4$ in the 
    graphic below. 
    \item[(ii)] Calculate the predicted value $\hat{y}$ for $\xv=4$ as the 
    unweighted mean of the k-nearest neighbours and draw it in the graphic 
    below.
  \end{enumerate}
  \item Would using the euclidean distance as distance measure in a) have made 
  a difference? Explain your answer.
\end{enumerate}  

\dlz

\loesung{

\begin{enumerate}
  \item $\{x_5\}$, $\{x_5, x_6\}$, $\{x_4, x_5, x_6\}$, $\{x_4, x_5, x_6, x_7\}$
  \item No. In the case of a single feature L1 and L2 are identical.
\end{enumerate} 
}

% ------------------------------------------------------------------------------
% INSPO
% ------------------------------------------------------------------------------

\dlz
\exinspo