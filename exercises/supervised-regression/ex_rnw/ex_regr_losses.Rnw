In this exercise, we will examine loss functions for regression tasks 
somewhat more in depth.

<<echo=FALSE, message=FALSE, fig.align="left", fig.height=2.5, fig.width=4>>=

set.seed(1L)
x <- runif(20L, min = 0L, max = 10L)
y <- 0.2 + 3 * x
y <- y + rnorm(length(x), sd = 0.8)

ggplot2::ggplot(data.frame(x = x, y = y), ggplot2::aes(x = x, y = y)) +
  ggplot2::geom_point() + 
  ggplot2::theme_bw() + 
  # ggplot2::geom_smooth(formula = y ~ x, method = "lm", se = FALSE) +
  ggplot2::annotate("point", x = 10L, y = 1L, color = "orange", size = 2)

@

\begin{enumerate}[a)]
  \item Consider the above linear regression task. How will the model 
  parameters be affected by adding the new outlier point (orange) if you use
  \begin{enumerate}[i)]
    \item $L1$ loss
    \item $L2$ loss
  \end{enumerate}
  in the empirical risk? (You do not need to actually compute the 
  parameter values.)
\end{enumerate}

<<echo=FALSE, message=FALSE, fig.align="left", fig.height=2.5, fig.width=4>>=

huber_loss <- function(res, delta = 0.5) {
  if (abs(res) <= delta) {
    0.5 * (res^2)
  } else {
    delta * abs(res) - 0.5 * (delta^2)
  }
}

x <- seq(-10L, 10L, length.out = 1000L)
y <- sapply(x, huber_loss, delta = 5L)

ggplot2::ggplot(data.frame(x = x, y = y), ggplot2::aes(x = x, y = y)) +
  ggplot2::geom_line() + 
  ggplot2::theme_bw()

@

\begin{itemize}
  \item[b)] The second plot visualizes another loss function popular in 
  regression tasks, the so-called \textit{Huber loss} (depending on 
  $\epsilon > 0$; here: $\epsilon = 5$). 
  Describe how the Huber loss deals with residuals as compared to $L1$ and $L2$ 
  loss.
  Can you guess its definition? 
  % \item[c)] \textcolor{orange}{Zu schwer?}
  % Show that $\mathit{median}(y)$ is the optimal constant prediction $c$
  % when using $L1$ loss. \\
  % Hint:
  % \begin{itemize}
  %   \item Employing the law of total expectation, we can find $c$ via
  %   $$\argmin_c \E\left[|y - c|\right] = \argmin_c
  %   \int_{-\infty}^\infty |y - c| ~ p(y) \text{d}y =
  %   \argmin_c \int_{-\infty}^c -(y - c)~p(y)~\text{d}y + \int_c^\infty
  %   (y - c)~p(y)~\text{d}y .$$
  %   \item Setting the derivative of this expression, found by application
  %   of Leibniz's rule, to zero yields
  %   $$0 \overset{^\text{!}}{=} \int_{-\infty}^c  ~p(y)~\text{d}y -
  %   \int_c^\infty ~p(y)~\text{d}y.$$
  % \end{itemize}
  \item[c)] Derive the least-squares estimator, i.e., the solution to the linear 
  model when using $L2$ loss, analytically via
  $$\thetabh = \argmint \| \yv - \Xmat \thetab \|_2^2.$$
\end{itemize}

