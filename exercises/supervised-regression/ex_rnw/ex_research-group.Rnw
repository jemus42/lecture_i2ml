A medical research group wants to train a supervised regression model to predict how long a patient stays in the hospital after admission (measured in days). They have gathered data on each patientâ€˜s age (in years), systolic blood pressure (in mmHg, a continuous scale), and weight (in kg). At first, they believe all of these three features should be modeled to have a linear effect on the target variable. To construct their supervised regression model, they remember how most of such models can be described:
\medbreak
\textbf{Learning = Hypothesis Space + Risk + Optimization.}
\medbreak
With the first of the three components of learning in his mind, researcher 1 comes up with the following hypothesis space:
\begin{align}
\Hspace = \{f: \fx = \theta_0 + \theta_1 \cdot age + \theta_2 \cdot blood\;pressure + \theta_3 \cdot weight \enspace | \enspace \thetab \in \N^{4}\}
\end{align}

\begin{enumerate}\bfseries
  \item[1)] Is the hypothesis space formulated above correct? If not, find the correct one.
\end{enumerate}

Researcher 2 recalls the second component of learning: risk. She remembers two slightly different functions for calculating the empirical risk, but is not sure what difference they will make in optimization:
\begin{align}
\riskef = & \sumin \Lxyi \\
\riskeb(f) = & \frac{1}{n}\sumin \Lxyi
\end{align}

\begin{enumerate}\bfseries
  \item[2)] What difference may the two functions make for optimization?
\end{enumerate}