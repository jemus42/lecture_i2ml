Assume the following (noisy) data-generating process: $y = 0.5 + \sin(2\pi x) + 
\epsilon$ with $\epsilon ~ \sim \mathcal{N}(0, 0.1)$.

<<echo=FALSE, message=FALSE, fig.align="left", fig.height=2.5, fig.width=4>>=

n <- 50L
x <- seq(0L, 1L, length.out = n)
set.seed(123L)
y <- 0.5 + 0.4 * sin(2 * pi * x) + rnorm(n, sd = 0.1)

ggplot2::ggplot(data.frame(x = x, y = y), ggplot2::aes(x = x, y = y)) +
  ggplot2::geom_point() + 
  ggplot2::theme_bw()

@

\begin{enumerate}[a)]
  \item We decide to model the data with a cubic polynomial (including intercept 
  term). State the corresponding hypothesis space.
  \item Demonstrate that this hypothesis space is simply a parameterized family 
  of curves by plotting in \texttt{R} curves for 3 different models belonging to 
  the considered model class.
  \item State the empirical risk w.r.t. $\thetab$ for a member of our hypothesis 
  space. Use $L2$ loss and be as explicit as possible.
  \item We can minimize this risk using gradient descent. In order to make 
  this somewhat easier, we will denote the transformed feature matrix, 
  containing $x$ to the power from 0 to 3, by $\tilde \Xmat$, such that we can 
  express our model by $\tilde \Xmat \thetab$ (note that the model is still 
  linear in its parameters, even if $\Xmat$ has been transformed in a non-linear 
  manner!). Derive the gradient of the empirical risk w.r.t $\thetab$.
  \item Using the result from d), state the calculation to update the current 
  parameter $\thetat$.
  \item You will not be able to fit the data perfectly with a cubic polynomial. 
  Should you opt for a more flexible model class (i.e., a hypothesis space 
  with higher capacity)? What might be disadvantageous about this?
\end{enumerate}