\begin{enumerate}[a)]
  \item $L2$ loss penalizes vertical distances to the regression line 
  \textit{quadratically}, while $L1$ only considers the \textit{absolute} 
  distance.
  As the outlier point lies pretty far from the remaining training data, it 
  will have a large loss with $L2$, and the regression line will pivot 
  to the bottom right to minimize the resulting empirical risk.
  A model trained with $L1$ loss is less susceptible to the outlier 
  and will adjust only slightly to the new data.
  
  
  \item The Huber loss combines the respective advantages of $L1$ and $L2$ loss: 
  it is smooth and (once) differentiable like $L2$ but does not punish larger 
  residuals as severely, leading to more robustness. 
  It is simply a (weighted) piecewise combination of both losses, 
  where $\epsilon$ marks where $L2$ transits to $L1$ loss. The exact definition 
  of Huber loss is:
    $$
  \Lxy = \begin{cases}
    \frac{1}{2}(y - \fx)^2  & \text{ if } |y - \fx| \le \epsilon \\
    \epsilon |y - \fx|-\frac{1}{2}\epsilon^2 \quad & \text{ otherwise }
    \end{cases}, \quad \epsilon > 0
  $$
  
  \item We solve this just like any other optimization problem: setting the 
  derivative to 0 and solving for $\thetab$.
  \begin{flalign*}
    % \mint \| \yv - \Xmat \thetab \|_2^2 \Longleftrightarrow 0 &= 
    \pd{\| \yv - \Xmat \thetab \|_2^2}{\thetab^\top} &= 0 \\ 
    \pd{\left( \left(\yv - \Xmat \thetab\right)^\top \left(\yv - \Xmat 
    \thetab \right) \right)}{\thetab^\top} &= 0 \\ 
    \pd{\left( \yv^\top \yv - 2 \thetab^\top \Xmat ^\top \yv +
    \thetab^\top \Xmat^\top \Xmat \thetab \right)}{\thetab^\top} &= 0 \\ 
    - 2 \Xmat^\top \yv + 2 \Xmat^\top \Xmat \thetab &= 0 \\
    \Xmat^\top \Xmat \thetab &= \Xmat^\top \yv \\
    \thetabh &= \olsest
  \end{flalign*}
  Don't be spooked by the matrix notation --
  just make sure you know basic linear algebra, e.g., $$(\Amat \mathbf{B})^\top
  = \mathbf{B}^\top \Amat^\top,$$ and make yourself familiar with the analogies 
  between scalar and matrix-valued calculations (e.g., $x^2$ translates 
  to $\Xmat^\top \Xmat$, and $\tfrac{1}{x}$ to $\Xmat^{-1}$).
  As this is a tool you will need to be able to handle, refresh your algebra 
  if necessary^. 
  
\end{enumerate}
