\begin{enumerate}[a)]
  \item $L2$ loss penalizes vertical distances to the regression line 
  \textit{quadratically}, while $L1$ only considers the \textit{absolute} 
  distance.
  As the outlier point lies pretty far from the remaining training data, it 
  will have a large loss with $L2$, and the regression line will pivot 
  to the bottom right to minimize the resulting empirical risk.
  A model trained with $L1$ loss is less susceptible to the outlier 
  and will adjust only slightly to the new data.
<<echo=FALSE, message=FALSE, fig.align="left", fig.height=3, fig.width=6>>=

set.seed(1L)
x <- runif(20L, min = 0L, max = 10L)
y <- 0.2 + 3 * x
y <- y + rnorm(length(x), sd = 0.8)
x <- c(x, 10L)
y <- c(y, 1L)
dt <- data.frame(x = x, y = y)

ggplot2::ggplot(dt, ggplot2::aes(x = x, y = y)) +
  ggplot2::geom_point() + 
  ggplot2::theme_bw() + 
  ggplot2::geom_smooth(
    formula = y ~ x, 
    method = "lm", 
    se = FALSE, 
    ggplot2::aes(col = "L2 loss"), 
    size = 0.7) +
  ggplot2::geom_quantile(quantiles = 0.5, ggplot2::aes(col = "L1 loss")) +
  ggplot2::scale_color_manual(
    "", values = c("L2 loss" = "blue", "L1 loss" = "red")) +
  ggplot2::annotate("point", x = 10L, y = 1L, color = "orange", size = 2)

@  
  
  
  \item The Huber loss combines the respective advantages of $L1$ and $L2$ loss: 
  it is smooth and (once) differentiable like $L2$ but does not punish larger 
  residuals as severely, leading to more robustness. 
  It is simply a (weighted) piecewise combination of both losses, 
  where $\epsilon$ marks where $L2$ transits to $L1$ loss. The exact definition 
  is:
    $$
  \Lxy = \begin{cases}
    \frac{1}{2}(y - \fx)^2  & \text{ if } \lone \le \epsilon \\
    \epsilon \lone-\frac{1}{2}\epsilon^2 \quad & \text{ otherwise }
    \end{cases}, \quad \epsilon > 0
  $$
  In the plot we can see how the parabolic shape of the loss around 0 evolves 
  into an absolute-value function at $\lone > \epsilon = 5$.
  
  \item We solve this just like any other optimization problem: setting the 
  derivative to 0 and solving for $\thetab$.
  \begin{flalign*}
    % \mint \| \yv - \Xmat \thetab \|_2^2 \Longleftrightarrow 0 &= 
    \pd{}{\thetab} \left \| \yv - \Xmat \thetab \right \|_2^2 &= 0 \\ 
    \pd{}{\thetab} \left( \left(\yv - \Xmat \thetab\right)^\top \left(\yv - \Xmat 
    \thetab \right) \right) &= 0 \\ 
    \pd{}{\thetab} \left( \yv^\top \yv - 2 \thetab^\top \Xmat ^\top \yv +
    \thetab^\top \Xmat^\top \Xmat \thetab \right) &= 0 \\ 
    - 2 \Xmat^\top \yv + 2 \Xmat^\top \Xmat \thetab &= 0 \\
    \Xmat^\top \Xmat \thetab &= \Xmat^\top \yv \\
    \thetabh &= \olsest
  \end{flalign*}
  Don't be spooked by the matrix notation --
  just make sure you know basic linear algebra, e.g., $$(\Amat \mathbf{B})^\top
  = \mathbf{B}^\top \Amat^\top,$$ and remind yourself of the analogies 
  between scalar and matrix-valued calculations (e.g., $x^2$ translates 
  to $\Xmat^\top \Xmat$, and $\tfrac{1}{x}$ to $\Xmat^{-1}$).
  As this is a tool you will need to handle frequently, refresh your algebra 
  if necessary. 
  
  A good reference in general is \enquote{Mathematics for Machine Learning} 
  by Deisenroth et al. (for the above, you might want to have a look at 
  \textit{vector calculus}), 
  freely available at \url{https://mml-book.github.io/book/mml-book.pdf}.
  
\end{enumerate}