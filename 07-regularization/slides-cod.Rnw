%This file is a child of preamble.Rnw in the style folder
%if you want to add stuff to the preamble go there to make
%your changes available to all childs


% Load all R packages and set up knitr
<<setup, child="../style/setup.Rnw", include = FALSE>>=
@

<<setup,, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
library(knitr)
library(mlr)
library(ggplot2)
library(BBmisc)
library(reshape)
@



\lecturechapter{Curse of Dimensionality}
\lecture{Introduction to Machine Learning}
\sloppy



\begin{vbframe}{Curse of Dimensionality: Example}
Assume that we are given $20$ emails, $10$ of them are spam and $10$ are not.

\lz 

Our goal is to predict if a new incoming mail is spam or not. 

\medskip

For each email, we extract the following features:

\begin{itemize}
\item frequency of exclamation marks (in \%)
\item the length of the longest sequence of capital letters
\item the frequency of certain words, e.g., \enquote{free} (in \%)
\item ... 
\end{itemize}

... and we could extract many more features!

\framebreak

\vspace*{1.5cm}

<<plot-example2, echo = FALSE, fig.width = 6, fig.height = 1, warnings =FALSE, message = FALSE, fig.align = 'center'>>=

spam = getTaskData(spam.task)

# cherry pick some good examples
idx = which(spam$type == "spam" & spam$charExclamation <= 0.5 & spam$capitalAve <= 5 & spam$charExclamation > 0)[5:15]
idx = c(idx, which(spam$type == "nonspam" & spam$charExclamation <= 0.5 & spam$capitalAve <= 5 & spam$charExclamation > 0)[5:15])

df = spam[idx, ]
# dfpred = data.frame(xmin = c(0, 0.25), xmax = c(0.25, 0.5), 
#                     ymin = c(-Inf, -Inf), ymax = c(Inf, Inf), 
#                     col = c("nonspam", "spam"))

p = ggplot()
p = p + theme_bw()
p = p + geom_vline(xintercept = c(0, 0.25, 0.5), colour = "grey")
p = p + geom_jitter(data = df, aes(x = charExclamation, y = 0, colour = type), height = 0, size = 1)
p = p + ylim(c(-0.05, 0.05)) + xlim(c(0, 0.5))
p = p + xlab("Number of exclamation marks")
p = p + theme(axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank(), panel.grid.major = element_blank(), axis.line = element_blank())
# p = p + geom_segment(data = dfpred, aes(x = xmin, xend = xmax, y = 0, yend = 0, colour = col), size = 2, alpha = 0.3)
p
@

Based on the number of exclamation marks, we train a very simple classifier (a decision stump with split point $\xb = 0.25$):

\begin{itemize}
\item We divide the input space in $2$ equally sized regions
\item In the second region $[0.25, 0.5]$, $7$ out of $10$ are spam
\item Given that at least $0.25\%$ of all letters are exclamation marks, an email is spam with a probability of $\frac{7}{10} = 0.7$
\end{itemize}
\framebreak


Let us feed more information into our classifier. We include a feature that contains the length of the longest sequence of capital letters:
\medskip

<<plot-example3, echo = FALSE, fig.width = 6, fig.height = 3, warnings =FALSE, message = FALSE, fig.align = 'center'>>=

p = ggplot(data = df, aes(x = charExclamation, y = capitalAve, colour = type))
p = p + theme_bw()
p = p + geom_jitter()
p = p + ylim(c(0, 5)) + xlim(c(0, 0.5))
p = p + xlab("Number of exclamation marks") + ylab("Longest sequence of capital letters")
p = p + geom_vline(xintercept = c(0, 0.25, 0.5), color = "grey")
p = p + geom_hline(yintercept = c(0, 2.5, 5), color = "grey")
p
@


\begin{itemize}
\item In the 1D case we had $20$ observations across $2$ regions
\item Now, the same number of observations is spread across $4$ regions
\end{itemize}
\framebreak


Let us further increase the dimensionality to 3 by using the frequency of the word \enquote{your} in an email.

\vspace*{-.6cm}
<<plot-example4, echo = FALSE, fig.height = 5, warnings =FALSE, message = FALSE, fig.align = 'center'>>=
library(scatterplot3d)

colors = c('#bbdf27', '#440154')
colors = colors[as.numeric(df$type)]
scatterplot3d(df[, c("charExclamation", "capitalAve", "your")], 
                xlim = c(0, 0.5), ylim = c(0, 5), zlim = c(0, 2), 
                pch = 16, color=colors, xlab = "Frequency of exclamation marks", 
                ylab = "Longest sequence of capital letters", zlab = "Frequency of word 'your'")

@

\framebreak

\begin{itemize}
\item When adding a third dimension, the same number of observations is spread across $8$ regions
\item In $4$ dimensions the data points are spread across $16$ cells, in $5$ dimensions across $32$ cells and so on ...
\item As dimensionality increases the data become \textbf{sparse}, some of the cells become empty
\item In each of the blocks, there might be too few data to understand the distribution of the data and to model it
\end{itemize}

\vspace*{-.5cm}

\begin{center}
\includegraphics[width = 0.6\textwidth]{figure_man/exponentialcubes.png}\\
\scriptsize{Bishop, Pattern Recognition and Machine Learning, 2006}
\end{center}

\end{vbframe}


\begin{vbframe}{Curse of dimensionality}


\begin{itemize}
\item The phenomenon of data becoming sparse in high-dimensional spaces is one effect of the \textbf{curse of dimensionality}
\item The \textbf{curse of dimensionality} refers to various phenomena that arise when analyzing data in high-dimensional spaces that do not occur in low-dimensional spaces
\item Our intuition about the geometry of a space is formed in two and three dimensions. 
\item This intuition is often misleading in high-dimensional spaces!
% \item Especially algorithms that build upon the distance between data points will fail
% \item There are many different manifestations of the curse of dimensionalty
\end{itemize}

\end{vbframe}

% \section{Geometry of High-Dimensional Spaces}

% \begin{vbframe}{The high-dimensional cube}

% \begin{itemize}
%   \item We embed a small cube with edge length $a$ inside a unit cube
%   \item How long does the edge length $a$ of this small hypercube have to be so that the hypercube covers $10\%, 20\%, ...$ of the volume of the unit cube (volume 1)?

%   \medskip
%   \begin{center}
%     \includegraphics[height = 4cm, width = 4cm]{figure_man/hypercube.png}
%   \end{center}

% \framebreak

% <<high-dimensionalcube-plot, echo = FALSE, fig.height = 3>>=
% grid = expand.grid(p = c(1, 3, 5, 7, 10), x = seq(0, 1, by = .001))
% grid$edgelen = with(grid, (x)^(1/p))

% p = ggplot(grid, aes(x = x, y = edgelen, colour = ordered(p), group = ordered(p))) 
% p = p + geom_line(show.legend = FALSE) 
% p = p + geom_vline(xintercept = .1)
% p = p + theme_bw()
% p = p + labs(x  = expression("Fraction of Volume"), y = "Edge length of cube")
% p = p + annotate("text", x = rep(.25, 5), color = pal_5, y = c(.3, .67, .79, .85, .91), label = paste0("p=",c(1, 3, 5, 7, 10)))
% p = p + geom_text(aes(x = .13, y = 0, label = "10 %"), colour = "black", text = element_text(size = 1))
% p
% @
% \medskip 

%   \begin{footnotesize}
%   \begin{eqnarray*}
%     a^p &=& \frac{1}{10} \Leftrightarrow a = \frac{1}{\sqrt[p]{10}}
%   \end{eqnarray*}
%   \end{footnotesize}
%   \vspace*{-0.5cm}
%   \item  So: covering $10\%$ of total volume in a cell requires cells with almost $50\%$ of the entire range in $3$ dimensions, $80\%$ in $10$ dimensions. 
% \end{itemize}

% \end{vbframe}

% \begin{columns}[T,onlytextwidth]
% \column{0.55\textwidth}
%   How long does the edge length $a$ of a hypercube have to be so that the hypercube covers 10\% of the volume of the unit cube (volume 1)?

%   \lz
%   \lz 


% \column{0.4\textwidth}
% <<curseofdim-hypercubesize-plot, echo=FALSE, fig.width = 4>>=
% d <- data.frame(p = 1:15, a = c(0.1, 10^-(1/(2:15))))

% p = qplot(data = d, x = p, y = a, geom = c("line", "point")) 
% p = p + scale_y_continuous(breaks = 1:10/10)
% p = p + theme_bw()
% p = p + scale_x_continuous(breaks = seq(1, 15, by = 3))  
% p = p + labs(y  = "a: Length of hypercube side", x = "p: Dimension of space")
% p

% @
% \end{columns}
% \end{vbframe}


% \begin{vbframe}{The high-dimensional sphere}


% Another manifestation of the \textbf{curse of dimensionality} is that the majority of data points are close to the outer edges of the sample.
 

% Consider a hypersphere of radius $1$. The fraction of volume that lies in the $\epsilon$-\enquote{edge}, $\epsilon := R - r$, of this hypersphere can be calculated by the formula

% \vspace*{-0.7cm}

% $$
% 1-(1-\epsilon)^p.
% $$

% \vspace*{-0.5cm}

% \begin{center}
% \includegraphics[width=0.9\textwidth]{figure_man/orange.png}
% \end{center}

% \vspace*{-0.5cm}

% If we peel a high-dimensional orange, there is almost nothing left. 

% \flushleft


% \framebreak

% Consider a $20$-dimensional sphere. Nearly all of the volume lies in its outer shell of thickness $0.2$:
% \medskip

% <<cursedim-fractionedge-plot, echo = FALSE, fig.height = 4>>=
% fractionedge <- expand.grid(p = c(1, 2, 5, 20), eps = seq(0,1,by=.01))
% fractionedge$vol <- with(fractionedge, 1-(1-eps)^p)

% p = ggplot(fractionedge, aes(x = eps, y = vol, colour = ordered(p), group = ordered(p))) 
% p = p + geom_line(show.legend = FALSE) 
% p = p + theme_bw()
% p = p + scale_x_continuous(breaks = seq(0, 1,l = 6)) 
% p = p + labs(x  = expression("Edge length"~epsilon), y = "Covered volume") 
% p = p + annotate("text", x = c(.5, .35, .21, .11), color = pal_4, y = c(.42, .5, .6, .75), label = paste0("p=",c(1, 2, 5, 20)))
% p = p + geom_vline(xintercept = 0.2, lty = 2)
% # p = p + geom_text(aes(x = .1, y = 0, label = "10 %"))
% p
% @

% \end{vbframe}


% \begin{vbframe}{Gaussians in high-dimensions}

% A further manifestation of the \textbf{curse of dimensionality} appears if we consider a standard Gaussian $N_p(\bm{0}, \id_p)$ in $p$ dimensions.

% \begin{itemize}
%     \item After transforming from Cartesian to polar coordinates and integrating out the directional variables, we obtain an expression for the density $p(r)$ as a function of the radius $r$ (i.e., the point's distance from the origin).
%     \item Thus $p(r) \delta r$ is the probability mass inside a thin shell of thickness $\delta r$ located at radius $r$. 
%     \framebreak 
%     \item We can see that for large $p$ the probability mass of the Gaussian is concentrated in a fairly thin \enquote{shell} rather far away from the origin.
% \end{itemize}

% \begin{center}
% \includegraphics[width=0.6\textwidth]{figure_man/cursedim_gaussian.png} \\
% \scriptsize{Bishop, Pattern Recognition and Machine Learning, 2006}
% \end{center}

% \normalsize

% \end{vbframe}

\section{COD and learning algorithms}

\begin{vbframe}{Example: k-NN}

Let's look at the performance of algorithms for increasing dimensionality. First, we consider the k-NN algorithm:

\begin{itemize}
\item In a high dimensional space, data points are spread across a huge space
\item The distance to the \textbf{next neighbor} becomes extremely large
\item The distance might even get that large that all points are \textbf{equally far} away - we cannot really determine the nearest neighbor anymore
\end{itemize}

\framebreak 

\framebreak 

<<nn-dist, eval = FALSE>>=
library(FNN)
library(purrr)

dist_p <- function(dims, n_per_dim = 1e4, k = 1) {
  x <- matrix(runif(dims * n_per_dim), ncol = dims)
  dist <- as.vector(dist(x))
  dist_nn <- FNN::knn.dist(x, k = k)
  data.frame(dim = dims,
    min = min(dist),
    mean = mean(dist),
    max = max(dist),
    mean_nn = mean(dist_nn),
    max_nn = max(dist_nn))
}

@

Minimal, mean and maximal (NN)-distances of $10^{4p}$ points uniformly distributed in the hypercube $[0,1]^p$:

<<nn-dist-table, results = 'asis'>>=
library(xtable)
distances <- readRDS("rsrc/distances.rds")
names(distances) <- c("$p$", "$\\min(d(x,y))$",  "$\\overline{d(x,y)}$", "$\\max(d(x,y))$",
  "$\\overline{d_{NN1}(x,x')}$", "$\\max(d_{NN1}(x,x'))$")
print(xtable(signif(distances, 2), digits = 2, display = rep("fg", 7), align = "rr|lllll"),
  row.names = FALSE, sanitize.colnames.function = function(x) x, include.rownames = FALSE,
  hline.after = 0, latex.environments = "small")
@

\framebreak

<<nn-dist-hist-plot, echo = FALSE, fig.height = 5.5, warnings =FALSE, message = FALSE, fig.align = 'center'>>=
dist_overall <- function(dims, n_per_dim = 1e3, k = 1) {
  x <- matrix(runif(dims * n_per_dim), ncol = dims)
  dist <- as.vector(dist(x))
  data.frame(distances = dist)
}

myplot = function(data, binwidth){
  p = ggplot(data, aes(x = distances))
  p = p + geom_histogram(binwidth = binwidth)
  p = p + theme_bw() 
  p 
}

plots = lapply(c(2, 5, 50, 500), function(p) {
  data = dist_overall(p)
  myplot(data, .05) + xlab(paste("p =", p)) + xlim(0, sqrt(p))
  })

do.call(grid.arrange, c(plots, ncol = 2, nrow = 2, top = "Histogram of distances for different dimensions"))
@

\framebreak

The consequences for the k-nearest neighbors approach can be summarized as follows:

\begin{itemize}
 \item At constant sample size $n$ and growing $p$, the distance between the observations increases exponentially\\
 $\rightarrow$ coverage of the $p$-dimensional space decreases\\
 $\rightarrow$ every point becomes isolated / far way from all other points
 \item The size of the neighborhood $N_k(x)$ also \enquote{increases}\\
       (at constant $k$) \\
       $\rightarrow$ it's no longer a \enquote{local} method.
 \item Reducing $k$ dramatically does not help much either,
since the fewer observations we average, the higher the variance of our fit.
 \item[$\rightarrow$] k-NN estimates get more inaccurate with increasing dimensionality of the data.
\end{itemize}

% \framebreak

% To demonstrate this, we generate an artificial data set of dimension $p$ as follows: We define $a = \frac{2}{\sqrt{p}}$ and generate

% \begin{itemize}
% \item with probability $\frac{1}{2}$ we generate a sample from class $1$ by sampling from a Gaussian with mean $\mu = (a, a, ..., a)$ and unit covariance matrix
% \item with probability $\frac{1}{2}$ we generate a sample from class $2$ by sampling from a  Gaussian with mean $- \mu = (-a, -a, ..., -a)$ and unit covariance matrix
% \end{itemize}

% % Let $x_1 \sim \mathcal{N}\left(\mu, \id\right)$ and $x_2 \sim \mathcal{N}\left(- \mu, \id\right)$.

% \framebreak 

% <<plot-knn-example, echo = FALSE, fig.height = 4>>=

% # 1d case
% d = 1
% a = rep(2 / sqrt(d), d)
% x = seq(-6, 6, length.out = 500)

% datagrid = data.frame(x = x)
% datagrid$`1` = dnorm(datagrid$x, mean = a)
% datagrid$`2` = dnorm(datagrid$x, mean = - a)
% df = melt(datagrid, id.vars = "x")

% samples = data.frame(`1` = rnorm(10, mean = a), `2` = rnorm(10, mean = -a))
% samplesr = melt(samples)
% levels(samplesr$variable) = c(1, 2)

% p1 = ggplot()
% p1 = p1 + theme_bw()
% p1 = p1 + geom_line(data = df, aes(x = x, y = value, colour = variable))
% p1 = p1 + geom_point(data = samplesr, aes(x = value, y = 0, colour = variable))
% p1 = p1 + xlab("x") + ylab("density") + labs(colour = "class")
% p1 = p1 + ggtitle(paste("Dimension p =", d)) + theme(legend.position = "none")
% p1 = p1 + coord_fixed(xlim = c(- 5, 5), ylim = c(0, 0.5), ratio = 20)

% # 2d case
% d = 2
% a = rep(2 / sqrt(d), d)
% x = seq(-6, 6, length.out = 100)
% datagrid = expand.grid(x1 = x, x2 = x)
% datagrid$`1` = dmvnorm(datagrid[, 1:d], mean = a)
% datagrid$`2` = dmvnorm(datagrid[, 1:d], mean = - a)
% df = melt(datagrid, id.vars = c("x1", "x2"))
% datagrid$density = datagrid$`1` + datagrid$`2`

% samples = data.frame(class = rep(1:2, each = 10))
% samples = cbind(samples, t(vapply(samples$class, function(x) rmvnorm(1, mean = rep((- 1)^(x + 1), 2)), numeric(2))))
% samples$class = factor(samples$class)
% names(samples)[2:3] = c("x1", "x2")

% p2 = ggplot()
% p2 = p2 + theme_bw()
% p2 = p2 + geom_raster(data = datagrid, aes(x = x1, y = x2, fill = density))
% p2 = p2 + geom_contour(data = df, aes(x = x1, y = x2, z = value, colour = variable))
% p2 = p2 + coord_fixed(xlim = c(- 5, 5), ylim = c(- 5, 5), ratio = 1)
% p2 = p2 + scale_fill_gradient(low = "#2B2B2B", high = "white")
% p2 = p2 + geom_point(data = samples, aes(x = x1, y = x2, colour = class), size = 2)
% p2 = p2 + ggtitle(paste("Dimension p =", d))
% p2 = p2 + labs(colour = "Class")


% grid.arrange(p1, p2, nrow = 1)
% @
% \framebreak

% This example is constructed such that the Bayes error is always constant and does not depend on the dimension $p$. 

% \lz 

% The Bayes optimal classifiers predicts $\hat y = 1$ iff

% \begin{footnotesize}
%   \begin{eqnarray*}
%   \P\left(y = 1 ~|~ \xb\right) &=& \frac{p(\xb ~|~y = 1)\P(y = 1)}{p(\xb)} = \frac{1}{2} \cdot \frac{p(\xb ~|~y = 1)}{p(\xb)}\\
%   &\ge& \frac{1}{2} \cdot \frac{p(\xb ~|~y = 2)}{p(\xb)} \\ &=& \frac{p(\xb ~|~y = 2)\P(y = 2)}{p(\xb)} = \P\left(y = 2 ~|~ \xb\right). 
%   \end{eqnarray*}
% \end{footnotesize}

% This is equivalent to 

% \vspace*{-0.5cm}

% \begin{footnotesize}
%   \begin{eqnarray*}
%   \hat y = 1 &\Leftrightarrow& \exp\left(-\frac{1}{2} \left(\xb - \mu\right)^\top \left(\xb - \mu\right)\right) \ge \exp\left(-\frac{1}{2} \left(\xb + \mu\right)^\top \left(\xb + \mu\right)\right) \\
%   &\Leftrightarrow& \xb^\top \mu \ge 0. 
%   \end{eqnarray*}
% \end{footnotesize}

% <<plot-knn-example3, echo = FALSE, fig.height = 4>>=
% p3 = p1 + geom_rect(data = data.frame(xmin = c(0, -Inf), xmax = c(Inf, 0), ymin = c(-Inf, -Inf), ymax = c(Inf, Inf), group = as.factor(c(1, 2))), mapping=aes(xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax, fill=group), color="black", alpha=0.2) 
% p3
% @

% \framebreak 

% The Bayes error corresponds to the shaded area in the following plot: 

% <<plot-knn-example4, echo = FALSE, fig.height = 4>>=
% a = 2
% p4 = p1 + stat_function(data = data.frame(x = c(1, 2)), aes(x), fun = function(x) dnorm(x, mean = -a ), geom = "area", alpha = 0.2, xlim = c(0, 5)) 
% p4 = p4 + stat_function(data = data.frame(x = c(1, 2)), aes(x), fun = function(x) dnorm(x, mean = a ), geom = "area", alpha = 0.2, xlim = c(- 5, 0)) 
% p4
% @

% \framebreak 

% We can calculate the corresponding expected misclassification error (Bayes error)

% \begin{footnotesize}
% \begin{eqnarray*}
% && \frac{1}{2} \cdot p(\xb^\top \mu \ge 0 ~|~ y = 2) + \frac{1}{2} \cdot p(\xb^\top \mu \le 0 ~|~ y = 1) \\
% &\overset{\text{symm.}}{=}& p(\xb^\top \mu \le 0 ~|~ y = 1) = p\left(\sum_{i = 1}^p a \xb_i \le 0 ~|~ y = 1\right)\\
%   &=& p\left(\sum_{i = 1}^p \xb_i \le 0 ~|~ y = 1\right). 
% \end{eqnarray*}
% \end{footnotesize}

% $\sum_{i = 1}^p \xb_i ~|~ y = 1\sim \mathcal{N}(p \cdot a,~ p)$ because it is the sum of independent normal random variables $\xb_i ~|~ y = 1 \sim \mathcal{N}\left(a, 1\right)$ (the vector $\xb ~|~ y = 1$ follows a $\mathcal{N}\left(\mu, \id\right)$ distribution with $\mu = \left(a, ..., a\right)$). 

% \framebreak 

% We get for the Bayes error: 

% \begin{eqnarray*}
%  &=& p\left(\frac{\sum_{i = 1}^p \xb_i - p \cdot a}{\sqrt{p}} \le \frac{- p\cdot a}{\sqrt{p}}\right) \\ &=& \Phi(- \sqrt{p} a) \overset{a = \frac{2}{\sqrt{p}}}{=} \Phi(- 2) \approx 0.0228,
% \end{eqnarray*}

% where $\Phi$ is the distribution function of a standard normal random variable. 

% \lz 

% We see that the Bayes error is independent of $p$. 

% <<plot-knn-example5, echo = FALSE, fig.height = 2.5, fig.align = 'center', include = FALSE>>=

% bres = readRDS("code/cod_knn.rds")
% p = ggplot(data = bres[bres$learner == "Bayes Optimal Classifier", ], aes(x = d, y = mmce.test.mean, colour = learner))
% p = p + theme_bw()
% p = p + geom_line()
% p = p + xlab("d") + ylab("Mean Misclassification Error") + labs(colour = "Learner")
% p = p + ylim(c(0, 0.4))
% p
% @
% \framebreak

% We also train a k-NN classifier for $k = 3, 7, 15$ for increasing dimensions and monitor its performance (evaluated by $10$ times repeated $10$-fold CV).
% \medskip

% <<plot-knn-example6, echo = FALSE, fig.height = 2.5, fig.align = 'center'>>=
% bres = readRDS("code/cod_knn.rds")
% p = ggplot(data = bres, aes(x = d, y = mmce.test.mean, colour = learner))
% p = p + theme_bw()
% p = p + geom_line()
% p = p + xlab("d") + ylab("Mean Misclassification Error") + labs(colour = "Learner")
% p = p + ylim(c(0, 0.4))
% p
% @

% $\to$ k-NN deteriorates quickly with increasing dimension

\end{vbframe}


\begin{vbframe}{Example: Linear Model}
We also investigate how the linear model behaves in high dimensional spaces.

\begin{itemize}
\item We take the Boston Housing data set, where the value of houses in the area around Boston is predicted based on $14$ features describing the region (e.g. crime rate, status of the population, etc.).
\item We train a linear model on the data. 
\item We artificially create a high-dimensional dataset by adding $100, 200, 300, ...$ noise variables (containing no information at all) and look at the performance of a linear model trained on this modified data ($10$ times repeated $10$-fold CV).
\end{itemize}
\framebreak

We compare the performance of a linear model to that of a regression tree.

\lz 

<<plot-lm-example1, echo = FALSE, fig.height = 2.5, fig.align = 'center'>>=
bres = readRDS("code/cod_lm_rpart.rds")
bres = melt(bres, id.vars = c("task.id", "learner.id", "d"))
p = ggplot(data = bres[bres$d != 500, ], aes(x = d, y = value, colour = learner.id))
p = p + geom_line()
p = p + xlab("Number of noise variables") + ylab("Mean Squared Error") + labs(colour = "Learner")
p = p + ylim(c(0, 300))
p
@

$\rightarrow$ The LM struggles with the added noise features, while our tree seems to nicely filter them out.

\vfill

\begin{footnotesize}
\textbf{Note}: Trees automatically perform feature selection as only one feature at a time is considered for splitting (the smaller the depth of the tree, the less features are selected). Thus, they often perform well in high-dimensional settings. 
\end{footnotesize}

\end{vbframe} 

\begin{vbframe}{COD: Ways out}

Many methods besides the LM struggle with the curse of dimensionality. A large part of ML is concerned with dealing with this problem and finding ways around it.

\medskip

Possible approaches are:
\begin{itemize}
\item Increasing the space coverage by gathering more observations (not always viable in practice!)
\item Reducing the number of dimensions before training (e. g. by using domain knowledge, PCA or feature selection)
\item Regularization
\end{itemize}


\end{vbframe}




\endlecture