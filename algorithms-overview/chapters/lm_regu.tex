\begin{frame}{Regularized LM -- Functionality}

\highlight{General idea}

\begin{itemize}
  \item Unregularized LM: risk of \textbf{overfitting} in high-dimensional 
  space with only few observations
  \item \textbf{Goal}: find compromise between model fit and generalization
\end{itemize}

\medskip

% \highlight{Hypothesis space} ~~\\
% $\Hspace = \left\{f: \Xspace \to \R ~|~\fx = \phi(\thetab^\top \xv)\right\}$, 
% where $\phi(\cdot)$ is a transformation function.


%$\Hspace = \{ \theta_0 + \thx\ |\ (\theta_0, \thetab) \in \R^{p+1} \} $

\medskip

\highlight{Empirical risk}

\begin{itemize}
  \item Empirical risk function \textbf{plus complexity penalty} 
  $J(\thetab)$, controlled by shrinkage parameter $\lambda > 0$: \\
  $\riskrt := \risket + \lambda \cdot J(\thetab).$ 
  \item Popular regularizers
  \begin{itemize} 
    \item \textbf{Ridge} regression: L2 penalty $J(\thetab) = \|\thetab\|_2^2 $
    \item \textbf{LASSO} regression: L1 penalty $J(\thetab) = \|\thetab\|_1 $
  \end{itemize}
\end{itemize}

\medskip

\highlight{Optimization}
\begin{itemize}
  \item \textbf{Ridge}: analytically with 
  $\thetabh_{\text{Ridge}} = (\Xmat^\top \Xmat  + \lambda \id)^{-1} \Xmat^\top 
  \yv$
  \item \textbf{LASSO}: numerically with, e.g., (sub-)gradient descent
\end{itemize}

\medskip

\highlight{Hyperparameters} ~~ Shrinkage parameter $\lambda$

\medskip

% \highlight{Runtime behavior} ~~ $\mathcal{O}(p^2 \cdot n + p^3)$ for $n$ 
% observations and $p$ features 

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Regularized LM -- Practical hints}

\highlight{Choice of regularization parameter}

\begin{itemize}
  \item Standard hyperparameter optimization problem
  \item E.g., choose $\lambda$ with minimum mean cross-validated error 
  (default in R package \texttt{glmnet})
\end{itemize}

\medskip

\highlight{Ridge vs. LASSO} 

\begin{itemize}
  \item \textbf{Ridge}
  \begin{itemize} 
    \item Overall smaller, but still dense $\thetab$
    \item Suitable with many influential features present, handling correlated 
    features by shrinking their coefficients equally
  \end{itemize}
  \item \textbf{LASSO}
  \begin{itemize} 
    \item Actual variable selection
    \item Suitable for sparse problems, ineffective with correlated 
    features (randomly selecting one)
  \end{itemize}  
  \item Neither overall better -- compromise: \textbf{elastic net} \\
  $\rightarrow$ weighted 
  combination of Ridge and LASSO regularizers
\end{itemize}

\medskip

\highlight{Implementation}

\begin{itemize}
    \item \textbf{R:} \texttt{mlr3} learners \texttt{LearnerClassifGlmnet} / 
    \texttt{LearnerRegrGlmnet}, calling \texttt{glmnet::glmnet()}
    \item \textbf{Python:} \texttt{LinearRegression} from package 
    \texttt{sklearn.linear\_model}, package for advanced statistical parameters 
    \texttt{statsmodels.api} 
  \end{itemize}

\end{frame}
