\documentclass{beamer}

\usepackage[orientation=landscape,size=a0,scale=1.4,debug]{beamerposter}
\mode<presentation>{\usetheme{mlr}}

\usepackage[utf8]{inputenc} % UTF-8
\usepackage[english]{babel} % Language
\usepackage{hyperref} % Hyperlinks
\usepackage{ragged2e} % Text position
\usepackage[export]{adjustbox} % Image position
\usepackage[most]{tcolorbox}
%\usepackage{nomencl}
%\makenomenclature
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}

\input{basic-math.tex}
\input{basic-ml.tex}
\input{ml-bagging.tex}
\input{ml-boosting.tex}
\input{ml-trees.tex}


\title{I2ML :\,: CHEAT SHEET} % Package title in header, \, adds thin space between ::
\newcommand{\packagedescription}{ % Package description in header
	The \textbf{I2ML}: Introduction to Machine Learning course offers an introductory and applied overview of "supervised" Machine Learning. It is organized as a digital lecture.
}

\newlength{\columnheight} % Adjust depending on header height
\setlength{\columnheight}{84cm} 

\newtcolorbox{codebox}{%
	sharp corners,
	leftrule=0pt,
	rightrule=0pt,
	toprule=0pt,
	bottomrule=0pt,
	hbox}

\newtcolorbox{codeboxmultiline}[1][]{%
	sharp corners,
	leftrule=0pt,
	rightrule=0pt,
	toprule=0pt,
	bottomrule=0pt,
	#1}

\begin{document}
\begin{frame}[fragile]{}
\begin{columns}
	\begin{column}{.31\textwidth}
		\begin{beamercolorbox}[center]{postercolumn}
			\begin{minipage}{.98\textwidth}
				\parbox[t][\columnheight]{\textwidth}{				
					\begin{myblock}{Basic Notations}
						Important \textbf{notations} used in the whole course  
						\\
						\begin{codebox}
						    $\Xspace$ : $p$-dim. \textbf{input space} $\pdfxyt$
						\end{codebox}
						\hspace*{1ex}Usually we assume $\Xspace = \R^p$, but categorical \textbf{features} can occur as well.
						\\
						\begin{codebox}
							$\Yspace$ : \textbf{target space}
						\end{codebox}
						\hspace*{1ex}For example, $\Yspace = \R$, $\Yspace = \lbrace 0, 1 \rbrace$, $\Yspace = \lbrace -1, 1 \rbrace$, $\Yspace = \gset$ or \hspace*{1ex}$\Yspace = \lbrace \textrm{label}_1 \ldots \textrm{label}_g \rbrace$.
						\\
						\begin{codebox}
							$x$ : \textbf{feature vector}
						\end{codebox}
						\hspace*{1ex}$x = \xvec \in \Xspace$.
						\\
						\begin{codebox}
							$y$ : \textbf{target / label / output}
						\end{codebox}
						\hspace*{1ex}$y \in \Yspace$.
						\\
						\begin{codebox}
							 $\P_{xy}$ : \textbf{probability distribution}
						\end{codebox}
						\hspace*{1ex}Joint probability distribution on $\Xspace \times \Yspace$.
						\\
						\begin{codebox}
							$\pdfxy$ or $\pdfxyt$ : \textbf{joint pdf}
						\end{codebox}
						\hspace*{1ex}Joint probability distribution function for $x$ and $y$.\\
						
						\textbf{Note:} This lecture is mainly developed from a frequentist perspective. If parameters appear behind the $|$, this is for better reading, and does not imply that we condition on them in a Bayesian sense (but this notation would actually make a Bayesian treatment simple). So formally, $p(x | \theta)$ should be read as $p_\theta(x)$ or $p(x, \theta)$ or $p(x; \theta)$.

					\end{myblock}
					\begin{myblock}{Definitions}
						\begin{codebox}
							$\xyi$ : $i$-th \textbf{observation} or \textbf{instance}
						\end{codebox}
						\hspace*{1ex}
						\begin{codebox}
						$\D = \Dset$
						\end{codebox}
						\hspace*{1ex}\textbf{data set} with $n$ observations.
					\end{myblock}\vfill
				}
			\end{minipage}
		\end{beamercolorbox}
	\end{column}
	\begin{column}{.31\textwidth}
		\begin{beamercolorbox}[center]{postercolumn}
			\begin{minipage}{.98\textwidth}
				\parbox[t][\columnheight]{\textwidth}{
					\begin{myblock}{}
						\begin{codebox}
							$\Dtrain$, $\Dtest$ : data for training and testing
						\end{codebox}
						\hspace*{1ex}Often, $\D = \Dtrain \dot{\cup} ~ \Dtest$.
						\\
						\begin{codebox}
							$\fx$ or $\fxt \in \R$ or $\R^g$ : prediction function (\textbf{model}) learned
						\end{codebox}
						\begin{codebox}
						    from data
						\end{codebox}
						\hspace*{1ex}We might suppress $\theta$ in notation.
						\\
						\begin{codebox}
							 $\hx$ or $\hxt \in \Yspace$
						\end{codebox}
						\hspace*{1ex}Discrete prediction for classification.
						\\
						\begin{codebox}
							$\thetab = (\theta_1, \theta_2, ..., \theta_d) \in \Theta$ : model \textbf{parameters}
						\end{codebox}
						\hspace*{1ex}Some models may traditionally use different symbols.
						\\
						\begin{codebox}
							$\Hspace$ : \textbf{hypothesis space}
						\end{codebox}
						\hspace*{1ex}$f$ lives here, restricts the functional form of $f$.
						\\
						\begin{codebox}
				            $\eps = y - \fx$ or $\epsi = \yi - \fxi$
						\end{codebox}
						\hspace*{1ex}\textbf{Residual} in regression.
						\\
						\begin{codebox}
				             $\yf$ or $\yfi$ : \textbf{margin} for binary classification
						\end{codebox}
						\hspace*{1ex}With, $\Yspace = \{-1, 1\}$.
						\\
						\begin{codebox} $\pikx = \postk$: \textbf{posterior probability} for class $k$, given $x$
						\end{codebox}
						\hspace*{1ex}In case of binary labels we might abbreviate $\pix = \post$.
						\\
						\begin{codebox}
						$\pi_k = \P(y = k)$:\textbf{ prior probability} for class $k$
						\end{codebox}
						\hspace*{1ex}In case of binary labels we might abbreviate $\pi = \P(y = 1)$.
						\\
						\begin{codebox}
						$\LLt$ and $\llt$ : Likelihood and log-Likelihood for a parameter $\theta$
						\end{codebox}
						\hspace*{1ex}These are based on a statistical model.
						\\
						\begin{codebox}
						 $\yh$, $\fh$, $\hh$, $\pikxh$, $\pixh$ and $\thetah$
						\end{codebox}
						\hspace*{1ex}These are learned functions and parameters ( These are estimators of \hspace*{1ex}corresponding functions and parameters).\\
						
						\textbf{Note:} With a slight abuse of notation we write random variables, e.g., $x$ and $y$, in lowercase, as normal variables or function arguments. The context will make clear what is meant.
						\end{myblock}
				}
			\end{minipage}
		\end{beamercolorbox}
	\end{column}
	\begin{column}{.31\textwidth}
		\begin{beamercolorbox}[center]{postercolumn}
			\begin{minipage}{.98\textwidth}
				\parbox[t][\columnheight]{\textwidth}{
				    \begin{myblock}{Important terms}
				        \begin{codebox}
						    \textbf{Model: } (or hypothesis) $f : \mathcal{X} \rightarrow \mathcal{Y}$ maps inputs (or input features)
						\end{codebox}
						\begin{codebox}
						    to outputs (or targets).
						\end{codebox}
						\hspace*{1ex}
						\begin{codebox}
						\textbf{Learner: } takes a data set with features and outputs (\textbf{training set},
						\end{codebox}
						\begin{codebox}
						$\in \mathcal{X}\, \times \,\mathcal{Y}$)  and produces a \textbf{model} (which is a function $f:\, \mathcal{X} \to \mathcal{Y}$)
						\end{codebox}
						\hspace*{1ex}
						\begin{codebox}
						\textbf{Inducer: }An inducer takes a data set with features
						\end{codebox}
						\hspace*{1ex}Outputs (\textbf{training set} $\in \mathcal{X}\, \times \,\mathcal{Y}$)  and produces a \textbf{model} (which is a \hspace*{1ex}function $f:\, \mathcal{X} \to \mathcal{Y}$).
						\\
						\begin{codebox}
						    \textbf{Learning = Representation + Evaluation  + Optimization}.
						\end{codebox}
						\hspace*{1ex}
						\begin{codebox}
							 \textbf{Representation: }(Hypothesis space) Defines which kind of model
						\end{codebox}
						\begin{codebox}
						 structure of \(f\) can be learned from the data.
						\end{codebox}
						\hspace*{1ex}Example: Linear functions, Decision trees etc.
						\\
						\begin{codebox}
						    \textbf{Evaluation: }A metric that quantifies how well a specific model performs
						\end{codebox}
						\begin{codebox}
						    on a given data set. Allows us to rank candidate models in order to
						\end{codebox}
						\begin{codebox}
						    choose the best one.
						\end{codebox}
						\hspace*{1ex}Example: Squared error, Likelihood etc.
						\\
						\begin{codebox}
							 \textbf{Optimization: }Efficiently searches the hypothesis space for good models.
						\end{codebox}
						\hspace*{1ex}Example: Gradient descent, Quadratic programming etc.
						\\
						\begin{codebox}
							 \textbf{Loss function: }The \enquote{goodness} of a prediction $\fx$ is measured by
						\end{codebox}
						\begin{codebox}
							 a loss function $\Lxy$
						\end{codebox} \hspace*{1ex}Through \textbf{loss}, we calculate the prediction error and the choice of the \hspace*{1ex}loss has a major influence on the final model
						\\
	                    \begin{codebox}
							 \textbf{Risk Minimization: }The ability of a model $f$ to reproduce the association
						\end{codebox}
						\begin{codebox}
							 between $x$ and $y$ that is present in the data $\D$ can be measured by the
						\end{codebox}
						\begin{codebox}
							average loss: the \textbf{empirical risk}.
						\end{codebox}
						$$\riske(f) = \frac{1}{n} \sumin \Lxyi.$$
						\\
						\hspace*{1ex}Learning then amounts to \textbf{empirical risk minimization} -- figuring out \hspace*{1ex}which model $f$ has the smallest average loss:$$\fh = \argmin_{f \in \Hspace} \riske(f).$$
					\end{myblock}
				}
			\end{minipage}
		\end{beamercolorbox}
	\end{column}
\end{columns}
\end{frame}
\end{document}
