## Loss Minimization

<!-- - Assume features  $x \in \R^p$, which shall be used to predict a dependent variable $y$.  -->
- Let $\mathbb P_{xy}$ be the joint distribution of $x$ and $y$. This defines all aspects of
  the generating process where our data comes from. Sadly, we usually do not $\mathbb P_{xy}$.

- The goodness of predictions -- per point -- of a model $y=\fxt$ -- w.r.t. a parameter vector $\theta$ --
  is measured by a loss function, e.g. squared error:
  \vspace{-0.2cm}\[ L(y, f(x))) = (y-\fxt)^2 \]
<!--
  and its expectation (**risk**)
  \vspace{-0.2cm}\[ E_P[L(y,f(x))] = \int L(y, f(x)) d\mathbb P_{xy}. \]
-->

- Obvious aim: minimization of prediction error over all $\theta$ with respect to data distribution $\mathbb P_{xy}$.
  This error is called the (true / theoretical) risk.


## Risk Minimization

- As we usually do not know / cannot compute the theoretical risk, we approximate this on our
  empirical data. This is called empirical risk:

\[
\risket = \sumin \Lxyit
\]

- Then minimize it over $\theta$. This is called empirical risk minimization:

 \[
 \argmin_\theta \risket = \argmin_\theta \sumin \Lxyit
 \]
