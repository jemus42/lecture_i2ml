## Classification
```{r, include=FALSE, cache=FALSE}
library(mlr)

n = 30
set.seed(1234L)
tar = factor(c(rep(1L, times = n), rep(2L, times = n)))
feat1 = c(rnorm(n, sd = 1.5), rnorm(n, mean = 2, sd = 1.5))
feat2 = c(rnorm(n, sd = 1.5), rnorm(n, mean = 2, sd = 1.5))
bin.data = data.frame(feat1, feat2, tar)
bin.tsk = makeClassifTask(data = bin.data, target = "tar")

ap = adjust_path(paste0(getwd(), "/figure"))
ap.code = adjust_path(paste0(getwd(), "/code"))
```

- Feature vector $x \in \Xspace$
- Label $y \in \Yspace = \gset$
- Observations of $x$ and $y$: $\D = \Dset$

Classification usually means to construct $g$ discriminant functions $f_1(x), \ldots f_g(x)$,
so that we choose our class as
$$h(x) = \argmax_k f_k(x)$$

## Classification

This divides the feature space into $g$ *decision regions* $\{x \in \Xspace \mid h(x) = k\}$.
These regions are separated by the *decision boundaries* where ties occur between these
regions.

```{r, echo=FALSE, fig.height=4,message=FALSE}
plotLearnerPrediction("classif.kknn", iris.task, k = 7, cv = 0, prob.alpha = FALSE)
```

## Classification

If these functions $f_k(x)$ can be specified as linear functions,
we will call the classifier a \emph{linear classifier}. We can write a
class separating decision boundary as $x^T\theta = \text{const}$.

```{r, echo=FALSE, fig.height=4,message=FALSE}
plotLearnerPrediction("classif.lda", iris.task, cv = 0, prob.alpha = FALSE)
```

