## $K$-medoids 

```{r, echo=FALSE, results='hide'}
Colors = c("#000000", "#E69F00", "#56B4E9", "#009E73", 
           "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
palette(Colors)
add.alpha <- function(col, alpha=1){
  if(missing(col))
    stop("Please provide a vector of colours.")
  apply(sapply(col, col2rgb)/255, 2, 
                     function(x) 
                       rgb(x[1], x[2], x[3], alpha=alpha))  
}
```

$K$-medoids clustering 

- is strongly related to $K$-means and is realized by the **Partitioning Around Medoids (PAM)** algorithm.
- uses cluster medoids as representative clusters, i.e. **real data points** instead of **artificial data points** (such as the cluster centers as in $K$-means) are used. 
- is less sensitive to outliers and more robust than $K$-means.
- can handle categorical features ($K$-means doesn't because it is based on calculating the cluster centers by taking the mean in each dimension).

<!-- - User also needs to specifiy $K$, which is the number of clusters. -->

## The PAM algorithm

1. **Initialization:** Randomly select $K$ data points as the medoids. 
2. **Assignment:** Assign each data point $x_i$ to its closest medoid $m$ and calculate the within-cluster variation for each medoid (by summing up the distances of the current medoid $m$ to all other data points associated to $m$) .
3. **Update:** Swap $m$ and $x_i$ and recompute the within-cluster variation to see if another medoid is more appropriate. Select the medoid $m$ with the lowest within-cluster variation.
4. Repeat steps (2) and (3) until medoids do not change.

## The PAM algorithm

The PAM algorithm typically uses the following two metrics to compute distances:

- The euclidean distance (root sum-of-squares of differences).
- The Manhattan distance (the sum of absolute distances).

**Note:** The Manhattan distance should give more robust results if your data contains outliers.
In all other cases, the results will be similar for both metrics.

## K-means vs. K-medoids

```{r, fig.width = 10, fig.height = 6, fig.align="center", out.width="\\textwidth", echo=FALSE, message=FALSE, results="asis"}
# Compute either K-means clustering or K-medoids clustering
#
# Arguments:
# x: data matrix, n observations (rows) by p features (cols).
# centers: a vector giving the starting centers. Defaults
#   to NULL in which case we choose k centers at random.
# k: number of clusters. If the centers argument is specified,
#   then this doesn't need to be specified.
# alg: algorithm. Can be either "kmeans" or "kmedoids". 
#   Defaults to "kmeans".
# maxiter: Maximum number of iterations before we quit. 
#   Defaults to 100.
#
# Returns:
# centers: a vector of length k, giving the final centers.
# cluster: a vector of length n, giving the final clustering
#   assignments.
# iter: number of iterations performed.
# cluster.history: a matrix of dimension iter by k, each row
#   giving the cluster assignments at the corresponding iteration.
# 
kclust = function(x, centers=NULL, k=NULL, alg="kmeans", maxiter=100) {
  n = nrow(x)
  p = ncol(x)
  if (is.null(centers)) {
    if (is.null(k)) stop("Either centers or k must be specified.")
    if (alg=="kmeans") centers = matrix(runif(k*p,min(x),max(x)),nrow=k)
    else centers = x[sample(n,k),]
  }
  k = nrow(centers)
  cluster = matrix(0,nrow=0,ncol=n)

  for (iter in 1:maxiter) {
    cluster.new = clustfromcent(x,centers,k)
    centers.new = centfromclust(x,cluster.new,k,alg)

    cluster = rbind(cluster,cluster.new)
    j = is.na(centers.new[,1])
    if (sum(j)>0) centers.new[j,] = centers[j,]
    centers = centers.new

    if (iter>1 & sum(cluster[iter,]!=cluster[iter-1,])==0) break
  }
  return(list(centers=centers,cluster=cluster[iter,],iter=iter,cluster.history=cluster))
}

# Compute clustering assignments, given centers
clustfromcent = function(x, centers, k) {
  n = nrow(x)
  dist = matrix(0,n,k)
  for (i in 1:k) {
    dist[,i] = colSums((t(x)-centers[i,])^2)
  }
  return(max.col(-dist,ties="first"))
}

# Compute centers, given clustering assignments
centfromclust = function(x, cluster, k, alg) {
  if (alg=="kmeans") return(avgfromclust(x,cluster,k))
  else return(medfromclust(x,cluster,k))
}

avgfromclust = function(x, cluster, k) {
  p = ncol(x)
  centers = matrix(0,k,p)
  for (i in 1:k) {
    j = cluster==i
    if (sum(j)==0) centers[i,] = rep(NA,p)
    else centers[i,] = colMeans(x[j,,drop=FALSE])
  }
  return(centers)
}

medfromclust = function(x, cluster, k) {
  p = ncol(x)
  centers = matrix(0,k,p)
  for (i in 1:k) {
    j = cluster==i
    if (sum(j)==0) centers[i,] = rep(NA,p)
    else {
      d = as.matrix(dist(x[j,],diag=T,upper=T)^2)
      ii = which(j)[which.min(colSums(d))]
      centers[i,] = x[ii,]
    }
  }
  return(centers)
}

# Compute within-cluster variation
wcv = function(x, cluster, centers) {
  k = nrow(centers)
  wcv = 0
  for (i in 1:k) {
    j = cluster==i
    nj = sum(j)
    if (nj>0) {
      wcv = wcv + sum((t(x[j,])-centers[i,])^2)
    }
  }
  return(wcv)
}

set.seed(1234)
pts = as.matrix(iris[, 3:4])

iters = 2
K = 3
km = kmed = vector("list", iters)
centers = kmed.centers = vector("list", iters + 1)

ind = sample.int(nrow(pts), K)
centers[[1]] = kmed.centers[[1]] = pts[c(99,93,91), , drop = FALSE]

par(mar = c(4,4,3,0))
plot(pts, pch = 19, col = "gray70")
points(centers[[1]][, 1], centers[[1]][, 2], pch = 4, cex = 2, lwd = 2, col = "red")
title(main = "Iteration 0", line = 2)
title(main = "Choose K arbitrary observations to be the initial cluster centers", line = 1, cex.main = 0.8)

par(mfrow = c(1, 2))
for (i in 1:iters) {
  cat("  \n##", "K-means vs. K-medoids  \n  \\addtocounter{framenumber}{-1}  \n")
  # kmeans
  km[[i]] = kmeans(pts, centers = centers[[i]], nstart = 1, algorithm = "Lloyd", iter.max = 1)
  col = km[[i]]$cluster
  # kmedoids
  kmed[[i]] = kclust(pts, centers = kmed.centers[[i]], alg = "kmedoids", maxiter = 1)
  col.kmed = kmed[[i]]$cluster
  
  # kmeans
  par(mar = c(4,3,3,0))
  plot(pts, pch = 19, col = col)
  title(main = paste("K-means iteration", i), line = 2)
  title(main = "Assignment step", line = 1, cex.main = 0.8)
  points(centers[[i]][, 1], centers[[i]][, 2], pch = 4, cex = 2, lwd = 2, col = "red")
  
  # kmedoids
  par(mar = c(4,3,3,0))
  plot(pts, pch = 19, col = col.kmed)
  title(main = paste("K-medoids iteration", i), line = 2)
  title(main = "Assignment step", line = 1, cex.main = 0.8)
  points(kmed.centers[[i]][, 1], kmed.centers[[i]][, 2], pch = 4, cex = 2, lwd = 2, col = "red")
  
  cat("  \n")
  cat("  \n##", "K-means vs. K-medoids  \n  \\addtocounter{framenumber}{-1}  \n")
  
  centers[[i + 1]] = km[[i]]$centers
  kmed.centers[[i + 1]] = medfromclust(pts, kmed[[i]]$cluster, K)
  
  par(mar = c(4,3,3,0))
  plot(pts, pch = 19, col = col)
  points(centers[[i + 1]][, 1], centers[[i + 1]][, 2], pch = 4, cex = 2, lwd = 2, col = "red")
  title(main = paste("K-means iteration", i), line = 2)
  title(main = "Update step", line = 1, cex.main = 0.8)
  
  par(mar = c(4,3,3,0))
  plot(pts, pch = 19, col = col)
  points(kmed.centers[[i + 1]][, 1], kmed.centers[[i + 1]][, 2], pch = 4, cex = 2, lwd = 2, col = "red")
  title(main = paste("K-medoids iteration", i), line = 2)
  title(main = "Update step", line = 1, cex.main = 0.8)
  # 
  # points(medfromclust(pts, kmed[[i]]$cluster.history[i,], K), col = 1:3, pch = 19, cex = 2)
  # points(medfromclust(pts, kmed[[i]]$cluster.history[i,], K), pch = 21, cex = 2)
  
  cat("  \n")
}
```

<!-- ## $K$-medoids in R  -->

<!-- - `pam` function of the cluster package.  -->
<!-- - `pamk` function of the fpc package. -->
<!-- - `kmedoids` function of the clue package. -->
<!-- - `KMedoids` function of the TSdist package for clustering time series data.  -->
<!-- - `fviz_nbclust()` function of the factoextra package provides a solution to estimate the optimal number of clusters. -->

## $K$-medoids in R 

The $K$-medoids algorithm is implemented in the `pam` function included in the `cluster` R-package.

We compare $K$-means and $K$-medoids on a modified iris data with additional outliers:

```{r, echo = FALSE, eval = FALSE}
set.seed(5)
random = data.frame(Petal.Length = runif(2, 2.0, 3.5), Petal.Width = runif(2,1.0,10.5))
example = rbind(random, iris[,3:4])
```

```{r, echo = TRUE}
library(cluster)
# add outliers to iris
out = data.frame(Petal.Length = c(2.3, 3), Petal.Width = c(9.7, 3.7))
iris2 = rbind(iris[, 3:4], out)

# K-medoid vs. K-means
kmedoid = pam(iris2, k = 3)
km = kmeans(iris2, centers = 3, iter.max = 100, nstart = 100)
```

## $K$-medoids in R 

```{r, fig.width = 10, fig.height = 6, fig.align="center", out.width="\\textwidth", echo=FALSE}
set.seed(5)
par(mfrow = c(1,2), mar = c(4,3,3,0))

plot(iris2, col=kmedoid$clustering, pch=19, cex=1, main = "K-Medoids")
points(kmedoid$medoids, col="red", pch=4, cex=3, lwd=3)

plot(iris2, col = km$cluster, pch = 19, cex = 1, main = "K-Means")
points(km$centers, col="red", pch=4, cex=3, lwd=3)
```

