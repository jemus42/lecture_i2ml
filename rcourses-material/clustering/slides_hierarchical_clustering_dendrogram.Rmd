## Example: Hierarchical Clustering

Agglomerative hierarchical clustering starts with all points forming their own cluster and iteratively merges them until all points form a single cluster containing all points.

Example:

\begin{columns}

\begin{column}{0.5\textwidth}
Step 1: $\{1\},\{2\},\{3\},\{4\},\{5\}$ \\
% Step 2: $\{1\},{\color{blue}\{2,3\}},\{4\},\{5\}$\\
% Step 3: $\{1\},{\color{blue}\{2,3\}},{\color{blue}\{4,5\}}$\\
% Step 4: ${\color{blue}\{1,2,3\}},\{4,5\}$\\
% Step 5: ${\color{blue}\{1,2,3,4,5\}}$
\end{column}
\begin{column}{0.4\textwidth}

```{r, echo=FALSE, out.width="\\textwidth", fig.width=4, fig.height=4}
par(mar = c(4,4,1,1))
simple.data = data.frame(x = c(0.2,0.3,0.4,0.85,0.85), 
  y = c(0.4,0.65,0.6,0.7,0.9))

lab = 1:nrow(simple.data)
plot(simple.data, xlim = c(0, 1), ylim = c(0, 1), xlab = "Dimension 1", ylab = "Dimension 2")
text(simple.data, labels = lab, adj = c(-1,0.5))
```

\end{column}
\end{columns}


## Example: Hierarchical Clustering

\addtocounter{framenumber}{-1}

Agglomerative hierarchical clustering starts with all points forming their own cluster and iteratively merges them until all points form a single cluster containing all points.

Example:

\begin{columns}

\begin{column}{0.5\textwidth}
Step 1: $\{1\},\{2\},\{3\},\{4\},\{5\}$\\
Step 2: $\{1\},{\color{blue}\{2,3\}},\{4\},\{5\}$ \\
% Step 3: $\{1\},{\color{blue}\{2,3\}},{\color{blue}\{4,5\}}$\\
% Step 4: ${\color{blue}\{1,2,3\}},\{4,5\}$\\
% Step 5: ${\color{blue}\{1,2,3,4,5\}}$
\end{column}
\begin{column}{0.4\textwidth}

```{r, echo=FALSE, out.width="\\textwidth", fig.width=4, fig.height=4}
par(mar = c(4,4,1,1))
simple.data = data.frame(x = c(0.2,0.3,0.4,0.85,0.85), 
  y = c(0.4,0.65,0.6,0.7,0.9))

lab = 1:nrow(simple.data)
plot(simple.data, xlim = c(0, 1), ylim = c(0, 1), xlab = "Dimension 1", ylab = "Dimension 2")
text(simple.data, labels = lab, adj = c(-1,0.5))
lines(simple.data[2:3,], col = "blue")
```

\end{column}
\end{columns}


## Example: Hierarchical Clustering

\addtocounter{framenumber}{-1}

Agglomerative hierarchical clustering starts with all points forming their own cluster and iteratively merges them until all points form a single cluster containing all points.

Example:

\begin{columns}

\begin{column}{0.5\textwidth}
Step 1: $\{1\},\{2\},\{3\},\{4\},\{5\}$\\
Step 2: $\{1\},\{2,3\},\{4\},\{5\}$ \\
Step 3: $\{1\},{\color{blue}\{2,3\}},{\color{red}\{4,5\}}$\\
% Step 4: ${\color{blue}\{1,2,3\}},\{4,5\}$\\
% Step 5: ${\color{blue}\{1,2,3,4,5\}}$
\end{column}
\begin{column}{0.4\textwidth}

```{r, echo=FALSE, out.width="\\textwidth", fig.width=4, fig.height=4}
par(mar = c(4,4,1,1))
simple.data = data.frame(x = c(0.2,0.3,0.4,0.85,0.85), 
  y = c(0.4,0.65,0.6,0.7,0.9))

lab = 1:nrow(simple.data)
plot(simple.data, xlim = c(0, 1), ylim = c(0, 1), xlab = "Dimension 1", ylab = "Dimension 2")
text(simple.data, labels = lab, adj = c(-1,0.5))
lines(simple.data[2:3,], col = "blue")
lines(simple.data[4:5,], col = "red")
```

\end{column}
\end{columns}


## Example: Hierarchical Clustering

\addtocounter{framenumber}{-1}

Agglomerative hierarchical clustering starts with all points forming their own cluster and iteratively merges them until all points form a single cluster containing all points.

Example:

\begin{columns}

\begin{column}{0.5\textwidth}
Step 1: $\{1\},\{2\},\{3\},\{4\},\{5\}$\\
Step 2: $\{1\},\{2,3\},\{4\},\{5\}$ \\
Step 3: $\{1\},\{2,3\},\{4,5\}$\\
Step 4: ${\color{blue}\{1,2,3\}},{\color{red}\{4,5\}}$\\
% Step 5: ${\color{blue}\{1,2,3,4,5\}}$
\end{column}
\begin{column}{0.4\textwidth}

```{r, echo=FALSE, out.width="\\textwidth", fig.width=4, fig.height=4}
par(mar = c(4,4,1,1))
simple.data = data.frame(x = c(0.2,0.3,0.4,0.85,0.85), 
  y = c(0.4,0.65,0.6,0.7,0.9))

lab = 1:nrow(simple.data)
plot(simple.data, xlim = c(0, 1), ylim = c(0, 1), xlab = "Dimension 1", ylab = "Dimension 2")
text(simple.data, labels = lab, adj = c(-1,0.5))
lines(simple.data[c(1:3, 1),], col = "blue")
lines(simple.data[4:5,], col = "red")
```

\end{column}
\end{columns}


## Example: Hierarchical Clustering

\addtocounter{framenumber}{-1}

Agglomerative hierarchical clustering starts with all points forming their own cluster and iteratively merges them until all points form a single cluster containing all points.

Example:

\begin{columns}

\begin{column}{0.5\textwidth}
Step 1: $\{1\},\{2\},\{3\},\{4\},\{5\}$\\
Step 2: $\{1\},\{2,3\},\{4\},\{5\}$ \\
Step 3: $\{1\},\{2,3\},\{4,5\}$\\
Step 4: $\{1,2,3\},\{4,5\}$\\
Step 5: ${\color{blue}\{1,2,3,4,5\}}$
\end{column}
\begin{column}{0.4\textwidth}

```{r, echo=FALSE, out.width="\\textwidth", fig.width=4, fig.height=4}
par(mar = c(4,4,1,1))
simple.data = data.frame(x = c(0.2,0.3,0.4,0.85,0.85), 
  y = c(0.4,0.65,0.6,0.7,0.9))

lab = 1:nrow(simple.data)
plot(simple.data, xlim = c(0, 1), ylim = c(0, 1), xlab = "Dimension 1", ylab = "Dimension 2")
text(simple.data, labels = lab, adj = c(-1,0.5))
lines(simple.data[c(1,2,3,5,4,1),], col = "blue")
```

\end{column}
\end{columns}

## Dendrogram

A Dendrogram is a tree showing which clusters / observations are merged after each step.
The `height' is proportional to the distance between the two merged clusters: 

```{r, echo = FALSE, results='hide'}
# compute distance matrix
euclid = dist(simple.data, method = "euclidean")
# do clustering with complete linkage
agglo = hclust(euclid, method = "complete")
agglo
```

```{r, echo=FALSE, out.width="\\textwidth", fig.width=8, fig.height=4}
par(mfrow = c(1,2), mar = c(4,4,1,1))
plot(simple.data, xlim = c(0, 1), ylim = c(0, 1), xlab = "Dimension 1", ylab = "Dimension 2")
text(simple.data, labels = lab, adj = c(-1,0.5))

par(mar = c(2,3,1,4))
plot(agglo, hang = -1, ylab = "", axes = F)
axis(2, line = 0, padj = 0.5)
title(ylab = "Height", line = 2)
abline(h = c(0, agglo$height), lty = 2, col = "gray70")
text(x = 5, y = c(0, agglo$height), labels = paste("Step ", 1:5), xpd = TRUE, col = "gray70", pos = 4)
```

<!-- # Dendrogram -->

<!-- A Dendrogram is a tree that visualizes the merging of clusters: -->

<!-- - Each leaf node is a singleton (i.e., a cluster containing a single  -->
<!--   observation). -->

<!-- - Each internal node has two daughter nodes (children), representing -->
<!--   the clusters that were merged to form it.  -->

<!-- - Each node represents a cluster. -->

<!-- - The root node is the cluster containing all observations. -->

<!-- - The $y$ axis shows the distance between two clusters when they were merged. -->

<!-- Remember: the choice of linkage determines how the distance between clusters is  -->
<!-- measured.  -->

<!-- If we fix the leaf nodes at height zero, then each internal node is
drawn at a height proportional to the dissmilarity between
its two daughter nodes -->

