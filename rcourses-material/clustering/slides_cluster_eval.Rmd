## Clustering Validation Measures
<!-- Source: http://www.sthda.com/english/wiki/clustering-validation-statistics-4-vital-things-everyone-should-know-unsupervised-machine-learning#internal-clustering-validation-measures -->

Recall: The goal of clustering algorithms is to find clusters, such that

  - the observations in each cluster are as "similar" as possibe, and
  - the clusters are as "far away" as possible from other clusters.

That is, we want

  - the (average) distance within all clusters to be as small as possible, and
  - the (average) distance between clusters to be as large as possible.

## Clustering Validation Measures

We therefore need

  - *compactness measures* that evaluate how close the observations within a cluster are,
    e.g. the average distances between observations within a cluster. $\rightarrow$ smaller is better.

  - *separation measures* that determine how well-separated a cluster is from other clusters,
    e.g. distances between cluster centers. $\rightarrow$ larger is better.

Most common clustering validation indices combine *compactness* and *separation* measures as follows:

$$\texttt{index} = \frac{\alpha \times \texttt{separation}}{\beta \times \texttt{compactness}},$$

where $\alpha$ and $\beta$ are weights.

## Example: Dunn Index

The Dunn index $D$ is calculated using

$$D = \tfrac{\texttt{separation}}{\texttt{compactness}} = \tfrac{\min_{i\neq j}{d(C_i, C_j)}}{\max_k d(\Delta C_k)}.$$
```{r, echo = FALSE, fig.height=5, out.width="0.8\\textwidth", message=FALSE}
Colors = colorspace::rainbow_hcl(3)
library(MASS)
library(cluster)
library(pdist)
set.seed(123456)
n = 6
cl1 = mvrnorm(n = n, mu = c(6,0), Sigma = diag(2)*0.2)
cl2 = mvrnorm(n = n, mu = c(3,-2), Sigma = diag(2)*0.3)
cl3 = mvrnorm(n = n, mu = c(2,1), Sigma = diag(2)*0.6)

e1 = ellipsoidhull(cl1)
e2 = ellipsoidhull(cl2)
e3 = ellipsoidhull(cl3)
dat = rbind(cl1, cl2, cl3)
d1 = as.matrix(pdist(cl1, cl2))
d2 = as.matrix(pdist(cl1, cl3))
d3 = as.matrix(pdist(cl2, cl3))

par(mar=c(0,0,0,0))

pr = (rbind(predict(e1), predict(e2), predict(e3)))
plot(dat, pch = 19, col = rep(Colors, each = n),
  xlab = "", ylab = "", axes = F, xlim = range(pr[,1]), ylim = range(pr[,2]))
lines(predict(e1), col = Colors[1])
lines(predict(e2), col = Colors[2])
lines(predict(e3), col = Colors[3])
text(e1$loc[1], e1$loc[2], label = expression(C[1]), col = Colors[1])
text(e2$loc[1], e2$loc[2], label = expression(C[2]), col = Colors[2])
text(e3$loc[1], e3$loc[2], label = expression(C[3]), col = Colors[3])

single = which(d1==min(d1), arr.ind = TRUE)
lines(rbind(cl1[single[1],], cl2[single[2],]), col = "gray")
xy = colMeans(rbind(cl1[single[1],], cl2[single[2],]))
text(xy[1], xy[2], label = expression(d(C[1], C[2])), col = "gray")

single = which(d2==min(d2), arr.ind = TRUE)
lines(rbind(cl1[single[1],], cl3[single[2],]), col = "gray")

single = which(d3==min(d3), arr.ind = TRUE)
lines(rbind(cl2[single[1],], cl3[single[2],]), col = "gray")

d = predict(e1)
#summary(d)
xy = dat[c(2,6),]
lines(xy, col = "gray")
xy = colMeans(dat[c(2,6),])
text(xy[1], xy[2], expression(d(Delta[C[1]])), col = "gray")
```

## Example: Dunn Index

The Dunn index $D$ is calculated using

$$D = \tfrac{\texttt{separation}}{\texttt{compactness}} = \tfrac{\min_{i\neq j}{d(C_i, C_j)}}{\max_k d(\Delta C_k)}.$$

  - *compactness* measure is computed as follows:
    - For each cluster, compute the distance between the observation in the same cluster.
    - Use the maximal distance as compactness measure.

  - *separation* measure is computed as follows:
    - For each cluster, compute the pairwise distance between each observation in
      a cluster and the observations in all other clusters.
    - Use the minimum as separation measure.

Large values of $D$ indicate better clustering.


## Silhouette coefficient

Silhouette coefficient of observation $i$ is defined as

$$S_i = \frac{b_i - a_i}{max(a_i, b_i)} \;\;\; S_i \in [-1, 1]$$

  - $a_i$ is the average distance between $i$ and all other observation of the cluster to which $i$ belongs.

  - $b_i = \min_{C(i) \neq k} d(C(i), C_k)$
  
    - $d(C(i), C_k)$ is the average distance between observation $i$ from cluster $C(i)$ to all observations in cluster $C_k$ to which $i$ does not belong. 
    - $b_i$ can be seen as the distance between $i$ and its closest cluster to which $i$ does not belong.

## The silhouette method

- Quality measure of $K$-means or a $K$-medoid clustering algorithms.
- Evaluates the mapping of objects to clusters.
- Observations within each cluster should be similar to their representative.
- Observations in different clusters should be dissimilar.
- A high average silhouette value indicates a good clustering. 

## The silhouette method in R 

```{r, echo = FALSE, size = "tiny", fig.align="center", out.width="70%"}
library(cluster)
library(factoextra)

fviz_nbclust(iris[,3:4], pam, method = "silhouette")
```

**Note:** The silhouette method just gives a good guess on the right number of clusters, e.g. in the iris dataset we have should use $k = 3$ as there are three different species. 

## Summary 

- The $K$-medoids algorithm is a robust alternative to $K$-means.
- Its sensitivity to outliers is lower compared to $K$-means.
- Each cluster "center" is represented by a selected observation within the data.
- User needs to specifiy $K$ or use the so-called silhouette method.
- Result and runtime strongly depend on the initial cluster assignment. 
- $K$-means and $K$-medoids often terminate in a local optimum, but methods for a good initialization exist.
