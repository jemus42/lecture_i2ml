## Optimal Partitioning Clustering

```{r, echo=FALSE, results='hide'}
Colors = c("#000000", "#E69F00", "#56B4E9", "#009E73", 
           "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
palette(Colors)
add.alpha <- function(col, alpha=1){
  if(missing(col))
    stop("Please provide a vector of colours.")
  apply(sapply(col, col2rgb)/255, 2, 
                     function(x) 
                       rgb(x[1], x[2], x[3], alpha=alpha))  
}
```

**Hierarchical clustering:** 
Stepwise merging (agglomerative methods) or dividing (divisive methods) of clusters based on distances and linkages. 
The number of clusters are selected by splitting the dendrogram at a specific threshold for the ``height'' after visual inspection.

**Partitioning clustering:** 
Partitions the $N$ observations into a predefined number of $K$ clusters by optimizing a numerical criterion. The most common partitioning methods are:

- $K$-means
- $K$-medians
- $K$-medoids (Partitioning Around Medoids (PAM))

## $K$-means

$K$-means partitions the $N$ observations into $K$ predefined clusters $C_1, C_2, \hdots, C_K$ by minimizing the **compactness**, i.e. the **within-cluster variation** of all clusters using
$$\textstyle\sum_{k=1}^K \sum_{i \in C_k} \|X_i-\bar{X}_k\|_2^2 \rightarrow \min,$$ 
where $\bar{X}_k = \frac{1}{N_k} \sum_{i \in C_k} X_i$ is the centroid of cluster $k$ and $N_k$ is the number of observations in cluster $k$. 

<!--Hence, equivalently we seek a clustering $C$ that minimizes the 
within-cluster variation.-->

```{r, echo = FALSE, fig.height=3.5, fig.width=6.5, out.width="0.6\\textwidth", message=FALSE}
Colors = colorspace::rainbow_hcl(3)
library(MASS)
library(cluster)
library(pdist)
set.seed(123456)
n = 6
cl1 = mvrnorm(n = n, mu = c(6,0), Sigma = diag(2)*0.2)
cl2 = mvrnorm(n = n, mu = c(3,-1.5), Sigma = diag(2)*0.3)
cl3 = mvrnorm(n = n, mu = c(2,0.25), Sigma = diag(2)*0.6)

e1 = ellipsoidhull(cl1)
e2 = ellipsoidhull(cl2)
e3 = ellipsoidhull(cl3)
dat = rbind(cl1, cl2, cl3)
d1 = as.matrix(pdist(cl1, cl2))
d2 = as.matrix(pdist(cl1, cl3))
d3 = as.matrix(pdist(cl2, cl3))

par(mar = c(0,0,0,0))

pr = (rbind(predict(e1), predict(e2), predict(e3)))
plot(dat, pch = 19, col = rep(Colors, each = n),
  xlab = "", ylab = "", axes = F, xlim = range(pr[,1]), ylim = range(pr[,2]))
lines(predict(e1), col = Colors[1])
lines(predict(e2), col = Colors[2])
lines(predict(e3), col = Colors[3])


centers = as.data.frame(rbind(
  c("x" = e2$loc[1], "y" = e2$loc[2]), 
  c("x" = e1$loc[1], "y" = e1$loc[2])))
#lines(centers, col = "black", lwd = 2)
arrows(x0 = centers$x[1], x1 = centers$x[2], 
  y0 = centers$y[1], y1 = centers$y[2], code = 3, lwd = 2, length = 0.1, col = "gray60")
xy = colMeans(centers)
text(xy[1], xy[2], label = "between cluster variation", col = "black")

text(e1$loc[1], e1$loc[2], label = expression(C[1]), col = Colors[1], pos = 1)
text(e2$loc[1], e2$loc[2], label = expression(C[2]), col = Colors[2], pos = 1)
text(e3$loc[1], e3$loc[2], label = expression(C[3]), col = Colors[3], pos = 3)

d = predict(e1)
#summary(d)
xy = dat[c(14,16),]
#lines(xy, col = "black", lwd = 2)
arrows(x0 = xy[1,1], x1 = xy[2,1], 
  y0 = xy[1,2], y1 = xy[2,2], code = 3, lwd = 2, length = 0.1, col = "gray60")
xy = colMeans(xy)
text(xy[1], xy[2], label = "within-cluster variation", col = "black")

```


## $K$-means

Idea: Consider every possible partition of $N$ observations into $K$ clusters and select the one with the lowest **within-cluster variation**.

Problem: Requires trying all possible assignments of $N$ observations into $K$ clusters, which in practice is nearly impossible (Hothorn et al., 2009, p.322):

|$N$     |$K$     | Number of possible partitions|
|-------:|:------:|:-----------------------------|
|15      | 3      | 2.375.101                    |
|20      | 4      | 45.232.115.901               |
|100     | 5      | $10^{68}$                    |

\tiny{Hothorn, T., Everitt, B. S. (2009). A handbook of statistical analyses using R. Chapman and Hall/CRC.}

## $K$-means
<!--
The $K$-means clustering algorithm approximately minimizes
the enlarged criterion by alternately minimizing over $C$
and $c_1,\hdots , c_K$

We start with an initial guess for $c_1,\hdots , c_K$ (e.g., pick $K$
points at random over the range of $X_1,\hdots , X_n$), then repeat:

1.  Minimize over $C$: for each $i=1,\hdots , n$, find the
    cluster center $c_k$ closest to $X_i$, and let $C(i)=k$

2.  Minimize over $c_1,\hdots , c_K$: for each
    $k=1,\hdots , K$, let $c_k = \bar{X}_k$, the average of points in
    group $k$

Stop when within-cluster variation doesnâ€™t change

In words:

1.  Cluster (label) each point based the closest center

2.  Replace each center by the average of points in its cluster

## $K$-means example

Here $X_i \in \R^2$, $n=300$, and $K=3$ -->

<!-- ![image](km0.pdf) -->

<!-- ![image](km1.pdf) -->

<!-- ![image](km2.pdf)\ -->
<!-- ![image](km3.pdf) -->

<!-- ![image](km9.pdf) --> 

Use an approximation:

1. **Initialization:** Choose $K$ arbitrary observations to be the initial cluster 
   centers.

2. **Assignment:** Assign every observation to the cluster with the closest center.

3. **Update:** Compute the new center of each cluster as the mean of its members.

4. Repeat (2) and (3) until the centers do not move.
 
<!--
1. Find some initial partition of the observations into the required number of clusters. 
2. Calculate the change in the clustering criterion produced by "moving" each 
individual from its own cluster to another cluster.
3. Make the change that leads to the greatest improvement in the value of the clustering criterion.
4. Repeat steps (2) and (3) until no move of an observation causes the clustering criterion to improve.
-->
<!-- Cool Example: http://www.edureka.co/blog/implementing-kmeans-clustering-on-the-crime-dataset/ -->

