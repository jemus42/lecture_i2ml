## Hierarchical Clustering
```{r include=FALSE, cache=FALSE}
library(knitr)
root = rprojroot::find_root(rprojroot::is_git_root)
ap = adjust_path(paste0(getwd(), "/figure"))
```

Hierarchical clustering is a recursive process that builds a hierarchy of clusters. 
We distinguish between:

1. Agglomerative (or bottom-up) clustering:
    - Start: Each observations is an *individual cluster*.
    - Repeat: Merge the two closest clusters.
    - Stop when there is only one cluster left.

2. Divisive (or top-down) clustering:
    - Start: All observations are within *one* cluster.
    - Repeat: Divide the cluster that results in two clusters with biggest distance.
    - Stop when each observation is an individual cluster.
    
<!-- We focus on agglomerative clustering methods as they are simpler. -->

## Hierarchical Clustering

Let $X_1,\hdots , X_N$ be observations with $P$ features (dimensions), where 
$X_i = (x_{i1}, \ldots, x_{iP})^\top$. A data set is a (N $\times$ P)-matrix of 
the form:

|        | feature $1$ | $\hdots$   | $\hdots$    | feature $P$|
|:------:|:-----------:|:----------:|:-----------:|:----------:|
|$X_1$   |     $x_{11}$|    $\hdots$|     $\hdots$|    $x_{1P}$|
|$\vdots$|     $\vdots$|    $\vdots$|     $\vdots$|    $\vdots$|
|$X_N$   |     $x_{N1}$|    $\hdots$|     $\hdots$|    $x_{NP}$|
 
<!-- |$\vdots$|     $\vdots$|    $\vdots$|     $\vdots$|    $\vdots$| -->
<!-- |$X_{150}$|         5.9|         3.0|          5.1|         1.8| -->

## Hierarchical Clustering 

For hierarchical clustering, we need a definition for

- distances $d(X_i, X_j)$ between two observations $X_i$ and $X_j$:

    - manhattan distance: 
      $$d(X_i,X_j)= ||X_i - X_j||_1 = \sum_{k=1}^P|x_{ik}-x_{jk}|$$
    - euclidean distance:
      $$d(X_i,X_j)= ||X_i - X_j||_2 = \sqrt{\sum_{k=1}^P(x_{ik}-x_{jk})^2}$$

- distances between two clusters (called linkage).

<!-- 
 -  Let $K$ be the number of clusters.
 - A clustering of observations $X_1,\hdots , X_N$ is a function $C$ that 
   assigns each observation $X_i$ to a cluster $k \in \{1,\hdots , K\}$. 
-->

## Distances between Observations

```{r, echo=FALSE, results='hide'}
Colors = c("#000000", "#E69F00", "#56B4E9", "#009E73", 
           "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
palette(Colors)
```

```{r,echo=FALSE, fig.align="center", fig.width=6, fig.height=4, out.width="0.5\\textwidth"}
getCurrentAspect = function() {
   uy <- diff(grconvertY(1:2,"user","inches"))
   ux <- diff(grconvertX(1:2,"user","inches"))
   uy/ux
}

par(mar = c(4,4,0,0))
plot(x = c(1L,5L), y = c(1L,4L), ylim = c(0,5), xlim = c(0,6), pch = 19, 
  xlab = "Dimension 1", ylab = "Dimension 2")
lines(x = c(1L,5L), y = c(1L,4L))
text(x = c(1L,5L), y = c(1L,4L), c(expression(X[i]), expression(X[j])), adj = c(1.5, 0))
lines(x = c(1L, 5L, 5L, 5L), y = c(1L, 1L, 1L, 4L), col = 2)
legend("topleft", lty = 1, legend = c("manhattan", "euclidean"), col = c(2,1))

text(x = 5, y = 1, expression(d(X[i],X[j])~"= |5-1| + |4-1| = 7"), adj = c(1,1), col = 2)

asp = getCurrentAspect()
text(x = 3, y = 2.5, expression(d(X[i],X[j])~"="~sqrt((5-1)^2 + (4-1)^2)~"= 5"),
  adj = c(0.5,0), col = 1, srt = 180/pi*atan(3/4*asp))
```


- \small manhattan: sum up the absolute distances in each dimension.

    In R: `dist(data, method = "manhattan")`
    
- \small euclidean: remember Pythagoras theorem from school?

    In R: `dist(data, method = "euclidean")`

- \small gower: can be used for mixed variables (categorical and numeric).

    In R: `gower_dist()` from the `gower` package

- \small see `?dist` for other distances.

## Gower Distance I

- The Gower's metric calculates the distance between observations $X_i$ and $X_j$ for each feature separately and based on its data type (i.e., categorical or numeric).

- For a categorical feature $X_k$, the distance between the $i$-th and the $j$-th observation $X_{ik}$ and $X_{jk}$ is defined by
  $$s_{ijk}=\begin{cases} 0\ \text{if}\ X_{ik}=X_{jk} \\ 1\ \text{if}\ X_{ik}\neq X_{jk}.\end{cases}$$

- For a numerical feature $X_k$, the distance between the $i$-th and the $j$-th observation $X_{ik}$ and $X_{jk}$ is defined by $$s_{ijk}=\frac{|X_{ik}-X_{jk}|}{\max(X_k)-\min(X_k)}, \;\;\; \text{so that } 0 \leq s_{ijk} \leq 1.$$ 

## Gower Distance II

The Gower's metric $S$ combines all individual distances of each feature by
$$S_{ij}=\dfrac{\sum_{k=1}^P w_{k} s_{ijk}}{\sum_{k=1}^P w_{k}},$$ where
      
  - $P$: number of features.
  - $w_k$: weight for feature $k$ (typically $w_k = 1$).
  - $s_{ijk}$: the difference (distance) between $X_{ik}$ and $X_{jk}$, i.e. the $i$-th and $j$-th observation of feature $k$.

  
## Gower Distance III - Example      

`r include_graphics(ap("gower_distance.png"))`


## Distances between Observations

It is often a good idea to *normalize* the data before computing distances, 
especially when the scale of features is different, e.g.:

```{r, echo=FALSE, fig.align="center", fig.height=4, fig.width=9}
par(mar = c(3.5,3.5,1,1), mfrow = c(1,2), mgp = c(2.5,1,0))

dat = data.frame(shoe.size = c(46, 40, 44), height = c(180, 172, 175))
plot(x = dat$shoe.size, y = dat$height, xlab = "shoe size", ylab = "body height (in cm)", 
  pch = 19, xlim = range(dat$shoe.size)*c(0.95, 1.05), ylim = range(dat$height)*c(0.98, 1.02))
lines(x = dat$shoe.size[-3], y = dat$height[-3])
asp = getCurrentAspect()
text(x = mean(dat$shoe.size[-3]), y = mean(dat$height[-3]), 
  bquote(paste(d(X[1],X[2])~"="~sqrt((46-40)^2 + (180-172)^2)~" = ", .(sqrt((46-40)^2 + (180-172)^2)))),
  adj = c(0.5,-0.25), col = 1, srt = 180/pi*atan(diff(dat$height[-3])/diff(dat$shoe.size[-3])*asp))
text(x = dat$shoe.size, y = dat$height, c(expression(X[2]), expression(X[1]), expression(X[3])), adj = c(-1, 0.5))


dat$height = dat$height/100
plot(x = dat$shoe.size, y = dat$height, xlab = "shoe size", ylab = "body height (in m)", 
  pch = 19, xlim = range(dat$shoe.size)*c(0.95, 1.05), ylim = range(dat$height)*c(0.98, 1.02))
lines(x = dat$shoe.size[-3], y = dat$height[-3])
asp = getCurrentAspect()
text(x = mean(dat$shoe.size[-3]), y = mean(dat$height[-3]), 
  bquote(paste(d(X[1],X[2])~"="~sqrt((46-40)^2 + (1.80-1.72)^2)~" = ", .(sqrt((46-40)^2 + (1.80-1.72)^2)))),
  adj = c(0.5,-0.25), col = 1, srt = 180/pi*atan(diff(dat$height[-3])/diff(dat$shoe.size[-3])*asp))
text(x = dat$shoe.size, y = dat$height, c(expression(X[2]), expression(X[1]), expression(X[3])), adj = c(-1, 0.5))

```

- On the right plot, the distance is dominated by `shoe size`.

## Distances between Observations

\small
The normalized feature $\tilde{X}_{\texttt{height}}$ is computed using 
$X_{\texttt{height}}$ by <!-- Normalization of the \texttt{height} feature means: -->
\[\tilde{X}_{\texttt{height}} = \tfrac{X_{\texttt{height}}-\texttt{mean}(X_{\texttt{height}})}{\texttt{sd}(X_{\texttt{height}})}.\]

Distances based on normalized data are better comparable and robust in terms of 
linear transformations (e.g. unit conversion).

```{r, echo=FALSE, fig.align="center", fig.height=4, fig.width=6, out.width="0.55\\textwidth"}
par(mar = c(3.5,3.5,0.1,0.1), mgp = c(2.5,1,0))

#dat = data.frame(shoe.size = c(45, 40, 42), height = 1000*c(85, 70, 72))
dat = as.data.frame(scale(dat))
plot(x = dat$shoe.size, y = dat$height, xlab = "normalized shoe size", ylab = "normalized body height", 
  pch = 19, xlim = range(dat$shoe.size)*c(1.1, 1.2), ylim = range(dat$height)*c(1.1, 1.1))
lines(x = dat$shoe.size[-3], y = dat$height[-3])
asp = getCurrentAspect()
text(x = mean(dat$shoe.size[-3]), y = mean(dat$height[-3]), 
  bquote(paste(d(X[1],X[2])~" = ", .(sqrt((dat$shoe.size[1]-dat$shoe.size[2])^2 + (dat$height[1]-dat$height[2])^2)))), 
  adj = c(0.5,0), col = 1, srt = 180/pi*atan(diff(dat$height[-3])/diff(dat$shoe.size[-3])*asp))
text(x = dat$shoe.size, y = dat$height, c(expression(X[2]), expression(X[1]), expression(X[3])), adj = c(-1, 0.5))

```

## Distances between Clusters (Linkage)

- Assume that all observations $X_1,\hdots , X_N$ belong to $K<N$ different clusters.

- The linkage of two clusters $C_r$ and $C_s$ is a "score" describing their distance.
  
The most popular and simplest linkages are

- *Single Linkage* <!-- the shortest distance between two observations in each cluster.  -->

- *Complete Linkage* <!-- the longest distance between two observations in each cluster. -->

- *Average Linkage* <!-- the average distance between each observation in one cluster to every observation in the other cluster. -->

- *Centroid Linkage*

```{r, echo=FALSE, message=FALSE}
library(MASS)
library(cluster)
library(pdist)
set.seed(12345)
n = 6
cl1 = mvrnorm(n = n, mu = c(6,2), Sigma = diag(2))
cl2 = mvrnorm(n = n, mu = c(1,-3), Sigma = diag(2)*2)
e1 = ellipsoidhull(cl1)
e2 = ellipsoidhull(cl2)
dat = rbind(cl1, cl2)
d = as.matrix(pdist(cl1, cl2))
```

## Single Linkage

```{r, echo=FALSE, fig.pos = "H", fig.align="center", fig.width=7, fig.height=4, out.width="0.7\\textwidth"}
single = which(d==min(d), arr.ind = TRUE)

par(mar=c(4,4,0,0))
plot(dat, pch = 19, xlim = c(-2,7), col = rep(c(2,3), each = n),
  xlab = "Dimension 1", ylab = "Dimension 2") 
lines(predict(e1), col = 2)
lines(predict(e2), col = 3)

lines(rbind(cl1[single[1],], cl2[single[2],]))
text(e1$loc[1], e1$loc[2], label = expression(C[r]), col = 2)
text(e2$loc[1], e2$loc[2], label = expression(C[s]), col = 3)
```

Single linkage defines <!-- we compute all pairwise distances for observations in 
opposite clusters.--> the distance of the *closest point pairs* from different clusters as the distance between two clusters:

\[d_{\text{single}}(C_r,C_s) = \min_{i \in C_r, \, j \in C_s} d(X_i,X_j)\]

## Complete Linkage

```{r, echo=FALSE, fig.pos = "H", fig.align="center", fig.width=7, fig.height=4, out.width="0.7\\textwidth"}
complete = which(d==max(d), arr.ind = TRUE)

par(mar=c(4,4,0,0))
plot(dat, pch = 19, xlim = c(-2,7), col = rep(c(2,3), each = n),
  xlab = "Dimension 1", ylab = "Dimension 2") 
lines(predict(e1), col = 2)
lines(predict(e2), col = 3)

lines(rbind(cl1[complete[1],], cl2[complete[2],]))
text(e1$loc[1], e1$loc[2], label = expression(C[r]), col = 2)
text(e2$loc[1], e2$loc[2], label = expression(C[s]), col = 3)
```

Complete linkage defines the distance of the *furthest point pairs* of different clusters as 
the distance between two clusters:

\[d_{\text{complete}}(C_r,C_s) = \max_{i \in C_r, \, j \in C_s} d(X_i,X_j)\]

## Average Linkage

```{r, echo=FALSE, fig.pos = "H", fig.align="center", fig.width=7, fig.height=4, out.width="0.7\\textwidth"}
par(mar=c(4,4,0,0))
plot(dat, pch = 19, xlim = c(-2,7), col = rep(c(2,3), each = n),
  xlab = "Dimension 1", ylab = "Dimension 2") 
lines(predict(e1), col = 2)
lines(predict(e2), col = 3)

for(i in 1:nrow(cl2)) lines(rbind(cl1[single[1],], cl2[i,]))
text(e1$loc[1], e1$loc[2], label = expression(C[r]), col = 2)
text(e2$loc[1], e2$loc[2], label = expression(C[s]), col = 3)
```

\begin{center}
{\scriptsize (Note: Plot only shows distances between all green points and 
\textit{one} red point)}
\end{center}

In average linkage, the distance between two clusters is defined as the average 
distance across *all* pairs of two different clusters.

## Centroid Linkage

```{r, echo=FALSE, fig.pos = "H", fig.align="center", fig.width=7, fig.height=4, out.width="0.65\\textwidth"}
par(mar=c(4,4,0,0))
plot(dat, pch = 19, xlim = c(-2,7), col = rep(c(2,3), each = n),
  xlab = "Dimension 1", ylab = "Dimension 2") 
lines(predict(e1), col = 2)
lines(predict(e2), col = 3)

text(e1$loc[1], e1$loc[2], label = expression(C[r]), col = 2)#, adj = c(0, 0.5))
text(e2$loc[1], e2$loc[2], label = expression(C[s]), col = 3)#, adj = c(1, 0.5))


#points(x = c(e1$loc[1], e2$loc[1]), y = c(e1$loc[2], e2$loc[2]), pch = 1, cex = 4)
lines(x = c(e1$loc[1], e2$loc[1]), y = c(e1$loc[2], e2$loc[2]), type = "b", cex = 3.5)
```

\small
Centroid linkage defines the distance between two clusters as the distance between the two cluster centroids.
The centroid of a cluster $C_s$ with $N_s$ points is the mean value of each dimension:

\[\bar{X}_s = \frac{1}{N_s} \sum_{i \in C_s} X_i\]
