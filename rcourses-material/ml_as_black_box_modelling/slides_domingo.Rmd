## Inducer Decomposition
```{r include=FALSE, cache=FALSE}
ap = adjust_path(paste0(getwd(), ""))
```

Nearly all ML supervised learning training algorithms can be described by three components:

\begin{center}
\textbf{Learning = Representation + Evaluation + Optimization}
\end{center}


- **Representation / Hypothesis Space:**
  Defines functional structures of $f$ we can learn.
- **Evaluation:**
  How well does a certain hypothesis score on a given data set? Allows us to choose
  better candidates over worse ones.
- **Optimization:**
  How do we search the hypothesis space? Guided by the evaluation metric.


## Inducer Decomposition

* All of these components represent important choices in ML which can have drastic effects:

* If we make smart choices here, we can tailor our inducer to our needs -
  but that usually requires quite a lot of experience and deeper insights into ML.

## Inducer Decomposition

| **Representation**              | **Evaluation**        | **Optimization**                |
|:------------------------------- |:--------------------- |:------------------------------- |
| Instances / Neighbours          | Squared error         | Gradient descent                |
| Linear functions                | Likelihood            | Stochastic gradient descent     |
| Decision trees                  | Information gain      | Quadratic programming           |
| Set of rules                    | K-L divergence        | Greedy optimization             |
| Neural networks                 |                       | Combinatorial optimization      |
| Graphical models                |                       |                                 |

Note: What is on the same line above does not belong together!

