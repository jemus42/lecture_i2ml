
## Quadratic discriminant analysis (QDA)

QDA follows a generative approach, each class density is modeled as a
\emph{multivariate Gaussian} with mean vector $\mu_k$
and covariance matrix $\Sigma_k$.
So the data, given that we are in class $k$, is assumed to be distributed:

\[
\mathcal{N}\left(\mu_k, \Sigma_k\right)
\]

<!--
\[
\pdfxyk = \frac{1}{(2\pi)^{\frac{p}{2}}\abs{\Sigma_k}^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k)\right)
\]
-->

```{r, echo=FALSE, fig.height=3}
library(ggplot2)
p = ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) +
  geom_point()
for (l in c(0.01, 0.05, 0.2, 0.5, 0.7, 0.8)) {
  p = p + stat_ellipse(type = "norm", level = l)
}
p
```


## Quadratic discriminant analysis (QDA)

- Parameters are estimated by per class means and covariances:\vspace{-0.5cm}
  \begin{eqnarray*}
  \hat{\pi}_k &=& n_k / n,\text{ where $n_k$ is the number of class $k$ observations} \\
  \hat{\mu}_k &=& \sum_{i\colon y_i = k} \frac{x_i}{n_k} \\
  \hat{\Sigma_k} &=& \frac{1}{n_k - 1}\sum_{i\colon y_i = k} (x_i - \hat{\mu}_k) (x_i - \hat{\mu}_k) ^T
  \end{eqnarray*}

- Afterwards, predicting new classes for given data $x_0$ is done by taking the
  highest probability of all estimated classes
  \[
  \hat{k} = \argmax_{k = 1, \dots, g}\ \hat{\pi}_k ~ f_{\mathcal{N}(\hat{\mu}_k, \hat{\Sigma_k})} (x_0)
  \]


## Quadratic discriminant analysis (QDA)

Due to the different covariances, the decision boundaries quadratic in input space:

```{r, echo=FALSE, fig.height=5}
mlr::plotLearnerPrediction(learner = "classif.qda", task = iris.task, cv = 0)
# Not working:
# plotLearnerPrediction(learner = "classif.qda", task = iris.task, err.mark = "none")
```

