## Logistic regression
```{r, include=FALSE, cache=FALSE}
library(mlr)

n = 30
set.seed(1234L)
tar = factor(c(rep(1L, times = n), rep(2L, times = n)))
feat1 = c(rnorm(n, sd = 1.5), rnorm(n, mean = 2, sd = 1.5))
feat2 = c(rnorm(n, sd = 1.5), rnorm(n, mean = 2, sd = 1.5))
bin.data = data.frame(feat1, feat2, tar)
bin.tsk = makeClassifTask(data = bin.data, target = "tar")

ap = adjust_path(paste0(getwd(), "/figure"))
ap.code = adjust_path(paste0(getwd(), "/code"))
```

For now, we will only look at the binary case $y \in \{0, 1\}$.

A naive approach would be to model
\[
\pix = \post = \theta^T x .
\]
Obviously this would allow predicted probabilities $\pix \not\in [0,1]$.

## Logistic regression

Logistic regression squashes the estimated linear scores to $[0,1]$ through a sigmoid function $s$:
\[
\pix = \post = \frac{\exp\left(\theta^Tx\right)}{1+\exp\left(\theta^Tx\right)} = s\left(\theta^T x\right)
\]
Note that we will suppress the intercept in notation.

```{r, echo=FALSE, fig.height=2.5}
t = seq(-7, 7, 0.01)
resp = exp(t) / (exp(t) + 1L)
resp.dat = data.frame(t, resp)
q = ggplot(data = resp.dat, aes(t, resp)) + geom_line()
q = q + ylab(expression(s(t)))
q
```

## Logistic regression

Logistic regression is usually fitted by maximum likelihood.

In order to minimize the loss (misclassification), we should predict $y=1$, if

\[ \pi\left(x, \thetah\right) = \P\left(y = 1 \mid x, \thetah\right) = \frac{\exp\left(\thetah^T x\right)}{1+\exp\left(\thetah^Tx\right)} \ge 0.5, \]

which is equal to
\[ \hat \theta^T x \ge 0. \]

So logistic regression gives us a *linear classifier*:
\[
\yh = h\left(\thetah^T x\right) =
\begin{cases}
1 & \text{ for } x^T\thetah \ge 0 \\
0 & \text{ otherwise}
\end{cases}
\]


## Logistic regression

```{r, echo=FALSE}
plotLearnerPrediction("classif.logreg", bin.tsk)
```

## Logistic Regression: Log Odds

A way to interpret linear regression is to use that the log-odds are linear
in $\theta$:
\[
\log \frac{\pix}{1-\pix} = \log\frac{\post}{\P\left(y = 0 \mid x\right)} = \theta^Tx.
\]

The chance for event 1 ...

- ... increases if $\theta_i > 0$
- ... remains if $\theta_i = 0$
- ... decreases if $\theta_i < 0$
