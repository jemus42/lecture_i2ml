## Learning = Representation + Evaluation + Optimization

All learners consist of three main components (all introducing bias):

- **Representation**: A model must be represented in a formal language that 
  the computer can handle.
    - Defines the concepts it can learn: *The hypothesis space*

- **Evaluation**: How to choose one hypothesis over the other?
    - The evaluation function, objective function, scoring function
    - Can differ from the external evaluation function (e.g. accuracy)

- **Optimization**: How do we search the hypothesis space?
    - Key to the efficiency of the learner
    - Defines how many optima it finds
    - Often starts from most simple hypothesis, relaxing it if needed to 
      explain the data

## Learning = Representation + Evaluation + Optimization

\scriptsize

Pedro Domingos summarizes the three components in 
"A Few Useful Things to Know about Machine Learning":

| **Representation**                      | **Evaluation**        | **Optimization**                     |
|:--------------------------------------- |:--------------------- |:------------------------------------ |
| Instances                               | Accuracy/Error rate   | Combinatorial optimization           |
| \hspace{0.3cm}K-nearest neighbor        | Precision and recall  | \hspace{0.3cm}Greedy search          |
| Support vector machines                 | Squared error         | Beam search                          |
| Hyperplanes                             | Likelihood            | \hspace{0.3cm}Branch-and-bound       |
| \hspace{0.3cm}Naive Bayes               | Posterior probability | Continuous optimization              |
| \hspace{0.3cm}Logistic regression       | Information gain      | \hspace{0.3cm}Unconstrained          |
| Decision trees                          | K-L divergence        | \hspace{0.6cm}Gradient descent       |
| Sets of rules                           | Cost/Utility          | \hspace{0.6cm}Conjugate gradient     |
| \hspace{0.3cm}Propositional rules       | Margin                | \hspace{0.6cm}Quasi-Newton methods   |
| \hspace{0.3cm}Logic programs            |                       | \hspace{0.3cm}Constrained            |
| Neural networks                         |                       | \hspace{0.6cm}Linear programming     |
| Graphical models                        |                       | \hspace{0.6cm}Quadratic programming  |
| \hspace{0.3cm}Bayesian networks         |                       |                                      |
| \hspace{0.3cm}Conditional random fields |                       |                                      |



\normalsize
