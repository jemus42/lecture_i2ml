## k-Nearest-Neighbors
```{r, include=FALSE, cache=FALSE}
library(mlr)

n = 30
set.seed(1234L)
tar = factor(c(rep(1L, times = n), rep(2L, times = n)))
feat1 = c(rnorm(n, sd = 1.5), rnorm(n, mean = 2, sd = 1.5))
feat2 = c(rnorm(n, sd = 1.5), rnorm(n, mean = 2, sd = 1.5))
bin.data = data.frame(feat1, feat2, tar)
bin.tsk = makeClassifTask(data = bin.data, target = "tar")

ap = adjust_path(paste0(getwd(), "/figure"))
ap.code = adjust_path(paste0(getwd(), "/code"))
```

- $k$-NN is a non-parametric method for regression and classification
- Models predictions $\yh$ for $x$ by looking at $\xyi$ near $x$
- Closeness implies distance measure (usually Euclidean)
- $N_k(x)$ is called the *neighborhood* of $x$, if it consists of the $k$-closest points $\xi$ to $x$ in the training sample


```{r, echo=FALSE, out.width=".4\\textwidth", fig.align = 'center', fig.height=3, fig.width=3.7, message=FALSE}
library(grid)
circleFun = function(center = c(0,0), diameter = 1, npoints = 100){
    r = diameter / 2
    tt = seq(0,2*pi,length.out = npoints)
    xx = center[1L] + r * cos(tt)
    yy = center[2L] + r * sin(tt)
    return(data.frame(x1 = xx, x2 = yy, class = NA))
}
set.seed(1234L)
n = 30L
x1 = rnorm(n)
x2 = rnorm(n)
mat = as.data.frame(cbind(x1, x2))
mat = rbind(mat, c(0, 0))
dists = sort(as.matrix(dist(mat))[n + 1L, ])
neighbs = as.numeric(names(dists[1:10L]))
mat$class = ifelse(1:(n+1) %in% neighbs, 1L, 0L)
mat[n + 1L, "class"] = 2L
mat$class = as.factor(mat$class)
circle.dat = circleFun(c(0, 0), 2.2, npoints = 100)
q = ggplot(mat, aes(x = x1, y = x2, color = class)) + geom_point(size = 3)
q = q + geom_polygon(data = circle.dat, alpha = 0.2, fill = "#619CFF")
q = q + theme(legend.position="none")
q
```


## k-Nearest-Neighbors


Predictions:

- For regression, average over neighbors' outputs:
  \[ \yh = \frac{1}{k} \sum_{\xi \in N_k(x)} \yi \]
- For classification: majority vote over neighbors' labels:
  \[ \yh = \argmax_l \sum_{\xi \in N_k(x)} \I\left(\yi = l\right) \]
  Posterior probabilities can be estimated with:
  \[ \hat{\pi}_l(x)= \frac{1}{k} \sum_{\xi \in N_k(x)} \I\left(\yi = l\right) \]


## k-Nearest-Neighbors: k = 3
```{r, echo = FALSE, message = FALSE}
plotLearnerPrediction("classif.kknn", iris.task, cv = 0, k = 3)
```

## k-Nearest-Neighbors: k = 10
```{r, echo = FALSE, message = FALSE}
plotLearnerPrediction("classif.kknn", iris.task, cv = 0, k = 10)
```

## k-Nearest-Neighbors: k = 30
```{r, echo = FALSE, message = FALSE}
plotLearnerPrediction("classif.kknn", iris.task, cv = 0, k = 30)
```

## k-Nearest-Neighbors: k = 50
```{r, echo = FALSE, message = FALSE}
plotLearnerPrediction("classif.kknn", iris.task, cv = 0, k = 50)
```


## k-Nearest-Neighbors

- No assumptions about the underlying data distribution.

- No training-step and is a very local model.

- Many more parameters than the simpler linear model.

- The smaller $k$, the less stable, less smooth and more "wiggly" the decision
  boundary becomes.

- Accuracy of $k$-NN can be severely degraded by the presence of noisy
  or irrelevant features, or if the feature scales are not consistent with
  their importance.

<!--
- We cannot simply use least-squares loss on the training data for picking $k$,
  because we would always pick $k=1$, instead we usually choose $k$ by
  cross-validation.
-->
<!--
- In binary classification, choose an odd $k$ to avoid ties.

- For $\yh$, we might inversely weigh neighbors with their distance to $x$,
  e.g., $w_i = 1/d(\xi, x)$
-->

