% Introduction to Machine Learning
% Day 4

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
library(rpart)
library(rpart.plot)
library(randomForest)
library(rattle)
library(smoof)
@

% Load all R packages and set up knitr
<<setup, child="../style/setup.Rnw", include = FALSE>>=
@


\lecturechapter{14}{Bagging and Random Forests}
\lecture{Introduction to Machine Learning}
\sloppy

\begin{vbframe}{Ensemble methods}

% \begin{itemize}

% \item A \enquote{\bf{base learner}} is often referred to as \enquote{weak learner}.
% \enquote{\bf{Weak learners}} (e.g. decision trees) are learning algorithms that should perform (slightly) better than random guessing, e.g. in case of a balanced  classification problem the misclassification rate should be (slightly) better than 0.5.
% \item The linear combination of the base learners potentially expands the hypothesis space.
% \end{itemize}

% \framebreak
% Common ensemble methods:
  \begin{itemize}
\item Ensemble methods combine the predictions of several \emph{base learners} and combine them into an aggregated estimator.
  \item Flavors:
    \begin{itemize}
    \item Bagging: Fit (identical) models on bootstrapped versions of the training data
    \item Boosting: same model is fit sequentially on reweighted data / residuals of the previous fits
      in order to improve the errors of the previous rounds. (Componentwise boosting: selects one of many different base learners in each iteration)
      \item Model stacking / \enquote{super learner}:  Fit different base learners on the same data or different \enquote{views} of the same data, then learn how to optimally aggregate their predictions, often with a second layer model.
    \end{itemize}
    \item Bagging and boosting are also called \enquote{homogeneous} ensembles, model stacking \enquote{heterogeneous} 
\end{itemize}
\framebreak

General homogeneous approach (often it works like this but not always)
  \begin{itemize}
  \item A \enquote{base learner} is selected and fitted multiple times to either resampled or reweighted versions of the original data.\\
  %\item The base learner is applied to either resampled or reweighted versions of the original dataset.
  % \item his results in $M$ prediction functions $g^{(1)}(x),\dots,g^{(M)}(x)$.
  This results in $M$ prediction functions $\bl{1},\dots,\bl{M}$.
  \item These $M$ function are aggregated, usually in a linear fashion.

  This results in the following final prediction function:
$$f(x) = \sum_{m=1}^M \betam \blm$$
with coefficients $\betai{1},\dots,\betai{M}$.
  \end{itemize}
\end{vbframe}


\begin{vbframe}{Bagging}

\begin{itemize}
  \item Bagging is based on \textbf{B}ootstrap \textbf{Agg}regation.
  \item Ensemble that improves instable / high variance learners
  \begin{itemize}
    \item Classification and regression trees
    \item Neural networks
    \item Piecewise variable selection in the regression case, etc.
  \end{itemize}
\end{itemize}

\framebreak

\begin{itemize}
\item Train on $B$ \textbf{bootstrap} samples of data $D$:
  \begin{itemize}
    \item Draw $n$ observations with replacement 
    \item Fit the base learner on each of the $B$ bootstrap samples
    \begin{center}
    \includegraphics[width=0.5\textwidth]{figure_man/bootstrapping.png}
    \end{center} 
  \end{itemize}
\item  \textbf{Aggregate} the predictions of the $B$ estimators:
  \begin{itemize}
    \item Aggregate via averaging or majority voting
    \item Posterior probabilities for $x$ in classification can be estimated by calculating class frequencies over the ensemble
  \end{itemize}
\end{itemize}

\framebreak
Making Predictions

\begin{itemize}
  \item In classification task, each tree's vote is calculated and the final output is determined with the majority.
  \item In regression task, output from each tree is averaged. 
\end{itemize}

\begin{center}
\includegraphics[width=0.8\textwidth]{figure_man/rf_majvot_averaging.png}
\end{center}


\framebreak

  \begin{itemize}
    \item Bagging is short for {\bf B}ootstrap {\bf Agg}regation.
    \item Proposed by Breiman (1996).
    \item Train on multiple bootstrap samples of data $\D$, then combine:

    \begin{enumerate}
      \item Create $M$ bootstrap samples of size $n$.
      \item Fit the base learner on each of the $M$ bootstrap samples.
      \item Aggregate the predictions of the $M$ estimators via averaging or majority voting.
    \end{enumerate}

  \end{itemize}

\framebreak

\begin{algorithm}[H]
  \small
  \setstretch{1.15}
  \caption*{Bagging algorithm}
  \begin{algorithmic}[1]
    \State {\bf Input: } Dataset $\D$, base learner, number of bootstraps $M$
    \For {$m = 1 \to M$}
      \State Draw a bootstrap sample $\D^{[m]}$ from $\D$.
      \State Train base learner on $\D^{[m]}$ to obtain model $\blm$
    \EndFor
    \State Aggregate the predictions of the $M$ estimators (via averaging or majority voting), to determine the bagging estimator:
    \begin{align*}
    \fM &= \frac{1}{M} \sum_{m=1}^M \blm \\
    \text{or}\quad \fM &= \argmax_{k \in \Yspace} \sum_{m=1}^M \I\left(\blm = k\right)
    \end{align*}
  \end{algorithmic}
\end{algorithm}

\framebreak

  \begin{itemize}
    \item Bagging reduces the variance of the estimator, but increases the bias.
    \item Bagging works best for unstable/high variance learners (learners where small perturbations of the training set can cause large changes in the prediction)

    \begin{itemize}
      \item Classification and regression trees
      \item Neural networks
      \item Step-wise/forward/backward variable selection for regression
    \end{itemize}

    \item For stable estimation methods bagging might degrade performance
    \begin{itemize}
      \item k-nearest neighbor
      \item discriminant analysis
      \item naive Bayes
      \item linear regression
    \end{itemize}

  \end{itemize}
\end{vbframe}

\begin{vbframe}{Why does bagging work?}
\begin{itemize}
  \item Suppose we have a numerical target variable and are looking at quadratic loss.
  \item The training datasets are given by $\D$, and the base learner estimators derived from it are $f(x)$. $f(x)$ is a random variable whose realized value depends on the values drawn from $\D$. 
  \item The datasets are sampled independently from distribution $\P_{xy}$ (data generating process).
  \item The {\em theoretical} aggregated estimator is given by
    \begin{align*}
      f_{\text{A}} (x) &= \E_\D[f(x)].
    \end{align*}

  \framebreak

 
 \item So: The more unstable or diverse $f(x)$ is, the more error reduction we can obtain by bagging.
 \item But the bagging estimator only approximates the theoretical $f_A$ (bootstrap), we therefore suffer from approximation error (bias) by using the empirical distribution function instead of the true data generating process and only performing $M$ bootstrap iterations instead of all possible bootstrap samples.
\item Bagging does not necessarily lead to an improved classifier -- (pathological) example:
\begin{itemize}
\item Binary outcome, $y = 1$ for all values of $x$
\item Consider random classifier $f$ with $\text{P}(\fx = 1) = 0.4$
(independent of $x$)
\item Expected misclassification rate for $f$ is 0.6
\item Expected misclassification rate for a majority-vote bagging estimator is 
$\P(z \leq \tfrac{M}{2}) \stackrel{M \to \infty}{\longrightarrow} 1$ for $z \sim B(M, p = 0.4)$.
\end{itemize}

%\framebreak
%\newcommand{\ambiblm}{\text{ambi}\left(\blm\right)}
%\newcommand{\ambifM}{\text{ambi}\left(\fM\right)}
% \item Bagging improves predictions if the baselearners in the ensemble are diverse:
% \begin{itemize}
% \item Measure diversity as \enquote{ambiguity} of baselearners and ensemble with $\ambiblm = \left(\blm- \fM \right)^2$,
%  $\ambifM = \tfrac{1}{M}\sum^M_{m} \ambiblm$
%  \item for quadratic loss, we can write:
%  \begin{scriptsize}
%  \begin{align*}
%  \ambifM &= \tfrac{1}{M}\sum^M_{m} \left(\blm- \fM\right)^2 \\
%          &= \tfrac{1}{M}\sum^M_{m} \left(\left(\blm - y\right)  + \left(y - \fM\right)\right)^2\\
%          &= \tfrac{1}{M}\sum^M_{m} L(y, \blm) + L(y, \fm) -\\
%          & \qquad\qquad\underbrace{- 2 \left(y - \tfrac{1}{M}\sum^M_{m}\blm\right)\left(y - \fM\right)}_{- 2 L(y, \fm)} \\
%   \text{so } \E_{xy}\left[L(y, \fm)\right] = \tfrac{1}{M}\sum^M_{m} \E_{xy}\left[L(y, \blm)\right] - \E_{xy}\left[\ambifM\right]
%   \end{align*}
%   \end{scriptsize}
%   \end{itemize}
%   \item The expected loss of the ensemble is always below the average loss of the single base learners, by the amount of ambiguity in the base learners.
%   \item The more accurate and diverse the base learners, the better the ensemble.
\end{itemize}



\end{vbframe}

\begin{vbframe}{Random Forests}

\begin{itemize}
  \item Modification of Bagging for Trees
  \item Proposed by Breiman (2001)
  \item Construction of bootstrapped **decorrelated** trees through randomized splits
  \item Trees are usually fully expanded, without aggressive early stopping or
    pruning, to increase variance
\end{itemize}

\framebreak
Bagging Predictor

Variance of the bagging prediction:

  \[ \rho \sigma^2 + \frac{1-\rho}{B} \sigma^2 = \left( \rho + (1 - \rho) \frac{1}{B} \right) \sigma^2 \]
  where $\sigma^2$ describes the variance of a tree and $\rho$ the positive correlation between trees
  
\begin{itemize}
  \item If trees are highly correlated ($\rho \approx 1$), variance $\rightarrow \sigma^2$
  \item If trees are uncorrelated ($\rho \approx 0$), variance $\rightarrow \frac{\sigma^2}{B}$
  \item Variance can be reduced by increasing the number of trees $B$
\end{itemize}

\framebreak
Bagging Predictor

<<eval=TRUE, echo=FALSE, fig.height= 3.5, fig.align="center">>=
# artificial graphic, numbers are fictive
rho = seq(0, 1, by = 0.001)
B = c(5, 50, 500)
sigma = 20 

grid = expand.grid(rho = rho, B = B)

grid$var = grid$rho * sigma + (1 - grid$rho) / grid$B * sigma
grid = grid[order(grid$B), ]
grid$B = as.factor(grid$B)

horizontal = data.frame(
  B = as.factor(B), 
  intercept = sigma / B,
  intercept.label = sigma / B + c(0, 0.7, -0.04))

p1 = ggplot(data = grid, aes(x = rho, y = var)) +
  geom_line(aes(group = B, colour = B)) +
  geom_hline(aes(yintercept = 20), colour = "black", lty = 2) +
  geom_hline(data = horizontal, aes(yintercept = intercept, colour = B), lty = 2) +
  xlab(expression(paste("Correlation of Trees ", rho))) +
  ylab("Theoretical Variance of a Bagging Prediction") +
  labs(colour = "Number of Trees") +
  annotate("text", x = 1.1, y = sigma, label = "sigma^2", parse = TRUE) +
  geom_text(data = horizontal, aes(x = rep(1.08, 3), y = intercept.label, color = B, 
    label = paste0("sigma^2 / ", B)), parse = TRUE, show.legend = FALSE, hjust = 0) +
  coord_cartesian(xlim = c(0, 1), clip = "off") +
  ylim(c(0, 20)) 

p1
@

\framebreak
Decorrelation of Trees

\begin{itemize}
  \item Draw bootstrap samples
  \item Instead of all $p$ features, draw $\text{mtry} \le p$ random split candidates. Recommended values:
  \begin{itemize}
    \item Classification:  $\lfloor \sqrt{p} \rfloor$
    \item Regression: : $\lfloor p/3 \rfloor$
  \end{itemize}
  \item Allow trees to slightly overfit by terminating them late
\end{itemize}

\framebreak
With 1 Tree on Iris
<<output4, eval=TRUE, echo=FALSE, fig.height= 4.5, fig.align="center">>=
plotLearnerPrediction("classif.randomForest", iris.task, cv = 0, ntree = 1)
@


\framebreak
With 10 Trees on Iris

<<output5, eval=TRUE, echo=FALSE, fig.height= 4.5, fig.align="center">>=
plotLearnerPrediction("classif.randomForest", iris.task, cv = 0, ntree = 10)
@

\framebreak
With 500 Trees on Iris

<<output6, eval=TRUE, echo=FALSE, fig.height= 4.5, fig.align="center">>=
plotLearnerPrediction("classif.randomForest", iris.task, cv = 0, ntree = 500)
@

\framebreak
OOB Error

\begin{center}
\includegraphics[width=0.65\textwidth]{figure_man/rF_oob_error.jpg}
\end{center}

\begin{itemize}
  \item OOB size: $P(\text{not drawn}) = \left(1 - \frac{1}{n}\right)^n \ \stackrel{n \to \infty}{\longrightarrow} \ \frac{1}{e} \approx 0.37$
  \item Predict all x with trees that didnt see it, average error
  \item Similar to CV, can be used for a quick model selection
\end{itemize}

\framebreak
OOB Error

Imagine a classification if an email is spam or not. With the Random 
Forest it is possible to get the mean misclassification errors inherently 
during the training:

<<eval=FALSE, echo = FALSE>>=


lrn = makeLearner("classif.randomForest", ntree = 150)

# download the dataset from OpenMl
d = OpenML::getOMLDataSet(data.id = 44)
spam = as_tibble(d)

task = makeClassifTask(data = spam, target = "class")
mod = train(lrn, task)
plot(getLearnerModel(mod))
@
<<echo=FALSE, fig.height=3.5, message=FALSE, warning=FALSE>>=
library(tidyr)
library(kernlab)

data(spam)
model = randomForest(type ~., data=spam, ntree=150, proximity=TRUE)
# layout(matrix(c(1,2),nrow=1), width=c(4,1))
# par(mar=c(5,4,4,0)) #No margin on the right side
# plot(model, log="y", lwd = 2, main = NULL)
# par(mar=c(5,0,4,2)) #No margin on the left side
# plot(c(0,1),type="n", axes=F, xlab="", ylab="")
# legend("top", colnames(model$err.rate),col=1:4,cex=0.8,fill=1:4)

data.frame(model$err.rate, iter = seq_len(nrow(model$err.rate))) %>%
  gather(key = "error.type", value = "error.measured", -iter) %>%
  ggplot(mapping = aes(x = iter, y = error.measured, color = error.type)) +
  geom_line() + 
  xlab("Number of Trees") +
  ylab("Mean Misclassification Error") +
  labs(color = "")
@

\framebreak

  \begin{itemize}
  \item Modification of bagging for trees proposed by Breiman (2001).
  \item Construction of bootstrapped {\bf decorrelated} trees
  \item Variance of the bagging prediction $\fM$ depends on (mean) correlation between trees $\rho = \corr\left(\bl{m}, \bl{m'}\right)$
      \begin{align*}
        \var\left(\fM\right) = \rho \sigma^2 + \frac{1-\rho}{M} \sigma^2,
      \end{align*}
      where $\sigma^2 = \var(\blm)$ describes the average variance of the trees.
\item we want to have as much $\var(\blm)$ as possible (diverse ensemble!), but mimize $\var\left(\fM\right)$ at the same time
\item[$\Rightarrow$] Reduce correlation by randomization in each node:
  Instead of all $p$ features, draw $\texttt{mtry} \le p$ random split candidates for each node.
\item[$\Rightarrow$] Trees $\blm$ are expanded liberally, without aggressive early stopping or pruning, to increase the diversity of the ensemble.
\end{itemize}


  \framebreak

  \begin{algorithm}[H]
  \caption*{Random Forest algorithm}
  \begin{algorithmic}[1]
  \State {\bf Input: }A dataset $\D$ of $n$ observations, number $M$ of trees
  in the forest, number $\texttt{mtry}$ of variables to draw for each split
  \For {$m = 1 \to M$}
  \State Draw a bootstrap sample $\D^{[m]}$ from $\D$
  \State Grow tree $\blm$ using $\D^{[m]}$
  \State For each split only consider $\texttt{mtry}$ randomly selected features
  \State Grow tree without early stopping or pruning
\EndFor
\State Aggregate the predictions of the $M$ estimators (via averaging or majority voting), to predict on new data.
\end{algorithmic}
\end{algorithm}

\framebreak

\begin{center}\includegraphics[width=0.95\textwidth]{figure_man/forest.png}\end{center}

\framebreak

\begin{itemize}
  \item The following values are recommended for $\texttt{mtry}$:
    \begin{itemize}
    \item Classification: $\lfloor \sqrt{p} \rfloor$
    \item Regression: $\lfloor p/3 \rfloor$
    \end{itemize}

  \item Out-of-bag error: On average $\approx$ 1/3 of points are not drawn.
    \begin{align*}
      \P(\text{Obs. not drawn}) &= \left(1 - \frac{1}{n}\right)^n
      \ \stackrel{n \to \infty}{\longrightarrow} \ \frac{1}{e} \approx
      \Sexpr{round(exp(-1), digits = 2)}.
      % e^x = lim_n(1 + x/n)^n
    \end{align*}

  To compute the OOB error, each observation $x$ is predicted only with those trees that did not use $x$ in their fit.
   \item The OOB error is similar to cross-validation estimation. It can also be used for a quicker model selection.
  \end{itemize}

\framebreak
% This pic has been created with powerpoint. To get a good quality pic
% I marked all the elements of the pic and copied them into IrfanView,
% where I can save these elements as a jpg.

% FS: this graphic sucks really badly. students don't understand it.
% should replace it with somethimg that uses notation from the rest of the slides. 

\begin{center}
\includegraphics{figure_man/rF_oob_error.jpg}
\end{center}

\framebreak

\begin{figure}
<<rf-friedman-plot1, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4.5>>=
set.seed(1)
lrn = makeLearner("regr.randomForest", predict.type = "response")
# remove this values from title
lrn$par.vals$se.boot = NULL
lrn$par.vals$ntree.for.se = NULL
lrn2 = makeLearner("classif.randomForest")
# remove this values from title
lrn2$par.vals$se.boot = NULL
lrn2$par.vals$ntree.for.se = NULL
task = convertMLBenchObjToTask("mlbench.friedman1", n = 500, sd = 0.1)
plotLearnerPrediction(lrn, task, cv = 0, ntree = 1) + 
  scale_f() + scale_c() +
  labs(caption = "M = 1")
@
\caption{randomForest trained on the \enquote{friedman1} regression task
from the \pkg{mlbench} \pkg{R}-package with increasing number of trees}
\end{figure}

\framebreak
<<rf-friedman-plot2, echo=FALSE, warning = FALSE>>=
plotLearnerPrediction(lrn, task, cv = 0, ntree = 10) + 
  scale_f() + scale_c() +
  labs(caption = "M = 10")
@
\framebreak
<<rf-friedman-plot3, echo=FALSE, warning = FALSE>>=
plotLearnerPrediction(lrn, task, cv = 0, ntree = 500) + 
  scale_f() + scale_c() +
  labs(caption = "M = 500")
@
\framebreak
\begin{figure}
<<iris-rf-plot1, echo=FALSE, fig.height=4.5, warning = FALSE>>=
plotLearnerPrediction(lrn2, iris.task, cv = 0, ntree = 1) + 
  scale_f_d() + scale_c_d() +
  labs(caption = "M = 1")
@
\caption{randomForest on \enquote{iris} for increasing number of trees}
\end{figure}
\framebreak
<<iris-rf-plot2, echo=FALSE, warning = FALSE>>=
plotLearnerPrediction(lrn2, iris.task, cv = 0, ntree = 10)+ 
  scale_f_d() + scale_c_d() +
  labs(caption = "M = 10")
@
\framebreak
<<iris-rf-plot3, echo=FALSE>>=
plotLearnerPrediction(lrn2, iris.task, cv = 0, ntree = 500)+ 
 scale_f_d() + scale_c_d() +
  labs(caption = "M = 500")
@
\framebreak

<<rf-oob-error-plot, echo=FALSE, out.height = '.8\\textheight'>>=
mod = train(lrn, task)$learner.model
plot(mod, main = "")
@
OOB error for different number of trees for regression forest example.
\end{vbframe}

\begin{vbframe}{Random Forests in R}

<<eval=TRUE>>=
library(OpenML)
library(tibble)

# required for loading the dataset
lrn = makeLearner("classif.randomForest", ntree = 150)

# download the dataset 
d = OpenML::getOMLDataSet(data.id = 44)
spam = as_tibble(d)

task = makeClassifTask(data = spam, target = "class")
mod = train(lrn, task)

# extract OOB predictions for evaluation
oob = getOOBPreds(mod, task) 
performance(oob) 
@

\framebreak

<<echo = FALSE, fig.height=4, message=FALSE, warning=FALSE>>=


# download the dataset 
d = OpenML::getOMLDataSet(data.id = 44)
spam = as_tibble(d)

model = randomForest(class ~., data = spam, ntree=150, proximity=T)

data.frame(model$err.rate, iter = seq_len(nrow(model$err.rate))) %>%
  gather(key = "error.type", value = "error.measured", -iter) %>%
  ggplot(mapping = aes(x = iter, y = error.measured, color = error.type)) +
  geom_line() + 
  xlab("Number of Trees") +
  ylab("Mean Misclassification Error") +
  labs(color = "")
@

\framebreak

<<message = FALSE>>=
lrn = makeLearner("regr.randomForest", ntree = 100)
mod = train(lrn, bh.task)
getLearnerModel(mod)
@


\framebreak

<<message = FALSE, fig.height= 4.5, fig.align="center">>=
plot(getLearnerModel(mod)) # Visualizes OOB error vs. number of trees
@
\end{vbframe}

\endlecture
