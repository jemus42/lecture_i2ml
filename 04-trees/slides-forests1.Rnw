% Introduction to Machine Learning
% Day 4

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
library(rpart)
library(rpart.plot)
library(randomForest)
library(rattle)
library(smoof)
@

% Load all R packages and set up knitr
<<setup, child="../style/setup.Rnw", include = FALSE>>=
@


\lecturechapter{Bagging and Random Forest 1}
\lecture{Introduction to Machine Learning}
\sloppy



\begin{vbframe}{Bagging}

\begin{itemize}
  \item Bagging is based on \textbf{B}ootstrap \textbf{Agg}regation.
  \item Ensemble that improves instable / high variance learners by variance smoothing
\end{itemize}

Train on $B$ \textbf{bootstrap} samples of data $D$:
\begin{itemize}
  \item Draw $n$ observations with replacement
  \item Fit the base learner on each of the $B$ bootstrap samples
\end{itemize}

\begin{center}
% FIGURE SOURCE: No source
\includegraphics[width=0.45\textwidth]{figure_man/bootstrapping.png}
\end{center}



\framebreak

\textbf{Aggregate} the predictions of the $B$ estimators:
  \begin{itemize}
    \item Aggregate via averaging (regression) or majority voting (classification)
    \item Posterior probabilities for $x$ in classification can be estimated by calculating class frequencies over the ensemble
  \end{itemize}

\begin{center}
% FIGURE SOURCE: No source
\includegraphics[width=0.7\textwidth]{figure_man/rf_majvot_averaging.png}
\end{center}

% \begin{algorithm}[H]
%   \small
%   \setstretch{1.15}
%   \caption*{Bagging algorithm}
%   \begin{algorithmic}[1]
%     \State {\bf Input: } Dataset $\D$, base learner, number of bootstraps $M$
%     \For {$m = 1 \to M$}
%       \State Draw a bootstrap sample $\D^{[m]}$ from $\D$.
%       \State Train base learner on $\D^{[m]}$ to obtain model $\blm$
%     \EndFor
%     \State Aggregate the predictions of the $M$ estimators (via averaging or majority voting), to determine the bagging estimator:
%     \begin{align*}
%     \fM &= \frac{1}{M} \sum_{m=1}^M \blm \\
%     \text{or}\quad \fM &= \argmax_{k \in \Yspace} \sum_{m=1}^M \I\left(\blm = k\right)
%     \end{align*}
%   \end{algorithmic}
% \end{algorithm}

\framebreak

  \begin{itemize}
    \item Reduces variance of the predictor, but (slightly) increases its bias
    \item Bagging works best for unstable/high variance learners (learners where small perturbations of the training set can cause large changes in the prediction)

    \begin{itemize}
      \item Classification and regression trees
      \item Neural networks
      \item Step-wise/forward/backward variable selection for regression
    \end{itemize}

    \item For stable estimation methods bagging might degrade performance
    \begin{itemize}
      \item k-nearest neighbor
      \item discriminant analysis
      \item naive Bayes
      \item linear regression
    \end{itemize}

  \end{itemize}
\end{vbframe}

% \begin{vbframe}{Why does bagging work?}
% \begin{itemize}
%   \item Suppose we have a numerical target variable and are looking at quadratic loss.
%   \item The training datasets are given by $\D$, and the base learner estimators derived from it are $f(x)$. $f(x)$ is a random variable whose realized value depends on the values drawn from $\D$.
%   \item The datasets are sampled independently from distribution $\P_{xy}$ (data generating process).
%   \item The {\em theoretical} aggregated estimator is given by
%     \begin{align*}
%       f_{\text{A}} (x) &= \E_\D[f(x)].
%     \end{align*}

%   \framebreak


%  \item So: The more unstable or diverse $f(x)$ is, the more error reduction we can obtain by bagging.
%  \item But the bagging estimator only approximates the theoretical $f_A$ (bootstrap), we therefore suffer from approximation error (bias) by using the empirical distribution function instead of the true data generating process and only performing $M$ bootstrap iterations instead of all possible bootstrap samples.
% \item Bagging does not necessarily lead to an improved classifier -- (pathological) example:
% \begin{itemize}
% \item Binary outcome, $y = 1$ for all values of $x$
% \item Consider random classifier $f$ with $\text{P}(\fx = 1) = 0.4$
% (independent of $x$)
% \item Expected misclassification rate for $f$ is 0.6
% \item Expected misclassification rate for a majority-vote bagging estimator is
% $\P(z \leq \tfrac{M}{2}) \stackrel{M \to \infty}{\longrightarrow} 1$ for $z \sim B(M, p = 0.4)$.
% \end{itemize}

%\framebreak
%\newcommand{\ambiblm}{\text{ambi}\left(\blm\right)}
%\newcommand{\ambifM}{\text{ambi}\left(\fM\right)}
% \item Bagging improves predictions if the baselearners in the ensemble are diverse:
% \begin{itemize}
% \item Measure diversity as \enquote{ambiguity} of baselearners and ensemble with $\ambiblm = \left(\blm- \fM \right)^2$,
%  $\ambifM = \tfrac{1}{M}\sum^M_{m} \ambiblm$
%  \item for quadratic loss, we can write:
%  \begin{scriptsize}
%  \begin{align*}
%  \ambifM &= \tfrac{1}{M}\sum^M_{m} \left(\blm- \fM\right)^2 \\
%          &= \tfrac{1}{M}\sum^M_{m} \left(\left(\blm - y\right)  + \left(y - \fM\right)\right)^2\\
%          &= \tfrac{1}{M}\sum^M_{m} L(y, \blm) + L(y, \fm) -\\
%          & \qquad\qquad\underbrace{- 2 \left(y - \tfrac{1}{M}\sum^M_{m}\blm\right)\left(y - \fM\right)}_{- 2 L(y, \fm)} \\
%   \text{so } \E_{xy}\left[L(y, \fm)\right] = \tfrac{1}{M}\sum^M_{m} \E_{xy}\left[L(y, \blm)\right] - \E_{xy}\left[\ambifM\right]
%   \end{align*}
%   \end{scriptsize}
%   \end{itemize}
%   \item The expected loss of the ensemble is always below the average loss of the single base learners, by the amount of ambiguity in the base learners.
%   \item The more accurate and diverse the base learners, the better the ensemble.
% \end{itemize}
% \end{vbframe}

\begin{vbframe}{Random Forests}


\begin{itemize}
  \item Modification of bagging for trees proposed by Breiman (2001)
  \item Construction of bootstrapped decorrelated trees through randomized splits
  \item Trees are usually fully expanded, without aggressive early stopping or
    pruning, to increase variance
\end{itemize}
\begin{center}
\includegraphics[width=0.55\textwidth]{figure_man/forest.png}
\end{center}
\end{vbframe}

\begin{vbframe}{Variance of Bagging}


  \[ \rho \sigma^2 + \frac{1-\rho}{B} \sigma^2 = \left( \rho + (1 - \rho) \frac{1}{B} \right) \sigma^2 \]
  $\sigma^2$ is variance of a tree and $\rho$ the correlation between trees

\begin{itemize}
  \item If trees are highly correlated ($\rho \approx 1$), variance $\rightarrow \sigma^2$
  \item If trees are uncorrelated ($\rho \approx 0$), variance $\rightarrow \frac{\sigma^2}{B}$
  \item Variance can be reduced by increasing the number of trees $B$
\end{itemize}

<<eval=TRUE, echo=FALSE, fig.height= 2.5, fig.align="center">>=
# artificial graphic, numbers are fictive
rho = seq(0, 1, by = 0.001)
B = c(5, 50)
sigma = 20

grid = expand.grid(rho = rho, B = B)

grid$var = grid$rho * sigma + (1 - grid$rho) / grid$B * sigma
grid = grid[order(grid$B), ]
grid$B = as.factor(grid$B)

horizontal = data.frame(
  B = as.factor(B),
  intercept = sigma / B,
  intercept.label = sigma / B + c(0, 0.7))

p1 = ggplot(data = grid, aes(x = rho, y = var)) +
  geom_line(aes(group = B, colour = B)) +
  geom_hline(aes(yintercept = 20), colour = "black", lty = 2) +
  geom_hline(data = horizontal, aes(yintercept = intercept, colour = B), lty = 2) +
  xlab(expression(paste("Correlation of Trees ", rho))) +
  ylab("Variance") +
  labs(colour = "Number of Trees") +
  annotate("text", x = 1.1, y = sigma, label = "sigma^2", parse = TRUE) +
  geom_text(data = horizontal, aes(x = rep(1.08, 2), y = intercept.label, color = B,
    label = paste0("sigma^2 / ", B)), parse = TRUE, show.legend = FALSE, hjust = 0) +
  coord_cartesian(xlim = c(0, 1), clip = "off") +
  ylim(c(0, 20))

p1
@
\end{vbframe}

\begin{vbframe}{Random feature sampling}

\begin{itemize}
  \item From our variance analysis we can see that decorrelating trees further
    might reduce the variance of the predictor
  \item Simple randomized approach:\\
    Instead of all $p$ features, draw $\text{mtry} \le p$ random split candidates. Recommended values:
  \begin{itemize}
    \item Classification: $\lfloor \sqrt{p} \rfloor$
    \item Regression: $\lfloor p/3 \rfloor$
  \end{itemize}
\end{itemize}
\end{vbframe}

% \begin{Random Forest Algorithm}
%   \begin{algorithm}[H]
%   \caption*{Random Forest algorithm}
%   \begin{algorithmic}[1]
%   \State {\bf Input: }A dataset $\D$ of $n$ observations, number $M$ of trees
%   in the forest, number $\texttt{mtry}$ of variables to draw for each split
%   \For {$m = 1 \to M$}
%   \State Draw a bootstrap sample $\D^{[m]}$ from $\D$
%   \State Grow tree $\blm$ using $\D^{[m]}$
%   \State For each split only consider $\texttt{mtry}$ randomly selected features
%   \State Grow tree without early stopping or pruning
% \EndFor
% \State Aggregate the predictions of the $M$ estimators (via averaging or majority voting), to predict on new data.
% \end{algorithmic}
% \end{algorithm}
% \end{vbframe}

\begin{vbframe}{Effect of ensemble size}
With 1 Tree on Iris
<<echo=FALSE, fig.height= 5.5, fig.align="center">>=
plotLearnerPrediction("classif.randomForest", iris.task, cv = 0, ntree = 1)
@


\framebreak
With 10 Trees on Iris

<<echo=FALSE, fig.height= 5.5, fig.align="center">>=
plotLearnerPrediction("classif.randomForest", iris.task, cv = 0, ntree = 10)
@

\framebreak
With 500 Trees on Iris

<<echo=FALSE, fig.height= 5.5, fig.align="center">>=
plotLearnerPrediction("classif.randomForest", iris.task, cv = 0, ntree = 500)
@
\end{vbframe}

\begin{vbframe}{Out-of-Bag Error Estimate}
With the RF it is possible to obtain unbiased estimates of generalization error directly
during training:

<<echo=FALSE, fig.height=4.5, message=FALSE, warning=FALSE>>=
library(tidyr)
library(kernlab)

data(spam)
model = randomForest(type ~., data=spam, ntree=150, proximity=TRUE)
# layout(matrix(c(1,2),nrow=1), width=c(4,1))
# par(mar=c(5,4,4,0)) #No margin on the right side
# plot(model, log="y", lwd = 2, main = NULL)
# par(mar=c(5,0,4,2)) #No margin on the left side
# plot(c(0,1),type="n", axes=F, xlab="", ylab="")
# legend("top", colnames(model$err.rate),col=1:4,cex=0.8,fill=1:4)

data.frame(model$err.rate, iter = seq_len(nrow(model$err.rate))) %>%
  gather(key = "error.type", value = "error.measured", -iter) %>%
  ggplot(mapping = aes(x = iter, y = error.measured, color = error.type)) +
  geom_line() +
  xlab("Number of Trees") +
  ylab("MCE") +
  labs(color = "")
@

\framebreak

\begin{center}
% FIGURE SOURCE: https://docs.google.com/presentation/d/16gg9PqtEV_Ii_ZBw0zA9GdUIled-6OVkoqArX1ouTmo/edit#slide=id.p
\includegraphics[width=0.9\textwidth]{figure_man/rF_oob_error_new.pdf}
\end{center}

\begin{itemize}
  \item OOB size: $P(\text{not drawn}) = \left(1 - \frac{1}{n}\right)^n \ \stackrel{n \to \infty}{\longrightarrow} \ \frac{1}{e} \approx 0.37$
  \item Predict all x with trees that didn't see it, average error
  \item Similar to 3-CV, can be used for a quick model selection
\end{itemize}


\end{vbframe}




% \begin{figure}
% <<rf-friedman-plot1, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4.5>>=
% lrn = makeLearner("regr.randomForest", predict.type = "response")
% # remove this values from title
% lrn$par.vals$se.boot = NULL
% lrn$par.vals$ntree.for.se = NULL
% lrn2 = makeLearner("classif.randomForest")
% # remove this values from title
% lrn2$par.vals$se.boot = NULL
% lrn2$par.vals$ntree.for.se = NULL
% task = convertMLBenchObjToTask("mlbench.friedman1", n = 500, sd = 0.1)
% plotLearnerPrediction(lrn, task, cv = 0, ntree = 1) +
%   scale_f() + scale_c() +
%   labs(caption = "M = 1")
% @
% \caption{randomForest trained on the \enquote{friedman1} regression task
% from the \pkg{mlbench} \pkg{R}-package with increasing number of trees}
% \end{figure}



% <<rf-oob-error-plot, echo=FALSE, out.height = '.8\\textheight'>>=
% mod = train(lrn, task)$learner.model
% plot(mod, main = "")
% @
% OOB error for different number of trees for regression forest example.


\endlecture
