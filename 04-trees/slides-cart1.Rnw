% Introduction to Machine Learning
% Day 4

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
library(rpart)
library(rpart.plot)
library(randomForest)
library(rattle)
library(smoof)
@

% Load all R packages and set up knitr
<<setup, child="../style/setup.Rnw", include = FALSE>>=
@


\lecturechapter{CART 1}
\lecture{Introduction to Machine Learning}
\sloppy

<<results='hide'>>=
draw_cart_on_iris = function(depth, with_tree_plot = FALSE) {
  task = makeClassifTask(data = iris[,3:5], target = "Species")
  lrn = makeLearner("classif.rpart", cp = 0, minbucket = 4, maxdepth = depth)
  model = train(lrn, task)
  p = plotLearnerPrediction(lrn, task, gridsize = 100, cv = 0, prob.alpha = FALSE, err.mark = "train")
  p = p + 
    theme(axis.line = element_line(colour = "black"),
        panel.grid.major = element_line(colour = "grey80"),
        panel.grid.minor = element_line(colour = "grey80"),
        panel.border = element_blank(),
        panel.background = element_blank()) + 
    ggtitle("Iris Data")
  print(p)
  if (with_tree_plot)
    # "palettes" should ensure that colors in tree growing in rectangles and tree match
    fancyRpartPlot(model$learner.model, sub = "", palettes = c("Reds", "Greens", "Blues"))
  invisible(model)
}
@

\begin{vbframe}{Tree Model and Prediction}
  \begin{itemize}
    \item Classification and Regression Trees, introduced by Breiman
    \item Binary splits are constructed top-down
    \item Only constant prediction in each leaf, either a numerical value,
  a class label or a probability vector.
  \end{itemize}
    \begin{figure}
    \centering
      % FIGURE SOURCE: No source
      \includegraphics[height = 5.0cm]{figure_man/labelling_of_tree.png}
    \end{figure}
\end{vbframe}

\begin{vbframe}{Tree Model and Prediction}
  \begin{itemize}
    \item During prediction, observations are passed down the tree, according to the splitting rules
      in each node
    \item An observation will end up in exactly one leaf node
    \item The constant label of that leaf node determines the prediction
  \end{itemize}
    \begin{figure}
    \centering
      % FIGURE SOURCE: No source
      \includegraphics[height = 5.0cm]{figure_man/labelling_of_tree.png}
    \end{figure}
\end{vbframe}

\begin{vbframe}{Tress as an additive model}
Each point in input space is assigned to one leaf node, and each leaf node has a set of input points leading to it, through axis-parallel splits. Hence, trees divide the feature space $\Xspace$ into rectangles. We can write the tree as a additive model over the leaf-rectangles:
  \begin{align*}
    \fx = \sum_{m=1}^M c_m \I(x \in Q_m),
  \end{align*}
  where $M$ rectangles $Q_m$ are used. $c_m$ is a predicted numerical response, a class label or a class
  distribution.

<<result='hide', fig.height=2.4>>=
model = draw_cart_on_iris(depth = 2)
@

\end{vbframe}

\begin{vbframe}{Tree Growing}
In the greedy top-down construction, features and split points are selected by exhaustive search. The training data is distributed to child nodes according to the splits - see below that nodes display the percentage of currently contained data.
<<result='hide', fig.height=2.2>>=
draw_cart_on_iris(depth = 1, with_tree_plot = TRUE)
@

\framebreak

We start with an empty tree, a full root node of all data. We search for a feature and split-point, which makes the label distribution in the resulting left and right child (or sub-rectangles) most pure (defined later). See below that nodes display their current label distribution.
<<result='hide', fig.height=2.2>>=
draw_cart_on_iris(depth = 1, with_tree_plot = TRUE)
@

\framebreak

We proceed then recursively for each child node:
Iterate over all features, and for each feature over all split points. Select the "best" split, divide training data from parent into left and right child.
<<result='hide', fig.height=2.2>>=
draw_cart_on_iris(depth = 2, with_tree_plot = TRUE)
@

\framebreak

We proceed then recursively for each child node:
Iterate over all features, and for each feature over all split points. Select the "best" split, divide training data from parent into left and right child.
<<result='hide', fig.height=2.2>>=
draw_cart_on_iris(depth = 3, with_tree_plot = TRUE)
@

\end{vbframe}

\begin{vbframe}{Split placement}
<<fig.height=4>>=
task = subsetTask(iris.task, seq(1, 150, by = 20))
lrn = makeLearner("classif.rpart", cp = 0, minbucket = 1, maxdepth = 1)
pl = plotLearnerPrediction(lrn, task, gridsize = 100,
  cv = 0, prob.alpha = FALSE, err.mark = "none")
pl = pl + theme(legend.position="none")
print(pl)
@
\lz
Splits are usually placed "in the middle" between the observations they split, so the large margin to the next observations ensures better generalization
\end{vbframe}

\begin{vbframe}{Regression Example}
\begin{columns}[T,onlytextwidth]
\column{0.2\textwidth}
<<out.width='\\textwidth'>>=
modForrester = makeSingleObjectiveFunction(
  name = "Modification Forrester et. all function",
  fn = function(x) (sin(4*x - 4)) * ((2*x - 2)^2) * (sin(20*x - 4)),
  par.set = makeNumericParamSet(lower = 0, upper = 1, len = 1L),
  noisy = TRUE
)
design = generateDesign(7L, getParamSet(modForrester), fun = lhs::maximinLHS)
design$y = modForrester(design)
ordered.design = design[order(design$x),]
rownames(ordered.design) = NULL
kable(ordered.design, digits = 3)
@

\hspace{0.5cm}
\column{0.7\textwidth}
\includegraphics[height = 0.56\textheight]{figure_man/CART_reg_example.pdf}
\end{columns}
\vspace{0.5cm}
Data points (red) were generated from the underlying function (black):

$ sin(4x - 4) * (2x - 2)^2 * sin(20x -4) $

% \framebreak

% BB: doesnt seem too useful to show this, nothing really new in here
% <<fig.height=5>>=
% regr.task = makeRegrTask(data = design, target = "y")
% regr.rpart = makeLearner("regr.rpart", par.vals = list(minsplit=1, minbucket = 1))
% regr.model = train(regr.rpart, regr.task)
% fancyRpartPlot(regr.model$learner.model, sub="")
% @
\end{vbframe}


\begin{vbframe}{Stopping Criteria}
  At some point we have to stop the recursive splitting of child nodes and produce a leaf. Multiple simple criteria for stopping can be defined:
  \begin{itemize}
    \item Minimal number of observations per node, for a split to be tried
    \item Minimal number of observations that must be contained in a leaf
    \item Minimal increase in label distribution purity that must be reached for the best split
    \item Maximum number of levels for the tree
  \end{itemize}
\end{vbframe}

\begin{vbframe}{Constant predictions in leafs}
  After growing, for each leaf, we know which training observations are assigned to it. We produce a constant optimal rule here, for the complete associated
  rectangle $Q_m$
  \begin{itemize}
    \item Regression: We usually use the average value of all $\yi$ contained in the leaf. This is the optimal constant prediction under squared loss.
      We could also fit the median, which would be optimal under L1 loss.
    \item Classification: We fit the most common label of the data contained in that node. In order to estimate probabilities, we simply count class proportions for the training data contained in that leaf.
  \end{itemize}
\end{vbframe}

\begin{vbframe}{Splitting criteria}

  Let $\Np \subseteq \D$ be the data of a parent node with two child nodes $\Nl$ and $\Nr$,
  which are created by a split w.r.t. feature $\xj$ at split point $t$:
  \begin{align*}
    \Nl &= \{ (x,y) \in \Np: \xj \leq t \} \text{ and } \Nr = \{ (x,y) \in \Np: \xj > t \}.
  \end{align*}
  In order to quantify the (negative) quality of the considered split we compute the empirical risks
  of both child nodes and sum it up
    $$\risk(j, t) = \risk(\Nl) + \risk(\Nr)$$
  The risk $\risk(N)$ for a node is simply the summed loss for the data contained in that node
  under a selected loss function $L$
  $$\risk(\Np) = \sum\limits_{(x,y) \in \Np} L(y, c), $$
  if we model the data in that node with an optimal constant $c = \argmin_c \risk(\Np)$.
  This is basically pretending that after split $(j,t)$ our two child nodes become leafs.

\end{vbframe}

\begin{vbframe}{Split criteria: Regression}
\begin{itemize}
 \item For regression, we usually use $L_2$ loss / the SSE-criterion
  $$\risk(\Np) = \sum\limits_{(x,y) \in \Np} (y - c)^2$$
 \item The best constant under $L_2$ is the mean
  $c = \bar{y}_\Np = \frac{1}{|\Np|} \sum\limits_{(x,y) \in \Np} y$
\item Up to a constant, we just computed the variance of the label distribution in $\Np$; we can also interpret this as a way of measuring the impurity of the distribution / fluctuation around the constant.
  \item We could have also used the $L_1$ loss and the median
\end{itemize}

<<result='hide', fig.height=2.2>>=
x = rnorm(20)
m = mean(x)
s = var(x)
d = data.frame(x = x, y = 1)
pl = ggplot(d, aes(x = x, y = y))
pl = pl + geom_point()
pl = pl + geom_segment(x = m, xend = m, y = 0.9, yend = 1.1)
pl = pl + geom_segment(x = m-s, xend = m+s, y = 1.05, yend = 1.05)
pl = pl + geom_text(x = m, y = 1.15, label = "mean +- 1 std")
pl = pl + xlab("y") + ylab("")
pl = pl + coord_map(xlim = c(-2, 2),ylim = c(0.75, 1.25))
print(pl)
@
\end{vbframe}

\begin{vbframe}{Splitting Criteria: Classification}

\begin{itemize}
\item We normally use either Brier (so $L_2$ loss on probabilities) or the Bernoulli loss (from logistic regression) as loss function
\item We usually model constant predictions in node $\Np$ by simply calculating the class proportions
$$ \pikN = \frac{1}{|\Np|} \sum\limits_{(x,y) \in \Np} [y = k] $$
This is the optimal constant under the 2 mentioned losses above
\end{itemize}

<<fig.height=2.2>>=
d = data.frame(prob = c(0.1, 0.7, 0.2), label = 1:3)
pl = ggplot(data = d, aes(x = label, y = prob, fill = label))
pl = pl + geom_bar(stat = "identity")  + theme(legend.position = "none")
pl = pl + ylab("Class prob.") + xlab("Label")
print(pl)
@
\end{vbframe}

\begin{vbframe}{Splitting Criteria: Comments}

\begin{itemize}
\item Tree splitting is usually introduced under the concept of "impurity reduction", but our approach above is simpler and more in line with empirical risk minimization and our previous concepts
\item Splitting on Brier score is normally called splitting on Gini impurity
$$I(\Np) = \sum_{k\neq k'} \pikN \hat\pi_{\Np k'} = \sum_{k=1}^g \pikN(1-\pikN)$$
\item Splitting on Bernoulli loss is normally called splitting on entropy impurity
$$I(\Np) = -\sum_{k=1}^g \pikN \log \pikN$$
\item The pairs Brier score / Gini and Bernoulli / entropy are equivalent (which is not hard to prove, but will not be done here)
\end{itemize}
\end{vbframe}

\begin{vbframe}{Splitting with misclassification loss}
\begin{itemize}
\item Why don't we simply split according to the misclassification loss? We could use the majority class in each child
  as "best constant" and count how many errors we make? Aren't we often interested in minimizing this error, but have to
  approximate it? We do not have to compute derivatives when we optimize the tree!
\item Actually, that is possible, but Brier score and Bernoulli loss are more sensitive to changes in the node probabilities, and
  therefore often preferred
\end{itemize}

\framebreak

Example: two-class problem with 400 obs in each class and two possible splits:
\begin{columns}[T,onlytextwidth]
\column{0.5\textwidth}
\begin{center}
\textbf{Split 1:} \\
\vspace{0.25cm}
<<split1>>=
class = as.factor(c(rep(0,400), rep(1,400)))
x1 = as.factor(c(rep(0,300), rep(1,400), rep(0,100)))
x2 = as.factor(c(rep(0,600), rep(1,200)))
tab = table(x1, class)
tab2 = table(x2, class)
rownames(tab) = c("Left node", "Right node")
rownames(tab2) = c("Left node", "Right node")
kable(tab, row.names = TRUE, col.names = c("class 0", "class 1"))
@
\end{center}
\column{0.5\textwidth}
\begin{center}
\textbf{Split 2:} \\
\vspace{0.25cm}
<<split2>>=
kable(tab2, row.names = TRUE, col.names = c("class 0", "class 1"))
@
\end{center}
\end{columns}

\lz

\begin{itemize}
\item Both splits misclassify 200 observations
\item Split 2 produces a pure node and is probably preferable.
\item Brier and Bernoulli loss (slightly) prefer the 2nd split
% \item The average node impurity for Split 2 is $\frac{1}{3}$ (Gini) or $0.344$ (Entropy)
 % Gini: 6/8 * 2 * 1/3 * 2/3
 % entropy: 6/8 * ((1/3 * log(1/3) + 2/3 * log(2/3)) / (2 * log(0.5)))
\item Calculation for Brier:\\
$Split1: 300(0-\frac{1}{4})^2 + 100(1-\frac{1}{4})^2 + 100(0-\frac{3}{4})^2+300(1-\frac{3}{4})^2 = 150$\\
$Split2: 400(0-\frac{1}{3})^2 + 200(1-\frac{1}{3})^2 = 133.3$
\end{itemize}
\end{vbframe}






% % BB: as we are not really talking too much about impurity anymore, I took this out
% % <<splitcriteria-plot, results='hide', fig.height=5>>=
% % Colors = pal_3
% % par(mar = c(5.1, 4.1, 0.1, 0.1))
% % p = seq(1e-6, 1-1e-6, length.out = 200)
% % entropy = function(p) (p * log(p) + (1 - p) * log(1 - p))/(2 * log(0.5))
% % gini = function(p) 2 * p * (1 - p)
% % missclassification = function(p) (1 - max(p, 1 - p))
% % plot(p, entropy(p), type = "l", col = Colors[1], lwd = 1.5, ylab = "",
% %   ylim = c(0, 0.6), xlab = expression(hat(pi)[Nk]))
% % lines(p, gini(p), col = Colors[2], lwd = 1.5)
% % lines(p, sapply(p, missclassification), col = Colors[3], lwd = 1.5)
% % legend("topright", c("Gini Index", "Entropy", "Misclassification Error"),
% %        col = Colors[1:3], lty = 1)
% % @


\endlecture
