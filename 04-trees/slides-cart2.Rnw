% Introduction to Machine Learning
% Day 4

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
library(rpart)
library(rpart.plot)
library(randomForest)
library(rattle)
library(smoof)
@

% Load all R packages and set up knitr
<<setup, child="../style/setup.Rnw", include = FALSE>>=
@


\lecturechapter{CART 2}
\lecture{Introduction to Machine Learning}

\sloppy

\begin{vbframe}{Monotone feature transformations}

Monotone transformations of one or several features will not change the value of the impurity measure, neither the structure of the tree (just the numerical value of the split point).
\vspace{0.5cm}
\begin{columns}[T]
\column{0.49\textwidth}
Original data
<<>>=
x = c(1,2,7,10,20)
y = c(1,1,0.5,10,11)
data = t(data.frame(x = x, y = y))
kable(data)
@
% use picture created in rsrc/monotone_trafo.R
\includegraphics[width = \textwidth]{figure_man/monotone_trafo1.png}
\column{0.49\textwidth}
Data with log-transformed $x$
<<fig.align='right'>>=
log.x = log(x)
data = t(data.frame(log.x, y))
rownames(data) = c("log(x)", "y")
kable(data,digits = 1)
@
% use picture created in rsrc/monotone_trafo.R
\includegraphics[width = \textwidth]{figure_man/monotone_trafo2.png}
\end{columns}
\vspace{0.5cm}
\centering
\end{vbframe}

\begin{vbframe}{CART: Categorical Predictors}
  \begin{itemize}
  \item A categorical split partitions the feature levels:
    $$x_j \in \{a,c,e\} \leftarrow node \rightarrow x_j \in \{b,d\} $$
  \item For a categorical feature with $m$ categories,
  there are ca. $2^m$ different possible partitions of the $m$ values into two groups ($2^{m-1} - 1$ exactly, considering symmetry and that we do not split off empty groups)
  \item Searching over these becomes prohibitive for larger values of $m$
  \item But for regression with squared loss and binary classification shortcuts exist.
  \end{itemize}

  \framebreak

For $0-1$ responses, in each node:
  \begin{enumerate}
  \item Calculate the proportion of 1-outcomes for each category of the feature.
  \item Sort the categories according to these proportions.
  \item The feature can then be treated as if ordered
  \end{enumerate}

  \vspace{0.3cm}

<<fig.height=3.4>>=
library(gridExtra)

# generate data (one categorcal variable with 4 categories, 0-1 response)
data = data.frame(category = sample(c("A", "B", "C", "D"), size = 150,
  replace = TRUE, prob = c(0.2, 0.1, 0.4, 0.3)),
  y = sample(c(0,1), size = 150, replace = TRUE, prob = c(0.3, 0.7)))

# calculates proportion of 1-outcomes and plot
subset.data = subset(data, y == 1)
plot.data = data.frame(prop.table(table(subset.data$category)))
colnames(plot.data) = c("Category", "Frequency")
p1 = ggplot(data = plot.data, aes(x = Category, y = Frequency, fill = Category)) +
  geom_bar(stat = "identity")  + theme(legend.position = "none") +
  ggtitle("1)") + ylab("Frequency of class 1") + xlab("Category of feature")

# sort by proportions
p2.pre = ggplot(data = plot.data, aes(x = reorder(Category, Frequency), y = Frequency, fill = Category)) +
  geom_bar(stat = "identity")  + theme(legend.position = "none") +
  ylab("Frequency of class 1") + xlab("Category of feature")
p2 = p2.pre + ggtitle("2)")


# decision tree to draw a vertical line where the spit is being made
mod = rpart::rpart(y ~., data = data)
lvls = levels(reorder(plot.data$Category, plot.data$Frequency))
vline.level = 'C'
p3 = p2.pre +  geom_vline(xintercept = which(lvls == vline.level) - 0.5, col = 'red', lwd = 1, linetype = "dashed") +
  ggtitle("3)")
grid.arrange(p1, p2, p3, ncol = 3)
@

\pagebreak

  \begin{itemize}
  \item This procedure obtains the optimal split
  \item This result also holds for regression trees (with squared error loss) -- the categories are ordered by increasing mean of the outcome
  \item The proofs are not trivial and can be found here:
    \begin{itemize}
    \item for 0-1 responses:
      \begin{itemize}
      \item Breiman, 1984, Classification and Regression Trees.
      \item Ripley, 1996, Pattern Recognition and Neural Networks.
      \end{itemize}
    \item for continuous responses:
      \begin{itemize}
      \item Fisher, 1958, On grouping for maximum homogeneity.
      \end{itemize}
    \end{itemize}
  \item Such simplifications are not known for multiclass problems.
  %\item The Algorithm prefers categorical variables with a large value
  %of categories $Q$
  \end{itemize}

\pagebreak

For continuous responses, in each node:
  \begin{enumerate}
  \item Calculate the mean of the outcome in each category
  \item Sort the categories by increasing mean of the outcome
  \end{enumerate}

\vspace{0.3cm}

<<fig.height=3.5>>=
library(gridExtra)

# generate data (one categorcal variable with 4 categories, 0-1 response)
data = rbind(data.frame(category = "A", y = runif(30, 5, 7.5)),
  data.frame(category = "B", y = runif(15, 6, 12)),
  data.frame(category = "C", y = runif(60, 5, 20)),
  data.frame(category = "D", y = runif(45, 1, 6)))

# calculate the mean of the outcome in each category
plot.data = aggregate(y ~ category, data = data, FUN = mean)
colnames(plot.data) = c("Category", "Mean")

# plot the categories wrt the mean of the outcome in each category
p1 = ggplot(data = plot.data, aes(x = Category, y = Mean, fill = Category)) +
  geom_bar(stat = "identity")  + theme(legend.position = "none") +
  ggtitle("1)") + ylab("Mean of outcome") + xlab("Category of feature")

# sort by increasing mean of the outcome
p2.pre = ggplot(data = plot.data, aes(x = reorder(Category, Mean), y = Mean, fill = Category)) +
  geom_bar(stat = "identity")  + theme(legend.position = "none") +
  ylab("Mean of outcome") + xlab("Category of feature")
p2 = p2.pre + ggtitle("2)")

# decision tree to draw a vertical line where the spit is being made
mod = rpart::rpart(y ~., data = data)
lvls = levels(reorder(plot.data$Category, plot.data$Mean))
vline.level = 'B'
p3 = p2.pre +  geom_vline(xintercept = which(lvls == vline.level) - 0.5, col = 'red', lwd = 1, linetype = "dashed") +
  ggtitle("3)")
grid.arrange(p1, p2, p3, ncol = 3)
@
\end{vbframe}


\begin{vbframe}{Pruning}
\begin{itemize}
\item Method to select optimal size of the tree, to avoid overfitting
\item During tree growing, it is hard to tell when we should stop, as we don't know if the addition
  of further splits down the line will dramatically decrease error (called horizon effect).
\item Aggressive (early) stopping criteria can be used, sometimes this is called pre-pruning,
  and you would have to tune over their settings
\item Or post-pruning: Grow a deep tree, then remove parts, so that the resulting smaller tree is optimal
  w.r.t. cross-validated error
\end{itemize}

\framebreak

<<size='footnotesize'>>=
lrn = makeLearner("regr.rpart", maxdepth = 2)
mod = train(lrn, bh.task)
mod = mod$learner.model
@

\framebreak

<<results='hide'>>=
# Pruning with every cp taken from the cptable
cps = rev(mod$cptable[, "CP"])
lapply(cps, function(x) {
    p = rpart::prune(mod, cp = x)
    sub_title = sprintf("Pruning with complexity parameter = %.4f.", x)
    rattle::fancyRpartPlot(p, sub = sub_title)})
@
\end{vbframe}


\begin{vbframe}{CART: Missing feature values}
  Two approaches:
  \begin{enumerate}
  \item CART often uses the so-called surrogate split principle to natively deal with
    missing values in features
  \item When splits are evaluated, only observations for which the feature is not missing are used
   \item When observations are passed down the tree (in training or prediction), and the feature value needed at a split is missing, we use a "surrogate split" to send the data down.
     These surrogate splits are created during training and resemble as close as possible the actual splitting rule.
  \end{enumerate}
\end{vbframe}

\begin{vbframe}{Advantages}
  \begin{itemize}
    \item Model is easy to comprehend, graphical representation
    \item Not much preprocessing required:
      a) native handling of categorical features and missing values
      b) noproblems with outliers in features
      c) Monotone transformations of features change nothing, so no feature scaling
    \item Interaction effects between features are easily possible
    \item Works for (some) non-linear functions, but see "disadvantages"
    \item Inherent feature selection
    \item Quite fast, scales well with larger data
    \item Trees are flexible by creating a custom split criterion and leaf-node prediction rule
      (clustering trees, semi-supervised trees, density estimation, etc.)
  \end{itemize}
\end{vbframe}

\begin{vbframe}{Disadvantage: Linear Dependencies}
<<fig.height=5, align='center'>>=
n = 100
data = data.frame(x1 = runif(n), x2 = runif(n))
data$y = as.factor(with(data, as.integer(x1 > x2)))
task = makeClassifTask(data = data, target = "y")
rpart = makeLearner("classif.rpart", cp = 0, minbucket = 2, maxdepth = 6)
pl = plotLearnerPrediction(rpart, task, gridsize = 300, cv = 0, err.mark = "none",
  prob.alpha = FALSE)
pl = pl + geom_abline(slope = 1, linetype = 2)
print(pl)
@
Linear dependencies must be modeled over several splits. Logistic regression would model this easily.
\end{vbframe}

\begin{vbframe}{Disadvantage: Smooth functions}
<<fig.height=5, align='center'>>=
fn = function(x) (sin(4*x - 4)) * ((2*x - 2)^2) * (sin(20*x - 4))
d = data.frame(x = seq(0.2, 1, length.out = 7))
d$y = fn(d$x)
task = makeRegrTask(data = d, target = "y")
lrn = makeLearner("regr.rpart", minbucket = 1, minsplit = 1)
pl = plotLearnerPrediction(lrn, task, cv = 0)
x = seq(0.2, 1, length.out = 500)
dd = data.frame(x = x, y = fn(x))
pl = pl + geom_line(data = dd, aes(x, y, color = "red"))
pl = pl + theme(legend.position = "none")
print(pl)
@
Prediction function isn't smooth because a step function is fitted.
\end{vbframe}

% \begin{vbframe}{Disadvantage: Instability}
% \begin{itemize}
% \item High instability (variance) of the trees:
% \item Small changes in the data could lead to completely different splits / trees
% \item This leads to a) less trust in interpretability b) is a reason why prediction error of trees is usually not best, compared with other models
% \end{itemize}

% \framebreak

% High instability of trees will be demonstrated using the Wisconsin Breast Cancer data set.
% It has 699 observations on 9 features and one target class with values \enquote{benign} and \enquote{malignant}.

% \begin{table}
% \begin{tabular}{ll}
% Feature name & Explanation\\
% \hline
% \code{Cl.thickness} & Clump Thickness\\
% \code{Cell.size} & Uniformity of Cell Size\\
% \code{Cell.shape} & Uniformity of Cell Shape\\
% \code{Marg.adhesion} & Marginal Adhesion\\
% \code{Epith.c.size} & Single Epithelial Cell Size\\
% \code{Bare.nuclei} & Bare Nuclei\\
% \code{Bl.cromatin} & Bland Chromatin\\
% \code{Normal.nucleoli} & Normal Nucleoli\\
% \code{Mitoses} & Mitoses\\
% \end{tabular}
% \end{table}

% \framebreak

% Tree fitted on complete Wisconsin Breast Cancer data
% <<results='hide', fig.height=5>>=
% # BB: i am using the BC dataset directly from mlbench
% # and convert features, see issue in i2ml nr 277
% library(mlbench)
% data(BreastCancer)
% d = BreastCancer
% d$Id = NULL
% for (i in 1:9)
%   d[,i] = as.integer(d[,i])
% print(str(d))
% lrn = makeLearner("classif.rpart")
% task1 = makeClassifTask(data = d, target = "Class")
% mod1 = train(lrn, task1)
% d2 = d[-13, ]
% task2 = makeClassifTask(data = d2, target = "Class")
% mod2 = train(lrn, task2)
% fancyRpartPlot(mod1$learner.model, sub = "")
% @
% \framebreak

% Tree fitted on Wisconsin Breast Cancer data without observation 13
% <<results='hide', fig.height=5>>=
% fancyRpartPlot(mod2$learner.model, sub = "")
% @
% \end{vbframe}

\begin{vbframe}{Disadvantages}
\begin{itemize}
\item Really not the best predictor: Combine with bagging (forest) or boosting!
\item High instability (variance) of the trees.
  Small changes in the data could lead to very different trees. This leads to a) less trust in interpretability b) is a reason why prediction error of trees is usually not best.
\item In regression, due to fitting piecewise constant models, trees often do not extrapolate well
\end{itemize}
\end{vbframe}

\begin{vbframe}{Further tree methodologies}

\begin{itemize}
\item AID (Sonquist and Morgan, 1964)
\item CHAID (Kass, 1980)
\item CART (Breiman et al., 1984)
\item C4.5 (Quinlan, 1993)
\item Unbiased Recursive Partitioning (Hothorn et al., 2006)
\end{itemize}

\end{vbframe}


\endlecture
