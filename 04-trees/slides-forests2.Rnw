% Introduction to Machine Learning
% Day 4

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
library(methods)
library(rpart)
library(rpart.plot)
library(randomForest)
library(rattle)
library(smoof)
@

% Load all R packages and set up knitr
<<setup, child="../style/setup.Rnw", include = FALSE>>=
@


\lecturechapter{16}{Random Forest cont.}
\lecture{Introduction to Machine Learning}

\sloppy


\begin{vbframe}{Variable importance}

\begin{itemize}
  \item Single trees are highly interpretable
  \item Random Forests as an ensemble of many trees lose this feature
  \item Hence, contributions of a single covariate to the fit are difficult to evaluate
  \item Way out: variable importance measures
\end{itemize}

\framebreak

\begin{algorithm}[H]
  \small
  \caption*{Measure based on permutations of OOB observations}
  \begin{algorithmic}[1]
    \State After growing tree $\blmh$, pass down OOB observations and record
    predictive accuracy.
    \State Permute OOB observations of $j$th variable.
    \State Pass down the permuted OOB observations and evaluate predictive accuracy again.
    \State The loss of goodness induced by permutation is averaged over all trees and is used as a measure for the importance of the $j^\text{th}$ variable.
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
  \small
  \caption*{Measure based on improvement in split criterion}
  \begin{algorithmic}[1]
    \State At each split in tree $\blmh$ the improvement in the split criterion is attributed as variable importance measure for the splitting variable.
    \State For each variable, this improvement is accumulated over all trees for the importance measure.
  \end{algorithmic}
\end{algorithm}
\end{vbframe}

\begin{vbframe}{Variable Importance based on permutations of OOB observations}
\begin{center}
\includegraphics[width = 10.3cm]{figure_man/rF_varImp_permutation.jpg}
\end{center}
\end{vbframe}

\begin{vbframe}{Variable importance}
\begin{figure}
<<size="footnotesize", fig.height=3>>=
model = randomForest(Species ~ ., data = iris, importance = TRUE)
randomForest::varImpPlot(model)
@
\caption{Two importance measures on iris.}
\end{figure}

\begin{figure}
<<size="footnotesize", fig.height=3>>=
v = generateFilterValuesData(iris.task, method = c("randomForest.importance", "cforest.importance"))
plotFilterValues(v)
@
\caption{RF importance as filters in mlr.}
\end{figure}

\end{vbframe}

\begin{vbframe}{Random Forest: Proximities}
\begin{itemize}
  \item the "closeness" or "nearness" between pairs of cases.
\item Algorithm
\begin{itemize}
\item After a tree is grown, put all of the data down the tree.
\item If cases $x_1$ and $x_2$ are in the same terminal node through one tree increase their proximity by one. 
\item At the end of the run of all trees, normalize the proximities by dividing by the number of trees.
\end{itemize}
\item The proximities originally form a $N \times N$ matrix.
\item Proximities are used in replacing missing data, locating outliers, and producing illuminating low-dimensional views of the data.

\end{itemize}
\end{vbframe}





% data(iris)
% iris[1:10,]
% library(randomForest)
% Try to predict glass type, based on 
% X <- iris[,-5]
% Y <- iris[,5]
% rf <- randomForest(X,Y,ntree=500,proximity=TRUE,oob.prox=TRUE)
% op <- par()
% par(mar = rep(2, 4))
% MDSplot(rf,Y)
% par(mar = op$mar)




\begin{vbframe}{}

We can visualize our Proximities $P(x_i, x_j)$ for each i $\in \{1,...,n\}$ example by MDS.\\

<<>>=
require(randomForest)
require(mlbench) 
data(Sonar)
library(randomForest)
@

Our  data contains 111 patterns obtained by bouncing sonar signals off a metal cylinder at various angles and under various conditions and  97 patterns obtained from rocks under similar conditions. 

%The data set contains signals obtained from a variety of different aspect angles, spanning 90 degrees for the cylinder and 180 degrees for the rock. Each pattern is a set of 60 numbers in the range 0.0 to 1.0. Each number represents the energy within a particular frequency band, integrated over a certain period of time. 

%The label associated with each record contains the letter "R" if the object is a rock and "M" if it is a mine (metal cylinder). 

\begin{center}
\textbf{} \\
\vspace{0.25cm}
<<>>=
  Sonar[1:10,53:61]
@
  \end{center}
  
  \framebreak


We try to predict the type, based on signals obtained from a variety of different aspect angles, spanning 90 degrees for the cylinder and 180 degrees for the rock

<<include=TRUE>>=
X <- Sonar[,-61]
Y <- Sonar[,61]
rf <- randomForest(X,Y,ntree=500,proximity=TRUE,oob.prox=TRUE)
@

We calculate the proximities $P(x_i, x_j)$ based on out-of-bag observations.

<<>>=
  rf
@



Now we can visualize proximities $P(x_i, x_j)$ by MDS, using as distance matrix D= 1 - P
\framebreak
<<include=TRUE>>=
op <- par()
par(mar = rep(1, 4))
MDSplot(rf,Y)
par(mar = op$mar)
 @

\end{vbframe}


\begin{vbframe}{Adaptive Nearest Neighbors}

Let $P(x, x_i)$ $ \in$ [0, 1] be the Proximity between the  observation x and our original point $x_i$.
\begin{itemize}
\item For classification, the prediction will be the weighted greatest number of hits, which is proportional to the proximities $P(x,x_i)$.
\item  For a regression we can calculate the prediction of Random Forests at  x as:
\begin{itemize}
\item if every leaf node contains the same number of observations. 
\begin{align*}
\hat{Y}_{RF}(x)  = \frac{ \sum_{n=0}^n P(x,x_i)Y_i}{\sum_{n=0}^n P(x,x_i)}
\end{align*}
%This can be seen as a weighted nearest neighbour scheme, where the weights are proportional to our proximities $P(x,x_i)$.
\item  if some leaf node contains the different number of observations, $P(x,x_i)$ is the
percentage of trees where x and $x_i$ fall into the same leaf node, and
weights are inversely proportional for each tree to the number of samples in
the leaf node where $x_i$ falls into.
\end{itemize}

\end{itemize}
\end{vbframe}


\begin{vbframe}{Random Forest: Advantages}

\begin{itemize}
  \item Easy to implement
  \item Can be applied to basically any model
  \item Easy to parallelize
  \item Often works well (enough)
  \item Enables variance analysis
  \item Integrated estimation of OOB error
  \item Can work on high-dimensional data
  \item Often not much tuning necessary
\end{itemize}

\end{vbframe}

\begin{vbframe}{Random Forest: Disadvantages}

\begin{itemize}
  \item Often suboptimal for regression
  \item Hard to interpret, especially interactions
  \item Does not really optimize loss aggressively
  \item No real way to adapt to problem\\
  (see e.g. loss in GBM, kernel in SVM)
  \item Implementations sometimes memory-hungry
  \item Prediction can be slow
\end{itemize}

\end{vbframe}



\endlecture
