% Introduction to Machine Learning
% Day 4

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
library(methods)
library(rpart)
library(rpart.plot)
library(randomForest)
library(rattle)
library(smoof)
@

% Load all R packages and set up knitr
<<setup, child="../style/setup.Rnw", include = FALSE>>=
@


\lecturechapter{15}{Random Forest cont.}
\lecture{Introduction to Machine Learning}

\sloppy


\begin{vbframe}{Variable importance}

\begin{itemize}
  \item Single trees are highly interpretable
  \item Random Forests as an ensemble of many trees lose this feature
  \item Hence, relevance of each single covariates to the fit is difficult to evaluate
  \item Way out: variable importance measures
\end{itemize}

\framebreak

\begin{algorithm}[H]
  \small
  \caption*{Measure based on permutations of OOB observations}
  \begin{algorithmic}[1]
    \State After growing tree $\blmh$, pass down OOB observations and record
    predictive accuracy.
    \State Permute OOB observations of $j$th variable.
    \State Pass down the permuted OOB observations and evaluate predictive accuracy again.
    \State The loss of predictive accuracy induced by permutation is averaged over all trees and is used as a measure for the importance of the $j^\text{th}$ variable.
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
  \small
  \caption*{Measure based on improvement in split criterion}
  \begin{algorithmic}[1]
    \State At each split in tree $\blmh$ the improvement in the split criterion is attributed as variable importance measure for the splitting variable.
    \State For each variable, this improvement is accumulated over all trees for the importance measure.
  \end{algorithmic}
\end{algorithm}
\end{vbframe}

\begin{vbframe}{Variable Importance based on permutations of OOB observations}
\begin{center}
\includegraphics[width = 10.3cm]{figure_man/rF_varImp_permutation.jpg}
\end{center}
\end{vbframe}

\begin{vbframe}{Variable importance}
\begin{figure}
<<rf-iris-varimp, size="footnotesize", fig.height=3>>=
model = randomForest(Species ~ ., data = iris, importance = TRUE)
randomForest::varImpPlot(model, main = "")
@
\caption{Two importance measures on the iris dataset.}
\end{figure}

<<size="footnotesize", fig.height=3, eval = FALSE>>=
v = generateFilterValuesData(iris.task,
  method = c("randomForest.importance", "cforest.importance"))
plotFilterValues(v)
@

\end{vbframe}

\begin{vbframe}{Random Forest: Proximities}
\begin{itemize}
  \item the "closeness" or "nearness" between pairs of cases.
\item Algorithm
\begin{itemize}
\item After a tree is grown, put all of the data down the tree.
\item If cases $x_1$ and $x_2$ are in the same terminal node through one tree increase their proximity by one. 
\item At the end of the run of all trees, normalize the proximities by dividing by the number of trees.
\end{itemize}
\item The proximities originally form a $N \times N$ matrix.
\item Proximities are used in replacing missing data, locating outliers, and producing illuminating low-dimensional views of the data.

\end{vbframe}

\begin{vbframe}{Random Forest: Advantages}

\begin{itemize}
  \item Easy to implement
  \item Can be applied to basically any model
  \item Easy to parallelize
  \item Often works well (enough)
  \item Enables variance analysis
  \item Integrated estimation of OOB error
  \item Can work on high-dimensional data
  \item Often not much tuning necessary
\end{itemize}

\end{vbframe}

\begin{vbframe}{Random Forest: Disadvantages}

\begin{itemize}
  \item Often suboptimal for regression
  \item Hard to interpret, especially interactions
  \item Does not really optimize loss aggressively
  \item No real way to adapt to problem\\
  (see e.g. loss in GBM, kernel in SVM)
  \item Implementations sometimes memory-hungry
  \item Prediction can be slow
\end{itemize}

\end{vbframe}

\begin{vbframe}{Benchmark: Random Forest vs. (bagged) CART vs. (bagged) k-NN}

  \begin{itemize}
    \item Goal: Compare performance of random forest against (bagged) stable and (bagged) unstable methods
    \item Algorithms:
    \begin{itemize}
      \item classification tree (CART, implemented in \code{rpart}, \code{max.depth}: 30, \code{min.split}: 20, \code{cp}: 0.01)
      \item bagged classification tree using 50 bagging iterations (\code{bagged.rpart})
      \item k-nearest neighbors (k-NN, implemented in \code{kknn}, $k=7$)
      \item bagged k-nearest neighbors using 50 bagging iterations (\code{bagged.knn})
      \item random forest with 50 trees (implemented in \code{randomForest})
    \end{itemize}
    \item Method to evaluate performance: 10-fold cross-validation
    \item Performance measure: mean missclassification error on test sets
    \end{itemize}

    \framebreak

    \begin{itemize}
    \item Datasets from \pkg{mlbench}:
    \end{itemize}

\begin{table}
\footnotesize
\begin{tabular}{p{1.5cm}p{2cm}p{0.5cm}p{0.5cm}p{5cm}}
Name & Kind of data &  n & p & Task\\
\hline
Glass & Glass identification data & 214 & 10 & Predict the type of glass (6 levels) on the basis of the chemical analysis of the glasses represented by the 10 features\\
Ionosphere & Radar data & 351 & 35 & Predict whether the radar returns show evidence of some type of structure in the ionosphere (\enquote{good}) or not (\enquote{bad}) \\
Sonar & Sonar data & 208 & 61 & Discriminate between sonar signals bounced off a metal cylinder (\enquote{M}) and those bounced off a cylindrical rock (\enquote{R})\\
Waveform & Artificial data & 100 & 21 & Simulated 3-class problem which is considered to be a difficult pattern recognition problem. Each class is generated by the waveform generator.\\
\hline
\end{tabular}
\end{table}

\framebreak
  
  \begin{center}\includegraphics[width=0.95\textwidth]{figure_man/bm_stable_vs_unstable.pdf}\end{center}{}

%\framebreak
  
\end{vbframe}

\begin{vbframe}{Benchmark: Random Forest vs. (bagged) CART vs. (bagged) k-NN}

  Bagging k-NN does not improve performance because:

  \begin{itemize}
    \item k-NN is stable w.r.t. perturbations
    \item In a 2-class problem, nearest neighbor based classification only changes under bagging if both
    \begin{itemize}
    \item the nearest neighbor in the learning set is \textbf{not} in at least half of the bootstrap samples, but the probability that any given observation is in the bootstrap sample is 63\% which is greater than 50\%,
    \item and, simultaneously, the \emph{new} nearest neighbor(s) all have a different label than the missing nearest neighbor in those bootstrap samples, which is unlikely for most regions of $\Xspace \times \Yspace$.
    \end{itemize}
\end{itemize}
\end{vbframe}


\endlecture
