% Introduction to Machine Learning
% Day 4

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
library(methods)
library(rpart)
library(rpart.plot)
library(randomForest)
library(rattle)
library(smoof)
@

% Load all R packages and set up knitr
<<setup, child="../style/setup.Rnw", include = FALSE>>=
@


\lecturechapter{Bagging and Random Forest 2}
\lecture{Introduction to Machine Learning}

\sloppy

% \begin{vbframe}{Variable importance}

% <<size="footnotesize", fig.height=3, eval = FALSE>>=
% v = generateFilterValuesData(iris.task,
  % method = c("randomForest.importance", "cforest.importance"))
% plotFilterValues(v)
% @
% \end{vbframe}

 \begin{vbframe}{Random Forest: Proximities}
 \begin{itemize}
  \item One of the most useful tools in random forests
  \item A measurement of similarity ("closeness" or "nearness")
  \item Calculated for each pair of cases, observations or sample points
  \item Definition
  \begin{itemize}
    \item The proximity between two cases $x_1$ and $x_2$ is calculated by measuring the number of times that these two cases are placed in the same terminal node of the same tree of random forest, divided by the number of trees in the forest
    \item The proximity of cases $x_1$ and $x_2$ can be written as \textbf{prox}($x_1$, $x_2$)
    \item The proximities form an intrinsic similarity measure between pairs of cases
  \end{itemize}
  \item The proximities originally form a $N \times N$ matrix.
  \item Proximity matrix is a symmetric matrix
  
  \framebreak 
  
  \item With large datasets, it is not possible to fit a $N \times N$ matrix into fast memory
  \begin{itemize}
  \item A modification is needed - reduce the required memory size to $N \times T$
  \item Here, $T$ is number of trees in the forest
  \end{itemize}

  \item Algorithm
 \begin{itemize}
 \item When a tree is fully developed, all of the data is put down the tree (both training and out of bag).
 \item If cases $x_1$ and $x_2$ are in the same terminal node of one tree, their proximity is increased by one. 
 \item At the end of the run of all trees, the proximities are normalized by dividing them by the number of trees.
 \end{itemize}
 
   \framebreak
 \item Proximities are not directly related to Euclidean distance rather they are based on the way the trees deal with the data. For example:
 \begin{itemize}
 \item Even though cases $x_1$ and $x_2$ are far away in Euclidean space, they might have high proximity because they stay together in all the trees.
 \item Even though cases $x_1$ and $x_2$ are too close in Euclidean space, they might have relatively small proximity if they are near the classification boundary.
 \end{itemize}
 
 \item Proximities are used in:
 \begin{itemize}
 \item Replacing missing data
 \item Locating outliers 
 \item Visualizing the forest 
 \item Identifying mislabeled data
 \item Computing prototypes etc.
 \end{itemize}

 \end{itemize}
 \end{vbframe}



\endlecture
