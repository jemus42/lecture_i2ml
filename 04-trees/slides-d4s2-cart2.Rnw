% Introduction to Machine Learning
% Day 4

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@

% Load all R packages and set up knitr
<<setup, child="../style/setup.Rnw", include = FALSE>>=
@


\lecturechapter{14}{Trees cont.}
\lecture{Introduction to Machine Learning}

\sloppy

\begin{vbframe}{Monotone feature transformations}

Monotone transformations of one or several features will not change the value of the impurity measure, neither the structure of the tree (just the numerical value of the split point).
\vspace{0.5cm}
\begin{columns}[T]
\column{0.49\textwidth}
Original data
<<>>=
x = c(1,2,7,10,20)
y = c(1,1,0.5,10,11)
data = t(data.frame(x = x, y = y))
kable(data)
@

\column{0.49\textwidth}
Data with log-transformed $x$
<<fig.align='right'>>=
log.x = log(x)
data = t(data.frame(log.x, y))
rownames(data) = c("log(x)", "y")
kable(data,digits = 1)
@
\end{columns}
\vspace{0.5cm}
\centering
\includegraphics[width = \textwidth]{figure_man/monotone_trafo.png}
\end{vbframe}

\begin{vbframe}{CART: Stopping-Criteria}
  \begin{itemize}
    \item Minimal number of observations per node, for a split to be tried
    \item Minimal number of observations that must be contained in a leaf
    \item Minimal increase in goodness of fit that must be reached for a split to be
      tried
    \item Maximum number of levels for your tree
  \end{itemize}
\end{vbframe}

\begin{vbframe}{CART: Overfitting}
  \begin{itemize}
  \item The CART-Algorithm could just continue until there is a single observation in each node

  \item [$\Rightarrow$] Complexity (and hence the danger of overfitting)
  increases with the number of splits / levels / leafs
  \end{itemize}
\end{vbframe}

\begin{vbframe}{CART: Categorical Predictors}
  \begin{itemize}
  \item For a nominal scaled feature with $Q$ categories,
  there are $2^{Q-1}-1$ possible partitions of the $Q$ values into two groups:
    \begin{itemize}
    \item There are $2^Q$ ways to assign $Q$ distinct values to the left or right node.
    \item Two of these configurations lead to an empty node, while the other one contains all observations.
    Discarding these configurations leads to $2^Q -2$ possible partitions.
    \item Symmetry halves the number of possible partitions: $\frac{1}{2}(2^Q - 2) = 2^{Q-1} - 1$
    \end{itemize}
    $\Rightarrow$ computations become prohibitive for large values of $Q$
  \item But for regression with squared loss and binary classification shortcuts exist.
  \end{itemize}

  \pagebreak

For $0-1$ responses, in each node:
  \begin{enumerate}
  \item Calculate the proportion of 1-outcomes for each category of the feature.
  \item Sort the categories according to these proportions.
  \item The feature can then be treated as if it were an ordered categorical feature ($Q-1$ possible splits).
  \end{enumerate}

  \vspace{0.3cm}

<<fig.height=3.4>>=
set.seed(1234)
# generate data (one categorcal variable with 4 categories, 0-1 response)
data = data.frame(category = sample(c("A", "B", "C", "D"), size = 150,
  replace = TRUE, prob = c(0.2, 0.1, 0.4, 0.3)),
  y = sample(c(0,1), size = 150, replace = TRUE, prob = c(0.3, 0.7)))

# calculates proportion of 1-outcomes and plot
subset.data = subset(data, y == 1)
plot.data = data.frame(prop.table(table(subset.data$category)))
colnames(plot.data) = c("Category", "Frequency")
p1 = ggplot(data = plot.data, aes(x = Category, y = Frequency, fill = Category)) +
  geom_bar(stat = "identity")  + theme(legend.position = "none") +
  ggtitle("1)") + ylab("Frequency of class 1") + xlab("Category of feature")

# sort by proportions
p2.pre = ggplot(data = plot.data, aes(x = reorder(Category, Frequency), y = Frequency, fill = Category)) +
  geom_bar(stat = "identity")  + theme(legend.position = "none") +
  ylab("Frequency of class 1") + xlab("Category of feature")
p2 = p2.pre + ggtitle("2)")


# decision tree to draw a vertical line where the spit is being made
mod = rpart::rpart(y ~., data = data)
lvls = levels(reorder(plot.data$Category, plot.data$Frequency))
vline.level = 'C'
p3 = p2.pre +  geom_vline(xintercept = which(lvls == vline.level) - 0.5, col = 'red', lwd = 1, linetype = "dashed") +
  ggtitle("3)")
grid.arrange(p1, p2, p3, ncol = 3)
@

\pagebreak

  \begin{itemize}
  \item This procedure obtains the optimal split for entropy and Gini index.
  \item This result also holds for regression trees (with squared error loss) -- the categories are ordered by increasing mean of the outcome (see next slide).
  \item The proofs are not trivial and can be found here:
    \begin{itemize}
    \item for 0-1 responses:
      \begin{itemize}
      \item Breiman, 1984, Classification and Regression Trees.
      \item Ripley, 1996, Pattern Recognition and Neural Networks.
      \end{itemize}
    \item for continuous responses:
      \begin{itemize}
      \item Fisher, 1958, On grouping for maximum homogeneity.
      \end{itemize}
    \end{itemize}
  \item Such simplifications are not known for multiclass problems.
  %\item The Algorithm prefers categorical variables with a large value
  %of categories $Q$
  \end{itemize}

\pagebreak

For continuous responses, in each node:
  \begin{enumerate}
  \item Calculate the mean of the outcome in each category.
  \item Sort the categories by increasing mean of the outcome.
  \item The feature can then be treated as if it were an ordered categorical feature ($Q-1$ possible splits).
  \end{enumerate}

\vspace{0.3cm}

<<fig.height=3.5>>=
set.seed(1234)
# generate data (one categorcal variable with 4 categories, 0-1 response)
data = rbind(data.frame(category = "A", y = runif(30, 5, 7.5)),
  data.frame(category = "B", y = runif(15, 6, 12)),
  data.frame(category = "C", y = runif(60, 5, 20)),
  data.frame(category = "D", y = runif(45, 1, 6)))

# calculate the mean of the outcome in each category
plot.data = aggregate(y ~ category, data = data, FUN = mean)
colnames(plot.data) = c("Category", "Mean")

# plot the categories wrt the mean of the outcome in each category
p1 = ggplot(data = plot.data, aes(x = Category, y = Mean, fill = Category)) +
  geom_bar(stat = "identity")  + theme(legend.position = "none") +
  ggtitle("1)") + ylab("Mean of outcome") + xlab("Category of feature")

# sort by increasing mean of the outcome
p2.pre = ggplot(data = plot.data, aes(x = reorder(Category, Mean), y = Mean, fill = Category)) +
  geom_bar(stat = "identity")  + theme(legend.position = "none") +
  ylab("Mean of outcome") + xlab("Category of feature")
p2 = p2.pre + ggtitle("2)")

# decision tree to draw a vertical line where the spit is being made
mod = rpart::rpart(y ~., data = data)
lvls = levels(reorder(plot.data$Category, plot.data$Mean))
vline.level = 'B'
p3 = p2.pre +  geom_vline(xintercept = which(lvls == vline.level) - 0.5, col = 'red', lwd = 1, linetype = "dashed") +
  ggtitle("3)")
grid.arrange(p1, p2, p3, ncol = 3)
@
\end{vbframe}


% \begin{vbframe}{CART: Minimal cost complexity pruning}
%   \begin{itemize}
%   \item Method to optimize the trade-off between goodness-of-fit and complexity
%   \item Criteria: cost function
%     \begin{align*}
%       R_{\alpha} &= R(T) + \alpha \cdot \# \text{leafs},
%     \end{align*}
%     where $R(T)$ represents the error of tree $T$ on the training set
%     $\rightarrow$ $R_{\alpha}$ = Training error + Complexity term.
%   \item For every $\alpha$ there is a distinctly defined smallest sub-tree of the original tree,
%   which minimizes the cost function.
%   \item $\hat{\alpha}$ can be assessed with cross-validation.
%   \item Final tree is fitted on the whole data, where $\hat{\alpha}$ is used to
%   to find the optimal size of the tree
%   \end{itemize}
% 
% \pagebreak
% 
% <<size='footnotesize'>>=
% lrn = makeLearner("regr.rpart")
% mod = train(lrn, bh.task)
% mod = mod$learner.model
% @
% 
% \pagebreak
% 
% <<results='hide'>>=
% # Pruning with every cp taken from the cptable
% cps = rev(mod$cptable[, "CP"])
% lapply(cps, function(x) {
%     p = rpart::prune(mod, cp = x)
%     sub_title = sprintf("Pruning with complexity parameter = %.4f.", x)
%     rattle::fancyRpartPlot(p, sub = sub_title)})
% @
% \end{vbframe}


\begin{vbframe}{CART: Missing predictor values}
  Two approaches:
  \begin{enumerate}
  \item Missing values of a categorical variable are treated as an own category
  \item When considering a predictor for a split,
  only use the observations for which the predictor is not missing.

  To pass observations with missing values down the tree (during fitting or predicting),
    we have to find surrogate variables, that produce similar splits.
  \end{enumerate}
\end{vbframe}

\begin{vbframe}{Advantages}
  \begin{itemize}
    \item Model is easy to comprehend, graphical representation
    \item Categorical features can easily be handled
    \item Missing values can be handled
    \item No problems with outliers in features
    \item Monotone transformations of features change nothing
    \item Interaction effects between features are easily possible
    \item Works for (some) non-linear functions
    \item Inherent feature selection
    \item Quite fast, scales well with larger data
    \item Trees are flexible by creating a custom split criterion and leaf-node prediction rule
      (clustering trees, semi-supervised trees, density estimation, etc.)
  \end{itemize}
\end{vbframe}

\begin{vbframe}{Disadvantages}
  \begin{itemize}
  \item High instability (variance) of the trees: Small changes in the data could lead to completely different splits, thus, to completely different trees $\rightarrow$ Decisions on the upper level influence decisions on lower levels (\enquote{mistakes} in upper levels proceed to the lower ones.)
  \item Prediction function isn't smooth because a step function is fitted.
  \item Linear dependencies must be modeled over several splits
    $\rightarrow$ Simple linear correlations must be translated into a complex tree structure
    (see the following example)
  \item Really not the best predictor: Combine with bagging (forest) or boosting!
  (This will also be illustrated in a small benchmark at the end of the random forest chapter.)
  \end{itemize}

\pagebreak

High instability of trees will be demonstrated using the Wisconsin Breast Cancer data set.
It has 699 observations on 9 features and one target class with values \enquote{benign} and \enquote{malignant}.

\begin{table}
\begin{tabular}{ll}
Feature name & Explanation\\
\hline
\code{Cl.thickness} & Clump Thickness\\
\code{Cell.size} & Uniformity of Cell Size\\
\code{Cell.shape} & Uniformity of Cell Shape\\
\code{Marg.adhesion} & Marginal Adhesion\\
\code{Epith.c.size} & Single Epithelial Cell Size\\
\code{Bare.nuclei} & Bare Nuclei\\
\code{Bl.cromatin} & Bland Chromatin\\
\code{Normal.nucleoli} & Normal Nucleoli\\
\code{Mitoses} & Mitoses\\
\end{tabular}
\end{table}

\pagebreak

Tree fitted on complete Wisconsin Breast Cancer data
<<results='hide', fig.height=5>>=
# Create learner and train learner on the Wisconsin Breast Cancer data
lrn = makeLearner("classif.rpart")
model = train(lrn, bc.task)

# Remove one observation from the data and train learner on modified dataset
bc.data = getTaskData(bc.task)
bc.data.mod = bc.data[-13, ]
bc.task.mod = makeClassifTask("bc.task modified",
  data = bc.data.mod, target = getTaskTargetNames(bc.task))
model.mod = train(lrn, bc.task.mod)

# Display tree on full data set
fancyRpartPlot(model$learner.model, sub = "")
@
\pagebreak

Tree fitted on Wisconsin Breast Cancer data without observation 13
<<results='hide', fig.height=5>>=
# Leaving out just one obs leads to a totally differnt tree
fancyRpartPlot(model.mod$learner.model, sub = "")
@

\pagebreak
\setkeys{Gin}{width=0.95\textwidth}
\begin{center}
<<results='hide', fig.height=5>>=
set.seed(123)
n = 100
data = data.frame(x1 = runif(n), x2 = runif(n))
data$y = as.factor(with(data, as.integer(x1 > x2)))

ggplot(data, aes(x = x1, y = x2, shape = y, col = y)) +
  geom_point(size = 3) +
  pers_theme +
  ggtitle("") +
  geom_abline(slope = 1, linetype = 2)
@
\end{center}
Linear dependencies must be modeled over several splits.
\pagebreak
\begin{center}
<<results='hide', fig.height=5>>=
problemtask = makeClassifTask(data = data, target = "y")
rpart = makeLearner("classif.rpart")
rpart = setHyperPars(rpart, cp = 0, minbucket = 2, maxdepth = 1)
model = train(rpart, problemtask)
# Plot
plotLearnerPrediction(rpart, problemtask, gridsize = 300, cv = 0) +
  pers_theme +
  ggtitle("") +
  geom_abline(slope = 1, linetype = 2)
@
\end{center}
Linear dependencies must be modeled over several splits.
\pagebreak
\begin{center}
<<results='hide', fig.height=5>>=
rpart = setHyperPars(rpart, cp = 0, minbucket = 2, maxdepth = 2)
model = train(rpart, problemtask)
# Plot
plotLearnerPrediction(rpart, problemtask, gridsize = 300, cv = 0) +
  pers_theme +
  ggtitle("") +
  geom_abline(slope = 1, linetype = 2)
@
\end{center}
Linear dependencies must be modeled over several splits.
\pagebreak
\begin{center}
<<results='hide', fig.height=5>>=
rpart = setHyperPars(rpart, cp = 0, minbucket = 2, maxdepth = 3)
model = train(rpart, problemtask)
# Plot
plotLearnerPrediction(rpart, problemtask, gridsize = 300, cv = 0) +
  pers_theme +
  ggtitle("") +
  geom_abline(slope = 1, linetype = 2)
@
\end{center}
Linear dependencies must be modeled over several splits.
\pagebreak
\begin{center}
<<results='hide', fig.height=5>>=
rpart = setHyperPars(rpart, cp = 0, minbucket = 2, maxdepth = 4)
model = train(rpart, problemtask)
# Plot
plotLearnerPrediction(rpart, problemtask, gridsize = 300, cv = 0) +
  pers_theme +
  ggtitle("") +
  geom_abline(slope = 1, linetype = 2)
@
\end{center}
Linear dependencies must be modeled over several splits.

\pagebreak
\begin{center}
<<results='hide', fig.height=5>>=
modForrester = makeSingleObjectiveFunction(
  name = "Modification Forrester et. all function",
  fn = function(x) (sin(4*x - 4)) * ((2*x - 2)^2) * (sin(20*x - 4)),
  par.set = makeNumericParamSet(lower = 0, upper = 1, len = 1L),
  noisy = TRUE
)
set.seed(9)
design = generateDesign(7L, getParamSet(modForrester), fun = lhs::maximinLHS)
design$y = modForrester(design)
design
regr.task = makeRegrTask(data = design, target = "y")
fn = function(x) (sin(4*x - 4)) * ((2*x - 2)^2) * (sin(20*x - 4))
regr.rpart = makeLearner("regr.rpart", par.vals = list(minbucket = 1, minsplit = 1))
pp = plotLearnerPrediction(regr.rpart, regr.task, cv = 0)
x = seq(0.07547466, 1, length.out = 500)
pp + geom_line(data = data.frame(x = x, y = fn(x)), aes(x, y, color = "True function"), color = "red")
@
\end{center}
Prediction function isn't smooth because a step function is fitted.
\end{vbframe}




\endlecture
