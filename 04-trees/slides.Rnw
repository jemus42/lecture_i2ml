% Introduction to Machine Learning
% Day 4

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@

% Load all R packages and set up knitr
<<setup, child="../style/setup.Rnw", include = FALSE>>=
@

<<load-tree-packages, include = FALSE, results='hide'>>=
library(smoof)
library(rpart)
library(rpart.plot)
library(rattle)
library(randomForest)
@


\newcommand{\pikN}{\hat{\pi}^\Np}
\input{../latex-math/ml-trees.tex}



\lecturechapter{3}{Trees}
\lecture{Introduction to Machine Learning}

\sloppy

\begin{vbframe}{Trees - Introduction}

Can be used for classification, regression (and much more!)

\begin{blocki}{Zoo of tree methodologies}
\item AID (Sonquist and Morgan, 1964)
\item CHAID (Kass, 1980)
\item CART (Breiman et al., 1984)
\item C4.5 (Quinlan, 1993)
\item Unbiased Recursive Partitioning (Hothorn et al., 2006)
\end{blocki}

\end{vbframe}

\begin{vbframe}{CART}
  \begin{itemize}
    \item Classification and Regression Trees, introduced by Breiman
    \item Binary splits are constructed top-down
    \item Only constant prediction in each leaf
    \begin{figure}
    \centering
      \includegraphics[width= 8cm, height = 5.5cm]{figure_man/labelling_of_tree.png}
    \end{figure}
    \item In the greedy top-down construction, features and split points are selected by exhaustive search.
    \item For each node, one iterates over all features, and for each feature over all split points.
    \item The best feature and split point, which make both created child nodes most pure, measured by a split criterion, are selected.
    \item The procedure then is applied to the child nodes in a recursive manner.
    \end{itemize}
\end{vbframe}


\begin{vbframe}{CART}
\begin{itemize}
\item Trees divide the feature space $\Xspace$ into rectangles and fit simple models (e.g: ~constant) in these:
  \begin{align*}
    \fx = \sum_{m=1}^M c_m \I(x \in R_m),
  \end{align*}
  where $M$ rectangles $R_m$ are used. $c_m$ is a predicted numerical response, a class label or a class
  distribution.
\end{itemize}

\pagebreak
\textbf{Example for Classification:} Iris-Data
\centering
<<results='hide', fig.height=2.4>>=
# Trees on Iris Data
data("iris")
pers_theme =   theme(axis.line = element_line(colour = "black"),
        panel.grid.major = element_line(colour = "grey80"),
        panel.grid.minor = element_line(colour = "grey80"),
        panel.border = element_blank(),
        panel.background = element_blank())

iristask = makeClassifTask(data = iris[,-(1:2)], target = "Species")
rpart = makeLearner("classif.rpart")
rpart = setHyperPars(rpart, cp = 0, minbucket = 4, maxdepth = 1) #Illustration
model = train(rpart, iristask)
# Plot
plotLearnerPrediction(rpart, iristask, gridsize = 300, cv = 0) +
  pers_theme +
  ggtitle("Iris Data")
@

<<results='hide', fig.height=3>>=
fancyRpartPlot(model$learner.model, sub = "")
@

\pagebreak
\textbf{Example for Classification:} Iris-Data
<<results='hide', fig.height=2.4>>=
rpart = setHyperPars(rpart, cp = 0, minbucket = 4, maxdepth = 2) #2
model = train(rpart, iristask)
# Plot
plotLearnerPrediction(rpart, iristask, gridsize = 300, cv = 0) +
  pers_theme +
  ggtitle("Iris Data")
@

<<results='hide', fig.height=3>>=
fancyRpartPlot(model$learner.model, sub = "")
@

\pagebreak
\textbf{Example for Classification:} Iris-Data
<<results='hide', fig.height=2.4>>=
rpart = setHyperPars(rpart, cp = 0, minbucket = 4, maxdepth = 3) #3
model = train(rpart, iristask)
# Plot
plotLearnerPrediction(rpart, iristask, gridsize = 300, cv = 0) +
  pers_theme +
  ggtitle("Iris Data")
@

<<results='hide', fig.height=3>>=
fancyRpartPlot(model$learner.model, sub = "")
@

\pagebreak
\textbf{Example for Regression:}
\vspace{0.5cm}
\begin{columns}[T,onlytextwidth]
\column{0.2\textwidth}
<<out.width='\\textwidth'>>=
modForrester = makeSingleObjectiveFunction(
  name = "Modification Forrester et. all function",
  fn = function(x) (sin(4*x - 4)) * ((2*x - 2)^2) * (sin(20*x - 4)),
  par.set = makeNumericParamSet(lower = 0, upper = 1, len = 1L),
  noisy = TRUE
)
set.seed(9)
design = generateDesign(7L, getParamSet(modForrester), fun = lhs::maximinLHS)
design$y = modForrester(design)
ordered.design = design[order(design$x),]
rownames(ordered.design) = NULL
kable(ordered.design, digits = 3)
@

\hspace{0.5cm}
\column{0.7\textwidth}
\includegraphics[height = 0.55\textheight]{figure_man/regression_tree}
\end{columns}
\vspace{0.5cm}
Data points (red) were generated from the underlying function (black):

$ sin(4x - 4) * (2x - 2)^2 * sin(20x -4) $

\pagebreak
\textbf{Example for Regression:}
<<fig.height=5>>=
regr.task = makeRegrTask(data = design, target = "y")
regr.rpart = makeLearner("regr.rpart", par.vals = list(minsplit=1, minbucket = 1))
regr.model = train(regr.rpart, regr.task)
fancyRpartPlot(regr.model$learner.model, sub="")
@
\end{vbframe}


%there were commented frames here in the original file

\begin{vbframe}{CART: Split criteria}

  Let $\Np \subseteq \D$ be a parent node with two child nodes $\Nl$ and $\Nr$.

  Dividing all of the data with respect to the split variable $\xj$ at split point $t$, leads to the following half-spaces:

  \begin{align*}
    \Nl(j,t) &= \{ (x,y) \in \Np: \xj \leq t \} \text{ and } \Nr(j,t) = \{ (x,y) \in \Np: \xj > t \}.
  \end{align*}

  Assume we can measure the impurity of the data in node $\Np$ (usually the label distribution) with function $I(\Np)$.
  This function should return an \enquote{average quantity per observation}.

  Potential splits created in a node $\Np$ are then evaluated via impurity reduction:

    $$  I(\Np) - \frac{|\Nl|}{|\Np|} I(\Nl) - \frac{|\Nr|}{|\Np|} I(\Nr) $$

  $|\Np|$ means number of data points contained in (parent) node $\Np$.

  \framebreak

  \begin{itemize}
  \item {\bf Continuous targets:} mean-squared error / variance

  $$I(\Np) = \frac{1}{|\Np|} \sum\limits_{(x,y) \in \Np} (y - \bar{y}_\Np)^2$$
  with $\bar{y}_\Np = \frac{1}{|\Np|} \sum\limits_{(x,y) \in \Np} y$.

  \vspace{0.3cm}

  Hence, the best prediction in a potential leaf $\Np$ is the mean of the contained y-values, i.e. impurity here is variance of y-values.

  \vspace{0.3cm}

  We can also obtain this by considering:
  \begin{align*}
    \min_{j,t} \left(\min_{c_1} \sum_{(x,y) \in \Nl} (y -
        c_1)^2 + \min_{c_2} \sum_{(x,y) \in \Nr} (y - c_2)^2
    \right).
  \end{align*}
  The inner minimization is solved through:
  $\hat{c}_1 = \bar{y}_1$ and $\hat{c}_2 = \bar{y}_2$

  \framebreak

  \item {\bf Categorical targets ($\mathbf{K}$ categories):} \enquote{Impurity Measures}
    \begin{itemize}
    \item Gini index:
      $$I(\Np) = \sum_{k\neq k'} \pikN_k \pikN_{k'} = \sum_{k=1}^g \pikN_k(1-\pikN_k)$$
    \item misclassification error:
      $$I(\Np) = 1 - \max_k \pikN_k$$
    \item Shannon entropy:
      $$I(\Np) = -\sum_{k=1}^g \pikN_k \log \pikN_k \ ,$$
    \end{itemize}
    where $\pikN_k$ corresponds to the relative frequency of category $k$ of the response.
  \end{itemize}

\framebreak

<<results='hide', fig.height=5>>=
Colors = colorspace::rainbow_hcl(3)
par(mar = c(5.1, 4.1, 0.1, 0.1))
p = seq(0, 1, length.out = 200)
entropy = function(p) (p * log(p) + (1 - p) * log(1 - p))/(2 * log(0.5))
gini = function(p) 2 * p * (1-p)
missclassification = function(p) (1 - max(p, 1 - p))
plot(p, entropy(p), type = "l", col = Colors[2], ylab = "", ylim = c(0, 0.6))
lines(p, gini(p), col = Colors[1])
lines(p, sapply(p, missclassification), col = Colors[3])
legend("topright", c("Gini Index", "Entropy", "Missclassification Error"),
       col = Colors[1:3], lty = 1)
@

\end{vbframe}


\begin{vbframe}{Impurity Measures}
\begin{itemize}
\item In general the three proposed splitting criteria are quite similar.
\item Entropy and Gini index are more sensitive to changes in the node probabilities.
\item \textbf{Example:} two-class problem with 400 obs in each class and two possible splits:
\end{itemize}
\begin{columns}[T,onlytextwidth]
\column{0.5\textwidth}
\begin{center}
\textbf{Split 1:} \\
\vspace{0.25cm}
<<>>=
class = as.factor(c(rep(0,400), rep(1,400)))
x1 = as.factor(c(rep(0,300), rep(1,400), rep(0,100)))
x2 = as.factor(c(rep(0,600), rep(1,200)))
tab = table(x1, class)
tab2 = table(x2, class)
rownames(tab) = c("Left node", "Right node")
rownames(tab2) = c("Left node", "Right node")
kable(tab, row.names = TRUE, col.names = c("class A", "class B"))
@
\end{center}
\column{0.5\textwidth}
\begin{center}
\textbf{Split 2:} \\
\vspace{0.25cm}
<<>>=
kable(tab2, row.names = TRUE, col.names = c("class A", "class B"))
@
\end{center}
\end{columns}

\framebreak
\begin{columns}[T,onlytextwidth]
\column{0.5\textwidth}
\begin{center}
\textbf{Split 1:} \\
\vspace{0.25cm}
<<>>=
kable(tab, row.names = TRUE, col.names = c("class A", "class B"))
@
\end{center}
\column{0.5\textwidth}
\begin{center}
\textbf{Split 2:} \\
\vspace{0.25cm}
<<>>=
kable(tab2, row.names = TRUE, col.names = c("class A", "class B"))
@
\end{center}
\end{columns}

\begin{itemize}
\item Both splits produce a misclassification rate of $\frac{200}{800}=0.25$
\item Split 2 produces a pure node and is probably preferable.
\item The average node impurity after a split based on $x_1$ is $0.375$ (Gini) or $0.406$ (Entropy) and $\frac{1}{3}$ (Gini) or $0.344$ (Entropy) after a split based on $x_2$.
% Gini: 6/8 * 2 * 1/3 * 2/3
% entropy: 6/8 * ((1/3 * log(1/3) + 2/3 * log(2/3)) / (2 * log(0.5)))
\item Both criteria prefer split 2 and \textit{choose} the result with a pure node.
\end{itemize}
\framebreak
\begin{itemize}
\item For metric features the exact split points can be ambiguous.
\item If the classes of the response (for classification trees) are completely separated regarding the value range of the feature, a split can be done anywhere between the extreme values of the feature in the classes and the impurity measures stay the same.
\item Look again at the Iris data and the classes \textit{setosa} and \textit{versicolor}:
\end{itemize}
<<results='hide', fig.height=3>>=
# Trees on Iris Data
iristask = makeClassifTask(data = iris[,-(1:2)], target = "Species")

rpart = makeLearner("classif.rpart")
rpart = setHyperPars(rpart, cp = 0, minbucket=4, maxdepth=1) #Illustration
model = train(rpart, iristask)
# Plot
plotLearnerPrediction(rpart, iristask, gridsize = 300, cv = 0) +
  pers_theme +
  ggtitle("Iris Data")
@
\end{vbframe}

\begin{vbframe}{Monotone feature transformations}

Monotone transformations of one or several features will not change the value of the impurity measure, neither the structure of the tree (just the numerical value of the split point).
\vspace{0.5cm}
\begin{columns}[T]
\column{0.49\textwidth}
Original data
<<>>=
x = c(1,2,7,10,20)
y = c(1,1,0.5,10,11)
data = t(data.frame(x = x, y = y))
kable(data)
@

\column{0.49\textwidth}
Data with log-transformed $x$
<<fig.align='right'>>=
log.x = log(x)
data = t(data.frame(log.x, y))
rownames(data) = c("log(x)", "y")
kable(data,digits = 1)
@
\end{columns}
\vspace{0.5cm}
\centering
\includegraphics[width = \textwidth]{figure_man/monotone_trafo.png}
\end{vbframe}

\begin{vbframe}{CART: Stopping-Criteria}
  \begin{itemize}
    \item Minimal number of observations per node, for a split to be tried
    \item Minimal number of observations that must be contained in a leaf
    \item Minimal increase in goodness of fit that must be reached for a split to be
      tried
    \item Maximum number of levels for your tree
  \end{itemize}
\end{vbframe}

\begin{vbframe}{CART: Overfitting}
  \begin{itemize}
  \item The CART-Algorithm could just continue until there is a single observation in each node

  \item [$\Rightarrow$] Complexity (and hence the danger of overfitting)
  increases with the number of splits / levels / leafs
  \end{itemize}
\end{vbframe}

\begin{vbframe}{CART: Minimal cost complexity pruning}
  \begin{itemize}
  \item Method to optimize the trade-off between goodness-of-fit and complexity
  \item Criteria: cost function
    \begin{align*}
      R_{\alpha} &= R(T) + \alpha \cdot \# \text{leafs},
    \end{align*}
    where $R(T)$ represents the error of tree $T$ on the training set
    $\rightarrow$ $R_{\alpha}$ = Training error + Complexity term.
  \item For every $\alpha$ there is a distinctly defined smallest sub-tree of the original tree,
  which minimizes the cost function.
  \item $\hat{\alpha}$ can be assessed with cross-validation.
  \item Final tree is fitted on the whole data, where $\hat{\alpha}$ is used to
  to find the optimal size of the tree
  \end{itemize}

\pagebreak

<<size='footnotesize'>>=
lrn = makeLearner("regr.rpart")
mod = train(lrn, bh.task)
mod = mod$learner.model
@

\pagebreak

<<results='hide'>>=
# Pruning with every cp taken from the cptable
cps = rev(mod$cptable[, "CP"])
lapply(cps, function(x) {
    p = rpart::prune(mod, cp = x)
    sub_title = sprintf("Pruning with complexity parameter = %.4f.", x)
    rattle::fancyRpartPlot(p, sub = sub_title)})
@
\end{vbframe}

\begin{vbframe}{CART: Categorical Predictors}
  \begin{itemize}
  \item For a nominal scaled feature with $Q$ categories,
  there are $2^{Q-1}-1$ possible partitions of the $Q$ values into two groups:
    \begin{itemize}
    \item There are $2^Q$ ways to assign $Q$ distinct values to the left or right node.
    \item Two of these configurations lead to an empty node, while the other one contains all observations.
    Discarding these configurations leads to $2^Q -2$ possible partitions.
    \item Symmetry halves the number of possible partitions: $\frac{1}{2}(2^Q - 2) = 2^{Q-1} - 1$
    \end{itemize}
    $\Rightarrow$ computations become prohibitive for large values of $Q$
  \item But for regression with squared loss and binary classification shortcuts exist.
  \end{itemize}

  \pagebreak

For $0-1$ responses, in each node:
  \begin{enumerate}
  \item Calculate the proportion of 1-outcomes for each category of the feature.
  \item Sort the categories according to these proportions.
  \item The feature can then be treated as if it were an ordered categorical feature ($Q-1$ possible splits).
  \end{enumerate}

  \vspace{0.3cm}

<<fig.height=3.4>>=
set.seed(1234)
# generate data (one categorcal variable with 4 categories, 0-1 response)
data = data.frame(category = sample(c("A", "B", "C", "D"), size = 150,
  replace = TRUE, prob = c(0.2, 0.1, 0.4, 0.3)),
  y = sample(c(0,1), size = 150, replace = TRUE, prob = c(0.3, 0.7)))

# calculates proportion of 1-outcomes and plot
subset.data = subset(data, y == 1)
plot.data = data.frame(prop.table(table(subset.data$category)))
colnames(plot.data) = c("Category", "Frequency")
p1 = ggplot(data = plot.data, aes(x = Category, y = Frequency, fill = Category)) +
  geom_bar(stat = "identity")  + theme(legend.position = "none") +
  ggtitle("1)") + ylab("Frequency of class 1") + xlab("Category of feature")

# sort by proportions
p2.pre = ggplot(data = plot.data, aes(x = reorder(Category, Frequency), y = Frequency, fill = Category)) +
  geom_bar(stat = "identity")  + theme(legend.position = "none") +
  ylab("Frequency of class 1") + xlab("Category of feature")
p2 = p2.pre + ggtitle("2)")


# decision tree to draw a vertical line where the spit is being made
mod = rpart::rpart(y ~., data = data)
lvls = levels(reorder(plot.data$Category, plot.data$Frequency))
vline.level = 'C'
p3 = p2.pre +  geom_vline(xintercept = which(lvls == vline.level) - 0.5, col = 'red', lwd = 1, linetype = "dashed") +
  ggtitle("3)")
grid.arrange(p1, p2, p3, ncol = 3)
@

\pagebreak

  \begin{itemize}
  \item This procedure obtains the optimal split for entropy and Gini index.
  \item This result also holds for regression trees (with squared error loss) -- the categories are ordered by increasing mean of the outcome (see next slide).
  \item The proofs are not trivial and can be found here:
    \begin{itemize}
    \item for 0-1 responses:
      \begin{itemize}
      \item Breiman, 1984, Classification and Regression Trees.
      \item Ripley, 1996, Pattern Recognition and Neural Networks.
      \end{itemize}
    \item for continuous responses:
      \begin{itemize}
      \item Fisher, 1958, On grouping for maximum homogeneity.
      \end{itemize}
    \end{itemize}
  \item Such simplifications are not known for multiclass problems.
  %\item The Algorithm prefers categorical variables with a large value
  %of categories $Q$
  \end{itemize}

\pagebreak

For continuous responses, in each node:
  \begin{enumerate}
  \item Calculate the mean of the outcome in each category.
  \item Sort the categories by increasing mean of the outcome.
  \item The feature can then be treated as if it were an ordered categorical feature ($Q-1$ possible splits).
  \end{enumerate}

\vspace{0.3cm}

<<fig.height=3.5>>=
set.seed(1234)
# generate data (one categorcal variable with 4 categories, 0-1 response)
data = rbind(data.frame(category = "A", y = runif(30, 5, 7.5)),
  data.frame(category = "B", y = runif(15, 6, 12)),
  data.frame(category = "C", y = runif(60, 5, 20)),
  data.frame(category = "D", y = runif(45, 1, 6)))

# calculate the mean of the outcome in each category
plot.data = aggregate(y ~ category, data = data, FUN = mean)
colnames(plot.data) = c("Category", "Mean")

# plot the categories wrt the mean of the outcome in each category
p1 = ggplot(data = plot.data, aes(x = Category, y = Mean, fill = Category)) +
  geom_bar(stat = "identity")  + theme(legend.position = "none") +
  ggtitle("1)") + ylab("Mean of outcome") + xlab("Category of feature")

# sort by increasing mean of the outcome
p2.pre = ggplot(data = plot.data, aes(x = reorder(Category, Mean), y = Mean, fill = Category)) +
  geom_bar(stat = "identity")  + theme(legend.position = "none") +
  ylab("Mean of outcome") + xlab("Category of feature")
p2 = p2.pre + ggtitle("2)")

# decision tree to draw a vertical line where the spit is being made
mod = rpart::rpart(y ~., data = data)
lvls = levels(reorder(plot.data$Category, plot.data$Mean))
vline.level = 'B'
p3 = p2.pre +  geom_vline(xintercept = which(lvls == vline.level) - 0.5, col = 'red', lwd = 1, linetype = "dashed") +
  ggtitle("3)")
grid.arrange(p1, p2, p3, ncol = 3)
@
\end{vbframe}


\begin{vbframe}{CART: Missing predictor values}
  Two approaches:
  \begin{enumerate}
  \item Missing values of a categorical variable are treated as an own category
  \item When considering a predictor for a split,
  only use the observations for which the predictor is not missing.

  To pass observations with missing values down the tree (during fitting or predicting),
    we have to find surrogate variables, that produce similar splits.
  \end{enumerate}
\end{vbframe}

\begin{vbframe}{Advantages}
  \begin{itemize}
    \item Model is easy to comprehend, graphical representation
    \item Categorical features can easily be handled
    \item Missing values can be handled
    \item No problems with outliers in features
    \item Monotone transformations of features change nothing
    \item Interaction effects between features are easily possible
    \item Works for (some) non-linear functions
    \item Inherent feature selection
    \item Quite fast, scales well with larger data
    \item Trees are flexible by creating a custom split criterion and leaf-node prediction rule
      (clustering trees, semi-supervised trees, density estimation, etc.)
  \end{itemize}
\end{vbframe}

\begin{vbframe}{Disadvantages}
  \begin{itemize}
  \item High instability (variance) of the trees: Small changes in the data could lead to completely different splits, thus, to completely different trees $\rightarrow$ Decisions on the upper level influence decisions on lower levels (\enquote{mistakes} in upper levels proceed to the lower ones.)
  \item Prediction function isn't smooth because a step function is fitted.
  \item Linear dependencies must be modeled over several splits
    $\rightarrow$ Simple linear correlations must be translated into a complex tree structure
    (see the following example)
  \item Really not the best predictor: Combine with bagging (forest) or boosting!
  (This will also be illustrated in a small benchmark at the end of the random forest chapter.)
  \end{itemize}

\pagebreak

High instability of trees will be demonstrated using the Wisconsin Breast Cancer data set.
It has 699 observations on 9 features and one target class with values \enquote{benign} and \enquote{malignant}.

\begin{table}
\begin{tabular}{ll}
Feature name & Explanation\\
\hline
\code{Cl.thickness} & Clump Thickness\\
\code{Cell.size} & Uniformity of Cell Size\\
\code{Cell.shape} & Uniformity of Cell Shape\\
\code{Marg.adhesion} & Marginal Adhesion\\
\code{Epith.c.size} & Single Epithelial Cell Size\\
\code{Bare.nuclei} & Bare Nuclei\\
\code{Bl.cromatin} & Bland Chromatin\\
\code{Normal.nucleoli} & Normal Nucleoli\\
\code{Mitoses} & Mitoses\\
\end{tabular}
\end{table}

\pagebreak

Tree fitted on complete Wisconsin Breast Cancer data
<<results='hide', fig.height=5>>=
# Create learner and train learner on the Wisconsin Breast Cancer data
lrn = makeLearner("classif.rpart")
model = train(lrn, bc.task)

# Remove one observation from the data and train learner on modified dataset
bc.data = getTaskData(bc.task)
bc.data.mod = bc.data[-13, ]
bc.task.mod = makeClassifTask("bc.task modified",
  data = bc.data.mod, target = getTaskTargetNames(bc.task))
model.mod = train(lrn, bc.task.mod)

# Display tree on full data set
fancyRpartPlot(model$learner.model, sub = "")
@
\pagebreak

Tree fitted on Wisconsin Breast Cancer data without observation 13
<<results='hide', fig.height=5>>=
# Leaving out just one obs leads to a totally differnt tree
fancyRpartPlot(model.mod$learner.model, sub = "")
@

\pagebreak
\setkeys{Gin}{width=0.95\textwidth}
\begin{center}
<<results='hide', fig.height=5>>=
set.seed(123)
n = 100
data = data.frame(x1 = runif(n), x2 = runif(n))
data$y = as.factor(with(data, as.integer(x1 > x2)))

ggplot(data, aes(x = x1, y = x2, shape = y, col = y)) +
  geom_point(size = 3) +
  pers_theme +
  ggtitle("") +
  geom_abline(slope = 1, linetype = 2)
@
\end{center}
Linear dependencies must be modeled over several splits.
\pagebreak
\begin{center}
<<results='hide', fig.height=5>>=
problemtask = makeClassifTask(data = data, target = "y")

rpart = setHyperPars(rpart, cp = 0, minbucket = 2, maxdepth = 1)
model = train(rpart, problemtask)
# Plot
plotLearnerPrediction(rpart, problemtask, gridsize = 300, cv = 0) +
  pers_theme +
  ggtitle("") +
  geom_abline(slope = 1, linetype = 2)
@
\end{center}
Linear dependencies must be modeled over several splits.
\pagebreak
\begin{center}
<<results='hide', fig.height=5>>=
rpart = setHyperPars(rpart, cp = 0, minbucket = 2, maxdepth = 2)
model = train(rpart, problemtask)
# Plot
plotLearnerPrediction(rpart, problemtask, gridsize = 300, cv = 0) +
  pers_theme +
  ggtitle("") +
  geom_abline(slope = 1, linetype = 2)
@
\end{center}
Linear dependencies must be modeled over several splits.
\pagebreak
\begin{center}
<<results='hide', fig.height=5>>=
rpart = setHyperPars(rpart, cp = 0, minbucket = 2, maxdepth = 3)
model = train(rpart, problemtask)
# Plot
plotLearnerPrediction(rpart, problemtask, gridsize = 300, cv = 0) +
  pers_theme +
  ggtitle("") +
  geom_abline(slope = 1, linetype = 2)
@
\end{center}
Linear dependencies must be modeled over several splits.
\pagebreak
\begin{center}
<<results='hide', fig.height=5>>=
rpart = setHyperPars(rpart, cp = 0, minbucket = 2, maxdepth = 4)
model = train(rpart, problemtask)
# Plot
plotLearnerPrediction(rpart, problemtask, gridsize = 300, cv = 0) +
  pers_theme +
  ggtitle("") +
  geom_abline(slope = 1, linetype = 2)
@
\end{center}
Linear dependencies must be modeled over several splits.

\pagebreak
\begin{center}
<<results='hide', fig.height=5>>=
modForrester = makeSingleObjectiveFunction(
  name = "Modification Forrester et. all function",
  fn = function(x) (sin(4*x - 4)) * ((2*x - 2)^2) * (sin(20*x - 4)),
  par.set = makeNumericParamSet(lower = 0, upper = 1, len = 1L),
  noisy = TRUE
)
set.seed(9)
design = generateDesign(7L, getParamSet(modForrester), fun = lhs::maximinLHS)
design$y = modForrester(design)
design
regr.task = makeRegrTask(data = design, target = "y")
fn = function(x) (sin(4*x - 4)) * ((2*x - 2)^2) * (sin(20*x - 4))
regr.rpart = makeLearner("regr.rpart", par.vals = list(minbucket = 1, minsplit = 1))
pp = plotLearnerPrediction(regr.rpart, regr.task, cv = 0)
x = seq(0.07547466, 1, length.out = 500)
pp + geom_line(data = data.frame(x = x, y = fn(x)), aes(x, y, color = "True function"), color = "red")
@
\end{center}
Prediction function isn't smooth because a step function is fitted.
\end{vbframe}

\begin{vbframe}{Tree building algorithms and their implementation in R}
  \begin{itemize}
  \item CART (Breiman et al., 1984):

  Package \pkg{rpart} with fitting function \code{rpart()}.
  \begin{itemize}
    \item No nice plotting functionalities.
    \item But \pkg{rattle} is able to create fancy visualizations.
  \end{itemize}

  \item Unbiased Recursive Partitioning (Hothorn et al., 2006):

  \pkg{partykit} (old: \pkg{party}) provides the function \code{ctree()} for recursive partitioning in a conditional inference framework.
  \begin{itemize}
    \item Supports continuous, censored, ordered, nominal and multivariate response variables.
    \item \code{ctree()} uses unbiased split selection.
    \item Nice plotting functionality.
  \end{itemize}

  \item C4.5 (Quinlan, 1993):

  Implemented in the R-package \pkg{RWeka} with function \code{J48()}.
  %\item \pkg{evtree} (function \code{evtree()}):

  %Uses evolutionary algorithms, which is a global optimization method to search over the parameter space of trees instead of performing a forward stepwise search like in CART.
  \end{itemize}
\end{vbframe}


\endlecture




\lecturechapter{4}{Bagging and Random Forests}
\lecture{Fortgeschrittene Computerintensive Methoden}

\begin{vbframe}{Ensemble methods}

% \begin{itemize}

% \item A \enquote{\bf{base learner}} is often referred to as \enquote{weak learner}.
% \enquote{\bf{Weak learners}} (e.g. decision trees) are learning algorithms that should perform (slightly) better than random guessing, e.g. in case of a balanced  classification problem the misclassification rate should be (slightly) better than 0.5.
% \item The linear combination of the base learners potentially expands the hypothesis space.
% \end{itemize}

% \framebreak
% Common ensemble methods:
  \begin{itemize}
\item Ensemble methods combine the predictions of several base learners and combine them into an aggregated estimator.
  \item Homogeneous ensembles (multiple models of same base learner)
    \begin{itemize}
    \item Bagging: Fit models on bootstrapped versions of training data
    \item Boosting: runs sequentially: each model on reweighted data version / the previous residuals so
      it improves the errors of the previous round
    \end{itemize}
  \item Heterogeneous ensembles (different base learners)
    \begin{itemize}
      \item Fit different base learners on the same data or different \enquote{views} of the same data.
        Then learn how to aggregate their predictions, often with a 2nd-layer model.
    \end{itemize}
  \end{itemize}

\framebreak

General homogenous approach (often it works like this but not always)
  \begin{itemize}
  \item A \enquote{base learner} is selected and fitted multiple times to either resampled or reweighted versions
  of the original data.
  \item The base learner is applied to either resampled or reweighted versions of the original dataset.
  % \item his results in $M$ prediction functions $g^{(1)}(x),\dots,g^{(M)}(x)$.
  This results in $M$ prediction functions $\bl{1},\dots,\bl{M}$.
  \item These $M$ function are aggregated, usually in a linear fashion.

  This results in the following final prediction function:
$$f(x) = \sum_{m=1}^M \betam \blm$$
with coefficients $\betai{1},\dots,\betai{M}$.
  \end{itemize}
\end{vbframe}

\begin{vbframe}{Bagging}
  \begin{itemize}
    \item Bagging is based on {\bf B}ootstrap {\bf Agg}regation.
    \item Proposed by Breiman (1996).
    \item Train on multiple bootstrap samples of data $\D$, then combine:

    \begin{enumerate}
      \item Create $M$ bootstrap samples of size $n$.
      \item Fit the base learner on each of the $M$ bootstrap samples.
      \item Aggregate the predictions of the $M$ estimators via averaging or majority voting.
    \end{enumerate}

    \item $M$ affects Monte-Carlo approximation error; main hyperparameter.
    \item Interpretability of the model becomes harder.
  \end{itemize}

\framebreak

\begin{algorithm}[H]
  \small
  \setstretch{1.15}
  \caption*{Bagging algorithm}
  \begin{algorithmic}[1]
    \State {\bf Input: } Dataset $\D$, base learner, number of bootstraps $M$
    \For {$m = 1 \to M$}
      \State Draw a bootstrap sample $\D^{[m]}$ from $\D$.
      \State Train base learner on $\D^{[m]}$, obtain model $\blm$
    \EndFor
    \State Aggregate the predictions of the $M$ estimators (via averaging or majority voting), to determine the bagging estimator:
    $$
    f(x) = \frac{1}{M} \sum_{m=1}^M \blm
    $$
  \end{algorithmic}
\end{algorithm}

\framebreak

  \begin{itemize}
    \item Bagging reduces the variance of the estimator, but increases the bias in return.
    \item Bagging works best for unstable/high variance learners (learners where small perturbations in training set lead to larger changes in the prediction)

    \begin{itemize}
      \item Classification and regression trees
      \item Neural networks
      \item Piece-wise variable selection in the regression case, etc.
    \end{itemize}

    \item For stable estimation methods bagging might degrade performance
    \begin{itemize}
      \item k-nearest neighbor
      \item discriminant analysis
      \item naive bayes
      \item linear regression
    \end{itemize}

  \end{itemize}
\end{vbframe}

\begin{vbframe}{Why does bagging work?}
\begin{itemize}
  \item Suppose we have a numerical dependent variable.
  \item The training datasets are given by $\D$ and base learner estimator by $f(x)$.
  \item The datasets are sampled independently from distribution $\P_{xy}$ (data generating process).
  \item The {\em theoretical} aggregated estimator is given by
    \begin{align*}
      f_{\text{A}} (x) &= \E_\D[f(x)].
    \end{align*}

  \framebreak

  \item Let x,y be a random sample from $\P_{xy}$ but independent of $\D$. The average error of the normal
    $f(x)$ is then
    $e = E_\D E_{xy} [(y - f(x))^2]$
    and of the aggregated estimator
    $e_A = E_{xy} [(y - f_A(x))^2]$.
 \item It follows:
   $$ e = E_\D E_{xy} [(y - f(x))^2] = E_{xy}[y^2] - 2 E_{xy}[y f_A] + E_{\D} E_{xy} [f^2(x)] $$

 \item And we apply Jensen's inequality to $e$:
   $$ e = E_{xy} E_\D [(y - f(x))^2] \geq E_{xy} (E_\D[y - f(x)])^2 = E_{xy} [(y - f_A(x)])^2] = e_A $$
   $$ = E_{xy}[y^2] - 2 E_{xy}[y f_A] + E_{xy} [f_A^2(x)] $$

 \framebreak
 \item The difference between $e$ and $e_A$ is $E_{\D} E_{xy} [f^2(x)] \geq \E_{xy} [f_A^2(x)] $

 \item The more unstable $f(x)$, the more error reduction we obtain.
 \item But the bagging estimator only approximates the theoretical $f_A$ (bootstrap), we therefore suffer from approximation error (bias) by using the empirical distribution function instead of the true data generating process and only perform $M$ bootstrap iterations instead of an infinite number.
\item Bagging does not necessarily lead to an improved classifier.
\begin{itemize}
\item Example: binary outcome, $y = 1$ for all values of $x$
\item Consider random classifier $f$ with $\text{P}(\fx = 1) = 0.4$
(independent of $x$)
\item Prediction error for $f$ is 0.6
\item Prediction error for the bagging estimator is 1
\end{itemize}
\end{itemize}

\end{vbframe}

\begin{vbframe}{Random Forests}
  \begin{itemize}
  \item Modification of bagging for trees.
  \item Proposed by Breiman (2001).
  \item Construction of bootstrapped {\bf decorrelated} trees
  \item The variance of the bagging prediction depends on the correlation between the trees $\rho$
      \begin{align*}
        \rho \sigma^2 + \frac{1-\rho}{M} \sigma^2,
      \end{align*}
      where $\sigma^2$ describes the variance of a tree.
  \end{itemize}
\begin{itemize}
\item[$\Rightarrow$] Reduce correlation by randomization in each split.
  Instead of all $p$ features, draw $mtry \le p$ random split candidates.
\item[$\Rightarrow$] Trees are expanded as much as possible, without aggressive early stopping or pruning,
  to increase variance.
\end{itemize}

  \framebreak

  \begin{algorithm}[H]
  \caption*{Random Forest algorithm}
  \begin{algorithmic}[1]
  \State {\bf Input: }A dataset $\D$ of $n$ observations, number $M$ of trees
  in the forest, number $mtry$ of variables to draw for each split
  \For {$m = 1 \to M$}
  \State Draw a bootstrap sample $\D^{[m]}$ from $\D$
  \State Grow tree $\blm$ using $\D^{[m]}$
  \State For each split only consider $mtry$ randomly selected features
  \State Grow tree without early stopping or pruning
\EndFor
\State Aggregate the predictions of the $M$ estimators (via averaging or majority voting), to predict on new data.
\end{algorithmic}
\end{algorithm}

\framebreak

\begin{center}\includegraphics[width=0.95\textwidth]{figure_man/forest.png}\end{center}

\framebreak

\begin{itemize}
  \item The following values are recommended for $mtry$:
    \begin{itemize}
    \item Classification: $\lfloor \sqrt{p} \rfloor$
    \item Regression: $\lfloor p/3 \rfloor$
    \end{itemize}

  \item Out-Of-Bag error: On average ca. 1/3 of points are not drawn.
    \begin{align*}
      \P(\text{Obs. not drawn}) &= \left(1 - \frac{1}{n}\right)^n
      \ \stackrel{n \to \infty}{\longrightarrow} \ \frac{1}{e} \approx
      \Sexpr{round(exp(-1), digits = 2)}.
    \end{align*}

  To compute the OOB error, each observation $x$ is predicted only with those trees that did not use $x$ in their fit.
   \item The OOB error is similar to cross-validation estimation. It can also be used for a quicker model selection.
  \end{itemize}

\framebreak
% This pic has been created with powerpoint. To get a good quality pic
% I marked all the elements of the pic and copied them into IrfanView,
% where I can save these elements as a jpg.
\begin{center}
\includegraphics{figure_man/rF_oob_error.jpg}
\end{center}

\framebreak

\begin{figure}
<<echo=FALSE, message=FALSE, warning=FALSE, fig.height=4.5>>=
library(mlr)
set.seed(1)
lrn = makeLearner("regr.randomForest", predict.type = "response")
# remove this values from title
lrn$par.vals$se.boot = NULL
lrn$par.vals$ntree.for.se = NULL
lrn2 = makeLearner("classif.randomForest")
# remove this values from title
lrn2$par.vals$se.boot = NULL
lrn2$par.vals$ntree.for.se = NULL
task = convertMLBenchObjToTask("mlbench.friedman1", n = 500, sd = 0.1)
plotLearnerPrediction(lrn, task, cv = 0, ntree = 1)
@
\caption{randomForest trained on the \enquote{friedman1} regression task
from the \pkg{mlbench} \pkg{R}-package with increasing number of trees}
\end{figure}

\framebreak
<<echo=FALSE>>=
plotLearnerPrediction(lrn, task, cv = 0, ntree = 10)
@
\framebreak
<<echo=FALSE>>=
plotLearnerPrediction(lrn, task, cv = 0, ntree = 500)
@
\framebreak
\begin{figure}
<<echo=FALSE, fig.height=4.5>>=
plotLearnerPrediction(lrn2, iris.task, cv = 0, ntree = 1)
@
\caption{randomForest on \enquote{iris} for increasing number of trees}
\end{figure}
\framebreak
<<echo=FALSE>>=
plotLearnerPrediction(lrn2, iris.task, cv = 0, ntree = 10)
@
\framebreak
<<echo=FALSE>>=
plotLearnerPrediction(lrn2, iris.task, cv = 0, ntree = 500)
@
\framebreak

<<echo=FALSE>>=
mod = train(lrn, task)$learner.model
plot(mod, main = "OOB error for different number of trees, call plot(model) on rf for this")
@

\end{vbframe}

\begin{vbframe}{Variable importance}

\begin{itemize}
  \item Single trees are highly interpretable
  \item Random Forests as an ensemble of many trees lose this feature
  \item Hence, contributions of a single covariate to the fit are difficult to evaluate
  \item Way out: variable importance measures
\end{itemize}

\framebreak

\begin{algorithm}[H]
  \small
  \caption*{Measure based on permutations of OOB observations}
  \begin{algorithmic}[1]
    \State After growing tree $\blmh$, pass down OOB observations and record
    predictive accuracy.
    \State Permute OOB observations of $j$th variable.
    \State Pass down the permuted OOB observations and evaluate predictive accuracy again.
    \State The loss of goodness induced by permutation is averaged over all trees and is used as a measure for the importance of the $j^\text{th}$ variable.
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
  \small
  \caption*{Measure based on improvement in split criterion}
  \begin{algorithmic}[1]
    \State At each split in tree $\blmh$ the improvement in the split criterion is attributed as variable importance measure for the splitting variable.
    \State For each variable, this improvement is accumulated over all trees for the importance measure.
  \end{algorithmic}
\end{algorithm}
\end{vbframe}

\begin{vbframe}{Variable Importance based on permutations of OOB observations}
\begin{center}
\includegraphics[width = 10.3cm]{figure_man/rF_varImp_permutation.jpg}
\end{center}
\end{vbframe}

\begin{vbframe}{Variable importance}
\begin{figure}
<<size="footnotesize", fig.height=3>>=
model = randomForest(Species ~ ., data = iris, importance = TRUE)
randomForest::varImpPlot(model)
@
\caption{Two importance measures on iris.}
\end{figure}

\begin{figure}
<<size="footnotesize", fig.height=3>>=
v = generateFilterValuesData(iris.task,
  method = c("randomForest.importance", "cforest.importance"))
plotFilterValues(v)
@
\caption{RF importance as filters in mlr.}
\end{figure}

\end{vbframe}

\begin{vbframe}{Random Forest: Advantages}

\begin{itemize}
  \item Easy to implement
  \item Can be applied to basically any model
  \item Easy to parallelize
  \item Often works well (enough)
  \item Enables variance analysis
  \item Integrated estimation of OOB error
  \item Can work on high-dimensional data
  \item Often not much tuning necessary
\end{itemize}

\end{vbframe}

\begin{vbframe}{Random Forest: Disadvantages}

\begin{itemize}
  \item Often suboptimal for regression
  \item Hard to interpret, especially interactions
  \item Does not really optimize loss aggressively
  \item No real way to adapt to problem\\
  (see e.g. loss in GBM, kernel in SVM)
  \item Implementations sometimes memory-hungry
  \item Prediction can be slow
\end{itemize}

\end{vbframe}

\begin{vbframe}{Benchmark: Random Forest vs. (bagged) rpart vs. (bagged) knn}

  \begin{itemize}
    \item Goal: Compare performance of random forest against (bagged) stable and (bagged) unstable methods
    \item Algorithms:
    \begin{itemize}
      \item classification tree (\code{rpart})
      \item bagged classification tree using 50 bagging iterations (\code{bagged.rpart})
      \item k-nearest neighbors (\code{knn})
      \item bagged k-nearest neighbors using 50 bagging iterations (\code{bagged.knn})
      \item random forest with 50 trees (\code{rF})
    \end{itemize}
    \item Method to evaluate performance: 10-fold cross-validation
    \item Performance measure: mean missclassification error
    \end{itemize}

    \framebreak

    \begin{itemize}
    \item Datasets from \pkg{mlbench}:
    \end{itemize}

\begin{table}
\footnotesize
\begin{tabular}{p{1.5cm}p{2cm}p{0.5cm}p{0.5cm}p{5cm}}
Name & Kind of data &  n & p & Task\\
\hline
Glass & Glass identification data & 214 & 10 & Predict the type of glass on the basis of the chemical analysis of the glasses represented by the 10 features\\
Ionosphere & Radar data & 351 & 35 & Predict whether the radar returns show evidence of some type of structure in the ionosphere (\enquote{good}) or not (\enquote{bad}) \\
Sonar & Sonar data & 208 & 61 & Discriminate between sonar signals bounced off a metal cylinder (\enquote{M}) and those bounced off a cylindrical rock (\enquote{R})\\
Waveform & Artificial data & 100 & 21 & Simulated 3-class problem which is considered to be a difficult pattern recognition problem. Each class is generated by the waveform generator.\\
\hline
\end{tabular}
\end{table}

  \framebreak

  \begin{center}
  \includegraphics[height = 7.2cm, keepaspectratio]{figure_man/bm_stable_vs_unstable.jpg}
  \end{center}

  \framebreak

  Bagging knn does not improve performance because:

  \begin{itemize}
    \item knn is stable w.r.t. perturbations
    \item Each bootstrap sample contains about 63\% unique observations
        $$ \P(\text{obs. drawn}) = \left(1 - \frac{1}{n}\right)^n = \ \stackrel{n \to \infty}{\longrightarrow} \ 1 - \frac{1}{e} \approx 0.63$$
    \item In a 2-class problem classification may only change if for a test case the nearest neighbor in the learning set is \textbf{not} in at least half of the bootstrap samples.
    \item But probability that an observation is drawn into the bootstrap sample is 63\% which is greater than 50\%.
    \end{itemize}

\end{vbframe}

\endlecture
