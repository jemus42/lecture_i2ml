%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure_man/optimization_steps.jpeg}
\newcommand{\learninggoals}{
\item Know the 0-1-loss
\item Derive the point-wise optimum for the 0-1-loss 
\item Understand the concept of the Bayes Optimal Classifier and the Bayes Error 
}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}


\begin{document}

\lecturechapter{Theoretical Considerations on Classification Losses}
\lecture{Introduction to Machine Learning}


\begin{vbframe}{Risk Minimization for Classification}

Let $y$ be categorical with $g$ classes, i. e.  $\Yspace = \{1, ..., g\}$ and let $f: \Xspace \to \R^g$. We assume our model $f$ outputs a $g$-dimensional vector of scores or probabilities, one per class.

\lz 

\textbf{Note}: In this section, we will consider loss for \textbf{binary classification} tasks, so $\fx$ and $\pix$ are univariate scalars.

\lz 

\textbf{Note}: We will (usually) encode labels as $y \in \{-1, 1\}$ for scoring classifiers $\fx$, and as $y \in \{0, 1\}$ for probabilistic classifiers $\pix$ unless explicitly stated differently. 


\lz

\textbf{Goal:} Find a model $f$ that minimizes the expected loss over random observations $\xy \sim \Pxy$ 

$$
\argmin_{f \in \Hspace} \riskf = \E_{xy}[\Lxy] = \int \Lxy ~ \text{d}\Pxy. 
$$

\end{vbframe}

\begin{vbframe}{Point-wise Optimum}


We can in general rewrite the risk as

\vspace*{-0.5cm}

\begin{eqnarray*}
  \riskf  & = & \Exy\left[\Lxy\right] = \E_x \left[ \E_{y|x} [ L(y, \fx) ] \right] \\
          & = & \E_x \left[\sum_{k \in \Yspace} L(k, \fx) \P(y = k~|~ \xv = \xv)\right]\,, \\
          % & = & E_x \sum_{k \in \Yspace} L(k, \fx) \pikx,
\end{eqnarray*}

with $\P(y = k| \xv = \xv)$ being the posterior probability for class $k$.

\lz 

The optimal model for a loss function $\Lxy$ is

\begin{eqnarray*}
  \fxh &=& \argmin_{f \in \Hspace} \sum_{k \in \Yspace} L(k, f(\bm{x})) \P(y = k| \xv = \xv)\,.  \\
\end{eqnarray*}

If we can estimate $\Pxy$ very well via $\pikx$ through a stochastic model, we can compute the loss-optimal classifications point-wise. 

\lz

\textbf{Example}: Assume that our data is generated by a Mixture of Gaussian distributions. 

\begin{center}
\includegraphics[width = 9cm ]{figure_man/bayes_error_1.png} \\
\end{center}

\framebreak 

We could try to approximate the $\P(y = k ~|~ \xv = \xv)$ via a stochastic model $\pix$ (shown as contour lines): 

\begin{center}
\includegraphics[width = 9cm ]{figure_man/bayes_error_2.png} \\
\end{center}

For each new $\xv$, we estimate the probability $P(y~|~\xv)$ directly with the given stochastic model $\pix$, and our best point-wise prediction is 

\begin{eqnarray*}
  \fxh &=& \argmin_{f \in \Hspace} \sum_{k \in \Yspace} L(k, f(\bm{x})) \pix\,.  \\
\end{eqnarray*}


\lz 

But usually we directly adapt to the loss via \textbf{empirical risk minimization}. 

$$
\fh = \argmin_{f \in \Hspace} \riske(f) = \argmin_{f \in \Hspace} \sumin \Lxyi.
$$
  

\end{vbframe}

\section{0-1-Loss and Bayes Optimal Predictor}

\begin{vbframe}{0-1-Loss}

\begin{itemize}
  \item Let us first consider a classifier $\hx$ that outputs discrete classes directly. 
  \item The most natural choice for $\Lhxy$ is of course the 0-1-loss that counts the number of misclassifications
  $$
  \Lhxy = \mathds{1}_{\{y \ne \hx\}} =
     \footnotesize \begin{cases} 1 \quad \text{ if } y \ne \hx \\ 0 \quad    \text{ if } y = \hx  \end{cases}.
  $$

\end{itemize}


\end{vbframe}


\begin{vbframe}{0-1-Loss: Point-wise Optimum}

For an (unrestricted) classifier $\hx$ and the 0-1-loss: 

$$
\min_{h \in \Hspace} \risk(h) = \E_{xy}[L(y, \hx)]. 
$$

The (point-wise) solution of the above minimization problem is

\begin{footnotesize}
  \begin{eqnarray*}  
  \hxh &=& \argmin_{l \in \Yspace} \sum_{k \in \Yspace} L(k, l) \cdot \P(y = k~|~\xv = \xv) \\
  &=& \argmin_{l \in \Yspace} \sum_{k \ne l} \P(y = k~|~\xv = \xv) = \argmin_{l \in \Yspace} 1 - \P(y = l~|~\xv = \xv) \\
  &=& \argmax_{l \in \Yspace} \P(y = l~|~ \xv = \xv)
  \end{eqnarray*}
\end{footnotesize}

which corresponds to predicting the most probable class. 

\end{vbframe}


\begin{vbframe}{Bayes optimal classifier}

$\hxh$ is called the \textbf{Bayes optimal classifier}. 

\lz

\textbf{Example: } Consider again the previous example, and assume the data generating distribution is known. The decision boundary of the Bayes optimal classifier is shown in orange: 

\begin{center}
\includegraphics[width = 9cm ]{figure_man/bayes_error_3.png} \\
\end{center}


\end{vbframe}


\begin{vbframe}{Bayes error rate}

However, there is an unavoidable error: Even if we know the underlying distribution perfectly, it is possible that a class 1 observations is more likely under $\Pxy(\xv ~|~y = 0)$ than under $\Pxy(\xv ~|~y = 1)$. There is an unavoidable error. 

\lz 

\textbf{Example: } Consider a one-dimensional variant of the Gaussian mixture model. The Bayes optimal classifier is shown in orange. 

\begin{center}
\includegraphics[width = 7cm ]{figure_man/bayes_error_4.png} \\
\end{center}


\framebreak 

The expected loss is called \textbf{Bayes loss} or \textbf{Bayes error rate} for the 0-1-loss. 

\lz 

\textbf{Example: } The Bayes error rate is highlighted as red area. 

\begin{center}
\includegraphics[width = 9cm ]{figure_man/bayes_error_5.png} \\
\end{center}

\end{vbframe}

\begin{vbframe}{0-1-Loss: Optimal constant Model}

The optimal constant model (featureless predictor) under 0-1 loss, with $y \in \setmp$, either for hard classifiers $\hx$ or scoring classifiers $\fx$ 

$$
  L(y, \hx) = \mathds{1}_{y \neq \hx}
$$

is the classifier that predicts the most frequent class in the data


$$
\hx = \text{mode} \left\{\yi\right\} \qquad \text{or} \qquad \fx = \text{mode} \left\{\yi\right\}.
$$

\textbf{Proof:} Exercise / Trivial. 

\lz 

While the \textbf{Bayes error rate} is the theoretically lowest error rate we can achieve for a given data generating process, the above classifier gives usually a lower baseline for the predictive performance. 

% \lz 

% For general hypothesis spaces, however, optimization becomes very hard or even intractable. 

\end{vbframe}


\endlecture

\end{document}