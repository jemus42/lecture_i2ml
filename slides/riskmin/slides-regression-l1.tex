%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure_man/optimization_steps.jpeg}
\newcommand{\learninggoals}{
  \item Know the risk minimizer of the L1-loss
\item Know the optimal constant model for the L1-loss
}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}


\begin{document}

\lecturechapter{Regression Losses: L1-loss}
\lecture{Introduction to Machine Learning}


\begin{vbframe}{L1-Loss}

% Description
\vspace*{-0.3cm}

$$
\Lxy = \left|y-\fx\right|
$$

\begin{itemize}
\item More robust than $L2$, outliers in $y$ are less problematic.
\item Analytical properties: convex, not differentiable for $y = f(\bm{x})$ (optimization becomes harder).
\end{itemize}

\vspace*{-6mm}

\begin{center}
  \includegraphics[width = 9cm]{figure_man/loss_absolute_plot1.png} \\
\end{center}


\end{vbframe}


\begin{vbframe}{L1-Loss: Risk Minimizer}

We calculate the (true) risk for the $L1$-Loss $\Lxy = \left|y-\fx\right|$ with unrestricted $\Hspace = \{f: \Xspace \to \Yspace\}$. 

\begin{itemize}
  \item We use the law of total expectation 
  $$
    \riskf = \E_x \left[\E_{y|x}\left[|y-\fx|~|\xv = \xv\right]\right]. 
  $$
  \item As the functional form of $f$ is not restricted, we can just optimize point-wise at any point $\xv = \xv$. The best prediction at $\xv = \xv$ is then 

  $$
    \fxh = \mbox{argmin}_c \E_{y|x}\left[|y - c|\right]\overset{(*)}{=} \text{med}_{y|x} \left[y ~|~ \xv \right]. 
  $$

  \framebreak 

  \item $^{(*)}$ Let $p(y)$ be the density function of $y$. Then: 
  \begin{eqnarray*}
  && \mbox{argmin}_c \E\left[|y - c|\right] = \mbox{argmin}_c \int_{-\infty}^\infty |y - c| ~ p(y) \text{d}y \\
  &=& \mbox{argmin}_c \int_{-\infty}^c -(y - c)~p(y)~\text{d}y + \int_c^\infty (y - c)~p(y)~\text{d}y 
  \end{eqnarray*}
  Setting the derivation w.r.t. $c$ to zero yields: 
  \begin{eqnarray*}
  0 &=& \int_{-\infty}^c p(y)~\text{d}y - \int_c^\infty p(y)~~\text{d}y \\
  &=& \P_y (y \le c) - \left(1 - \P_y (y \le c)\right)\\
  &=& 2 \cdot \P_y (y \le c) - 1 \\
  \Leftrightarrow 0.5 &=& \P_y (y \le c),
  \end{eqnarray*}
  which yields $c = \text{med}_y(y)$.  



\end{itemize}

\end{vbframe}


\begin{vbframe}{L1-Loss: Optimal constant model}

The optimal constant model in terms of the theoretical risk for the L1 loss is the median over $y$:

\begin{eqnarray*}
  \fx &=& \text{med}_{y|x} \left[y ~|~ \xv \right] \overset{\text{drop } \xv}{=}  \text{med}_{y} \left[y \right]
  \end{eqnarray*} 

The optimizer of the empirical risk is $\text{med}(\yi)$ over $\yi$, which is the empirical estimate for $\text{med}_{y} \left[y \right]$. 

\vspace*{-0.3cm}

\begin{center}
\includegraphics[width = 0.5\textwidth ]{figure_man/l1_vs_l2.png} \\
\end{center}


\framebreak 

\textbf{Proof}: 

\begin{itemize}
  \item Firstly note that for $n = 1$ the median $\thetah = \text{med}(\yi) = y^{(1)}$ obviously minimizes the empirical risk $\riske$ associated to the $L1$ loss $L$. 


% Take proof from this page? 
% https://math.stackexchange.com/questions/113270/the-median-minimizes-the-sum-of-absolute-deviations-the-l-1-norm/1024462#1024462

  \item Hence let $n > 1$ in the following: Let 

  $$
    S_{a,b}:\mathbb{R} \rightarrow \mathbb{R}^+_0, \thetab \mapsto |a- \thetab| + |b-\thetab|
  $$

  for $a, b \in \mathbb{R}$. It holds that
  \begin{eqnarray*}
  S_{a,b}(\thetab) &=& \begin{cases}|a-b| ,& \text{ for } \thetab \in [a,b]\\ |a-b| + 2\cdot\min\{|a-\thetab|,|b-\thetab|\}
  ,& \text{ otherwise. }\end{cases}
  \end{eqnarray*}
  Thus, any $\thetah \in [a,b]$ minimizes $S_{a,b}$. \\

  Let us define $i_{\max} = n / 2$ for $n$ even and $i_{\max} = (n - 1) / 2$ for $n$ odd and consider the intervals 
  $$
    \mathcal{I}_i := [y^{(i)},y^{(n+1-i)}], i \in \{1, ..., i_{\max}\}. 
  $$

  By construction $\mathcal{I}_{j+1} \subseteq \mathcal{I}_j$ for $j \in \{1,\dots,i_{\max}-1\}$ and $\mathcal{I}_{i_{\max}} \subseteq \mathcal{I}_i$. With this, $\riske$ can be expressed as
  \begin{footnotesize}
  \begin{eqnarray*}
  \riske(\thetab) &=& \sumin L(\yi,\thetab) = \sumin|\yi - \thetab| \\ 
  &=& \underbrace{|\yi[1] - \thetab| + |\yi[n] - \thetab|}_{= S_{\yi[1], \yi[n]}(\thetab)} + \underbrace{|\yi[2] - \thetab| + |\yi[n - 1] - \thetab|}_{= S_{\yi[2], \yi[n - 1]}(\thetab)} + ...  \\
  &=& \begin{cases} \sum\limits_{i = 1}^{i_{\max}} S_{\yi, \yi[n + 1 - i]}(\thetab) & \text{ for } n \text{ is even} \\
  \sum\limits_{i = 1}^{i_{\max}} \left(S_{\yi, \yi[n + 1 - i]}(\thetab)\right) + |\yi[n + 1] - \thetab| & \text{ for } n \text{ is odd}. \end{cases}
  \end{eqnarray*}
  \end{footnotesize}

  % \begin{eqnarray*}
  % \begin{cases}
  % \sum^{\overbrace{n/2}^{=:i_{\max}}}_{i=1} \overbrace{S_{y^{(i)},y^{(n+1-i)}}(c)}^{=:S_i(c)},&  \text{ for } n \text{ is even}\\
  % (\sum^{\overbrace{(n-1)/2}^{=:i_{\max}}}_{i=1} \underbrace{S_{y^{(i)},y^{(n+1-i)}}(c)}_{=:S_i(c)}) + \underbrace{|y^{(n+1)/2}-c|}_{=:S_0(c)},& \text{ for } n \text{ is odd}. 
  % \end{cases}
  % \end{eqnarray*}
  % \end{footnotesize}
  % Now we define for $i \in \{1,\dots,i_{\max}\} \; \mathcal{I}_i := [y^{(i)},y^{(n+1-i)}].$ \\
  % From construction it follows that for  $j \in \{1,\dots,i_{\max}-1\}$
  % $$\mathcal{I}_{j+1} \subseteq \mathcal{I}_j \Rightarrow \forall i \in \{1,\dots,i_{\max}\}:  \mathcal{I}_{i_{\max}} \subseteq \mathcal{I}_i.$$
  \framebreak 


  From this follows that
  \begin{itemize}
  \item for \enquote{$n$ is even}: $\thetah \in  \mathcal{I}_{i_{\max}} = [y^{(n/2)},y^{(n/2+1)}]$ minimizes $S_i$ for all $i \in \{1,\dots, i_{\max}\} \; \Rightarrow  \riske$ reaches its global minimum at $\thetah$,
  \item for \enquote{$n$ is odd}: $\thetah = y^{(n+1)/2} \in \mathcal{I}_{i_{\max}}$ minimizes $S_i$ for all $i \in \{1,\dots, i_{\max}\} \; \Rightarrow  \riske$ reaches its global minimum at $\thetah$.
  \end{itemize}

  Since the median fulfills these conditions, we can conclude that it minimizes 
  the $L1$ loss.
\end{itemize}

% \framebreak


% \begin{center}
% \includegraphics[width = 11cm ]{figure_man/L1-loss.png} \\
% \end{center}

% \framebreak 

% We see that the $L1$-Loss is more robust w.r.t. outliers than the $L2$-Loss. 
% \vspace{0.3cm}


% \begin{center}
% \includegraphics[width = 9cm ]{figure_man/L1andL2-loss.png} \\
% \end{center}

\framebreak 

\end{vbframe}


\begin{vbframe}{Robustness}

We see that the $L1$-Loss is more robust w.r.t. outliers than the $L2$-Loss. 
\vspace{0.3cm}

\begin{center}
\includegraphics[width = 9cm ]{figure_man/L1andL2-loss.png} \\
\end{center}

\end{vbframe}


\endlecture

\end{document}