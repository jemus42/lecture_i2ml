%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure_man/optimization_steps.jpeg}
\newcommand{\learninggoals}{
  \item Understand why the choice of the loss function matters
  \item Learn about basic properties of regression losses
  \item Learn about some advanced loss functions
}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}


\begin{document}

\lecturechapter{Advanced Regression Losses}
\lecture{Introduction to Machine Learning}



\begin{vbframe}{The role of Loss Functions}

Why should we care about how to choose the loss function $\Lxy$?

\begin{itemize}
% \item For regression, the loss usually only depends on residual $\Lxy = L\left(y - \fx\right) = L(\eps)$, this is a \emph{translation invariant} loss
\item \textbf{Statistical} properties of $f$: Choice of loss implies statistical properties of $f$ like robustness and an implicit error distribution.
\item \textbf{Computational / Optimization} complexity of the optimization problem: The complexity of the optimization problem
$$
\argmin_{\thetab \in \Theta} \risket
$$
is influenced by the choice of the loss function, i.e.\

  \begin{itemize}
    \item Smoothness of the objective \\
    \begin{footnotesize} 
    Some optimization methods require smoothness (e.g. gradient methods).
    \end{footnotesize}
    \item Uni- or multimodality of the problem \\
    \begin{footnotesize} 
    If $\Lxy$ is convex in its second argument, and $\fxt$ is linear in $\thetab$, then $\risket$ is convex; every local minimum of $\risket$ is a global one. If $L$ is not convex, $\risket$ might have multiple local minima (bad!).
    \end{footnotesize}
  \end{itemize}
\end{itemize}


\end{vbframe}





\begin{vbframe}{Properties of Losses}

\begin{itemize}
  \item Regression losses usually only depend on the \textbf{ residuals}

  \vspace*{-0.5cm}

  \begin{eqnarray*}
    \eps &:=& y - \fx %\\
    %r^{(i)} &:=& \yi - \fxi .
  \end{eqnarray*}

  \item A loss is called \textbf{distance-based} if
  \begin{itemize}
    \item it can be written in terms of the residual
    $$
      \Lxy = \psi (\eps) \text{ for some } \psi: \R \to \R
    $$
    \item $\psi(\eps) = 0 \Leftrightarrow \eps = 0$ .
  \end{itemize}
  \item A loss is \textbf{translation-invariant}, if $L(y + a, \fx + a) = \Lxy$.
  \item Losses are called \textbf{symmetric} if $\Lxy = L\left(\fx, y\right)$. 
\end{itemize}
  % We will see later that in case of the L2-loss, pseudo-residuals correspond to the residuals - hence the name.

\end{vbframe}



\begin{vbframe}{Loss Plots}

We call the plot that shows the point-wise error, i.e. the loss $\Lxy$ vs. the \textbf{residuals} $\eps := y - \fx$ (for regression), \textbf{loss plot}. The pseudo-residual corresponds to the slope of the tangent in $\left(y - \fx, \Lxy \right)$. 

\vspace*{0.5cm}


\begin{figure}
\includegraphics[width = 1\linewidth]{figure_man/loss.png}
\end{figure}

%<<echo=FALSE, warning=FALSE, message=FALSE, fig.height = 4>>=
%   xx = seq(-2, 2, by = 0.01); 
%   yy = xx^2
%   plot(xx, yy, type = "l", xlab = "y - f(x)", ylab = "L(y, f(x))")
%   points(1, 1, col = "red")
%   lines(x = c(1, 1), y = c(0, 1), col = "red")
%   points(xx, 2 * xx - 1, type = "l", col = "red")
%  @

% We will define a similar plot for classification later on in this chapter.

\end{vbframe}




\begin{vbframe}{Risk Minimization}

Now, we will discuss the most common loss functions and the optimal solution with respect to 


\begin{itemize}
  \item the theoretical risk 
  $$
  \riskf =  \E[\Lxy] = \int_{\Xspace \times \Yspace} \Lxy ~ d\Pxy
  $$ 
  and
  \item the empirical risk 
  $$
 \riskef = \sumin \Lxyi. 
  $$
\end{itemize}

\end{vbframe}


\begin{vbframe}{Common Regression Loss Functions}

We already have seen two typical regression losses: 

\begin{itemize}
  \item L2-loss / squared error: 
  \begin{eqnarray*}
    \Lxy &=& \left(y - \fx\right)^2
  \end{eqnarray*}
  \item L1-loss / absolute error: 
  \begin{eqnarray*}
    \Lxy &=& \left|y - \fx\right|^2
  \end{eqnarray*}  
\end{itemize}

We will cover further common regression losses in this lecture: 

\begin{itemize}
  \item Huber loss
  \item Log Barrier loss
\end{itemize}

\end{vbframe}



\section{Huber-Loss}


\begin{vbframe}{Huber-Loss}

% Description
\vspace*{-0.3cm}

$$
\Lxy = \begin{cases}
  \frac{1}{2}(y - \fx)^2  & \text{ if } |y - \fx| \le \delta \\
  \delta |y - \fx|-\frac{1}{2}\delta^2 \quad & \text{ otherwise }
  \end{cases}, \delta > 0
$$

\begin{itemize}
\item Piecewise combination of $L1$ and $L2$ loss
\item Analytic properties: Convex, differentiable, robust
\item Combines advantages of $L1$ and $L2$ loss: differentiable + robust
\end{itemize}

\vspace*{-1cm}

\begin{center}
\includegraphics[width = 9cm]{figure_man/loss_huber_plot1.png} \\
\end{center}

\framebreak

The following plot shows the Huber loss for different values of $\delta$.

\begin{center}
\includegraphics[width = 11cm]{figure_man/loss_huber_plot2.png} 
\end{center}

\end{vbframe}

\begin{vbframe}{Huber Loss: Optimal constant model}

What is the optimal constant model $\fx = \thetab$ w.r.t. the Huber loss?

\vspace{-0.2cm}
\begin{eqnarray*}
f &=& \argmin_{f \in \Hspace} \riskef\\
\Leftrightarrow
\hat \thetab&=& \argmin_{\thetab\in \R} \sumin L(y, \thetab)
\end{eqnarray*}

with $L(y, \thetab) = \begin{cases}
  \frac{1}{2}(y - \thetab)^2  & \text{ if } |y - \thetab| \le \delta \\
  \delta |y - \thetab|-\frac{1}{2}\delta^2 \quad & \text{ otherwise }
  \end{cases}. $

\begin{itemize}
\item There is no closed-form solution.
\item Numerical optimization methods are necessary.
\item $\to$  the \enquote{optimal} solution can only be approached to a certain degree of accuracy via iterative optimization.
\end{itemize}


\framebreak

\vspace{0.2cm}

\begin{center}
\includegraphics[width = 11cm ]{figure_man/Huber1.png} \\
\end{center}


\framebreak 

And when adding an outlier we see that the loss is similarly robust w.r.t. outliers as the L1 loss. 

\vspace{0.2cm}

\begin{center}
\includegraphics[width = 9cm ]{figure_man/Huber-outlier.png} \\
\end{center}

% We see that the constant model fitted w.r.t. Huber loss in fact lies between L1- and L2-Loss.

\end{vbframe}

\begin{vbframe}{Linear Model: $L1$- vs. $L2$- vs. Huber Loss}
\begin{itemize}
\item \textbf{Optimization}: $L2$ loss can be differentiated and the empirical risk minimization problem has a closed-form solution; $L1$ is not differentiable and has no closed-form solution.
\item \textbf{Robustness}: $L1$ loss penalizes large residuals less than $L2$ loss, thus, $L1$ loss is more robust to outliers.
\item Huber loss has the robustness of $L1$ loss where residuals are large and flexibility of $L2$ loss where residuals are small.
\end{itemize}

\vspace*{0.1cm}
\begin{center}
\includegraphics[width = 11cm ]{figure_man/different_losses.png}
\end{center}

\end{vbframe}




\section{Log-Barrier Loss}

\begin{vbframe}{Log-Barrier Loss}

\begin{footnotesize}
\[
  \Lxy = \left\{\begin{array}{lr}
        -a^{2} \cdot \log \Bigl( 1 - \Bigl(\frac{\left|y - \fx\right|}{a}\Bigr)^2 \Bigr), & \text{if } \left|y-\fx\right| \leq a \\
        \infty, & \text{if } \left|y-\fx\right|  > a
        \end{array}\right.
  \]
\end{footnotesize}
\begin{itemize}
\item Behaves like L2 loss for small residuals
\item We use this, if we don't want residuals larger than $a$ at all
\item No guarantee that the risk minimization problem has a solution
\item Plot shows Log-Barrier Loss for $a=2$
\end{itemize}

\begin{figure}
\includegraphics[width = 0.8\textwidth]{figure_man/log-barrier01.png}
\end{figure}


\end{vbframe}



\begin{vbframe}{Log Barrier: Optimal constant model}

\begin{itemize}
 \item Similarly to the Huber loss, there is no closed-form solution for the optimal constant model $\fx = \thetab$ w.r.t. the Log Barrier loss. 
 \item Again, numerical optimization methods are necessary. 
\end{itemize}

\vspace{0.2cm}

\begin{center}
\includegraphics[width = 9cm ]{figure_man/log_barrier2.png} \\
\end{center}


\framebreak 


Note that the optimization problem has no (finite) solution, if there is no way to fit a constant with residuals smaller than $a$. 

\vspace{- 0.2cm}


\begin{center}
\includegraphics[width = 0.8\textwidth]{figure_man/log_barrier_2_1.png} \\
\end{center}

% We see that the constant model fitted w.r.t. Huber loss in fact lies between L1- and L2-Loss.

\end{vbframe}





% \section{Comparison of Loss Functions}
% 
% 
% \begin{vbframe}{Comparison of Loss Functions}
% 
% 
% \begin{center}
% \includegraphics[width = 10cm]{figure_man/2_7_loss_comparison_plot1.png} \\
% \end{center}
% 
% 
% \framebreak
% 
% 
% This plot shows the optimal constant model $\fx = \thetab$ (also \enquote{featureless predictor}) for the losses that have been discussed.
% 
% \begin{center}
% \includegraphics[width = 10cm, height = 6.5cm]{figure_man/2_8_loss_comparison_constant_model_plot1.png} \\
% \end{center}
% 
% \end{vbframe}



\section{Summary}

\begin{vbframe}{Summary of Loss Functions}

\begin{table}[]
\begin{tabular}{c|cccc}

& $L2$ & $L1$ & Huber & Log-Barrier \\ \hline 
Point-wise optimum & $\E_{y|x}\left[y ~|~ \xv\right]$ & $\text{med}_{y~|~\xv}\left[y ~|~ \xv\right]$ & n.a. & n.a. \\
Best constant & $\frac{1}{n}\sumin \yi$ & $\text{med}\left(\yi\right)$ & n.a. & n.a.\\
Differentiable & \checkmark & \xmark & \checkmark & \checkmark \\
Convex & \checkmark & \checkmark & \checkmark & \checkmark \\
Robust & \xmark & \checkmark & \checkmark & \xmark \\
\end{tabular}
\end{table}

There are many other loss functions for regression tasks, for example:

\begin{itemize}
  \item Quantile-Loss
  \item $\epsilon$-insensitive-Loss  
  % \item Log-Barrier-Loss
\end{itemize}

Loss functions might also be customized to an objective that is defined by an application. 

% \framebreak 
% \begin{itemize}
  % \item In future chapters, we will talk about different learning algorithms.
  % \lz
  % \item We will come back to loss functions introduced in this chapter or we will introduce new ones. 
  % \lz
  % \item We will see how learning algorithms are composed into specific hypothesis classes and loss functions; we will come back to the loss functions introduced in this chapter or introduce new ones.
% \end{itemize}


% When introducing different learning algorithms, we will come back to the loss functions introduced in this chapter or even introduce new ones. For example:  

% \begin{itemize}
%   \item Ordinary Linear Regression: L2-loss
%   \item Logistic Regression: Logistic loss
%   \item Support Vector Machine Classification: Hinge-Loss (to be introduced) (see \textbf{SVM} chapter)
%   \item Support Vector Machine Regression: $\epsilon$-insensitive loss (see \textbf{SVM} chapter)
%   \item AdaBoost: Exponential loss (see \textbf{Boosting} chapter)
% \end{itemize}

% Once knowing the theory of risk minimization and properties of loss functions, we can combine model classes and loss functions as needed or even tailor loss functions to our needs. 

\end{vbframe}


\endlecture

\end{document}