%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure_man/optimization_steps.jpeg}
\newcommand{\learninggoals}{
\item Understand that an ML model is simply a parametrized curve
\item Understand that the hypothesis space lists all admissible models
    for a learner
\item Understand the relationship between the hypothesis space and the parameter space
}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}

\section{Risk Minimization for Classification}

\begin{vbframe}{Risk Minimization for Classification}

Let $y$ be categorical with $g$ classes, i. e.  $\Yspace = \{1, ..., g\}$ and let $f: \Xspace \to \R^g$. We assume our model $f$ outputs a $g$-dimensional vector of scores or probabilities, one per class.

\lz 

\textbf{Note}: In this section, we will consider loss for \textbf{binary classification} tasks, so $\fx$ and $\pix$ are univariate scalars.

\lz 

  We will (usually) encode labels as $y \in \{-1, 1\}$ for scoring classifiers $\fx$, and as $y \in \{0, 1\}$ for probabilistic classifiers $\pix$ unless explicitly stated differently. 


\lz

\textbf{Goal:} Find a model $f$ that minimizes the expected loss over random observations $\xy \sim \Pxy$ 

$$
\argmin_{f \in \Hspace} \riskf = \E_{xy}[\Lxy] = \int \Lxy ~ \text{d}\Pxy. 
$$


\begin{itemize}
  \item As for regression before, losses measure prediction errors \textbf{point-wise}.
  \item In classification, however, we need to distinguish the different types of prediction functions:
  \item Losses can either be defined on
  \begin{itemize}
    \item hard labels $\hx$ or
    \item (class) scores $\fx$ or
    \item (class) probabilities $\pix$. 
  \end{itemize}
  \item For multiclass classification, loss functions will be defined on vectors of scores ($f_1(\xv), ..., f_g(\xv)$) or on vectors of probabilities ($\pi_1(\xv), ..., \pi_g(\xv)$).
\end{itemize}


\framebreak 

\begin{figure}
  \includegraphics[width=0.8\textwidth]{figure_man/classifiers.png}
\end{figure}

Note that for a \textbf{binary scoring classifier} $\fx$,

$$
\hx = \sign(\fx) \in \{-1, 1\}
$$
and for a \textbf{probabilistic classifier} $\pix$

$$
\hx = \mathds{1}_{\{ \pix > c \}} \in \{0, 1\}
$$

(e.g. $c = 0.5$) will be the corresponding label. 


\end{vbframe}

\begin{vbframe}{Margins} 

When considering scoring classifiers $\fx$ we usually define loss functions on the so-called \textbf{margin}

$$
r = y\cdot \fx =  \begin{cases} > 0  \quad &\text{ if } y = \sign(\fx) \text{ (correct classification)}\,, \\
                      < 0 \quad &\text{ if } y \ne \sign(\fx) \text{ (misclassification)}\,, \end{cases}
$$

$|\fx|$ is called \textbf{confidence}.

% \framebreak

% We define the \textbf{loss plot} for binary scoring classifiers as the plot that shows the point-wise loss $\Lxy$, vs. the \textbf{margin} $y \cdot \fx$. Large positive values of $y \cdot \fx$ are good and penalized less.

% \lz

% \textbf{Example:} 0-1-Loss

% <<echo=FALSE, warning=FALSE, message=FALSE, fig.height = 3>>=
% x = seq(-2, 2, by = 0.01); y = as.numeric(x < 0)
% qplot(x, y, geom = "line", xlab = expression(y %.% f(x)), ylab = expression(L(y, f(x))))
%   @

% For probabilistic classifiers, loss plots show the Loss $L(y, \pix)$ versus the class probability $\pix$.


\end{vbframe}

\begin{vbframe}{Point-wise Optimum}


We can in general rewrite the risk as

\vspace*{-0.5cm}

\begin{eqnarray*}
  \riskf  & = & \Exy\left[\Lxy\right] = \E_x \left[ \E_{y|x} [ L(y, \fx) ] \right] \\
          & = & \E_x \left[\sum_{k \in \Yspace} L(k, \fx) \P(y = k~|~ \xv = \xv)\right]\,, \\
          % & = & E_x \sum_{k \in \Yspace} L(k, \fx) \pikx,
\end{eqnarray*}

with $\P(y = k| \xv = \xv)$ being the posterior probability for class $k$.

\lz 

The optimal model for a loss function $\Lxy$ is

\begin{eqnarray*}
  \fxh &=& \argmin_{f \in \Hspace} \sum_{k \in \Yspace} L(k, f(\bm{x})) \P(y = k| \xv = \xv)\,.  \\
\end{eqnarray*}

If we can estimate $\Pxy$ very well via $\pikx$ through a stochastic model, we can now compute the loss-optimal classifications point-wise. 

But usually we directly adapt to the loss via \textbf{empirical risk minimization}. 

$$
\fh = \argmin_{f \in \Hspace} \riske(f) = \argmin_{f \in \Hspace} \sumin \Lxyi.
$$
  

\end{vbframe}


\section{0-1-Loss}

\begin{vbframe}{0-1-Loss}

\begin{itemize}
  \item Let us first consider a classifier $\hx$ that outputs discrete classes directly. 
  \item The most natural choice for $\Lhxy$ is of course the 0-1-loss that counts the number of misclassifications
  $$
  \Lhxy = \mathds{1}_{\{y \ne \hx\}} =
     \footnotesize \begin{cases} 1 \quad \text{ if } y \ne \hx \\ 0 \quad    \text{ if } y = \hx  \end{cases}.
  $$
  \item We can express the 0-1-loss also for a scoring classifier $\fx$ based on the margin $r$

  $$
  \Lr = \mathds{1}_{\{r < 0\}} = \mathds{1}_{\{y\fx < 0\}} = \mathds{1}_{\{y \neq \hx\}}.
  $$

\end{itemize}


\framebreak 

$$
  \Lr = \mathds{1}_{\{r < 0\}} = \mathds{1}_{\{y\fx < 0\}} = \mathds{1}_{\{y \neq \hx\}} 
$$

\begin{itemize}
\item Intuitive, often what we are interested in.
\item Analytic properties:  Not continuous, even for linear $f$ the optimization problem is NP-hard and close to intractable.
\end{itemize}

\begin{center}
\includegraphics[width = 11cm ]{figure_man/0-1-loss.png} \\
\end{center}


\end{vbframe}

\begin{vbframe}{0-1-Loss: Point-wise Optimum}

For an (unrestricted) classifier $\hx$ and the 0-1-loss: 

$$
\min_{h \in \Hspace} \risk(h) = \E_{xy}[L(y, \hx)]. 
$$

The (point-wise) solution of the above minimization problem is

\begin{footnotesize}
  \begin{eqnarray*}  
  \hxh &=& \argmin_{l \in \Yspace} \sum_{k \in \Yspace} L(k, l) \cdot \P(y = k~|~\xv = \xv) \\
  &=& \argmin_{l \in \Yspace} \sum_{k \ne l} \P(y = k~|~\xv = \xv) = \argmin_{l \in \Yspace} 1 - \P(y = l~|~\xv = \xv) \\
  &=& \argmax_{l \in \Yspace} \P(y = l~|~ \xv = \xv)
  \end{eqnarray*}
\end{footnotesize}

which corresponds to predicting the most probable class. 

\lz 

$\hxh$ is called the \textbf{Bayes optimal classifier}. The expected loss is called \textbf{Bayes loss} or \textbf{Bayes error rate} for the 0-1-loss.


\end{vbframe}

\begin{vbframe}{0-1-Loss: Optimal constant Model}

The optimal constant model (featureless predictor) under 0-1 loss, with $y \in \setmp$, either for hard classifiers $\hx$ or scoring classifiers $\fx$ 

$$
  L(y, \hx) = \mathds{1}_{y \neq \hx}
$$

is the classifier that predicts the most frequent class in the data


$$
\hx = \text{mode} \left\{\yi\right\} \qquad \text{or} \qquad \fx = \text{mode} \left\{\yi\right\}.
$$

\textbf{Proof:} Exercise / Trivial. 

% \lz 

% For general hypothesis spaces, however, optimization becomes very hard or even intractable. 

\end{vbframe}



% \begin{vbframe}{Classification Losses: (Naive) L2-Loss}


% $$
% \Lxy = (1 - y\fx)^2,
% $$


% \begin{itemize}
%   \item L2-loss defined on scores
%   \item Predictions with high confidence $|f(x)|$ are penalized  regardless of whether the signs of $y$ and $f(x)$ match.
%   \item Squared loss on the loss functions is thus not the best choice.

%   <<loss-squareclass-plot, echo=FALSE, results='hide', fig.height= 1.8, fig.asp = 0.4>>=
%  x = seq(-2, 5, by = 0.01)
%  plot(x, (1-x)^2, type = "l", xlab = expression(y %.% f(x)), ylab = expression(paste((1-yf(x))^2)), main = "Square loss")
%  box()
% @

%   \item The theoretical risk becomes
%     \begin{eqnarray*}
%     \risk(f) &=& \mathbb{E}_x [(1-\fx)^2 (P(1 | x)) + (1+\fx)^2 (1-P(1 | x))] \\
%     &=& \mathbb{E}_x [1 + 2\fx + \fx^2-4\fx P(1 | x)].
%     \end{eqnarray*}
%   \item By differentiating w.r.t. $f(x)$ we get the minimizer of $\risk(f)$ for the square loss function

%     \begin{eqnarray*}
%     f(\xv) &=& 2\cdot P(1 | \xv) - 1.
%     \end{eqnarray*}
%     \item The empiricla optimzer is then
%     $$
%     \fh(\xv) = \frac{2}{n}\cdot \sumin \I[y^{(i)} = 1] - 1.
%     $$


% \end{itemize}

% \end{vbframe}


\section{Brier Score}

\begin{vbframe}{Brier Score}

The binary Brier score is defined on probabilities $\pix \in [0, 1]$ and 0-1-encoded labels $y \in \{0, 1\}$ and measures their squared distance (L2 loss on probabilities).

\begin{eqnarray*}
L\left(y, \pix\right) &=& (\pix - y)^2
\end{eqnarray*}

\vspace{0.2cm}
\begin{center}
\includegraphics[width = 11cm ]{figure_man/Brier-score.png} \\
\end{center}


\end{vbframe}

\begin{vbframe}{Brier Score: Point-wise Optimum}

The minimizer of the (theoretical) risk $\risk(f)$ for the Brier score  

\begin{eqnarray*}
\hat \pi(\xv) &=& \P(y~|~\xv = \xv),
\end{eqnarray*}

which means that the Brier score would reach its minimum if the prediction equals the \enquote{true} probability of the outcome. 

\lz 

\textbf{Proof: }We have seen that the (theoretical) optimal prediction $c$ for an arbitrary loss function at fixed point $\xv$ is

$$
\argmin_{c} \sum_{k \in \Yspace} L(y, c) \P(y = k| \xv = \xv)\,.
$$

We plug in the Brier score

\vspace*{-0.3cm}

\begin{footnotesize}
  \begin{eqnarray*}
    && \argmin_c L(1, c) \underbrace{\P(y = 1| \xv = \xv)}_p + L(0, c) \underbrace{\P(y = 0| \xv = \xv)}_{1 - p} \\ 
    &=&  \argmin_c \quad (c - 1)^2 p + c^2 (1 - p)\\
    &=&  \argmin_c \quad (c - p)^2.
  \end{eqnarray*}
\end{footnotesize}

The expression is minimal if $c = p = \P(y = 1~|~\xv = \xv)$.

\end{vbframe}

\begin{vbframe}{Brier Score: Optimal constant Model}

The optimal constant probability model $\pix = \theta$ w.r.t. the Brier score for labels from $\Yspace = \setzo$ is:

\begin{eqnarray*}
  \min_{\theta} \riske(\theta) &=& \min_{\thetab} \sumin \left(\yi - \theta\right)^2 \\
  \Leftrightarrow \frac{\partial \riske(\theta)}{\partial \theta} &=& - 2 \cdot \sumin (\yi - \theta) = 0 \\
  \hat \theta &=& \frac{1}{n} \sumin \yi.   
\end{eqnarray*}

This is the fraction of class-1 observations in the observed data.\\
(This also directly follows from our $L2$-proof for regression).

\end{vbframe}


\section{Bernoulli Loss}

\begin{vbframe}{Bernoulli Loss}

\vspace*{-0.5cm}
\begin{eqnarray*}
  L_{-1, +1}(y, \fx) &=& \ln(1+\exp(-y\fx)) \\
  L_{0, 1}(y, \fx) &=& - y \cdot \fx + \log(1 + \exp(\fx)). 
\end{eqnarray*}

\begin{itemize}
  \item Two equivalent formulations: Labels $y \in \{-1, 1\}$ or $y \in \{0, 1\}$
  \item Negative log-likelihood of Bernoulli model, e.g., logistic regression
  \item Convex, differentiable
  \item Pseudo-Residuals (0,1 case): $\tilde{r} = y - \frac{1}{1+\exp(-\fx)}$\\   
    Interpretation: $L1$ distance between 0/1-labels and posterior prob!
\end{itemize}

\vspace{0.2cm}
\begin{center}
\includegraphics[width = 9cm ]{figure_man/bernoulli.png} \\
\end{center}

\end{vbframe}


\begin{vbframe}{Bernoulli loss: Point-wise Optimum}

The theoretical point-wise optimum for scores under Bernoulli loss is actually
the point-wise log-odds:

\begin{eqnarray*}
\fh(\xv) &=&  \ln \biggl(\frac{\P(y~|~\xv = \xv)}{1-\P(y~|~\xv = \xv)}\biggr).
\end{eqnarray*}

The function is undefined when $P(y~|~\xv = \xv) = 1$ or $P(y~|~\xv = \xv) = 0$, but predicts a smooth curve which grows when $P(y~|~\xv = \xv)$ increases and equals $0$ when $P(y~|~\xv = \xv) = 0.5$.

\lz 

\textbf{Proof: } We consider the case $\Yspace = \{-1, 1\}$. We have seen that the (theoretical) optimal prediction $c$ for an arbitrary loss function at fixed point $\xv$ is

$$
\argmin_{c} \sum_{k \in \Yspace} L(y, c) \P(y = k| \xv = \xv)\,.
$$

\framebreak 

We plug in the Bernoulli loss

\vspace*{-0.3cm}

\begin{footnotesize}
  \begin{eqnarray*}
    && \argmin_c L(1, c) \underbrace{\P(y = 1| \xv = \xv)}_p + L(-1, c) \underbrace{\P(y = -1| \xv = \xv)}_{1 - p} \\ 
    &=&  \argmin_c \ln(1 + \exp(-c)) p + \ln(1 + \exp(c)) (1 - p).
  \end{eqnarray*}
\end{footnotesize}

\vspace*{-0.3cm}

Setting the derivative w.r.t. $c$ to zero yields

\vspace*{-0.3cm}

\begin{footnotesize}
  \begin{eqnarray*}
  0 &=& - \frac{\exp(-c)}{1 + \exp(-c)} p + \frac{\exp(c)}{1 + \exp(c)} (1 - p) \\ 
   &=& - \frac{\exp(-c)}{1 + \exp(-c)} p + \frac{1}{1 + \exp(- c)} (1 - p) \\ 
  % &=& -  \frac{\exp(-c)}{1 + \exp(-c)} p + \frac{1}{1 + \exp(-c)} - \frac{1}{1 + \exp(-c)} p \\
  &=& - p + \frac{1}{1 + \exp(-c)} \\
  \Leftrightarrow p &=& \frac{1}{1 + \exp(-c)} \\
  \Leftrightarrow c &=& \ln\left(\frac{p}{1 - p}\right)
  \end{eqnarray*}
\end{footnotesize}


% \framebreak 

% Another common equivalent formulation of this loss function is defined on labels $y \in \{0, 1\}$ instead of $y \in \{-1, 1\}$

% $$
%   L_{0, 1}(y, \fx) = - y \cdot \fx + \log(1 + \exp(\fx)). 
% $$

% We indicate this by subscripts of the loss function. 

\end{vbframe}

\begin{vbframe}{Bernoulli loss on probabilities}

If scores are transformed into probabilities by the logistic function  $\pix = \left(1 + \exp(- \fx)\right)^{-1}$, we arrive at another equivalent formulation of the loss

  $$
    L_{0,1}(y, \pix) = - y \log \left(\pix\right) - (1 - y) \log \left(1 - \pix\right). 
  $$

\begin{center}
\includegraphics[width = 10cm ]{figure_man/bernoulli-loss.png} \\
\end{center}

Via this form it is easy to show that the point-wise optimum for probability estimates is $\pixh = \P(y~|~\xv = \xv)$.

\end{vbframe}

\begin{vbframe}{Bernoulli: Optimal constant Model}

The optimal constant probability model $\pix = \theta$ w.r.t. the Bernoulli loss for labels from $\Yspace = \{0, 1\}$) is:

\begin{eqnarray*}
  \thetah = \argmin_{\theta} \risket = \frac{1}{n} \sumin \yi
\end{eqnarray*}

Again, this is the fraction of class-1 observations in the observed data.
We can simply prove this again by setting the derivative of the risk to 0 and solving for $\theta$.

\framebreak

The optimal constant score model $\fx = \theta$ w.r.t. the Bernoulli loss labels from $\Yspace = \setmp$ or $\Yspace = \setzo$ is:

\begin{eqnarray*}
  \thetah = \argmin_{\theta} \risket = \ln \frac{n_{+1}}{n_{-1}} = \ln \frac{n_{+1}/n}{n_{-1}/n} 
\end{eqnarray*}

where $n_{-1}$ and $n_{+1}$ are the numbers of negative and positive observations, respectively.

\lz

This again shows a tight (and unsurprising) connection of this loss to log-odds.

\lz

Proving this is also a (quite simple) exercise.

\end{vbframe}

\begin{vbframe}{Bernoulli-Loss: Naming Convention}

We have seen three loss functions that are closely related. In the literature, there are different names for the losses: 

\begin{eqnarray*}
  L_{-1+1}(y, \fx) &=& \ln(1+\exp(-y\fx)) \\
  L_{0, 1}(y, \fx) &=& - y \cdot \fx + \log(1 + \exp(\fx)). 
\end{eqnarray*}

are referred to as Bernoulli, Binomial or logistic loss. 

  $$
    L_{0,1}(y, \pix) = - y \log \left(\pix\right) - (1 - y) \log \left(1 - \pix\right). 
  $$

is referred to as cross-entropy or log-loss. 

\lz 

For simplicity, we will call all of them \textbf{Bernoulli loss}, and rather make clear whether they are defined on labels $y \in \{0, 1\}$ or $y \in \{-1, 1\}$ and on scores $\fx$ or probabilities $\pix$. 

\end{vbframe}


% \vspace*{-0.2cm}

% \begin{eqnarray*}
%   \Lxy = \log \left[1 + \exp \left(-y\fx\right)\right].
% \end{eqnarray*}

% We transform scores into probabilities by

% $$
% \pix = \P(y = 1 ~|~\xv) = s(\fx) = \frac{1}{1 + \exp(- \fx)},
% $$

% with $s(.)$ being the logistic sigmoid function as introduced in chapter 2.

% \framebreak

% As already shown before, an equivalent approach that directly outputs probabilities $\pix$ is minimizing the \textbf{Bernoulli loss}

% \begin{eqnarray*}
% \Lxy = -y \log \left(\pix\right) - (1 - y) \log \left(1 - \pix\right)
% \end{eqnarray*}

% for $\pix$ in the hypothesis space

% \begin{eqnarray*}
%   \Hspace = \left\{\pi: \Xspace \to [0, 1] ~|~\pix = s(\thetab^\top \xv)\right\}
% \end{eqnarray*}

% with $s(.)$ again being the logistic function.

% \framebreak


% Logistic Regression with one feature $\xv \in \R$. The figure shows how $\xv \mapsto \pix$.

% <<fig.height=4>>=
% set.seed(1234)
% n = 20
% x = runif(n, min = 0, max = 7)
% y = x + rnorm(n) > 3.5
% df = data.frame(x = x, y = y)

% model = glm(y ~ x,family = binomial(link = 'logit'), data = df)
% df$score = predict(model)
% df$prob = predict(model, type = "response")
% x = seq(0, 7, by = 0.01)
% dfn = data.frame(x = x)
% dfn$prob = predict(model, newdata = dfn, type = "response")
% dfn$score = predict(model, newdata = dfn)

% p2 = ggplot() + geom_line(data = dfn, aes(x = x, y = prob))
% p2 = p2 + geom_point(data = df, aes(x = x, y = prob, colour = y), size = 2)
% p2 = p2 + xlab("x") + ylab(expression(pi(x)))
% p2 = p2 + theme(legend.position = "none")
% p2
% @

% \framebreak

% Logistic regression with two features:


\section{Summary}

\begin{vbframe}{Summary of Loss Functions}

\vspace*{-0.5cm}

\begin{table}[] 
\footnotesize
\renewcommand{\arraystretch}{1.5} %<- modify value to suit your needs
\begin{tabular}{c|lll}
Name & Formula & Differentiable \\ \hline
0-1 & $L(y, \hx) = [y \neq \hx]$  & \xmark \\
Brier & $L(y, \pix) = \left(\pix - y\right)^2$ & \checkmark \\
Bernoulli & $L_{-1+1}(y, \fx)= \ln[1 + \exp(-y\fx)]$ & \checkmark \\
Bernoulli & $L_{0,1}(y, \fx) = - y \fx + \log(1 + \exp(\fx))$ & \checkmark \\
Bernoulli & $L_{0,1}(y, \pix) = - y \log\pix - (1 - y) \log (1 - \pix)$ & \checkmark \\
\end{tabular}
\end{table}

\begin{table}[] 
\footnotesize
\renewcommand{\arraystretch}{1.5} %<- modify value to suit your needs
  \begin{tabular}{c|lll}
  Name & Point-wise Opt. & Optimal Constant\\ \hline
  0-1 & $\hxh = \argmax_{l \in \Yspace} \P(y = l~|~ \xv)$  & $\hx = \text{mode} \left\{\yi\right\}$ \\
  Brier & $\pixh = \P(y = 1~|~\xv = \xv)$ & $\hat{\pi} = \frac{1}{n} \sumin \yi$\\
  Bernoulli & $\pixh = \P(y = 1~|~\xv = \xv)$ & $\hat{\pi} = \frac{1}{n} \sumin \yi$\\
  Bernoulli & $\fxh = \log\left(\frac{\P(y = 1 ~|~\xv)}{1 - \P(y = 1 ~|~\xv)}\right)$ & $\fh = \ln \frac{n_{+1}}{n_{-1}}$  
  \end{tabular}
\end{table}


\framebreak 

There are other loss functions for classification tasks, for example:

\begin{itemize}
  \item Hinge-Loss 
  \item Exponential-Loss
\end{itemize}

As for regression, loss functions might also be customized to an objective that is defined by an application. 


\end{vbframe}




\endlecture
\end{document}


