%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure_man/optimization_steps.jpeg}
\newcommand{\learninggoals}{
\item Understand that an ML model is simply a parametrized curve
\item Understand that the hypothesis space lists all admissible models
    for a learner
\item Understand the relationship between the hypothesis space and the parameter space
}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}

\begin{vbframe}{Excursus: Numerical Optimization}

We are searching for the parameter $\thetab \in \Theta$ which minimizes the empirical risk

\begin{eqnarray*}
\min_{\thetab \in \Theta} \risket = \min_{\thetab \in \Theta} \sumin \Lxyit \cr 
\end{eqnarray*}

What if there is no closed-form solution to the problem above? 

\lz 

$\longrightarrow\quad$ Numerical Optimization

\end{vbframe}


\section{Gradient Descent}


\begin{vbframe}{Gradient Descent}

\begin{itemize}
\item To find local minima, gradient descent takes steps in direction of the \textbf{negative gradient} of $\risket$ at the current point $\thetab^{[t]}$.
\item The iterative update rule is 
$$
  \thetab^{[t + 1]} = \thetab^{[t]} - \alpha^{[t]} \cdot \nabla_{\thetab} \left.\risket\right|_{\thetab = \thetab^{[t]}} ,
$$
where the \textbf{step-size} $\alpha^{[t]}  \in [0,1]$ needs to be chosen (fixed, line-search, ...).
\item As it uses the gradient, gradient descent is a first-order iterative optimization algorithm.
\end{itemize}

\end{vbframe}
  
\begin{vbframe}{GD in ML and Pseudo-Residuals}

By using the chain rule we see that 

\vspace*{-0.5cm}

\begin{footnotesize}
\begin{eqnarray*}
\nabla_{\thetab} \risket &=&\sumin \underbrace{\left.\frac{\partial L\left(\yi, f\right)}{\partial f} \right|_{f = \fxit} }_{= - \tilde r^{(i)}}
\cdot \nabla_{\thetab} \fxit \\ 
&=& - \sumin \tilde r^{(i)} \cdot \nabla_{\thetab} \fxit
\end{eqnarray*}
\end{footnotesize}

For risk minimization, the update rule for the parameter $\thetab$ is 
\begin{footnotesize}
\begin{eqnarray*}
\thetab^{[t+1]} &\leftarrow & \thetab^{[t]} - \alpha^{[t]}  \sumin \nabla_{\thetab} \left. \Lxyit \right|_{\thetab = \thetab^{[t]}} \\
\thetab^{[t+1]} &\leftarrow & \thetab^{[t]} + \alpha^{[t]} \sumin \tilde r^{(i)} \cdot \left. \nabla_{\thetab} \fxit \right|_{\thetab = \thetab^{[t]}} 
\end{eqnarray*}
\end{footnotesize}
$\alpha^{[t]} \in [0,1]$ is called \enquote{learning rate} in this context.
\end{vbframe}

% \begin{vbframe}{A Note on Pseudo-Residuals}

% \begin{itemize}
% 	\item In gradient descent, we try to move in the direction of the negative gradient in each step by updating the model accordingly 
% 	$$
% 	 \thetab^{[j + 1]} = \thetab^{[t]} - \alpha^{[t]} \cdot \nabla_{\thetab} \left.\risket\right|_{\thetab = \thetab^{[t]}}		
% 	$$
% 	\item This can be seen as approximating the unexplained information (measured by the loss) through a model update
% 	\item The unexplained information - the negative gradient - can be thought of as residuals, which is therefore also called pseudo-residuals
% 	\item For the L2-loss, pseudo-residuals and residuals actually coincide.
% \end{itemize}

% \end{vbframe}


\begin{vbframe}{Gradient Descent for Huber Loss}

\textbf{Example:} Huber Loss

\vspace*{0.3cm}

We can calculate the pseudo-residuals for the Huber loss 

\begin{footnotesize}
$$
\tilde r^{(i)} = \begin{cases}
  - \frac{\partial}{\partial f}\frac{1}{2}\left(\yi - \fxi\right)^2 = \left(\yi - \fxi\right) & \text{ if } |\yi - \fxi| \le \delta \\
  - \frac{\partial}{\partial f} \delta |\yi - \fxi|-\frac{1}{2}\delta^2 = \delta \cdot \text{sgn}\left(\yi - \fxi\right)\quad & \text{ otherwise }
  \end{cases}
$$
\end{footnotesize}

For the constant model $\fx = \theta$ this results in the following update rule: 

\begin{eqnarray*}
\theta^{[t+1]} &\leftarrow& \theta^{[t]} +  \alpha^{[t]}\sumin \tilde r^{(i)}
\end{eqnarray*}

The steps of gradient descent are shown in the following plots. The orange line shows the \enquote{optimal} constant model w.r.t. Huber loss, the black line shows the iterations of gradient descent.


\framebreak 


\begin{center}
\foreach \x in{1, 2, 3, 4} {
\begin{figure}
	\includegraphics{figure_man/empirical_risk_plot_constant_\x.pdf}
	\caption{Huber loss optimized by gradient descent for a constant model $\fx = \theta$.}
\end{figure}
}
\end{center}

\framebreak 

\begin{center}
\foreach \x in{1, 2, 3, 4} {
	\begin{figure}
	\includegraphics{figure_man/empirical_risk_plot_linear_\x.pdf}
	\caption{Huber loss optimized by gradient descent for a linear model $\fx = \theta^\top \xv$.}
	\end{figure}
}
\end{center}

% \begin{center}
% \includegraphics{figure_man/empirical_risk_plot_constant_2.pdf}
% \end{center}
% \framebreak 

% \begin{center}
% \includegraphics{figure_man/empirical_risk_plot_constant_3.pdf}
% \end{center}

% \framebreak 

% \begin{center}
% \includegraphics{figure_man/empirical_risk_plot_constant_4.pdf}
% \end{center}

% \framebreak 

% \begin{center}
% \includegraphics{figure_man/empirical_risk_plot_linear_1.pdf}
% \end{center}
% \framebreak 

% \begin{center}
% \includegraphics{figure_man/empirical_risk_plot_linear_2.pdf}
% \end{center}
% \framebreak 

% \begin{center}
% \includegraphics{figure_man/empirical_risk_plot_linear_3.pdf}
% \end{center}

% \framebreak 

% \begin{center}
% \includegraphics{figure_man/empirical_risk_plot_linear_4.pdf}
% \end{center}

\end{vbframe}

% \begin{vbframe}{Gradient Descent vs. Steepest Descent}

% \begin{itemize}
	% \item \textbf{Gradient descent} performs updates in the direction of the negative gradient $v = - \nabla \risket$. 
	% \item \textbf{Steepest descent} performs updates in the direction $\frac{v}{\|v\|}$ of the smallest directional derivative 
	% $$
	% \argmin_v \{\nabla \risket^\top v\}. 
	% $$
% \end{itemize}

% Both directions coincide if $\|.\|$ is the euclidean norm $\|.\|_2$. Gradient descent and steepest descent are equivalent in this case. 

% \lz 
% 
% If we consider the $L_1$ norm, for example, both methods would perform different updates. For example, if you want to perform descent on a sparse dataset and you want to penalize $\|.\|_1$ norm, then you would probably want to use steepest gradient descent with $L_1$-norm. 

% \end{vbframe}




\section{Stochastic Gradient Descent}


\begin{vbframe}{Stochastic Gradient Descent}
\begin{itemize}
\item \textbf{Stochastic Gradient Descent (SGD)} is a stochastic approximation of gradient descent.
\item SGD is applied if $\sumin \nabla_{\thetab^{[t]}} \Lxyit$ is expensive in terms of evaluations (every summand needs to be evaluated).
\item SGD approximates the gradient using just a random observation $i$ leading to a simplified updating rule:

\begin{eqnarray*}
\thetab^{[t+1]} \leftarrow \thetab^{[t]} - \alpha^{[t]} \nabla_{\theta} \left. \Lxyit \right|_{\thetab = \thetab^{[t]}}
\end{eqnarray*}

\item The sequence of parameters $\{\thetab^{[1]}, \thetab^{[2]}, \hdots \}$ is stochastic since it depends on the randomly drawn observation in every step.
\end{itemize}

\end{vbframe}

\begin{vbframe}{SGD Mini Batches}

\begin{itemize}
  \item Stochastic gradient is computationally cheap compared to standard gradient descent, but might be very noisy.
\item A trade-off between standard gradient descent (uses all observations for computation of the gradient) and stochastic gradient (uses one observation for approximation of the gradient) is \textbf{mini-batch gradient descent}.
\item Mini-batch gradient descent uses a set of randomly drawn observations $I \subset \{1, 2, ..., n\}$ for approximation of the gradient 

 \begin{eqnarray*}
\thetab^{[t+1]} \leftarrow \thetab^{[t]} - \alpha^{[t]} \sum_{i \in I} \nabla_{\theta} \left. \Lxyit \right|_{\thetab = \thetab^{[t]}}
\end{eqnarray*}

\end{itemize}

For further details on multivariate optimization see CIM1 - Statistical
Computing.
\end{vbframe}

\endlecture
\end{document}


