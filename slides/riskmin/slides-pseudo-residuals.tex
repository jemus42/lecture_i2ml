%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure_man/optimization_steps.jpeg}
\newcommand{\learninggoals}{
\item Know the concept of pseudo-residuals 
\item Understand the relationship between pseudo-residuals and gradient descent 
}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}


\begin{document}

\lecturechapter{Pseudo-residuals and Gradient Descent}
\lecture{Introduction to Machine Learning}


\begin{vbframe}{Pseudo-Residuals}

\begin{itemize}
	\item In regression, residuals are defined as 
	\begin{eqnarray*}
		\eps &:=& y - \fx.
	\end{eqnarray*}
\item We further define \textbf{pseudo-residuals} as the negative first derivatives of loss functions w.r.t. $\fx$

  \begin{eqnarray*}
    % \tilde r &:=& - \frac{\partial \Lxy}{\partial \fx} \qquad 
    % \tilde r^{(i)} := - \frac{\partial \Lxyi}{\partial \fxi} 
    \tilde \eps\left(y, \fx\right) &:=& - \frac{\partial \Lxy}{\partial \fx}.  % \qquad 
    % \tilde r^{(i)} := - \frac{\partial \Lxyi}{\partial f} 
  \end{eqnarray*}
  Note that pseudo-residuals are functions of $y$ and $\fx$. \\ We write $\tilde \eps^{(i)} := \eps\left(\yi, \fxi\right)$. 
\end{itemize}

\lz 

\textcolor{red}{@BB: More information / intuition about pseudo-residuals! }

\end{vbframe}


\begin{vbframe}{GD in ML and Pseudo-Residuals}

Let us consider gradient descent

$$
	 \thetab^{[t + 1]} = \thetab^{[t]} - \alpha^{[t]} \cdot \nabla_{\thetab} \left.\risket\right|_{\thetab = \thetab^{[t]}}		
$$

with step size $\alpha^{[t])}$.  

\lz 

By using the chain rule we see that 

\vspace*{-0.5cm}

\begin{eqnarray*}
\nabla_{\thetab} \risket &=&\sumin \underbrace{\left.\frac{\partial L\left(\yi, f\right)}{\partial f} \right|_{f = \fxit} }_{= - \tilde \eps^{(i)}}
\cdot \nabla_{\thetab} \fxit \\ 
&=& - \sumin \tilde \eps^{(i)} \cdot \nabla_{\thetab} \fxit
\end{eqnarray*}

% For risk minimization, the update rule for the parameter $\thetab$ is 
% \begin{footnotesize}
% \begin{eqnarray*}
% \thetab^{[t+1]} &\leftarrow & \thetab^{[t]} - \alpha^{[t]}  \sumin \nabla_{\thetab} \left. \Lxyit \right|_{\thetab = \thetab^{[t]}} \\
% \thetab^{[t+1]} &\leftarrow & \thetab^{[t]} + \alpha^{[t]} \sumin \tilde r^{(i)} \cdot \left. \nabla_{\thetab} \fxit \right|_{\thetab = \thetab^{[t]}} 
% \end{eqnarray*}
% \end{footnotesize}
% $\alpha^{[t]} \in [0,1]$ is called \enquote{learning rate} in this context.
\end{vbframe}

\begin{vbframe}{A Note on Pseudo-Residuals}

\begin{itemize}
	\item In gradient descent, we try to move in the direction of the negative gradient in each step by updating the model accordingly 
	$$
	 \thetab^{[t + 1]} = \thetab^{[t]} - \alpha^{[t]} \cdot \nabla_{\thetab} \left.\risket\right|_{\thetab = \thetab^{[t]}}		
	$$
	\item This can be seen as approximating the unexplained information (measured by the loss) through a model update.
	\item The unexplained information - the negative gradient - can be thought of as residuals, which is therefore also called pseudo-residuals.
	\item For the L2-loss, pseudo-residuals and residuals actually coincide.
\end{itemize}

\textcolor{red}{@BB: Improve above text? }


\end{vbframe}


\endlecture

\end{document}