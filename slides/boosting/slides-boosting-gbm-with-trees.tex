\input{../../style/preamble} 
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-bagging.tex}
\input{../../latex-math/ml-boosting.tex}
\input{../../latex-math/ml-trees.tex}

\newcommand{\titlefigure}{figure/gbm_sine_title.png}
\newcommand{\learninggoals}{
  \item \textcolor{blue}{XXX}
  \item \textcolor{blue}{XXX}
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Gradient Boosting with Trees}
\lecture{Introduction to Machine Learning}

% ------------------------------------------------------------------------------

\begin{vbframe}{Gradient boosting with trees}

Trees are mainly used as base learners for gradient boosting in ML.
A great deal of research has been done on this combination so far, and it often provides the best results.

\begin{blocki}{Reminder: advantages of trees}
\item No problems with categorical features.
\item No problems with outliers in feature values.
\item No problems with missing values.
\item No problems with monotone transformations of features.
\item Trees (and stumps!) can be fitted quickly, even for large $n$.
\item Trees have a simple, built-in type of variable selection.
%\item Interpretation of Trees is rather easy.
\end{blocki}
The gradient-boosted trees method retains all of them, and strongly improves the trees' predictive power.
Furthermore, it is possible to adapt gradient boosting to tree learners in a targeted manner.

\framebreak

% ------------------------------------------------------------------------------

One can write a tree as: $ b(\xv) = \sum_{t=1}^{T} c_t \mathds{1}_{\{\xv \in R_t\}} $,
where $R_t$ are the terminal regions and $c_t$ the corresponding constant parameters.

\vspace*{0.2cm}

For a fitted tree with regions $R_t$, the special additive structure can be exploited in boosting:  %Finished here

\begin{align*}
  \fm(\xv) &= \fmd(\xv) +  \betam \bmm(\xv) \\
         &= \fmd(\xv) +  \betam \sum_{t=1}^{\Tm} \ctm \mathds{1}_{\{\xv \in \Rtm\}}.
\end{align*}

Actually, we do not have to find $\ctm$ and $\betam$ in two separate steps
(fitting against pseudo-residuals, then line search) but find optimal 
\textcolor{blue}{$\ctmt$} (including $\betam$).
Also note that the \textcolor{blue}{$\ctmt$} will not really be loss-optimal as 
we used squared error loss
to fit them against the pseudo-residuals.

% ------------------------------------------------------------------------------

\framebreak
\begin{footnotesize}
We do the same steps as before: (1) calculate the pseudo-residuals, (2) fit a tree against pseudo-residuals, \textbf{but now} we keep only the structure of the tree and optimize the $c$ parameter in a (further) post-hoc step.

$$
\fm(\xv) = \fmd(\xv) +  \sum_{t=1}^{\Tm} \ctmt \mathds{1}_{\{\xv \in \Rtm\}}. 
$$

We want to find the constant value $c$ that drives down risk the most w.r.t the squared error loss,
when added to the respective terminal region.
We can determine/change all $\ctmt$ individually and directly $L$-optimally:


%\vspace{-0.2cm}

$$ \ctmt = \argmin_{c} \sum_{\xi \in \Rtm} L(\yi, \fmd(\xi) + c). $$

\vspace{-0.5cm}

\begin{center}

\includegraphics[width=0.38\textwidth]{figure_man/gbm_leaf_adjustment.pdf}

\end{center}

\end{footnotesize}

\framebreak

% ------------------------------------------------------------------------------

\input{algorithms/gradient_tree_boosting_algorithm.tex}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Visualization 1}
\begin{footnotesize}
\textbf{Simulation setting:}
\begin{itemize}
\item Given: one feature $x$ and one numeric target variable $y$ of 50 observations.
\item $x$ is uniformly distributed between 0 and 10.
\item $y$ depends on $x$ as follows: $y^{(i)} = \sin{(x^{(i)})} + \epsilon^{(i)}$ with $\epsilon^{(i)} \sim \mathcal{N}(0, 0.01)$, $\forall i \in \{1, \dots, 50\}$.
\end{itemize}

\vfill

\begin{minipage}[c]{0.55\textwidth}
  \vspace{0pt}%
  \includegraphics[width = 0.9\textwidth]{figure/gbm_anim_data.png}
\end{minipage}%
\begin{minipage}[c]{0.02\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[c]{0.4\textwidth}
  \vspace{0pt}%
  \raggedright
  \textbf{Aim:} we want to fit a gradient boosting model to the data by using 
  stumps as base learners.
  
  \lz
  Since we are facing a regression problem, we use $L2$ loss.
\end{minipage}%

\framebreak

% ------------------------------------------------------------------------------

% !!! ADJUST VALUES IF YOU MODIFY SIMULATION (rsrc file contains kable commands)

\textbf{Iteration 0:} initialization by optimal constant (mean) prediction 
$\hat f^{[0](i)}(x) = \bar{y} \approx 0.2$.

\vfill

\begin{minipage}[c]{0.35\textwidth}
  \vspace{0pt}%
  \scriptsize
  \centering
  \begin{tabular}{r|r|r|r}
    $i$ & $x^{(i)}$ & $\yi$ & $\hat{f}^{[0]}$ \\
    \hline
    1 & 0.03 & 0.16 & 0.20 \\
    2 & 0.03 & -0.06 & 0.20 \\
    3 & 0.07 & 0.09 & 0.20 \\
    \vdots & \vdots & \vdots & \vdots \\
    50 & 9.69 & -0.08 & 0.20 \\
  \end{tabular}
\end{minipage}%
\begin{minipage}[c]{0.05\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[c]{0.6\textwidth}
  \vspace{0pt}%
  \includegraphics{figure/gbm_anim_init.png}
\end{minipage}%

\end{footnotesize}

\framebreak

% ------------------------------------------------------------------------------

% !!! ADJUST VALUES IF YOU MODIFY SIMULATION (rsrc file contains kable commands)

\begin{footnotesize}

\textbf{Iteration 1:} (1) Calculate residuals $\tilde{r}^{[m](i)}$ and (2) 
fit a regression stump $b^{[m]}.$ %which is multiplied by the learning rate $\beta$% (here: $\hat \beta^{[1]} = 1$).

\vfill

\begin{minipage}[c]{0.5\textwidth}
  \vspace{0pt}%
  \centering
  \scriptsize
  \begin{tabular}{r|r|r|r|r|r}
    $i$ & $x^{(i)}$ & $\yi$ & $\hat{f}^{[0]}$ & $\tilde{r}^{[1](i)}$ & 
    $\hat{b}^{[1](i)}$\\ 
    \hline
    1 & 0.03 & 0.16 & 0.20 & -0.04 & -0.17 \\
    2 & 0.03 & -0.06 & 0.20 & -0.25 & -0.17 \\
    3 & 0.07 & 0.09 & 0.20 & -0.11 & -0.17 \\
    \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
    50 & 9.69 & -0.08 & 0.20 & -0.27 & 0.33 \\
  \end{tabular}
\end{minipage}%
\begin{minipage}[c]{0.05\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[c]{0.45\textwidth}
  \vspace{0pt}%
  \includegraphics{figure/gbm_anim_init.png}
\end{minipage}%

\vfill

(3) Update model by $\hat{f}^{[1]}(x) = \hat{f}^{[0]}(x) + \hat{b}^{[1]}.$
\end{footnotesize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{frame}{Visualization 1}

\footnotesize
Repeat step (1) to (3):

\begin{center}
  \only<1>{ \includegraphics[width=\textwidth]{figure/gbm_anim_02.png} }
  \only<2>{ \includegraphics[width=\textwidth]{figure/gbm_anim_03.png} }
  \only<3>{ \includegraphics[width=\textwidth]{figure/gbm_anim_04.png} }
  \only<4>{ \includegraphics[width=\textwidth]{figure/gbm_anim_05.png} }
  \only<5>{ \includegraphics[width=\textwidth]{figure/gbm_anim_06.png} }
  \only<6>{ \includegraphics[width=\textwidth]{figure/gbm_anim_51.png} }
\end{center}

\end{frame}

% ------------------------------------------------------------------------------

\begin{vbframe}{Visualization 2}

\begin{center}
  \includegraphics[width=\textwidth]{figure/gbm_sine.png}
\end{center}

\vfill

\footnotesize
\begin{itemize}
  \item Iterating this very simple tree-stump base learner yields a rather nice
  approximation of a smooth model in the end.
  \item Severe overfitting apparent in the noisy case. We will discuss and solve 
  this problem later.
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------
\begin{vbframe}{Visualization 3}

\textbf{Simulation setting:}
\begin{itemize}
  \item Given: 2-dimensional regression problem
  \item Aim: reconstruct $ y = \fx = f (x_1, x_2) $
\end{itemize}

\href{http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html}{\beamergotobutton{Open in browser.}}


\end{vbframe}

\begin{frame}{Visualization 3}
Tree depth: 2 \\
Number of trees: 
\only<1>{1}
\only<2>{3}
\only<3>{5}
\only<4>{7}
\only<5>{10}

\begin{columns}
  \begin{column}{0.49\textwidth}
    \begin{center}
    Prediction of previous trees 
        \only<1>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_2_target_1.png}}
        \only<2>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_2_target_3.png}}
        \only<3>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_2_target_5.png}}
        \only<4>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_2_target_7.png}}
        \only<5>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_2_target_10.png}}
        
    \end{center}  
  \end{column}
  \begin{column}{0.49\textwidth}
    \begin{center}
    Residuals 
        \only<1>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_2_res_1.png}}
        \only<2>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_2_res_3.png}}
        \only<3>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_2_res_5.png}}
        \only<4>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_2_res_7.png}}
        \only<5>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_2_res_10.png}}
    \end{center}
  \end{column}
\end{columns}
\end{frame}


\begin{frame}{Visualization 3}
Tree depth: 1 \\
Number of trees: 
\only<1>{1}
\only<2>{3}
\only<3>{5}
\only<4>{7}
\only<5>{10}

\begin{columns}
  \begin{column}{0.49\textwidth}
    \begin{center}
    Prediction of previous trees 
        \only<1>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_1_target_1.png}}
        \only<2>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_1_target_3.png}}
        \only<3>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_1_target_5.png}}
        \only<4>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_1_target_7.png}}
        \only<5>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_1_target_10.png}}
        
    \end{center}  
  \end{column}
  \begin{column}{0.49\textwidth}
    \begin{center}
    Residuals 
        \only<1>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_1_res_1.png}}
        \only<2>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_1_res_3.png}}
        \only<3>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_1_res_5.png}}
        \only<4>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_1_res_7.png}}
        \only<5>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_1_res_10.png}}
    \end{center}
  \end{column}
\end{columns}
\end{frame}


\endlecture
\end{document}
