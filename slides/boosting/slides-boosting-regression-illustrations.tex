\input{../../style/preamble} 
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-bagging.tex}
\input{../../latex-math/ml-boosting.tex}
\input{../../latex-math/ml-trees.tex}

\newcommand{\titlefigure}{figure/illustration_title.png}
\newcommand{\learninggoals}{
  \item Understand impact of different loss functions and
  \item Understand impact of different base learners for regression
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Gradient Boosting - Illustration}
\lecture{Introduction to Machine Learning}

% ------------------------------------------------------------------------------

\begin{vbframe}{Gradient boosting illustration - GAM}

We now compare different loss functions and base learners.
We start with a GAM as base learner and compare the $L2$ loss with
the $L1$ loss.\\
\vspace*{0.1cm}

\textbf{Reminder:} Pseudo-residuals
\begin{itemize}
\item $L2$: $\tilde{r}(f) = r(f) = y - f(\xv)$
\item $L1$: $\tilde{r}(f) = sign(y - f(\xv))$
\end{itemize}
We consider a regression task with a single feature $x$ and target $y$, with 
the following true underlying relationship:
\vspace{-0.2cm}
\begin{align*}
y^{(i)} &=  -1 + 0.2 \cdot x^{(i)} + 0.1 \cdot sin(x^{(i)}) + \epsilon^{(i)} \\
&\text{with } n = 50 \text{ and } \epsilon \sim \mathcal{N}(0, 0.1) \quad 
\forall i \in \nset
\end{align*}

\vfill

\begin{figure}
  \includegraphics[width = 0.45\textwidth]{figure/illustration_data_normal.png}
\end{figure}

\framebreak

% ------------------------------------------------------------------------------

\begin{enumerate}
  \item We start with the simplest model, the optimal constant -- mean of the 
  target variable in the case of $L2$ loss and median in the case of $L1$ loss.
  \item We improve the model by calculating the pointwise 
  pseudo-residuals on the training data, and fit a GAM on the residuals.
  \begin{enumerate}
    \item The GAM base learners model the conditional mean via cubic B-splines 
    with 40 knots. 
    % i.e., $\bmmxth = \sum_{k = 1}^{K + \ell} B_k^{(3)}(\xv)$, with $\ell = 3$ 
    % and $K = 40$ knots.
    \item In each step, the GAM fitted on the current pseudo-residuals is
    multiplied by a constant learning rate of $0.2$ and added to the previous
    model.
    \item This procedure is repeated multiple times.
  \end{enumerate}
\end{enumerate}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{frame}{GAM illustration 1}

Top: $L2$ loss, bottom: $L1$ loss

\vfill

% \begin{center}
\only<1>{ 
\includegraphics[width=\textwidth]{figure/illustration_gaussian_L2_1.png}
\includegraphics[width=\textwidth]{figure/illustration_gaussian_L1_1.png}

\vfill

Iteration 1
}

\only<2>{ 
\includegraphics[width=\textwidth]{figure/illustration_gaussian_L2_2.png}
\includegraphics[width=\textwidth]{figure/illustration_gaussian_L1_2.png}

\vfill

Iteration 2
}
\only<3>{ 
\includegraphics[width=\textwidth]{figure/illustration_gaussian_L2_3.png}
\includegraphics[width=\textwidth]{figure/illustration_gaussian_L1_3.png}

\vfill

Iteration 3
}
\only<4>{ 
\includegraphics[width=\textwidth]{figure/illustration_gaussian_L2_10.png}
\includegraphics[width=\textwidth]{figure/illustration_gaussian_L1_10.png}

\vfill

Iteration 10
}

\only<5>{ 
\includegraphics[width=\textwidth]{figure/illustration_gaussian_L2_100.png}
\includegraphics[width=\textwidth]{figure/illustration_gaussian_L1_100.png}

\vfill

Iteration 100
}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{GAM illustration 1}
We can also use Huber loss, which is closer to $L2$ for large $\delta$ values and closer to $L1$ for 
smaller $\delta$ values. Top: $\delta$ = 2, bottom: $\delta$ = 0.2.

\vfill

\includegraphics[width=\textwidth]{figure/illustration_gaussian_huber_2_10.png}
\includegraphics[width=\textwidth]{figure/illustration_gaussian_huber_02_10.png}

\vfill

Iteration 10

\vfill

\footnotesize
We see how for smaller $\delta$ (bottom) pseudo-residuals are often effectively
bounded, resulting in $L1$-like behavior, while the upper plot more closely 
resembles $L2$ loss.
% The lower right plot shows that pseudo-residuals often take values of the upper and lower
% bounds (depending on $\delta$) and thus the behaviour is closer to the $L1$ loss while the upper plots show a
% similar behavior to the $L2$ loss. 

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{GAM illustration 2}
Instead of normally distributed noise we can assume a t-distribution, leading to 
outliers in the observed target values.
Top: $L2$, bottom: $L1$.

\vfill

\only<1>{ 
\includegraphics[width=\textwidth]{figure/illustration_tdist_L2_10.png}
\includegraphics[width=\textwidth]{figure/illustration_tdist_L1_10.png}

\vfill
Iteration 10
}
\only<2>{ 
\includegraphics[width=\textwidth]{figure/illustration_tdist_L2_100.png}
\includegraphics[width=\textwidth]{figure/illustration_tdist_L1_100.png}

\vfill
Iteration 100
}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{LM illustration}
Instead of using a GAM as base learner we now use a simple linear model.
It follows that this is equivalent to applying gradient descent on linear 
regression.
Top: $L2$, bottom: $L1$.

\vfill

\only<1>{ 
\includegraphics[width=\textwidth]{figure/illustration_gaussian_L2_lin_1.png}
\includegraphics[width=\textwidth]{figure/illustration_gaussian_L1_lin_1.png}

\vfill
Iteration 1
}
\only<2>{ 
\includegraphics[width=\textwidth]{figure/illustration_gaussian_L2_lin_2.png}
\includegraphics[width=\textwidth]{figure/illustration_gaussian_L1_lin_2.png}

\vfill
Iteration 2
}
\only<3>{ 
\includegraphics[width=\textwidth]{figure/illustration_gaussian_L2_lin_3.png}
\includegraphics[width=\textwidth]{figure/illustration_gaussian_L1_lin_3.png}

\vfill
Iteration 3
}
\only<4>{ 
\includegraphics[width=\textwidth]{figure/illustration_gaussian_L2_lin_10.png}
\includegraphics[width=\textwidth]{figure/illustration_gaussian_L1_lin_10.png}

\vfill
Iteration 10
}

\only<5>{ 
\includegraphics[width=\textwidth]{figure/illustration_gaussian_L2_lin_100.png}
\includegraphics[width=\textwidth]{figure/illustration_gaussian_L1_lin_100.png}

\vfill
Iteration 100
}

\end{frame}
% 
% \textbf{Iteration 1:} Step 1 to 3
% \begin{figure}
%   \includegraphics[width=0.7\textwidth]{figure_man/illustration2.png}
% \end{figure}
% 
% 
% \framebreak
% \textbf{Iteration 2:} Repeat Step 2 and 3
% \begin{figure}
%   \includegraphics[width=0.7\textwidth]{figure_man/illustration3.png}
% \end{figure}
% 
% 
% \framebreak
% \textbf{Iteration 3:} Repeat Step 2 and 3
% \begin{figure}
%   \includegraphics[width=0.7\textwidth]{figure_man/illustration4.png}
% \end{figure}
% 
% 
% \framebreak
% \textbf{Iteration 5:} Repeat Step 2 and 3
% \vspace{0.2cm}
% \begin{figure}
%   \includegraphics[width=0.7\textwidth]{figure_man/illustration5.png}
% \end{figure}
% 
% 
% \framebreak
% \textbf{Iteration 10:} Repeat Step 2 and 3
% \vspace{0.2cm}
% \begin{figure}
%   \includegraphics[width=0.7\textwidth]{figure_man/illustration6.png}
% \end{figure}
% 
% 
% \framebreak
% \textbf{Iteration 100:} Repeat Step 2 and 3
% \vspace{0.2cm}
% \begin{figure}
%   \includegraphics[width=0.7\textwidth]{figure_man/illustration7.png}
% \end{figure}
% 
% 
% \end{vbframe}

% ------------------------------------------------------------------------------

\endlecture
\end{document}

