\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-bagging.tex}
\input{../../latex-math/ml-boosting.tex}
\input{../../latex-math/ml-trees.tex}

\newcommand{\titlefigure}{figure_man/split-finding02.png}
\newcommand{\learninggoals}{
  \item \textcolor{blue}{XXX}
  \item \textcolor{blue}{XXX}
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Modern Boosting Techniques}
\lecture{Introduction to Machine Learning}

% sources: https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf
% sources: https://towardsdatascience.com/boosting-algorithm-xgboost-4d9ec0207d
% sources: https://devblogs.nvidia.com/parallelforall/gradient-boosting-decision-trees-xgboost-cuda/

\begin{vbframe}{Beyond XGboost}

Next to XGBoost to other important modern boosting libraries exist:

\lz

\begin{itemize}
  \item \pkg{LightGBM} by \textbf{Ke et al. (2017)}
  \item \pkg{CatBoost} by \textbf{Prokhorenkova et al. (2017)}
\end{itemize}

\lz

Both libraries extend the ideas of \pkg{XGBoost} in several areas:

\lz

\begin{enumerate}
  \item Tree growing efficiency
  \item Data sampling
  \item Feature compression
\end{enumerate}

\end{vbframe}

\begin{vbframe}{Tree Growing Efficiency}

Recall: \pkg{XGBoost} grows a balanced tree with \texttt{max\_depth} and prunes leaves that do not improve the loss.

\lz

\textbf{Leaf-wise (Best-first) Tree Growth} allows the growing of unbalanced trees as nodes are compared when deciding for a split.

\lz

\textbf{Gradient Boosted Decision Tables} are a generalization of decision trees that map a sequence of bolean tests to a constant prediction, e.g.

\begin{align*}
x_4 < 2 \wedge &\dots \wedge x_7 < 5 \rightarrow \hat{b}^{[1]} \\
&\dots \\
x_1 < 9 \wedge &\dots \wedge x_5 < 0 \rightarrow \hat{b}^{[M]}
\end{align*}.

%These tables can be represented efficiently and allow for very fast predictions of large ensembles.

\end{vbframe}

\begin{vbframe}{Data Sampling}

Gradient-based One-Side Sampling (GOSS) - \pkg{LightGBM}:

\lz

\begin{itemize}
  \item To evaluate a split GOSS only uses the $a \times n$ observations with largest (absolute) gradients and $b \times n$ randomly sampled observations remaining.
  \item The randomly sampled observations with smaller gradients are weighted by $\frac{1 - a}{b}$.
  \item Default values are $a=0.2$ and $b=0.1$.
  \item GOSS is only used after $m=\frac{1}{\nu}$ iterations of XGBoost type updates.
\end{itemize}

\framebreak

Minimal Variance Sampling (MVS) - \pkg{CatBoost}:

\lz

\begin{itemize}
  \item MVS computes weights and selection probabilities for observations on tree level from the gradients of $\fmd$.
  \item The weighting is computed from the regularized absolute value $\hat{g}^{[m]}(\xi)=\sqrt{g^{[m]}(\xi)^2 + \lambda h^{[m]}(\xi)^2}$.
  \item Observations with a value of $\hat{g}^{[m]}(\xi) > \mu$ are always used and other observations are selected with probability $\frac{\hat{g}^{[m]}(\xi)}{\mu}$.
  \item $\mu$ has a closed-form nearly optimal solution for minimizing the risk of a tree base learner \textbf{(Ibragimov et al. 2019)}.
  \item For the tree fit each observation is weighted inversily proportional to its selection probability.
\end{itemize}

\end{vbframe}


\begin{vbframe}{Feature compression}

For high dimensional data it can be helpful to bundle similar features together to speed up split computations.

\lz

\textbf{Exclusive feature bundling} looks for mutually exclusive features, i.e. features that never take nonzero values simultaniously.

\lz

\begin{itemize}
  \item A single histogram for approximate split finding in boosting can be built from multiple mutually exclusive features without the loss of information.
  \item Such an approach is only useful for sparse data.
  \item This approach speeeds up the histogram building from $\mathcal{O}(np)$ to $\mathcal{O}(nb)$ where $b$ is the number of feature bundles.
  \item While finding the optimal partitioning is np-hard, greedy approximations give good results empirically.
\end{itemize}

\end{vbframe}


\begin{vbframe}{Comparison of Boosting frameworks}

\end{vbframe}

\begin{vbframe}{Benchmark}

\end{vbframe}

\endlecture
\end{document}
