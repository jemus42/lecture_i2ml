\input{../../style/preamble} 
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-ensembles.tex}
\input{../../latex-math/ml-trees.tex}

\newcommand{\titlefigure}{figure/fig-gb-concept-2.png}
\newcommand{\learninggoals}{
  \item Understand idea of forward stagewise modelling
  \item Understand fitting process of gradient boosting for regression problems
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Gradient Boosting}
\lecture{Introduction to Machine Learning}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Gradient Boosting}

% \begin{vbframe}{Gradient descent}
% % \begin{vbframe}{Forward stagewise additive modeling}
% 
% % Let's recall gradient descent from numerical optimization.
% Let $\risk(\theta)$ be (just for the next few slides) an arbitrary, differentiable, unconstrained objective function, which we want to minimize. The gradient $\nabla \risk(\theta)$ is the direction of the steepest ascent, $-\nabla \risk(\theta)$ of \textbf{steepest descent}.
% 
% \lz
% 
% For an intermediate solution $\theta^{[k]}$ during minimization, we can iteratively improve by updating
% $$
% \theta^{[k+1]} = \theta^{[k]} - \beta \nabla \risk(\theta^{[k]}) \qquad  \text{for } 0 < \beta \leq c\left(\theta^{[k]}\right)
% $$
% % \enquote{Walking down the hill, towards the valley.}
% 
% % $f(x_1, x_2) = -\sin(0.8 x_1) \cdot \frac{1}{2\pi} \exp\left( (x_2 x_1 + \pi / 2)^2 \right)$
% <<sd-plot>>=
% #modified from ../../../cim1/2017/11-Optimierung/functions.
% 
% sd_plot = function(col = terrain_hcl, theta = 40, phi = 40, xlab = "x", ylab = "y") {
%   if (is.function(col)) col = col(nrow(z) * ncol(z))
% 
%   par(mfrow = c(1, 2))
% 
%   par(mar = rep(0.5, 4))
%   require("colorspace")
%   pmat = persp2(x, y, z, theta = theta, phi = phi, ticktype = "detailed",
%       xlab = xlab, ylab = ylab, zlab = "", col = col, lwd = 0.3, border = NA)
% 
%   for (j in seq_along(p)) {
%     t3d = trans3d(p[[j]][[1]], p[[j]][[2]], do.call(foo, p[[j]]), pmat)
%     if (j > 1) {
%       t3d2 = trans3d(p[[j - 1]][[1]], p[[j - 1]][[2]], do.call(foo, p[[j- 1]]), pmat)
%       lines(c(t3d$x, t3d2$x), c(t3d$y, t3d2$y))
%       points(x = t3d2$x, y = t3d2$y, pch = 16, col = heat_hcl(1))
%     }
%     points(x = t3d$x, y = t3d$y, pch = 16, col = heat_hcl(1))
%   }
%   par(mar = c(4.1, 4.1, 1.1, 1.1))
%   image(x, y, z, col = col, xlab = xlab, ylab = ylab, useRaster = TRUE)
%   contour(x, y, z, add = TRUE, nlevels = 15)
%   for (j in seq_along(p)) {
%     if (j > 1) {
%       lines(c(p[[j]][1], p[[j - 1]][1]), c(p[[j]][2], p[[j - 1]][2]))
%       points(p[[j - 1]][1], p[[j - 1]][2], pch = 16, col = heat_hcl(1))
%     }
%     points(p[[j]][1], p[[j]][2], pch = 16, col = heat_hcl(1))
%   }
%   invisible(NULL)
% }
% 
% @
% <<gradient-descent, fig.align="center", echo=FALSE, fig.width=8, fig.height=4, out.height="3cm", out.width="6cm">>=
% foo = function(x, y) {
%   -1 * sin(.8 * pi*x) * dnorm(-y * x, mean = pi / 2, sd = 0.8)
% }
% 
% x = seq(0, 2.5, length = 50)
% y = seq(-3, 1, length = 50)
% z = outer(x, y, foo)
% p = c(list(list(1.8, -.5)), optim0(1.8, -.5, FUN = foo, maximum = FALSE, maxit = 19))
% 
% sd_plot(phi = 35, theta = -20, xlab = "x_1", ylab = "x_2", col = viridis::viridis)
% @
% 
% % {\scriptsize step size  $\beta = 1$}
% 
% % \framebreak
% 
% % $\beta$ is called \textbf{step size}, and can be set by
% % \begin{itemize}
% % \item fixing it to a (smallish) constant
% % \item adapting it according to previous gradient values, the local Hessian, etc.
% % \item line search methods, which solve $\beta^{[k]} = \argmin_{\beta} f\left(x^{[k]} - \beta \nabla f\left(x^{[k]}\right)\right)$. Only one real parameter $\beta$, i.e, \enquote{easy} to solve\dots
% % \end{itemize}
% 
% 
% 
% 
% \end{vbframe}


\begin{vbframe}{Forward stagewise additive modeling}

Assume a regression problem for now (as this is simpler to explain);
and assume a space of base learners $\mathcal{B}$.

\lz

% A weak learner should have the property that it delivers better predictions than by random chance (e.g. for a balanced training set a misclassification error less than 1/2).

% \lz

We want to learn an additive model:

$$
\fx = \sum_{m=1}^M \betam \blxt.
$$

Hence, we minimize the empirical risk:

$$
\riskef = \sum_{i=1}^n L\left(\yi,\fxi \right) =
\sum_{i=1}^n L\left(\yi, \sum_{m=1}^M \betam \blxt\right)
$$


% \framebreak

% A common \textbf{loss} for \textbf{regression} is the \textbf{squared error} with
% $\Lxy = (y-\fx)^2$.

% \lz

% Apparently, $\risk$ depends on the \textbf{base learners} $b(x, \thetam)$,
% or rather their parameters $\thetam$ and weights $\betam$. Hence, we have to optimize these.

% \lz

\framebreak
\textbf{Why is gradient boosting a good choice for this problem?}
\begin{itemize}
\item Because of the additive structure it is difficult to jointly minimize $\riskef$ w.r.t. $\left(\left(\beta^{[1]}, \bm{\theta}^{[1]}\right), \ldots, \left(\beta^{[M]}, \bm{\theta}^{[M]}\right)\right)$, which is a very high-dimensional parameter space (though this is less of a problem nowadays, especially in the
case of numeric parameter spaces).
% - however, this is nowadays especially in the case of numeric parameter spaces not a real problem anymore.
\item Considering trees as base learners is worse as we would have to grow $M$ trees in parallel so they
  work optimally together as an ensemble.
\item Stagewise additive modeling has nice properties, which we want to make use of, e.g. for regularization, early stopping, \dots
\end{itemize}

\framebreak

Hence, we add additive components in a greedy fashion by sequentially minimizing the risk only w.r.t. the next additive component:

$$ \min \limits_{\beta, \bm{\theta}} \sum_{i=1}^n L\left(\yi, \fmdh\left(\xi\right) + \beta b\left(\xi, \bm{\theta}\right)\right) $$

\lz

Doing this iteratively is called \textbf{forward stagewise additive modeling}.

\input{algorithms/forward_stagewise_additive_modeling.tex}

\end{vbframe}


% \section{Gradient Boosting}

\begin{vbframe}{Gradient boosting}

% \begin{footnotesize}
This is not really an algorithm, but an abstract principle.
To find $b\left(\xv, \thetam\right)$ and $\betam$, we use gradient descent, but in function space!

\lz
\begin{columns}
\column{5cm}
% \textbf{Thought experiment:}
Consider a model $f$ whose predictions we can arbitrarily define for each training $\xi$, i.e.,
$f$ is a finite vector
  $$\left(f\left(\xv^{(1)}\right), \ldots,  f\left(\xv^{(n)}\right)\right)^\top $$

This implies $n$ parameters $\fxi$ (and the model would provide no generalization...).

Also, we let's assume $L$ to be differentiable.

\column{5cm}
\begin{center}
  \vspace{-1cm}
  \includegraphics[width=\textwidth]{figure/fig-gb-concept-2.png}
\end{center}

\end{columns}
% \end{footnotesize}
\end{vbframe}

\begin{vbframe}{Gradient boosting}

\textbf{Aim:} Define a movement in function space so we can push our current function towards the data points.

\vspace*{0.1cm}
\textbf{Given:} Regression problem with one feature $x$ and target variable $y$.

\vspace*{0.1cm}
\textbf{Initialization:} Set all parameters to the optimal constant value (e.g., the mean of $y$ for $L2$).

\begin{figure}
  \includegraphics[width=\textwidth]{figure/fig-gb-concept-1.png}
\end{figure}



\end{vbframe}

\begin{frame}{Pseudo Residuals}
% \begin{footnotesize}
How do we distort our $f$ to move it towards the labels and reduce risk?
\vspace*{0.2cm}
Let's minimize risk with GD.

So, we calculate the (negative) gradient of the risk for each parameter, which (weirdly) here are the outputs $\fxi$ 
(0 if $i \neq j$):
% \footnote{The gradient of all terms with $i \neq j$ are 0.}

$$
\tilde{r}^{(i)} = - \pd{\riske}{\fxi} = - \pd{\sum_j L(y^{(j)}, f(\xv^{(j)}))}{\fxi} = - \pd{\Lxyi}{\fxi}.
$$

\vspace*{0.1cm}

At each point, we would like to change the output via:
$ \tilde{r}(f) = - \pd{L(y, f)}{f} .$

\begin{columns}
\begin{column}{0.45\textwidth}
\begin{center}
  \textbf{L2 Example:} The PRs  
  match the usual residuals:
  $$    \tilde{r}(f) = - \pd{0.5(y - f)^2}{f} = y - f $$
\end{center}
\end{column}
\begin{column}{0.55\textwidth}
\begin{center}
  \includegraphics[width=0.7\textwidth]{figure/pseudo_residual_1.png}
\end{center}
\end{column}
\end{columns}

% \end{footnotesize}

\end{frame}

\begin{vbframe}{Boosting as Gradient Descent}

%\begin{footnotesize}
\vspace*{0.2cm}
Combining this with \enquote{forward stagewise modeling}, we are at $\fmd$ during minimization.
Here, we calculate the direction of the negative gradient or vector of PRs:

$$ \rmi = -\left[\pd{\Lxyi}{f(\xi)}\right]_{f=\fmd} $$

\lz

The gradient descent update for each vector component of $f$ is:

$$
  \fm (\xi) =  \fmd (\xi) + \beta \rmi.
$$

Like this, we should \enquote{nudge} $f$ in the direction best risk reduction.

%\end{footnotesize}
\end{vbframe}


\begin{vbframe}{Gradient boosting}

\textbf{Iteration 1:}

Let's move our function $\fxi$ a fraction towards the pseudo-residuals with a learning rate of $\beta = 0.6$.


\begin{figure}
  \includegraphics[width=\textwidth]{figure/fig-gb-concept-pseudo-resi-1.png}
\end{figure}

\framebreak

\textbf{Iteration 2:}

Let's move our function $\fxi$ a fraction towards the pseudo-residuals with a learning rate of $\beta = 0.6$.


\begin{figure}
  \includegraphics[width=\textwidth]{figure/fig-gb-concept-pseudo-resi-2.png}
\end{figure}

\framebreak
% \begin{footnotesize}
% We find our $\betam$ by minimizing with line search:

% $$
  % \betam = \argmin_{\beta} \sumin L(\yi, \fmdh(x) + \beta b(x, \thetamh)),
% $$

% where $h(x, \thetam) = \rmm$.

% \lz

%What is the point of doing all this? 
As said, such a model parameterization is pointless.

\vspace*{0.3cm}


So, we restrict our additive components to $b\left(\xv, \thetam\right) \in \mathcal{B}$.

% \framebreak

The pseudo-residuals are calculated exactly as stated above,
then we fit a simple model $b(\xv, \thetam)$ to them:
$$ \thetamh = \argmin_{\bm{\theta}} \sum_{i=1}^n \left(\rmi - b(\xi, \bm{\theta})\right)^2. $$

\lz

\begin{columns}
\column{5cm}
So, evaluated on the training data,
$b(\xv, \thetam)$ corresponds as closely as possible to the negative risk gradient and generalizes over $\Xspace$.


\column{5cm}
\vspace*{-1cm}
\begin{figure}[th]
  \includegraphics[width=\textwidth]{figure/fig-gb-concept-idea.png}
\end{figure}


\end{columns}
% \end{footnotesize}

\framebreak
% \begin{footnotesize}
\textbf{In a nutshell}: One boosting iteration is exactly one approximated gradient descent step in function space,
which minimizes the empirical risk as much as possible.


\vspace*{0.1cm}
\textbf{Iteration 1:}
\begin{figure}
  \includegraphics[width=0.9\textwidth]{figure/fig-gb-concept-idea-1.png}
\end{figure}
% \end{footnotesize}
\framebreak
% \begin{footnotesize}
Instead of moving the function values for each observation by a fraction closer to the observed data, we fit a regression base learner to the pseudo-residuals (right plot). 


\vspace*{0.1cm}
\textbf{Iteration 2:}
\begin{figure}
  \includegraphics[width=0.9\textwidth]{figure/fig-gb-concept-idea-2.png}
\end{figure}
% \end{footnotesize}
\framebreak
% \begin{footnotesize}
This BL is added to the of the ensemble, weighted by a learning rate (here: $\beta = 0.4$).
Then we iterate.


\vspace*{0.1cm}
\textbf{Iteration 3:}
\begin{figure}
  \includegraphics[width=\textwidth]{figure/fig-gb-concept-idea-3.png}
\end{figure}

% \begin{footnotesize}
% This procedure is continued stepwise until the boosting algorithm terminates.
% \end{footnotesize}
% \end{footnotesize}
\end{vbframe}
%% Combining this with the iterative additive procedure
%% of \enquote{forward stagewise modelling}, we are at the spot $\fmd$ during minimization.
%% At this point, we now calculate the direction of the negative gradient:
%%
%% $$ \rmi = -\left[\pd{\Lxyi}{f(\xi)}\right]_{f=\fmd} $$
%%
%% We will call these $\rmi$ \textbf{pseudo residuals}. For squared loss they match the usual residuals
%%
%%
%% $$
%% - \pd{\Lxy}{\fx} = - \pd{0.5(y - \fx)^2}{\fx} = y - \fx
%% $$
%%
%%
%% \framebreak
%%
%% % We find our $\betam$ by minimizing with line search:
%%
%% % $$
%%   % \betam = \argmin_{\beta} \sumin L(\yi, \fmdh(x) + \beta b(x, \thetamh)),
%% % $$
%%
%% % where $h(x, \thetam) = \rmm$.
%%
%% % \lz
%%
%% What is the point in doing all this? A model parameterized in this way is senseless,
%% as it is just memorizing the instances of the training data...?
%%
%% \lz
%%
%% So, we restrict our additive components to $b\left(x, \thetam\right) \in \mathcal{B}$.
%%
%% % \framebreak
%%
%% The pseudo-residuals are calculated exactly as stated above,
%% then we fit a regression model $b(\bm{x}, \thetam)$ to them:
%% $$ \thetamh = \argmin_{\thetab} \sum_{i=1}^n (\rmi - b(\xi, \thetab))^2 $$
%% So, evaluated on the training data,
%% our $b(x, \thetam)$ corresponds as closely as possible to the negative
%% loss function gradient and generalizes to the whole space.
%%
%% \lz
%%
%% \textbf{In a nutshell}: One boosting iteration is exactly one approximated gradient step in function space,
%% which minimizes the empirical risk as much as possible.
%%
%% \end{vbframe}

\begin{vbframe}{Gradient boosting algorithm}

\input{algorithms/gradient_boosting_general.tex}

Note that we also initialize the model in a loss-optimal manner. %Also, the constant learning rate can be replaced by a line-search, however a small constant learning rate is commonly used.

\end{vbframe}

\begin{vbframe}{Line Search}
The learning rate, as always, influences how we converge. 
Although a small constant LR is commonly used, we can also run line search,
% to solve this one-dimensional optimization problem:
$$\betamh = \argmin_{\beta} \sumin L(\yi, \fmd(\xv) + \beta b(\xv, \thetamh))$$

Alternatively, an (inexact) backtracking line search can be used to find the 
$\betam$ that minimizes the above equation.

\end{vbframe}

\endlecture
\end{document}

