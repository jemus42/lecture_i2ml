\input{../../style/preamble} 
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-ensembles.tex}
\input{../../latex-math/ml-trees.tex}

\newcommand{\titlefigure}{figure/boosting_classif_title.png}
\newcommand{\learninggoals}{
  \item Transfering gradient boosting for regression to binary classification problems
  \item Introducing gradient boosting for multiclass problems
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Gradient Boosting for Classification}
\lecture{Introduction to Machine Learning}


\begin{vbframe}{Binary classification}


For $\Yspace = \{0, 1\}$, we simply have to select an appropriate loss function, so let us
use Bernoulli loss as in logistic regression:

$$ \Lxy = - y \cdot \fx + \log(1 + \exp(\fx)).$$

Then,

\vspace{-0.5cm}

\begin{align*}
\tilde{r}(f) &=-\pd{\Lxy}{\fx} \\
&= y - \frac{\exp(\fx)}{1 + \exp(\fx)} \\
&= y - \frac{1}{1 + \exp(-\fx)} = y - s(\fx).
\end{align*}

Here, $s(\fx)$ is the logistic function, applied to a scoring model.
Hence, effectively, the pseudo-residuals are $y - \pix$.

Through $\pix = s(\fx)$ we can also estimate posterior probabilities.

\framebreak
%

\begin{itemize}
\item Using this loss function, we can simply run GB as for regression.
  NB: Also here, we fit regression base learners against our numerical 
  vector of pseudo-residuals with $L2$ loss. 
%\item The tree extension for boosting for the binary case works exactly as in regression.
\item  We could also have used the exponential loss for classification with 
  GB. It can be shown that the resulting GB algorithm is basically equivalent 
    to AdaBoost. In practice there is no big difference, although Bernoulli loss 
    makes a bit more sense from a theoretical (maximum likelihood) perspective.
\item It follows that GB is a generalization of AdaBoost which can also use other loss functions and be used for different ML scenarios.
\end{itemize}


\end{vbframe}

\begin{vbframe}{Example}

We now illustrate the boosting iterations for a classification example in a similar manner as we did for regression.
However, we will now look at a simulation example with 2 instead of 1 influencial features and one binary target variable.

\begin{columns}
\column{5.5cm}
\begin{itemize}
\item We used the \texttt{mlbench} dataset \texttt{circle} with $n = 50$ observations.
\item We used the Bernoulli loss to calculate pseudo-residuals and fitted in each iteration a base learner (regression tree with max. depth of 3) on them.
\item We initialized with $f^{[0]} = log(1) = 0.$
\end{itemize}
\column{4.5cm}
\begin{center}
\includegraphics[width=\textwidth]{figure_man/boosting_classif_example.png}
\end{center}

\end{columns}



\end{vbframe}

\begin{frame}{Example}
Left: Background color refers to prediction probabilites $\hat{\pi}^{[m]}$ and points to $y$ (probabilities);  
Right: Background color refers to predictions of base learner $\hat{b}^{[m]}$ and points to $\tilde{r}$. 


\vfill

\only<1>{ 
\includegraphics[width=\textwidth]{figure_man/boosting_classif_1.png}


\vfill
Iteration 1
}
\only<2>{ 
\includegraphics[width=\textwidth]{figure_man/boosting_classif_2.png}

\vfill
Iteration 2
}



\only<3>{ 
\includegraphics[width=\textwidth]{figure_man/boosting_classif_5.png}


\vfill
Iteration 5
}
\only<4>{ 
\includegraphics[width=\textwidth]{figure_man/boosting_classif_10.png}

\vfill
Iteration 10
}

\only<5>{ 
\includegraphics[width=\textwidth]{figure_man/boosting_classif_100.png}

\vfill
Iteration 100
}


\end{frame}

%\section{Interactive Playgrounds}

% \begin{vbframe}{Gradient Boosting Playground}
% \begin{center}
% 
% \includegraphics[width=0.7\textwidth]{figure_man/gbm_playground.png}
% 
% \href{http://arogozhnikov.github.io/2016/07/05/gradient_boosting_playground.html}{\beamergotobutton{Open in browser.}}
% 
% \end{center}
% \end{vbframe}

\begin{vbframe}{mlrPlayground}
\begin{center}

\includegraphics[width=\linewidth]{figure_man/mlrplayground_welcome.png}

\href{https://compstat-lmu.shinyapps.io/mlrPlayground/}{\beamergotobutton{Open in browser.}}

\end{center}
\end{vbframe}

% \section{Gradient Boosting for Multiclass Problems}

\begin{vbframe}{Multiclass problems}

We proceed as in softmax regression and model a categorical distribution with multinomial / log loss.
For $\Yspace = \{1, \ldots, g\}$, we create $g$ discriminant functions $\fkx$, one for each class and each one being an \textbf{additive} model of base learners.

We define the $\pi_k(\xv)$ through the softmax function:
$$ \pikx = s_k(f_1(\xv), \ldots, f_g(\xv)) = \exp(\fkx) / \sum_{j=1}^g \exp(f_j(\xv)). $$

Multinomial loss $L$:
$$ L(y, f_1(\xv), \ldots f_g(\xv)) = - \sumkg \mathds{1}_{\{y = k\}} \ln \pikx. $$

Pseudo-residuals:
$$-\pd{L(y, f_1(\xv), \ldots, f_g(\xv))}{\fkx} =  \mathds{1}_{\{y = k\}} - \pikx. $$


\framebreak

\begin{algorithm}[H]
  \begin{footnotesize}
  \begin{center}
  \caption{GB for Multiclass}
    \begin{algorithmic}[1]
      \State Initialize $f_{k}^{[0]}(\xv) = 0,\ k = 1,\ldots,g$
      \For{$m = 1 \to M$}
      \State Set $\pik^{[m]}(\xv) = \frac{\exp(f_k^{[m]}(\xv))}{\sum_j \exp(f_j^{[m]}(\xv))}, k = 1,\ldots,g$
            \For{$k = 1 \to g$}
            \State For all $i$: Compute $\rmi_k = \mathds{1}_{\{\yi = k\}} - \pik^{[m]}(\xi)$
              \State Fit a regression base learner $\hat{b}^{[m]}_k$ to the pseudo-residuals $\rmi_k$.
              \State Obtain $\betamh_k$ by constant learning rate or line-search.
              \State Update $\hat{f}_k^{[m]} = \hat{f}_k^{[m-1]} + \betamh_k \hat{b}^{[m]}_k$
            \EndFor
      \EndFor
    \State Output $\hat{f}_1^{[M]}, \ldots, \hat{f}_g^{[M]}$
    \end{algorithmic}
    \end{center}
    \end{footnotesize}
\end{algorithm}

\end{vbframe}



\endlecture
\end{document}

