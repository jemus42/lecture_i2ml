\input{../../style/preamble} 
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-bagging.tex}
\input{../../latex-math/ml-boosting.tex}
\input{../../latex-math/ml-trees.tex}

\newcommand{\titlefigure}{figure_man/componentwise-gb.png}
\newcommand{\learninggoals}{
  \item \textcolor{blue}{XXX}
  \item \textcolor{blue}{XXX}
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Componentwise Gradient Boosting}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{Componentwise gradient boosting}

Gradient boosting, especially when using trees, has strong predictive
performance but is difficult to interpret unless the base learners are stumps.

\lz

The aim of componentwise gradient boosting is to find a model that:

\begin{itemize}
  \item
    has strong predictive performance,

  \item
    has components that are still interpretable,

  \item
    does automatic selection of components,

  \item
    is sparser than a model fitted with maximum-likelihood estimation.
\end{itemize}

\lz

This is achieved by using \enquote{nice} base learners, which yield familiar statistical models
in the end.

\lz

Because of this, componentwise gradient boosting is also often referred to as \textbf{model-based boosting}.

\framebreak

In classical gradient boosting only one kind of base learner is used, e.g., regression trees.

\lz

For componentwise gradient boosting we define a whole set of base learners

$$
  b_j^{[m]}(\xv,\pmb\theta^{[m]}) \quad j = 1,\dots, J\,,
$$

where $j$ indexes the type of base learner.

\lz

In each iteration, only the best base learner $b_{j^\ast}^{[m]}(\xv,\pmb\theta^{[m]})$ is selected and updated.

\framebreak

We restrict these base learners to additive models, i.e.,

$$
 b_j^{[m]}(\xv,\pmb\theta^{[m]}) + b_j^{[m^\prime]}(\xv,\pmb\theta^{[m^\prime]}) = b_j(\xv,\pmb\theta^{[m]} + \pmb\theta^{[m^\prime]}).
$$

\lz

Often base learners are not defined on the full $\xv$, but only a single feature $x_j$:

$$
  b_j^{[m]}(x_j,\thetam) \quad j = 1,\dots, p.
$$

\lz
This directly incorporates a variable selection mechanism into the fitting process,
since in each iteration only the best base learner is selected, and each base learner can be (substantially) more complex than a stump (e.g., univariate linear effects or splines).

\framebreak


\input{algorithms/componentwise_gradient_boosting.tex}

\framebreak

In the simplest case we use linear models (without intercept) on single features as base learners:

$$
  b(x_j,\thetam) = x_j\beta^{[m]} \quad j = 1,\dots,p.
$$

This definition will result in an ordinary \textbf{linear regression} model.\footnote{Note: a linear model base learner without intercept only makes sense if the covariates are centered (see \texttt{mboost} tutorial, page7)}

\lz

If we let the boosting algorithm converge, i.e., let it run for a really long time, the parameters estimated by the boosting model will converge to the \textbf{same solution} as the ML estimate. This means that, by specifying a loss function according to the negative likelihood of a distribution from an exponential family and defining a link function accordingly, this kind of boosting is equivalent to a (regularized) \textbf{generalized linear model (GLM)}.

\framebreak

But: We do not \emph{need} an exponential family and thus are able to fit models to all kinds of other distributions and losses, as long as we can calculate (or approximate) a derivative of the loss. Note, however, that this does not imply that the algorithm does something meaningful (e.g., non-convex loss functions would still require some additional effort).

\lz

Usually we do not let the boosting model converge fully, but \textbf{stop early}, for regularization and feature selection.

\lz

Even though the resulting model looks like a GLM, we do not have valid standard errors for our coefficients,
so cannot provide confidence or prediction intervals or perform tests etc.
$\longrightarrow$ Post-selection inference



\framebreak

We can visualize the updates of the $\theta_j$ together with the number of iterations to see which base learner was selected when.

\begin{center}
\includegraphics[width=0.9\textwidth]{figure_man/componentwise-gb.png}
\end{center}

\framebreak

Depending on the value of $M$, a different number of features is included in the final model. $\rightarrow$ A sparse logistic regression is fitted

\begin{center}
\includegraphics[width=0.7\textwidth]{figure_man/mstop5.png}
\end{center}


\begin{center}
\includegraphics[width=0.7\textwidth]{figure_man/mstop12.png}
\end{center}

\end{vbframe}

\begin{vbframe}{Nonlinear base learners}

Nonlinear base learners like $P$- or $B$-splines make the model equivalent to a \textbf{generalized additive model (GAM)}, as long as the base learners keep their additive structure (which is the case for splines).
\vspace{0.5cm}

\begin{center}
\includegraphics[width=1\textwidth]{figure_man/NBL01.png}
\end{center}


\begin{center}
\includegraphics[width=1\textwidth]{figure_man/NBL02.png}
\end{center}

\framebreak

We are often interested in solutions that are as simple as possible. For example, we want to have a linear base learner if there is just a linear dependency and not an unnecessarily complex spline base learner.

\lz

A useful decomposition was proposed by Kneib~et~al. (2009): Each base learner is segmented into a constant, a linear and a nonlinear part. The boosting algorithm will automatically decide which feature to include -- linear, nonlinear, or none at all.

\vspace{-0.5cm}

\begin{align*}
b_j(x_j, \pmb\theta) & = b_{j,\text{const.}}(x_j, \pmb\theta) + b_{j,\text{lin.}}(x_j, \pmb\theta) + b_{j,\text{nonlin.}}(x_j, \pmb\theta)\\
 & = \theta_\text{const.} + x_j\cdot\theta_{j,\text{lin.}} + s_j(x_j, \pmb\theta_{j,\text{nonlin.}}),
\end{align*}

where
\begin{itemize}
  \item $\theta_\text{const.}$ is a constant term over all base learners.
  \item $x_j\cdot\theta_{j,\text{lin.}}$ is a feature-specific linear base learner.
  \item $s_j(x_j, \pmb\theta_{j,\text{nonlin.}})$ is a (centered) nonlinear base learner capturing deviation from the linear effect.
\end{itemize}

\end{vbframe}

\begin{vbframe}{Spam example: continued}

This time we fit a componentwise gradient boosting model to the spam dataset.

\lz

We specify a linear and nonlinear ($P$-spline) base learner for each feature and let the boosting model decide which to select.


\lz

To fit such a model we use the R package \texttt{mboost}.

% Unfortunately it's rather ugly to write down the decomposition

% <<spam-formula, eval = FALSE, echo = TRUE>>=
% formula <-
%   spam ~ bols(word_freq_make) + bbs(word_freq_make, df = 2, center = TRUE) +
%     bols(word_freq_address) + bbs(word_freq_address, df = 2, center = TRUE) +
%     bols(word_freq_all) + bbs(word_freq_all, df = 2, center = TRUE) +
% #  ...
% mboost(formula, data = spam, family = Binomial(), control = boost_control(mstop = 100))
% @

\lz

With $M = 100$ the model selects $17$ different base learners (out of 114).

\lz

These can be visualized nicely.

\framebreak

\vspace{0.3cm}

\begin{center}
\includegraphics[width=0.9\textwidth]{figure_man/spam-example.png}
\end{center}

\end{vbframe}

\begin{vbframe}{Fair Base Learner Selection}

\begin{itemize}

  \item
    Using splines and linear base learners in componentwise boosting will select the more complex spline base learner over the linear base learner.

  \item
    This makes it harder to achieve the desired behavior of the base learner decomposition as explained previously.

  \item
    To conduct a fair base learner selection we set the degrees of freedom of all base learners equal.

  \item
    The idea is to set the regularization/penalty term of a single learner in a manner that their complexity is treated equally.

\end{itemize}

\framebreak

Especially for linear models and GAMs it is possible to transform the degrees of freedom into a corresponding penalty term.
\begin{itemize}

  \item
    Parameters of the base learners are estimated via:
    $$
    \hat{\pmb\theta}_j^{[m]} = \left(\mathbf{Z}_j^T \mathbf{Z}_j + \lambda_j \mathbf{K}_j\right)^{-1}\mathbf{Z}_j^T \rmm\,,
    $$
    with $\mathbf{Z}_j$ the design matrix of the $j$-th base learner, $\lambda_j$ the penalty term, and $\mathbf{K}_j$ the penalty matrix.

  \item
    Having that kind of model, we use the hat matrix $\mathbf{S}_j = \mathbf{Z}_j\left(\mathbf{Z}_j^T \mathbf{Z}_j + \lambda_j \mathbf{K}_j\right)^{-1}\mathbf{Z}_j^T$ to define the degrees of freedom:
    $$
    \operatorname{df}(\lambda_j) = \trace\left(2\mathbf{S}_j - \mathbf{S}_j^T \mathbf{S}_j\right).
    $$
    \textbf{Note:} With $\lambda_j = 0$, $\mathbf{S}_j$ is the projection matrix into the target space with $\trace(\mathbf{S}_j) = \operatorname{rank}(\mathbf{X})$, which corresponds to the number of parameters in the model.

\end{itemize}

\framebreak

It is possible to calculate $\lambda_j$ by applying the Demmler-Reinsch orthogonalization (see Hofer et. al. (2011).\textit{\enquote{A framework for unbiased model selection based on boosting.}}). Suppose the following example with a GAM using splines with 24 parameters:
\begin{itemize}

  \item
    Setting $\operatorname{df} = 24$ gives $B$-splines with $\lambda_j = 0$.

  \item
    Setting $\operatorname{df} = 4$ gives $P$-splines with $\lambda_j = 418$.

  \item
    Setting $\operatorname{df} = 2$ gives $P$-splines with $\lambda_j = 42751174892$.

\end{itemize}

\begin{center}
\includegraphics[width=0.6\textwidth]{figure_man/df_to_lambda.pdf}
\end{center}

\end{vbframe}

\begin{vbframe}{Available base learners}

There is a large amount of possible base learners, e.g.:

\begin{itemize}
  \item Linear effects and interactions (with or without intercept)
  \item Uni- or multivariate splines and tensor product splines
  \item Trees
  \item Random effects and Markov random fields
  \item Effects of functional covariates
  \item ...
\end{itemize}

\lz

In combination with the flexible choice of loss functions, this allows boosting to fit  a huge number of different statistical models with the same algorithm. Recent extensions include GAMLSS-models, where multiple additive predictors are boosted to model different distribution parameters (e.g. conditional mean and variance for a Gaussian model).

\end{vbframe}



% \begin{vbframe}{RF vs AdaBoost vs GBM vs Blackboost}

% Again the Spirals data from mlbench. Blackboost: mboost with regression trees as base learners
% \end{vbframe}

\begin{vbframe}{Take-home message}

Componentwise gradient boosting is the statistical re-interpretation of gradient boosting.

\lz

We can fit a large number of statistical models, even in high dimensions ($p \gg n$).

\lz

A drawback compared to statistical models is that we do not get valid inference for coefficients $\rightarrow$ Post-Selection Inference.
% This can be (partially) solved by bootstrap inference or related methods.

\lz

In most cases, componentwise gradient boosting will have worse predictive performance than gradient boosting with trees. This is often because additive base learner motivated by regression models (LM, GLM, GAM) usually do not include higher-order interaction terms.

\end{vbframe}

\endlecture
\end{document}
