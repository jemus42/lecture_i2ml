\input{../../style/preamble} 
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-ensembles.tex}
\input{../../latex-math/ml-trees.tex}

\newcommand{\titlefigure}{figure_man/adaboost_example_adjusted.PNG}
\newcommand{\learninggoals}{
  \item Briefly cover the older AdaBoost and its general idea
  \item Understand difference between bagging and boosting
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Introduction to Boosting / AdaBoost}
\lecture{Introduction to Machine Learning}

% ------------------------------------------------------------------------------

\begin{vbframe}{Introduction to boosting}
  \begin{itemize}
    \item
      Boosting is one of the most powerful learning ideas since 1990.
    \item
      Originally designed for classification, (especially gradient) boosting handles regression (and many other supervised tasks) naturally nowadays.
    \item
      Homogeneous ensemble method (like bagging), but fundamentally different approach.
    \item
      {\bf Idea:} Take a weak classifier and sequentially apply it to modified versions of the training data.
    \item
      We will begin by describing an older, simpler boosting algorithm designed for binary classification, the popular \enquote{AdaBoost}.
  \end{itemize}
\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Boosting vs. Bagging}

  \begin{itemize}
      \item Homogeneous ensemble method (like bagging). 
  \item {\bf Idea:} Take a weak classifier and sequentially apply it to modified versions of the training data.
    \item Sequential not parallel model building, to minimize loss instead of variance reduction.
  \end{itemize}


% The general concept of boosting is a sequential fitting of weak learner on the error:
\begin{center}
\includegraphics[width=0.40\textwidth]{figure_man/bagging_vs_boosting.png}
\end{center}

% In bagging, the models are fitted parallel and not sequential.

\end{vbframe}

\begin{vbframe}{Boosting as a theoretical problem}

Boosting was developed as the answer to a theoretical problem:

\lz

\enquote{Does the existence of a weak learner for a certain problem imply
the existence of a strong learner?} (Kearns, 1988)

\lz

\begin{itemize}
\item \textbf{Weak learners} are defined as a prediction rule with a correct classification rate that is at least slightly better than random guessing (> 50\% accuracy on a balanced binary problem).
\item We call a learner a \textbf{strong learner} \enquote{if there exists a polynomial-time algorithm that achieves low error with high confidence for all concepts in the class} (Schapire, 1990).

\end{itemize}

% In practice it is typically easy to construct weak learners, but difficult to build a strong one.

% The proof of this ground-breaking idea generated the first boosting algorithm.

\begin{itemize}
    \item Any weak (base) learner can be iteratively boosted to become
a strong learner (Schapire and Freund, 1990).
  \item Idea was refined into \textbf{AdaBoost} (Adaptive Boosting).
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

% \section{AdaBoost}

\begin{vbframe}{AdaBoost}

% Any weak (base) learner can be iteratively boosted to become
% a strong learner (Schapire and Freund, 1990).
% The proof of this ground-breaking idea generated the first boosting algorithm.


% Leo Breiman (referring to the success of AdaBoost):
% \enquote{Boosting is the best off-the-shelf classifier in the world.}

% \framebreak

\begin{itemize}
  \item Assume binary classification with $y$ encoded as $\setmp$.
  \item We use binary classifiers as weak base learners (e.g., tree stumps) from a hypothesis space $\mathcal{B}$,
      denoted $\bl$ (or $\bl(\xv)$ or $\blxt$), which are hard labelers and output from $\setmp$.
  \item Decision score of the ensemble of size $M$ is weighted average:
    $$
    \fx = \sum_{m=1}^{M} \betam \blxt \in \R
    $$
  \item The base learner is sequentially applied to weighted training observations. 
      After each base learner fit, currently misclassified observations receive a higher weight for
    the next iteration, so we focus more on instances that are harder to classify.
\item BLs with higher predictive accuracy receive higher weights $\betam$.
  % \item The number of iterations $M$ is the main tuning parameter.
  % \item The discrete prediction function is $h(\xv) = \text{sign}(\fx) \in \setmp$.
\end{itemize}

% \framebreak
% 
% \begin{algorithm}[H]
%   \begin{algorithmic}[1]
%     \State Initialize observation weights: $w^{[1](i)} = \frac{1}{n} \quad \forall i \in \nset$
%     \For {$m = 1 \to M$}
%       \State Fit classifier to training data with weights $\wm$ and get $\blh$
%       \State Calculate weighted in-sample misclassification rate
%       $$
%         \errm = \frac{\sumin \wmi \cdot \mathds{1}_{\{\yi \,\neq\, \blh(\xi)\}}}{\sumin \wmi}
%       $$
%       \State Compute: $ \betamh = \frac{1}{2} \log \left( \frac{1 - \errm}{\errm}\right)$
%       \State Set: $w^{[m+1](i)} = \wmi \cdot \exp\left(\betamh \cdot
%         \mathds{1}_{\{\yi \,\neq\, \blh(\xi)\}} \right)$
%     \EndFor
%     \State Output: $\fxh = \sum_{m=1}^{M} \betamh \blh(\xv)$
%   \end{algorithmic}
%   \caption{AdaBoost}
% \end{algorithm}

%\end{vbframe}

\framebreak

\begin{algorithm}[H]
  \begin{algorithmic}[1]
    \State Initialize observation weights: $w^{[1](i)} = \frac{1}{n} \quad \forall i \in \nset$
    \For {$m = 1 \to M$}
      \State Fit classifier to training data with weights $\wm$ and get $\blh$
      \State Calculate weighted in-sample misclassification rate
      $$
        \errm = \sumin \wmi \cdot \mathds{1}_{\{\yi \,\neq\, \blh(\xi)\}}
      $$
      \State Compute: $ \betamh = \frac{1}{2} \log \left( \frac{1 - \errm}{\errm}\right)$
      \State Set: $w^{[m+1](i)} = \wmi \cdot \exp\left(- \betamh \cdot
        \yi \cdot \hat{h}(\xi)\right) $
      \State Normalize $w^{[m+1](i)}$ such that $\sumin w^{[m+1](i)} = 1$
    \EndFor
    \State Output: $\fxh = \sum_{m=1}^{M} \betamh \blh(\xv)$
  \end{algorithmic}
  \caption{AdaBoost}
\end{algorithm}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Adaboost illustration}
\begin{footnotesize}

\textbf{Example}

\begin{itemize}
  \item $n = 10$ observations and two features $x_1$ and $x_2$ 
  \item Tree stumps as base learners $\bl(\xv)$
  \item Balanced classification task with $y$ encoded as $\setmp$
  \item $M = 3$ iterations $\Rightarrow$ initial weights 
  $w^{[1](i)} = \frac{1}{10} \quad \forall i \in 1,\dots ,10$. 
  % \item The label of every observation is represented by triangle or circle.
  % \item Dark grey area: Base model in iteration $m$ predicts triangle.
  % \item Light grey area: Base model in iteration $m$ predicts circle.
\end{itemize}

\vfill

\begin{minipage}[b]{0.45\textwidth}
  \includegraphics[width=0.9\textwidth]{figure/adaboost_viz_mlr3_1.png}\\
  \textbf{Iteration} $m$ = 1:
  \begin{itemize}
    \item $\text{err}^{[1]} = 0.3$
    % \item The ratio $(1 - \errm) / \errm$ used to calculate the weights 
    \item $\hat{\beta}^{[1]} = \frac{1}{2} \log \left( \frac{1 - 0.3}{0.3} 
    \right) \approx 0.42$
  \end{itemize}
\end{minipage}%
\begin{minipage}[b]{0.55\textwidth}
  New observation weights:
  \begin{itemize}
    %\item $\wmi \cdot \exp \left(\betamh \cdot \mathds{1}_{\{\yi \neq \blh(\xi)\}} \right)$
    \item Prediction correct: \\
      $w^{[2](i)} = w^{[1](i)} \cdot \exp \left(-\hat \beta^{[1]} \cdot 1 
      \right)$\\ $\approx 0.065.$
    \item For 3 misclassified observations: \\
      $w^{[2](i)} = w^{[1](i)} \cdot \exp \left(-\hat \beta^{[1]} \cdot (-1) 
      \right)$\\ $\approx 0.15.$
    \item After normalization: 
    \begin{itemize}
      \begin{footnotesize}
      \item correctly classified: $w^{[2](i)} \approx 0.07$
      \item misclassified: $w^{[2](i)} \approx 0.17$
      \end{footnotesize}
    \end{itemize}
\end{itemize}
\end{minipage}

\end{footnotesize}

% ------------------------------------------------------------------------------

\framebreak

\begin{minipage}[c]{0.4\textwidth}
  \includegraphics[width = \textwidth]{figure/adaboost_viz_mlr3_2.png}
\end{minipage}%
\begin{minipage}[c]{0.05\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[c]{0.6\textwidth}
  \begin{footnotesize}
  \textbf{Iteration} $m = 2$:
  \begin{itemize}
    \item $\text{err}^{[2]} = 3 \cdot 0.07 = 0.21$ 
    \item $\hat{\beta}^{[2]} \approx 0.65$
    % \item The ratio $(1 - \errm) / \errm$ used to calculate the weights 
  \end{itemize}
  New observation weights:
  \begin{itemize}
    \item E.g., for 3 misclassified observations:
      $w^{[3](i)} = w^{[2](i)} \cdot \exp \left(-\hat \beta^{[1]} \cdot (-1) 
      \right) \approx 0.14.$
    \item After normalization: $w^{[3](i)} \approx 0.17$ (misclassified)
  \end{itemize}
  \textbf{Iteration} $m = 3$:
  \begin{itemize}
    \item $\text{err}^{[3]} = 3 \cdot 0.045 \approx 0.14$ 
    \item $\hat{\beta}^{[3]} \approx 0.92$
  % \item The ratio $(1 - \errm) / \errm$ used to calculate the weights 
  \end{itemize}
  \end{footnotesize}
\end{minipage}

\vfill

\begin{footnotesize}
\textbf{Note:} the smaller the error rate of a base learner, the larger the 
weight, e.g., $\text{err}^{[3]} \approx 0.14 < \text{err}^{[1]} \approx 0.3$ 
and $\hat \beta^{[3]} \approx 0.92 > \hat \beta^{[1]} \approx 0.42.$
\end{footnotesize}

\framebreak

% ------------------------------------------------------------------------------

With $\fxh = \sum_{m=1}^{M} \betamh \blh(\xv)$ and $h(\xv) = \text{sign}(\fx) 
\in \setmp$, \\we get:

\begin{center}
  \includegraphics[trim = 0 20 0 10, clip, width = 0.8\textwidth]{figure_man/adaboost_example_adjusted.png}
\end{center}

Hence, when all three base classifiers are combined, all samples are classified 
correctly.

\end{vbframe}

%\begin{vbframe}{Adaboost illustration}

%\begin{columns}
%\column{7cm}

%%\includegraphics[width=7cm]{figure_man/adaboost_example2.png}
%\includegraphics[width=7cm]{figure_man/adaboost_example_adjusted.PNG}
%%

%{\footnotesize Schapire, Boosting, 2012.}

%\column{3cm}

%The three base models are combined into one classifier.

%All observations are correctly classified.

%\end{columns}

%\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Bagging vs Boosting}

\begin{minipage}[t]{0.47\textwidth}
  \textbf{Random forest}
  \begin{itemize}
    \item Base learners are typically deeper decision trees (not only stumps!)
    \item Equal weights for BL
    \item BLs fitted independently
    \item Aim: variance reduction
    \item Tends \textbf{not} to overfit if ensemble size grows
  \end{itemize}
\end{minipage}%
\begin{minipage}[t]{0.06\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[t]{0.47\textwidth}
  \textbf{AdaBoost}
  \begin{itemize}
    \item BLs are weak learners, e.g., only stumps
    \item BL are weighted
    \item Sequential fitting of BLs
    \item Aim: loss reduction
    \item Tends to overfit with more iterations
  \end{itemize}
\end{minipage}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Bagging vs Boosting Stumps}

Random forest versus AdaBoost (both with stumps) on \texttt{spirals} data from 
\texttt{mlbench} ($n=200$, $sd=0$), with $5 \times 5$ repeated CV.

% \begin{minipage}[c]{0.6\textwidth}
%   \includegraphics[width = \textwidth]{figure_man/stump_plot_ntree.png}
% \end{minipage}%
% \begin{minipage}[c]{0.4\textwidth}
%   \begin{minipage}[c]{\textwidth}
%     \includegraphics[width = \textwidth]{figure_man/stump_plot_rf.png}
%   \end{minipage}
%   \begin{minipage}[c]{\textwidth}
%     \includegraphics[width = \textwidth]{figure_man/stump_plot_boost.png}
%   \end{minipage}
% \end{minipage}

\vfill

\includegraphics[width=\textwidth]{figure/stump_plots.png}

\vfill

Weak learners do not work well with bagging as only variance, but no bias reduction happens.

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Overfitting behavior}

Historically, the overfitting behavior of AdaBoost was often discussed.
% Random Forest versus AdaBoost on \texttt{Spirals} data from \texttt{mlbench} ($n=200$, $sd=0.3$).
% With $5 \times 5$ repeated CV
Increasing standard deviation in \texttt{spirals} to $sd = 0.3$ and allowing for more flexibility in 
the base learners, AdaBoost overfits with increasing number of trees while the 
RF only saturates.
The overfitting of AdaBoost here is quite typical as data is very noisy.

\vfill

\includegraphics[width=\textwidth]{figure/stump_plots_noisy.png}

\end{vbframe}

% ------------------------------------------------------------------------------

% \begin{vbframe}{Overfitting behavior}

% A long-lasting discussion in the context of AdaBoost is its overfitting behavior.

% % \lz

% % \emph{When a prediction rule concentrates too much on peculiarities of the specific sample of training observations, it will often perform poorly on a new data set.}

% % \lz

% The main instrument to avoid overfitting is the stopping iteration $M$:
% \begin{itemize}
% \item High values of $M$ lead to complex solutions. Overfitting?
% \item Small values of $M$ lead to simple solutions. Underfitting?
% \end{itemize}
% Although it will overfit eventually, AdaBoost in general shows a rather slow overfitting behavior.
% \framebreak

% As an example we have a look on random forest vs. AdaBoost on the \textit{Spiral} data
% from \texttt{mlbench} (n = 200, sd = 0.3). Performance (mmce) is measured
% with 5-fold CV.

% <<comparing-rf-ada1, echo=FALSE, fig.width=8, fig.height=4>>=
% load("rsrc/overfitting_ada_rf.Rdata")
% result$M = as.numeric(as.character(result$M))
% p = ggplot(result, aes(x = M, y = mmce, group = learner)) + 
%   geom_line()  +
%   xlab("number trees")
% p
% @

% AdaBoost overfits with increasing number of trees while
% the mmce of the random forest fluctuates on a constant level for the test data.
% \framebreak

% This becomes even more apparent when we take a look on the prediction surface. 
  
% <<comparing-rf-ada2, echo=FALSE, fig.width=13, fig.height=4.5 >>=
% load("rsrc/overfitting_plots.RData")
% learnerPredPlot1 = learnerPredPlot1  + ggplot2::ggtitle("rf") 
% learnerPredPlot2 = learnerPredPlot2 + ggplot2::ggtitle("adaboost")
% gridExtra::grid.arrange(learnerPredPlot1, learnerPredPlot2, ncol = 2)
% @

% \end{vbframe}

% % \begin{vbframe}{Software in R}
% %   \begin{itemize}
% %     \item Bagging: Package \pkg{mlr} is able to bag any learner with \code{makeBaggingWrapper()}.
% %   \item Random Forests: Package \pkg{randomForest} with function
% %     \code{randomForest()} based on CART.
% %   \item AdaBoost: included in the packages \code{ada} and \code{boosting}.
% % \end{itemize}
% % \end{vbframe}



\endlecture
\end{document}

