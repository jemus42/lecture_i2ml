\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-ensembles.tex}
\input{../../latex-math/ml-trees.tex}

\newcommand{\titlefigure}{figure/compboost-illustration-2.png}
\newcommand{\learninggoals}{
  \item Learn the concept of componentwise boosting and its relation to GLM
  \item Understand the built-in feature selection process
  \item Understand the problem of fair base learner selection
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Componentwise Gradient Boosting}
\lecture{Introduction to Machine Learning}

% ------------------------------------------------------------------------------

\begin{vbframe}{Componentwise gradient boosting}

Gradient boosting, especially when using trees, has strong predictive
performance but is difficult to interpret unless the base learners are stumps.

\lz

The aim of componentwise gradient boosting is to find a model that:

\begin{itemize}
  \item
    has strong predictive performance,

  \item
    has components that are still interpretable,

  \item
    does automatic selection of components,

  \item
    is sparser than a model fitted with maximum-likelihood estimation.
\end{itemize}

\lz

This is achieved by using \enquote{nice} base learners which yield familiar
statistical models
in the end.

\lz

Because of this, componentwise gradient boosting is also often referred to as \textbf{model-based boosting}.

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Base learners}

In classical gradient boosting only one kind of base learner $\mathcal{B}$ is used, e.g.,
regression trees.

\lz

For componentwise gradient boosting we generalize this to multiple base learner sets $\{ \mathcal{B}_1, ... \mathcal{B}_J \}$ with associated parameter spaces
$\{ \bm{\Theta}_1, ... \bm{\Theta}_J \}$,
% $$
%   % b_j^{[m]}(\xv,\pmb\theta^{[m]}) \quad j = 1,\dots, J\,,
%   \{ \bmm_j(\xv, \thetam): j = 1, 2, \dots, J \},
% $$
%
 where $j \in \{ 1, 2, \dots, J \}$ indexes the type of base learner.
%
\lz

Again, in each iteration, only the best base learner
% $\bmm_{\hat{j}}(\xv, \thetam)$
is selected and updated.

\framebreak

Common examples of base learners are

\begin{minipage}{0.4\textwidth}
    \includegraphics[width=\linewidth]{figure/compboost-base-learner-linear.png}
\end{minipage}\hfill
\begin{minipage}{0.5\textwidth}
  linear effect
\end{minipage}

\begin{minipage}{0.4\textwidth}
    \includegraphics[width=\linewidth]{figure/compboost-base-learner-spline.png}
\end{minipage}\hfill
\begin{minipage}{0.5\textwidth}
  non-linear (spline) effect
\end{minipage}

\begin{minipage}{0.4\textwidth}
    \includegraphics[width=\linewidth]{figure/compboost-base-learner-ridge.png}
\end{minipage}\hfill
\begin{minipage}{0.5\textwidth}
  dummy encoded linear model of a categorical feature
\end{minipage}

\begin{minipage}{0.4\textwidth}
    \includegraphics[width=\linewidth]{figure/compboost-base-learner-tensor.png}
\end{minipage}\hfill
\begin{minipage}{0.5\textwidth}
  product tensor of two base learners for interaction modelling (e.g. two splines)
\end{minipage}

\vspace{\baselineskip}

More advanced base learners could also be  Markov random fields, random effects, or trees.

\end{vbframe}
% ------------------------------------------------------------------------------

\begin{vbframe}{Componentwise boosting algorithm}

\input{algorithms/componentwise_gradient_boosting.tex}

\end{vbframe}
% ------------------------------------------------------------------------------
\begin{vbframe}{Additive model structure}
\begin{footnotesize}
We restrict these base learners to additive models, i.e., having a base learner $ b_j(\xv, \thetab^{[1]})$ and another base learner $b_j$ of the same type but with a different parameter vector $\thetab^{[2]}$, then it is possible to combine them to a new base learner of the same type:

$$
 b_j(\xv, \thetab^{[1]}) + b_j(\xv, \thetab^{[2]}) =
 b_j(\xv, \thetab^{[1]} + \thetab^{[2]}).
$$
\vspace*{0.1cm}

Thus, if $\{ b_j(\xv, \thetab^{[1]}), b_j(\xv, \thetab^{[2]}) \} \in \mathcal{B}_j$, then $b_j(\xv, \thetab^{[1]} + \thetab^{[2]}) \in \mathcal{B}_j$.
\lz

Often base learners are not defined on the entire feature vector $\xv$ but on
a single feature $x_j$:

$$
  b_j(x_j, \theta) \quad \text{for } j = 1, 2, \dots, p.
$$

This directly incorporates a variable selection mechanism into the fitting
process, since in each iteration only the best base learner is selected in
combination with the associated feature, and each base learner can be
(substantially) more complex than a stump (e.g., univariate linear effects or
splines).
\end{footnotesize}
\end{vbframe}



% ------------------------------------------------------------------------------

\begin{vbframe}{relation to glm}

In the simplest case we use linear models (without intercept) on single features
as base learners:

$$
  b_j(x_j,\theta) = \theta x_j  \quad \text{for } j = 1, 2, \dots, p \quad
  \text{and with } b_j \in \mathcal{B}_j = \{\theta x_j  ~\rvert~ \theta \in
  \mathbb{R} \}.
$$


This definition will result in an ordinary \textbf{linear regression} model.

% .\footnote{Note: a linear model base learner without intercept only makes sense if the covariates are centered (see \texttt{mboost} tutorial, page7)}


\begin{itemize}
  \item Note that linear base learners without intercept only make sense for
  covariates that have been centered before.
  \item If we let the boosting algorithm converge, i.e., let it run for a really
  long time, the parameters will converge to the \textbf{same solution} as the
  ML estimate.
  \item This means that, by specifying a loss function according to the negative
  likelihood of a distribution from an exponential family and defining a link
  function accordingly, this kind of boosting is equivalent to a (regularized)
  \textbf{generalized linear model (GLM)}.
\end{itemize}

\framebreak

% ------------------------------------------------------------------------------

But: We do not \emph{need} an exponential family and thus are able to fit models
to all kinds of other distributions and losses, as long as we can calculate (or
approximate) a derivative of the loss.
% Note, however, that this does not imply that the algorithm does something
% meaningful (e.g., non-convex loss functions would still require some
% additional effort).

\lz

Usually we do not let the boosting model converge fully, but \textbf{stop
early} for the sake of regularization and feature selection.

\lz

Even though the resulting model looks like a GLM, we do not have valid standard
errors for our coefficients,
so cannot provide confidence or prediction intervals or perform tests etc.
$\rightarrow$ post-selection inference.

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{intercept handling}

\textcolor{red}{@Janek}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{handling of categorical features}

\textcolor{red}{@BB}

\begin{itemize}
  \item Basically, there are two options for encoding categorical features:
  \begin{itemize}
    \item Specifying a single base learner for the entire variable
    \item One-hot encoding, i.e., creating a binary feature with associated
    base learner for each category
  \end{itemize}
  \item The \texttt{compboost} package currently implements the latter variant.
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{example: boston housing}

\begin{minipage}[c]{0.4\textwidth}
  \small
  \raggedright
  Consider the \texttt{Boston housing} regression task, for which we fit a
  CWB model with linear base learners (with intercept) to predict median home
  value.
  Using \texttt{compboost} with $M = 100$ iterations, we can
  visualize which base learner was selected when:
\end{minipage}%
\begin{minipage}[c]{0.05\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[c]{0.55\textwidth}
  \tiny
  \begin{tabular}{l|l}
    \textbf{variable} & \textbf{description} \\
    \hline
    medv &	median value of owner-occupied homes in k USD \\
    \hline
    crim & per capita crime rate by town \\
    zn &	proportion of residential land zoned for lots $>$ 25k sq.ft \\
    indus &	proportion of non-retail business acres per town \\
    chas &	Charles River dummy (1 if tract bounds river, 0 otherwise) \\
    nox &	nitric oxides concentration (parts per 10m) \\
    rm &	average number of rooms per dwelling \\
    age &	proportion of owner-occupied units built prior to 1940 \\
    dis &	weighted distances to five Boston employment centres \\
    rad &	index of accessibility to radial highways \\
    tax &	full-value property-tax rate per USD 10k \\
    ptratio &	pupil-teacher ratio by town \\
    b &	$1000(B - 0.63)^2$,  $B$ as proportion of blacks by town \\
    lstat &	percentage of lower status of the population \\
  \end{tabular}
\end{minipage}%

\vfill

\begin{center}
\includegraphics[width = \textwidth]{figure/compboost-illustration-1.png}
\end{center}

\framebreak

% ------------------------------------------------------------------------------

The number of features effectively included in the final model depends on the
number of total iterations $M$.

\vfill

$\rightarrow$ A sparse linear regression is fitted.

\vfill

\includegraphics[width = \textwidth]{figure/compboost-illustration-2.png}

\end{vbframe}



\begin{vbframe}{Relation to GLM - continued}

The following figure shows the parameter values after $m \in \{250, 500, 1000, 5000, 10000\}$ iterations as well as the estimates from a linear model as crosses (GLM with normally distributed errors):

\begin{center}
\includegraphics[width=\textwidth]{figure/compboost-to-glm-iter250.png}
\end{center}

\end{vbframe}

\begin{vbframe}{Relation to GLM - continued}

The following figure shows the parameter values after $m \in \{250, 500, 1000, 5000, 10000\}$ iterations as well as the estimates from a linear model as crosses (GLM with normally distributed errors):

\begin{center}
\includegraphics[width=\textwidth]{figure/compboost-to-glm-iter500.png}
\end{center}

\end{vbframe}

\begin{vbframe}{Relation to GLM - continued}

The following figure shows the parameter values after $m \in \{250, 500, 1000, 5000, 10000\}$ iterations as well as the estimates from a linear model as crosses (GLM with normally distributed errors):

\begin{center}
\includegraphics[width=\textwidth]{figure/compboost-to-glm-iter1000.png}
\end{center}

\end{vbframe}

\begin{vbframe}{Relation to GLM - continued}

The following figure shows the parameter values after $m \in \{250, 500, 1000, 5000, 10000\}$ iterations as well as the estimates from a linear model as crosses (GLM with normally distributed errors):

\begin{center}
\includegraphics[width=\textwidth]{figure/compboost-to-glm-iter5000.png}
\end{center}

\end{vbframe}

\begin{vbframe}{Relation to GLM - continued}

The following figure shows the parameter values after $m \in \{250, 500, 1000, 5000, 10000\}$ iterations as well as the estimates from a linear model as crosses (GLM with normally distributed errors):

\begin{center}
\includegraphics[width=\textwidth]{figure/compboost-to-glm-iter10000.png}
\end{center}

\end{vbframe}


\endlecture
\end{document}
