\input{../../style/preamble} 
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-bagging.tex}
\input{../../latex-math/ml-boosting.tex}
\input{../../latex-math/ml-trees.tex}

\newcommand{\titlefigure}{figure/gbm_sine_title}
\newcommand{\learninggoals}{
  \item Introducing the three main regularization options: number of iterations, tree depth and shrinkage
  \item Examples how regularization parameters influence model fit
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Gradient Boosting: Regularization}
\lecture{Introduction to Machine Learning}



\begin{vbframe}{Regularization and shrinkage}

If GB runs for a large number of iterations, it can overfit due to its aggressive loss
minimization.

\begin{blocki}{Options for regularization:}
\item Limit the number of boosting iterations $M$ (\enquote{early stopping}), i.e., limit the number of additive components.
\item Limit the depth of the trees. This can also be interpreted as choosing the order of interaction.
\item Shorten the learning rate $\betam$ of each iteration.
\end{blocki}

% The latter is achieved by multiplying $\betam$ with a small $\nu \in (0,1]$:
% $$ 
% \fm(\xv) = \fmd(\xv) + \betam b(\xv, \thetam) \,.
% $$

Note: If $\betam$ is found via line search, we multiply the next base learner by an additional constant \textbf{shrinkage parameter} $\nu \in (0,1]$ to shorten the step length.
In the case of a (commonly used) constant learning rate $\beta$, $\nu$ can be neglected by choosing a smaller value for $\beta$.
Hence, on the following slides, we call $\beta$ the shrinkage parameter or learning rate.


\framebreak

Obviously, the optimal values for $M$ and $\beta$ strongly depend on each other:
by increasing $M$ one can use a smaller value for $\beta$ and vice versa.

\lz

In practice, a common recommendation to find a good first model without tuning both parameters is thus to choose $\beta$ quite small and determine $M$ by cross-validation. 

\lz

It is probably best to tune all three parameters ($M$ and $\beta$ as well as 
the tree depths) jointly based on the training data
via cross-validation or a related method.

\end{vbframe}

\begin{vbframe}{Stochastic gradient boosting}

\textbf{Stochastic gradient boosting} is a minor modification to boosting to 
incorporate the advantages of bagging into the method.
The idea was formulated quite early by Breiman.

\lz

Instead of fitting on all the data points, a random subsample is drawn in each iteration.

\lz

Especially for small training sets, this simple modification often leads to
substantial empirical improvements.
How large the improvements are depends on data structure, size of the dataset,
base learner and size of the subsamples (so this is another tuning parameter).


\end{vbframe}



%\begin{vbframe}{Variable importance}

%As for random forests, we can construct a variable importance measure to help
%interpret the model.

%\lz
%For a regression tree $b$, we can define such a measure $I^2_j(b)$ as the sum over the reduction of
%impurity ($\rightarrow$ reduction of variance) at all inner knots where the tree splits with respect to feature $x_j$.

%\framebreak

%For an additive boosting model one just takes

%$$ I^2_j = \frac{1}{M} \sum_{m=1}^M I^2_j(\bmm) $$

%%From those importance values we take the squared root and scale them so that the most important feature gets the value 100.
%To get the relative influence measures, the resulting $I^2_j, j = 1, \dots, p$ are scaled so that they sum to 1.

%\lz
%For a $g$-class problem one has $g$

%$$ \fxk = \sum_{m=1}^M b_k^{[m]}(x), \quad
%  I^2_{jk} = \frac{1}{M} \sum_{m=1}^M I^2_j(b_k^{[m]}) $$

%$$ I^2_j = \frac{1}{g} \sum_{k=1}^g I^2_{jk} $$

%\end{vbframe}


\begin{vbframe}{Example: Spam detection}

% The data set we will examine briefly in the following was collected at the Hewlett Packard laboratories
% to train a personalized spam mail detector.

% \lz

% It contains data of 4601 emails. 2788 mails were regular mails and 1813 were spam.
% There are 57 numerical predictors available measuring e.g. the frequency of the most frequent words
% and special characters as well as runlengths of words in all capitals.

% \lz

% We use the R package \pkg{gbm}, which implements the introduced version of gradient boosting.

% \framebreak

% <<gbm-spam-example, eval = FALSE, echo = TRUE>>=
% library(gbm)
% data(spam, package = "ElemStatLearn")
% spam$spam = as.numeric(spam$spam) - 1 # gbm requires target to be 0/1
% gbm(spam ~ ., data = spam,
%   distribution = "bernoulli", # classification
%   n.trees = 100, # M = 100
%   interaction.depth = 2, # max. tree depth = 2
%   shrinkage = 0.001, # nu = 0.001
% )
% @
%
% \lz

We fit a gradient boosting model for different parameter values:

\begin{table}[]
\footnotesize
\centering
\begin{tabular}{l|l}
Parameter name      & Values                         \\
\hline
Loss        & Bernoulli (for classification) \\
Number of trees $M$ & $\{0, 1,\dots,10000\}$              \\
Shrinkage $\beta$     & $\{0.001, 0.01, 0.1\}$           \\
Max. tree depth     & $\{1, 3, 5, 20\}$
\end{tabular}
\end{table}

\vfill
We consider the 3-CV test error to find the optimal parameters (red):


% \framebreak

% Misclassification rates for different hyperparameter settings (shrinkage and maximum tree depth) of gradient boosting:

\begin{center}
\includegraphics[width=\textwidth]{figure/gbm_spam.png}
\end{center}

% \begin{figure}
%   \includegraphics[width=10cm, height=6cm]{figure_man/gbm_spam_effects.pdf}
% \end{figure}

% \framebreak

% \begin{figure}
  % \includegraphics[width=8cm]{figure_man/gbm_spam_imp_ggplot.pdf}
  % \caption{\footnotesize Variable Importance for model with $\nu = 0.1, M = 1380$ and tree depth of $4$.}
% \end{figure}

% \framebreak

% \begin{figure}
%  \includegraphics[width=6cm]{figure_man/gbm_spam_gbmperf.pdf}
%  \caption{\footnotesize Deviance}
% \end{figure}

%
% \begin{figure}
%   \includegraphics[width=8cm, height=5cm]{figure_man/gbm_spam_partdep.pdf}
%   \caption{\footnotesize Partial Dependency Plot for 2 important features.
%   Plotted is f in dependency of one feature, if all other features are integrated over.}
% \end{figure}

\end{vbframe}




% \begin{vbframe}{Visualization 2}
% 
% \begin{center}
%   \includegraphics[width=\textwidth]{figure/gbm_sine.png}
% \end{center}
% 
% \vfill
% 
% \footnotesize
% \begin{itemize}
%   \item Iterating this very simple tree-stump base learner yields a rather nice
%   approximation of a smooth model in the end.
%   \item Severe overfitting apparent in the noisy case. We will discuss and solve 
%   this problem later.
% \end{itemize}
% 
% \end{vbframe}

\begin{vbframe}{example: spirals data}

Consider the \texttt{spirals} data set with $\mathit{sd} = 0.1$ and $n = 300$.
We examine the effect of the shrinkage parameter, holding the number of trees 
fixed at 10k and choose a tree depth of 10:

\vfill

\includegraphics[width = \textwidth]{figure/gbm_regu_oversmoothing_overfitting}

\vfill

We observe an oversmoothing effect in the left scenario with strong 
regularization (i.e., very small learning rate) and overfitting when 
regularization is too weak (right). $\beta = 0.001$  yields a pretty good fit.

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{example: sinusoidal data}

Here we choose a tree-stump base learner to model univariate data 
with sinusoidal behavior.

% \vfill

\begin{center}
  \includegraphics[width=\textwidth]{figure/gbm_sine.png}
\end{center}

% \vfill

\small
\begin{itemize}
  \item Iterating this very simple base learner achieves a rather nice
  approximation of a smooth model in the end.
  \item Again, the model overfits the noisy case with less 
  regularization present.
\end{itemize}

\end{vbframe}

\endlecture
\end{document}
