\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-hpo.tex}


\newcommand{\titlefigure}{figure_man/train_valid_test.pdf}
\newcommand{\learninggoals}{
\item Understand how to fulfill the untouched test set principle by a 3-way split of the data
\item Understand how thereby the tuning step can be seen as part of a more complex training procedure}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}
\lecturechapter{Training - Validation - Test}
\lecture{Introduction to Machine Learning}

\begin{frame}{Tuning Problem}

\vskip 3em
Remember:\\
\vskip 3em
We need to 

\begin{itemize}
\item \textbf{select an optimal learner} 
\item without compromising the \textbf{accuracy of the performance estimate} for that learner
\end{itemize}

-- for that we need an \textbf{untouched test set}!

\end{frame}

\begin{vbframe}{Train - Validation - Test}
Simplest method to achieve this: a 3-way split
\begin{itemize}
\item During tuning, a learner is trained on the \textbf{training set},
  evaluated on the  \textbf{validation set}
\item After the best model configuration $\lams$ has been selected, we re-train on the joint (training+validation) set and evaluate the model's performance on the \textbf{test set}.
\end{itemize}
\begin{center}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1kBxfxdUzyUP2-_Y5kWWJ1TnWsm-C4AjHQeTaYPeFo24/edit?usp=sharing
\includegraphics[width=0.7\textwidth]{figure_man/train_valid_test.pdf}
\end{center}
\end{vbframe}


\begin{vbframe}{Tuning as part of model building}
\begin{itemize}
\item Effectively, the tuning step is now simply part of a more complex training procedure.
\item We could see this as removing the hyperparameters from the inputs of the algorithm and making it \enquote{self-tuning}.
\end{itemize}
\begin{center}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1JUtguuVBgidcqD0IdFFIiKH9zqYzM6YRjCqC53V90dA/edit?usp=sharing
\includegraphics[width=0.8\textwidth]{figure_man/autotune_in_model_fit.pdf}
\end{center}


\framebreak

More precisely: the combined training \& validation set is actually the training set for the \enquote{self-tuning} endowed algorithm. 

\vspace{1cm}
\begin{columns}[c, onlytextwidth]
\column{0.45\textwidth}
\hspace*{-0.3cm}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1kBxfxdUzyUP2-_Y5kWWJ1TnWsm-C4AjHQeTaYPeFo24/edit?usp=sharing
\includegraphics[width=1.5\textwidth]{figure_man/train_valid_test.pdf}
\column{0.45\textwidth}
\hspace*{-0.7cm}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1JUtguuVBgidcqD0IdFFIiKH9zqYzM6YRjCqC53V90dA/edit?usp=sharing
\includegraphics[width=1.5\textwidth]{figure_man/autotune_in_model_fit.pdf}
\end{columns}
\end{vbframe}

\endlecture
\end{document}
