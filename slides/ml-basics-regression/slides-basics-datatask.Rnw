% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../../style/preamble.Rnw")
@
% Load all R packages and set up knitr
<<setup-r, child="../../style/setup.Rnw", include = FALSE>>=
@

%! includes: basics-notation, basics-definition

\begin{frame}{Supervised Tasks and Data}

Supervised Learning comes in two flavours:

\begin{itemize}
\item \textbf{Regression}: Given features $x$, predict corresponding output from $\mathcal{Y} \in \mathbb{R}^m, 1 \leq m < \infty$.
\item \textbf{Classification}: Assigning an observation with features $x$ to one class of a finite set of classes $\mathcal{Y} = \{C_1,...,C_g\}, 2 \leq g < \infty$. (Details later.)
% \item \textbf{Density estimation}: Given an input $x$, predict the probability distribution $p(y|x)$ on $\mathcal{Y}$.
\end{itemize}

\end{frame}

\begin{vbframe}{Regression Task - Income Prediction} 
\begin{center}
  % FIGURE SOURCE: Screnshot from website (https://www.dice.com/salary-calculator)
  \includegraphics[width=\textwidth]{figure_man/salary_prediction.png}
\end{center}
\vspace{-0.5cm}
\begin{flushright}
  \tiny https://www.dice.com/salary-calculator
\end{flushright}

\end{vbframe}

\begin{vbframe}{More Regression Tasks}
\begin{enumerate}
\item \textbf{Predict house prices}
\medskip
\begin{itemize}
\item \textbf{Aim}: Predict the price for a house in a certain area
\item \textbf{Features}: e. g.
\begin{itemize}
\item square footage
\item number of bedrooms
\item district
\end{itemize}
\end{itemize}
\item \textbf{Predict the length-to-stay in hospital at the time of admission}
\begin{itemize}
\item \textbf{Aim}: Predict the number of days a single patient has to stay in hospital
\item \textbf{Features}: e. g.
\begin{itemize}
\item diagnosis category (heart disease, injury,...)
\item admission type (urgent, emergency, newborn,...)
\item age
\item gender
\end{itemize}
\end{itemize}
\end{enumerate}
\end{vbframe}


\begin{vbframe}{Data}

Imagine you want to investigate how salary and workplace conditions
affect productivity of employees. Therefore, you collect data about
their worked minutes per week (productivity), how many people work in the
same office as the employees in question and the employees' salary.

\begin{center}\includegraphics[width=0.6\textwidth]{figure_man/data_table} \end{center}

\end{vbframe}

\begin{frame}{Target and Features Relationship}

\begin{itemize}
\item For our observed data we know which outcome is produced
\item For new employees we can only observe the features, but not the target
\end{itemize}

\vspace{-0.5cm}

\scriptsize

% FIGURE SOURCE: https://drive.google.com/open?id=1WLPubv9vxLL-JIlHAtsvTBBG5pbF4xgRGW_prkOAEnE Page 2
\begin{center}\includegraphics[width=0.9\textwidth]{figure_man/new_data1_web} \end{center}

\normalsize

\vspace{-0.5cm}

\(\implies\) The goal is to predict the target variable for
\textbf{unseen new data} by using a \textbf{model} trained on the
already seen \textbf{training data}.\\

\end{frame}


\begin{vbframe}{Notation for Data}

\scriptsize

\begin{center}\includegraphics[width=0.6\textwidth]{figure_man/data_table} \end{center}

\normalsize

\vspace{-0.5cm}

In supervised machine learning, we are given a dataset
\[
\D = \Dset \subset \left(\Xspace \times \Yspace\right)^n.
\]

We call

\begin{itemize}
  \item $\Xspace$  the input space with $p = \text{dim}(\Xspace)$ (for now: $\Xspace \subset \R^p$),
  \item $\Yspace$ the output / target space (e.g. $\Yspace = \R$ for regression or $\Yspace = \{C_1, ..., C_g\}$, $g \ge 2$, for classification),
  \item the tuple \(\xyi\) $\in \Xspace\times \Yspace$ the \(i\)-th observation,
  \item $\bm{x}_j = \left(x^{(1)}_j, ..., x^{(n)}_j\right)$ the j-th feature vector
\end{itemize}

\end{vbframe}


\begin{vbframe}{Data Generating Process}

\begin{itemize}
\item We assume that there is a probability distribution
$$
\P_{xy}
$$

defined on $\Xspace \times \Yspace$ induced by the process that generates the observed data $\D$.
\item Depending on the context, we denote the random variables following this distribution by $\bm{x}$ and $y$.
\item Usually, we assume that the data is drawn i.i.d. from the joint probability density function (pdf) / probability mass function (pmf) $\pdfxy$.
\item Often, distributions are parameterized by a parameter $\theta \in \Theta$. We then write $\pdfxyt$.
\end{itemize}

\framebreak

\textbf{Remarks:}
\begin{itemize}
\item With a slight abuse of notation we write random variables, e.g., $\bm{x}$ and $y$, in lowercase, as normal
variables or function arguments. The context will make clear what is meant.
\item This lecture is mainly developed from a frequentist perspective. If parameters appear behind the |, this is
for better reading, and does not imply that we condition on them in a Bayesian sense (but this notation
would actually make a Bayesian treatment simple).
So formally, $p(x | \theta)$ should be read as $p_\theta(x)$ or $p(x, \theta)$ or $p(x; \theta)$.
\end{itemize}

\end{vbframe}




\endlecture

