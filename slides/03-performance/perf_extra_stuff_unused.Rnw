\begin{vbframe}{Regression: Defining a custom loss}

Assume a use case, where the target variable can have a wide range of values across different orders of magnitude.
A possible solution would be to use a loss functions that allows for better model evaluation and comparison.
The \textbf{Mean Squared Logarithmic Absolute Error} is not strongly influenced by large values due to the logarithm.

\[
\frac{1}{n} \sumin (\log(|\yi - \yih| + 1))^2
\]

<<echo=FALSE, out.width="0.7\\textwidth", fig.width = 7, fig.height = 3>>=
plotModAbsLogQuadLoss(data, model = model, pt_idx = c(1,4))
@
\end{vbframe}

######################################################################################

\begin{vbframe}{List of Classification Performance Measures}

\small

\begin{center}
\begin{tabular}{c l}
\hline

 \textbf{Classification}     & \textbf{Explanation}                                               \\
\hline
 \textit{Accuracy}           &  Fraction of correct classifications                               \\
 \textit{Balanced Accuracy}  &  Fraction of correct classifications in each class                 \\
 \textit{Recall}             &  Fraction of positives a classifier captures                       \\
 \textit{Precision}          &  Fraction of positives in instances predicted as positive          \\
 \textit{F1-Score}           &  Tradeoff between precision and recall                             \\
 \textit{AUC}                &  Measures calibration of predicted probabilities                   \\
 \textit{Brier Score}        &  Squared difference between predicted probability and  \\
  & true label   \\
 \textit{LogLoss}            &  Emphasizes errors for predicted probabilities close to \\
 & 0 and 1    \\
\hline
\end{tabular}
\end{center}

\normalsize

\end{vbframe}

######################################################################################

\begin{vbframe}{Bias-Variance Tradeoff}

We can decompose the generalization error for $L_2$-loss as follows:
\begin{eqnarray*}
  \GED &=& \E( L(y, \fhD(x)) | \D) \\
  &=& \E((y-\hat{f}(x))^2) \\
  &\overset{Var(y)=\E(y^2)-\E(y)^2}{=}& Var(y) + \E(y)^2 + Var(\hat f(x)) + \\
  &&\E(\hat f(x))^2 - 2 \E(y) \E(\hat f(x)) \\
  &\overset{\E(y) = f(x)}{=}& Var(y) + Var(\hat f(x)) + (f(x) - \E(\hat f(x)))^2\\
  &=& \sigma^2 + Var(\hat{f}(x)) + Bias(\hat{f}(x))^2
\end{eqnarray*}

where

\begin{itemize}
  \item $\sigma^2$: intrinsic variability of the data, cannot be avoided
  \item $Var(\hat f(x))$: variance of the model, the learners's tendency to learn random things irrespective of the signal (\textit{overfitting})
  \item $Bias(\hat f(x))^2$: systematic bias of the model (\textit{underfitting})
\end{itemize}

\end{vbframe}

\begin{vbframe}{Bias-Variance Tradeoff}

  \begin{itemize}
    \item We can reduce the model's variance on the cost of its bias and vice versa by controlling the model complexity.
    \item We search for the perfect Bias-Variance-Tradeoff that minimizes our expected prediction error.
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=7cm]{figure_man/bias_variance.png}
\end{figure}

\end{vbframe}


