\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure_man/domingos_biasvariance.png}
\newcommand{\learninggoals}{
  \item Understand how to decompose the generalization error of an inducer into 
  \begin{itemize}
    \item \footnotesize Bias of the inducer
    \item \footnotesize Variance of the inducer
    \item \footnotesize Noise in the data
  \end{itemize} 
  }

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Bias-Variance Decomposition}
\lecture{Introduction to Machine Learning}




\begin{vbframe} {Bias-Variance decomposition}

Let us take a closer look at the generalization error of a learning algorithm $\inducer_{L, O}$.
This is the expected error an induced model, on trainings sets of size $n$, when this is applied to a fresh, random test observation.
  $$\GEind = \E_{\D_n \sim \Pxy, (\xv, y) \sim \Pxy}\left( L\left(y, \fh_{\D_n}(\xv)\right) \right) = \E_{\D_n, xy}\left( L\left(y, \fh_{\D_n}(\xv)\right) \right)  $$
We therefore need to take the expectation over all training sets of size $n$, as well as the independent test observation.

\lz 

We assume that the data is generated by 
$$
y = \ftrue(\xv) + \epsilon\,,
$$
with normally distributed error $\epsilon \sim \mathcal{N}(0, \sigma^2)$ independent of $\xv$.  

\framebreak 

By plugging in the $L2$ loss $L(y, f(\xv)) = (y - f(\xv))^2$ we get

\begin{footnotesize}
\begin{eqnarray*}
\GEind &=& \E_{\D_n, xy}\left( L\left(y, \fh_{\D_n}(\xv)\right)\right) = \E_{\D_n, xy}\left(\left(y - \fh_{\D_n}(\xv)\right)^2\right) \\
&=& \E_{xy}\underbrace{\left[\E_{\D_n}\left(\left(y - \fh_{\D_n}(\xv)\right)^2~|~ \xv, y\right)\right]}_{(*)} 
\end{eqnarray*}
\end{footnotesize}

Let us consider the error $(*)$ conditioned on one fixed test observation $(\xv, y)$ first. (We omit the $~|~\xv, y$ for better readability for now.)

\begin{footnotesize}
\begin{eqnarray*}
(*) &=& \E_{\D_n}\left(\left(y - \fh_{\D_n}(\xv)\right)^2\right)\\
&=& \underbrace{\E_{\D_n}\left(y^2\right)}_{= y^2} + \underbrace{\E_{\D_n}\left(\fh_{\D_n}(\xv)^2\right)}_{(1)}  - 2\underbrace{\E_{\D_n}\left(y\fh_{\D_n}(\xv)\right)}_{(2)} 
% &=& y^2 + \underbrace{\E_{xy}^2\left(y\right)}_{(1)} + \underbrace{\E_{\D_n, xy}\left(\fh_{\D_n}(\xv)^2\right)}_{(2)}  - 2\underbrace{\E_{\D_n, xy}\left(y\fh_{\D_n}(\xv)\right)}_{(3)} \\
\end{eqnarray*}
\end{footnotesize}

by using the linearity of the expectation.  %and $\var(y) = \E(y^2) - \E^2(y)$. 

\framebreak
\begin{footnotesize}
$$
\E_{\D_n}\left(\left(y - \fh_{\D_n}(\xv)\right)^2\right) = 
y^2 + \underbrace{\E_{\D_n}\left(\fh_{\D_n}(\xv)^2\right)}_{(1)}  - 2\underbrace{\E_{\D_n}\left(y\fh_{\D_n}(\xv)\right)}_{(2)} =
$$
\end{footnotesize}


\begin{footnotesize}
By using that $\E(z^2) =\var(z) + \E^2(z)$, we see that
% \begin{eqnarray*}
$$
= y^2 + \var_{\D_n}\left(\fh_{\D_n}(\xv)\right) + \E^2_{\D_n}\left(\fh_{\D_n}(\xv)\right) - 2y \E_{\D_n}\left(\fh_{\D_n}(\xv))\right) 
$$
Plug in the definition of $y$
$$
 = \ftrue(\xv)^2 +2\epsilon\ftrue(\xv) + \epsilon^2 + \var_{\D_n}\left(\fh_{\D_n}(\xv)\right) + \E^2_{\D_n}\left(\fh_{\D_n}(\xv)\right) - 2 (\ftrue(\xv)+\epsilon)\E_{\D_n}\left(\fh_{\D_n}(\xv))\right) 
$$
Reorder terms and use the binomial formula
$$
= \epsilon^2 + \var_{\D_n}\left(\fh_{\D_n}(\xv)\right) + 
  \left(\ftrue(\xv) - \E_{\D_n}\left(\fh_{\D_n}(\xv)\right)\right)^2
 +2 \epsilon \left(\ftrue(\xv) - \E_{\D_n}\left(\fh_{\D_n}(\xv)\right)\right)
$$
% \end{eqnarray*}
\end{footnotesize}


\framebreak 

\begin{footnotesize}
$$
(*) = \epsilon^2 + \var_{\D_n}\left(\fh_{\D_n}(\xv)\right) + 
  \left(\ftrue(\xv) - \E_{\D_n}\left(\fh_{\D_n}(\xv)\right)\right)^2
 +2 \epsilon \left(\ftrue(\xv) - \E_{\D_n}\left(\fh_{\D_n}(\xv)\right)\right)
$$
\end{footnotesize}

Let us come back to the generalization error by taking the expectation over all fresh test observations $(\xv, y) \sim \Pxy$: 

\begin{footnotesize}
\begin{eqnarray*}
\GEind 
% &=& \var_{xy}(y) + \E_{xy}^2\left(\ftrue(\xv)\right) + \E_{xy}\left[\var_{\D_n}\left(\fh_{\D_n}(\xv) ~|~\xv, y\right)\right]  \\ &+& \E_{xy}\left[\E^2_{\D_n}\left(\fh_{\D_n}(\xv) ~|~\xv, y\right)\right]  - 2\E_{xy}\left[\E_{\D_n}\left(\ftrue(\xv)~|~\xv, y\right) \E_{\D_n}\left(\fh_{\D_n}(\xv)~|~\xv, y\right)\right] \\
% &=& \underbrace{\E_{xy}(y^2 - \ftrue(\xv)^2~|~\xv, y)}_{= \sigma^2} + \E_{xy}\left[\var_{\D_n}\left(\fh_{\D_n}(\xv) ~|~\xv, y\right)\right] \\ &+& \E_{xy}\left[\E^2_{\D_n}\left(\ftrue(\xv)-\fh_{\D_n}(\xv)~|~\xv, y\right)\right] \\
  &=& \underbrace{\sigma^2}_{\text{Variance of the data}} + \E_{xy}\underbrace{\left[\var_{\D_n}\left(\fh_{\D_n}(\xv) ~|~\xv, y\right)\right]}_{\text{Variance of inducer at } (\xv, y)} \\ &+& \E_{xy}\underbrace{\left[\left(\ftrue(\xv)-\E_{\D_n}\left(\fh_{\D_n}(\xv)\right)^2~|~\xv, y\right)\right]}_{\text{Squared bias of inducer at } (\xv, y)} + \underbrace{0}_{\text{As $\epsilon$ is zero-mean and independent}}
\end{eqnarray*}
\end{footnotesize}


\framebreak 

\begin{footnotesize}
$\GEind =$  
$$
 \underbrace{\sigma^2}_{\text{Variance of the data}} + \E_{xy}\underbrace{\left[\var_{\D_n}\left(\fh_{\D_n}(\xv) ~|~\xv, y\right)\right]}_{\text{Variance of inducer at } (\xv, y)} + \E_{xy}\underbrace{\left[\left(\ftrue(\xv)-\E_{\D_n}\left(\fh_{\D_n}(\xv)\right)^2~|~\xv, y\right)\right]}_{\text{Squared bias of inducer at } (\xv, y)}  
$$
\end{footnotesize}
\begin{enumerate}
  \item The first term expresses the variance of the data. 
    This is \textbf{noise} present in the data.
    Also called Bayes, intrinsic or unavoidable error.
    No matter what we do, we will never get below this error.
  \item The second term expresses how the predictions fluctuate on test-points on average, if we vary the training data. Expresses also the learner's tendency to learn random things irrespective of the real signal (overfitting).
  \item The third term says how much we are "off" on average at test locations (underfitting).
    Models with high capacity have low \textbf{bias} and models with low capacity have high \textbf{bias}.
\end{enumerate}


% So for the squared error loss, the generalized prediction error can be decomposed into

% \begin{itemize}
% \item \textbf{Noise}: Intrinsic error, independent from the learner, cannot be avoided
% \item \textbf{Variance}: Learner's tendency to learn random things irrespective of the real signal (overfitting)
% \item \textbf{Bias}: Learner's tendency to \textbf{consistently} misclassify certain instances (underfitting)
% \end{itemize}




% \framebreak

% For the $k$-NN learning algorithm, which outputs models of the shape $$\fh_{\D}(x) = \frac{1}{k}\sum_{i: \xi \in N_k(x)} \yi,$$
% the generalization error becomes 

% \begin{eqnarray*}
% \GE(\mathcal{I}_{L, O}) &=& \sigma^2 + \var\left(\fh_{\D}(\xv)\right) + \text{Bias}\left(\fh_{\D}(\xv)\right)^2 \\
% &=&\sigma^2 + \frac{\sigma^2}{k} + \E_x \left(f(\xv) - \frac{1}{k}\sum_{\xi \in N_k(\xv)}\fxi\right)^2 \\
% \end{eqnarray*}

% where we assumed for simplicity that training inputs $\xv^{(i)}$ are fixed and the randomness arises only from $y$.



\framebreak

\begin{figure}
    \centering
      \scalebox{0.8}{\includegraphics{figure_man/domingos_biasvariance.png}}
\caption{\textit{Left}: A model with high bias is unable to fit the curved relationship present in the data. \textit{Right}: A model with no bias and high variance can, in principle, learn the true pattern in the data. However, in practice, the learner outputs wildly different hypotheses for different training sets.}
\end{figure}




\end{vbframe}


\endlecture
\end{document}


