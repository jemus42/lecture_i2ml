\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure_man/lcurve_1.png}
\newcommand{\learninggoals}{
  \item Know that the capacity of a hypothesis space impacts generalization
  \item Know that low capacity carries the risk of underfitting
  \item Know that too high capacity carries the risk of overfitting
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Capacity \& Overfitting}
\lecture{Introduction to Machine Learning}



\begin{vbframe} {Capacity}
  \begin{figure}
    \centering
      \scalebox{0.7}{\includegraphics{figure_man/lcurve_1.png}}
      \tiny{\\ Credit: Ian Goodfellow}
  \end{figure}
  \vspace{-0.3cm}
  \begin{itemize}
    \item The performance of a learner depends on its ability to:
    \begin{itemize}
      \item Minimize the training error
      \item Generalize well to new data
    \end{itemize}
    \item Failure to obtain a sufficiently low training error is known as \textbf{underfitting}.
    \item On the other hand, if there is a large difference in training and test error, this is known as \textbf{overfitting}.
  \end{itemize}
   % {\tiny{Ian Goodfellow, Yoshua Bengio and Aaron Courville (2016). Deep Learning. \url{http://www.deeplearningbook.org/}}\par}
  \framebreak

  \begin{figure}
    \centering
      \scalebox{0.7}{\includegraphics{figure_man/lcurve_1.png}}
      \tiny{\\ Credit: Ian Goodfellow}
  \end{figure}
  \begin{itemize}
    \item The tendency of a model to over-/underfit is a function of its \textbf{capacity}, determined by the type of hypotheses it can learn.
    \item Loosely speaking, a model with low capacity can only learn a few simple hypotheses, whereas a model with large capacity can learn many, possibly complex, hypotheses.
    \item As the figure shows, the test error is minimized when the model neither underfits nor overfits, that is, when it has the right capacity.
  \end{itemize}
\end{vbframe}

\begin{vbframe} {Overfitting}
  \begin{itemize}
    \item The capacity (or \enquote{complexity}) of a model can be increased by increasing the size of the hypothesis space. 
    \item This (usually) also increases the number of learnable parameters. 
    \item Examples: Increasing the degree of the polynomial in linear regression, increasing the depth of a decision tree or a neural network, adding additional predictors, etc.  
    
  % Example - Polynomial regression 
  % \begin{itemize}
  %   \item Degree 1 : $ \hat{y} = \theta_0 + \theta_1 x $       
  %   \item Degree 2 : $ \hat{y} = \theta_0 + \theta_1 x + \theta_2x^2$
  %   \item Degree n : $ \hat{y} = \theta_0 + \sum_{i=1}^n \theta_ix^i$
  % \end{itemize}
  
  \item As the size of the hypothesis space increases, the tendency of a model to overfit also increases.
  \item Such a model might fit even the random quirks in the training data, thereby failing to generalize.

  \end{itemize} 
\end{vbframe}
\begin{vbframe} {Overfitting: Polynomial Regression}
    \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{figure_man/polynom_1.png}}
      % \caption{\footnotesize{Overfitting occurs when the the algorithm tries to model even the random quirks in the training data.}}
  \end{figure}
  
  \framebreak
  
    \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{figure_man/polynom_2.png}}
  \end{figure}
  
\end{vbframe}

\begin{vbframe} {Overfitting: Decision Trees}
    \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{figure_man/dectree_1.png}}
      % \caption{\footnotesize{Overfitting occurs when the the algorithm tries to model even the random quirks in the training data.}}
  \end{figure}
  
  \framebreak
    
    \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{figure_man/dectree_2.png}}
  \end{figure}
  
\end{vbframe}

\begin{vbframe} {Overfitting: k-nearest neighbors}
    \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{figure_man/knn_1.png}}
      % \caption{\footnotesize{Overfitting occurs when the the algorithm tries to model even the random quirks in the training data.}}
  \end{figure}
  
  \framebreak
    
    \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{figure_man/knn_2.png}}
  \end{figure}
  
\end{vbframe}

\endlecture
\end{document}

