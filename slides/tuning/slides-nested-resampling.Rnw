<<set-parent, include = FALSE>>=
knitr::set_parent("../../style/preamble.Rnw")
@

% Load all R packages and set up knitr
<<setup, child="../../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{Nested Resampling}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{Motivation}
In model selection, we are interested in selecting the best model from a set of potential candidate models (e.g., different model classes, different hyperparameter settings, different feature sets).

\begin{blocki}{Problem}
    \item We cannot evaluate our finally selected learner on the same resampling splits that we have used to perform model selection for it, e.g., to tune its hyperparameters.
    \item By repeatedly evaluating the learner on the same test set, or the same CV splits, information
      about the test set \enquote{leaks} into our evaluation.
    \item Danger of overfitting to the resampling splits / overtuning!
    \item The final performance estimate will be optimistically biased.
    \item One could also see this as a problem similar to multiple testing.
\end{blocki}
\end{vbframe}

\begin{vbframe}{Instructive and problematic example}
\begin{itemize}
    \item Assume a binary classification problem with equal class sizes.
    \item Assume a learner with hyperparameter $\lambda$.
    \item Here, the learner is a (nonsense) feature-independent classifier,
          where $\lambda$ has no effect. The learner simply
          predicts random labels with equal probability.
    \item Of course, it's true generalization error is 50\%.
    \item A cross-validation of the learner (with any fixed $\lambda$) will easily show this
      (given that the partitioned data set for CV is not too small).
    \item Now lets \enquote{tune} it, by trying out 100 different $\lambda$ values.
    \item We repeat this experiment 50 times and average results.
\end{itemize}

\framebreak

<<fig.height=3.5, echo = FALSE>>=
# rsrc data from rsrc/overtuning-example.R
ggd = BBmisc::load2("rsrc/overtuning-example.RData", "res2.mean")
ggd = reshape2::melt(ggd, measure.vars = colnames(res2), value.name = "tuneperf");
colnames(ggd)[1:2] = c("data.size", "iter")
ggd = BBmisc::sortByCol(ggd, c("data.size", "iter"))
ggd$data.size = as.factor(ggd$data.size)


pl = ggplot(ggd, aes(x = iter, y = tuneperf, col = data.size))
pl =  pl + geom_line() + ylab("Tuning Error") + xlab("Iteration") + labs(colour = "Data Size")
print(pl)
@

\begin{itemize}
\item Plotted is the best \enquote{tuning error} (i.e. the performance of the model with fixed $\lambda$ as evaluated by the cross-validation) after $k$ tuning iterations.
\item We have performed the experiment for different sizes of learning data
      that where cross-validated.
\end{itemize}

<<fig.height=3.5, echo = FALSE>>=

size = 200
p = 0.5
dens.x = seq(0, 1, length.out = 100)
myd = function(x, size, p) dbinom(round(x*size), p = p, size = size)
dens.y = myd(dens.x, p = p, size = size)
d.dens = data.frame(x = dens.x, y = dens.y)
sample_some_errs = function(k) rbinom(k, size = size, p = p) / size
errs = sample_some_errs(1000)
d.errs = data.frame(x = errs, y = 0.0)

plot_dens_with_errs = function(k) {
  pl = ggplot(mapping = aes(x = x, y = y)) + geom_line(data = d.dens)
  pl = pl + geom_point(data = d.errs[1:k,])
  minerr = min(d.errs[1:k, "x"])
  pl = pl + geom_vline(xintercept = minerr, col = "red")
  # pl = pl + geom_label()
  label = sprintf("n=%i; #runs=%i; best err=%.2f", size, k, minerr)
  pl = pl + ggtitle(label = label)
  pl = pl + xlab("Test Error") + ylab("Density") +  theme(plot.title = element_text(size = 12))
  return(pl)
}

pl1 = plot_dens_with_errs(k = 1)
pl2 = plot_dens_with_errs(k = 10)
grid.arrange(pl1, pl2, ncol = 2)
@

\begin{itemize}
\item For 1 experiment, the CV score will be nearly 0.5, as expected
\item We basically sample from a (rescaled) binomial distribution when we calculate error rates
\item And multiple experiment scores are also nicely arranged around the expected mean 0.5
\end{itemize}

\framebreak

<<fig.height=3.5, echo = FALSE>>=
pl3 = plot_dens_with_errs(k = 100)
pl4 = plot_dens_with_errs(k = 1000)
grid.arrange(pl3, pl4, ncol = 2)
@

\begin{itemize}
\item But in tuning we take the minimum of those! So we don't really estimate the "average performance" anymore, we get an estimate of "best case" performance instead.
\item The more we sample, the more "biased" this value becomes. 
\end{itemize}
\end{vbframe}

\begin{vbframe}{Untouched Test Set Principle}
\begin{itemize}
\item Again, simply simulate what happens in model application.
\item All parts of the model building (including model selection,
  preprocessing) should be embedded in the model-finding process
  \textbf{on the training data}.
\item The test set we should only touch once, so we have no way of \enquote{cheating}. The test dataset is only used once a model is completely trained, after deciding for example on specific hyper-parameters. Performances obtained from the test set are
  \textbf{unbiased estimates} of the true performance.
\item For steps that themselves require resampling (e.g., hyperparameter tuning) this results
  in two \textbf{nested resampling} loops, i.e., a resampling strategy for both tuning and outer evaluation.
\end{itemize}
\end{vbframe}


\begin{vbframe}{Tuning as part of model building}

\begin{itemize}
\item It conceptually helps to see the tuning step as now effectively part of a more complex training procedure.
 \item We could see this as removing the hyperparameters from the inputs of the algorithm and making it \enquote{self-tuning}.
 \end{itemize}

\begin{center}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1JUtguuVBgidcqD0IdFFIiKH9zqYzM6YRjCqC53V90dA/edit?usp=sharing
\includegraphics[width=0.8\textwidth]{figure_man/autotune_in_model_fit.pdf}
\end{center}
\end{vbframe}


\begin{vbframe}{Train Validation Test}

\begin{itemize}
\item Simple 3-way split; during tuning, a learner is trained on the training set,
  evaluated on the validation set
\item After the final model is selected, we fit on joint (training+validation) set and evaluate a final time on the test set
\end{itemize}
\begin{center}

% FIGURE SOURCE: https://docs.google.com/presentation/d/1kBxfxdUzyUP2-_Y5kWWJ1TnWsm-C4AjHQeTaYPeFo24/edit?usp=sharing
\includegraphics[width=0.8\textwidth]{figure_man/train_valid_test.pdf}
\end{center}

\framebreak

More precisely: the joint train + valid set is actually the training test for the "self-tuning" endowed algorithm. 

\vspace{1cm}
\begin{columns}[c, onlytextwidth]
\column{0.45\textwidth}
\hspace*{-0.3cm}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1kBxfxdUzyUP2-_Y5kWWJ1TnWsm-C4AjHQeTaYPeFo24/edit?usp=sharing
\includegraphics[width=1.5\textwidth]{figure_man/train_valid_test.pdf}
\column{0.45\textwidth}
\hspace*{-0.7cm}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1JUtguuVBgidcqD0IdFFIiKH9zqYzM6YRjCqC53V90dA/edit?usp=sharing
\includegraphics[width=1.5\textwidth]{figure_man/autotune_in_model_fit.pdf}
\end{columns}
\end{vbframe}

\begin{vbframe}{Nested Resampling}
As we can generalize holdout splitting to resampling, we can generalize the train+valid+test approach to nested resampling. This results in two nested resampling loops, i.e., a resampling strategy for both tuning and outer evaluation.


% FIGURE SOURCE: No source
\begin{center}\includegraphics[width = 0.7\textwidth]{figure_man/Nested_Resampling.png}\end{center}

\framebreak

Assume we want to tune over a set of candidate HP configurations $\lambda_i; i = 1, \dots$ with 4-fold CV in the inner resampling and 3-fold CV in the outer loop. The outer loop is visualized as the light green and dark green parts.

\vspace*{-0.3cm}

% FIGURE SOURCE: No source
\begin{center}\includegraphics[width = 0.7\textwidth]{figure_man/Nested_Resampling.png}\end{center}

\framebreak

\begin{footnotesize}
In each outer loop we do:
\begin{itemize}
\item Split off light green testing data
\item Run the tuner on the dark green part, e.g.,
  evaluate each $\lambda_i$ through 4CV on the dark green part
\end{itemize}
\end{footnotesize}

% FIGURE SOURCE: No source
\begin{center}\includegraphics[width = 0.7\textwidth]{figure_man/Nested_Resampling.png}\end{center}
 
\framebreak

\begin{footnotesize}
In each outer loop we do:
\begin{itemize}
\item Return the winning $\lambda^*$
\item Re-train the model on the full outer dark green train set
\item Predict on the outer light green test set
\end{itemize}
\end{footnotesize}

% FIGURE SOURCE: No source
\begin{center}\includegraphics[width = 0.7\textwidth]{figure_man/Nested_Resampling.png}\end{center}
  
\framebreak

\begin{footnotesize}
The error estimates on the outer samples (light green) are unbiased because this data was strictly excluded from the model-building process of the model that was tested on.
\end{footnotesize}

% FIGURE SOURCE: No source

\begin{center}\includegraphics[width = 0.7\textwidth]{figure_man/Nested_Resampling.png}\end{center}

\end{vbframe}

\begin{vbframe}{Nested Resampling - Instructive Example}

Taking again a look at the motivating example and adding a nested resampling outer loop, we get the expected behavior:

<<fig.height=3.5, echo = FALSE>>=
load("rsrc/overtuning_data.rds")
library(dplyr)
library(ggplot2)

df_overtuning %>%
  group_by(data_dim, tuning_method, number_points) %>%
  dplyr::summarize(performance = mean(performance)) %>%
  mutate(number_points = as.integer(as.character(number_points))) %>%
  ggplot(aes(x = number_points, y = performance, color = data_dim, linetype = tuning_method)) +
    geom_line() +
    xlab("Amount of tried hyperparameter configurations") +
    ylab("Tuned Performance") +
    scale_linetype_discrete(name = "")
@


\end{vbframe}

\endlecture
