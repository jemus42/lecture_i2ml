\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R



\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}
\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
  %Define standard arrow tip
  >=stealth',
  %Define style for boxes
  punkt/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=6.5em,
    minimum height=2em,
    text centered},
  % Define arrow style
  pil/.style={
    ->,
    thick,
    shorten <=2pt,
    shorten >=2pt,}
}
\usepackage{subfig}


% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
\newcommand{\titlefigure}{figure_man/riskmin_bilevel2.png}
\newcommand{\learninggoals}{
\item Understand the difference between model parameters and hyperparameters
\item Know different types of hyperparameters
\item Be able to explain the goal of hyperparameter tuning}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}



\begin{document}

% Load all R packages and set up knitr

% This file loads R packages, configures knitr options and sets preamble.Rnw as parent file
% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...








% Defines macros and environments
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-lm.tex}
\input{../../latex-math/ml-automl.tex}

%! includes: basics-riskminimization, evaluation-overfitting

\lecturechapter{Hyperparameter Tuning - Introduction}
\lecture{Introduction to Machine Learning}
\sloppy

\begin{vbframe}{Motivating Example} 

\begin{itemize}
\item Given a data set, we want to train a classification tree. 
\item We feel that a maximum tree depth of $4$ has worked out well for us previously, so we decide to set this hyperparameter to $4$. 
\item The learner ("inducer") $\inducer$ takes the input data, internally performs \textbf{empirical risk minimization}, and returns a fitted tree model $\fxh = f(\xv, \thetah)$ of at most depth $\lambda = 4$ that minimizes the empirical risk.
\end{itemize}

% FIGURE SOURCE: https://docs.google.com/presentation/d/14xwcs5zncTjFL4hIHAprjZMmyGIPqk5vs8DS32vEAvQ/edit?usp=sharing
\begin{center}
\begin{figure}
\includegraphics[width=0.6\textwidth]{figure_man/riskmin_bilevel1.png}
\end{figure}
\end{center}

\framebreak 

\begin{itemize}
\item We are \textbf{actually} interested in the \textbf{generalization performance} $\GEf$ of the estimated model on new, previously unseen data. 
\item We estimate the generalization performance by evaluating the model $\fh$ on a test set $\Dtest$: $$
\GEh{\Dtest}\left(\fh\right) = \frac{1}{|\Dtest|} \sum\limits_{(\xv, y) \in \Dtest} L\left(y, \fxh \right)
$$
\end{itemize}
\vspace*{-0.6cm}
% FIGURE SOURCE: https://docs.google.com/presentation/d/14xwcs5zncTjFL4hIHAprjZMmyGIPqk5vs8DS32vEAvQ/edit?usp=sharing
\begin{center}
\begin{figure}
\includegraphics[width=0.75\textwidth]{figure_man/riskmin_bilevel2.png}
\end{figure}
\end{center}

\framebreak 

\begin{itemize}
\item But many ML algorithms are sensitive w.r.t. a good setting of their hyperparameters,
  and generalization performance might be bad if we have chosen a suboptimal configuration:
\begin{itemize}
\item The data may be too complex to be modeled by a tree of depth $4$ 
\item The data may be much simpler than we thought, and a tree of depth $4$ overfits
\end{itemize}
\item[$\implies$] Algorithmically try out different values for the tree depth. For each maximum depth $\lambda$, we have to train the model \textbf{to completion} and evaluate its performance on the test set. 
\item We choose the tree depth $\lambda$ that is \textbf{optimal} w.r.t. the generalization error of the model. 
\end{itemize}

% $\to$ Finding the hyperparameter $\lambda$ that is optimal w.r.t. the generalization performance constitutes an optimization problem. 

\end{vbframe}

% \begin{vbframe}{The role of hyperparameters}

% \begin{itemize}
% \item Hyperparameters often control the complexity of a model, i.e., how flexible the model is.
% \item But they can in principle influence any structural property of a model or computational part of the training process.
% \item If a model is not flexible enough, it cannot approximate the relationship between the features and the output well and will underfit.
% \item If a model is too flexible so that it simply \enquote{memorizes} the training data, we will face the dreaded problem of overfitting.
% \item[$\implies$] Hence, controlling the model's capacity, i.e., finding suitable hyperparameters,
% can prevent overfitting the model on the training set.
% \end{itemize}

% \end{vbframe}

\begin{vbframe}{Model Parameters vs. Hyperparameters}

It is critical to understand the difference between model parameters and hyperparameters. 

\lz

\textbf{Model parameters} are optimized during training, typically via loss minimization. They are an \textbf{output} of the training. Examples:
\begin{itemize}
\item The splits and terminal node constants of a tree learner 
\item Coefficients $\thetab$ of a linear model $\fx = \thx$
\end{itemize}

\framebreak
  
In contrast, \textbf{hyperparameters} (HPs) are not decided during training. They must be specified before the training, they are an \textbf{input} of the training. 
Hyperparameters often control the complexity of a model, i.e., how flexible the model is.
But they can in principle influence any structural property of a model or computational part of the training process.

\lz

Examples:

\begin{itemize}
\item The maximum depth of a tree 
\item $k$ and which distance measure to use for $k$-NN
\item The number and maximal order of interactions to be included in a linear regression model
\end{itemize}

\end{vbframe}


\begin{vbframe}{Types of hyperparameters}

We summarize all hyperparameters we want to tune over in a vector $\lambdav \in \Lambda$ of (possibly) mixed type. HPs can have different types: 

\begin{itemize}
\item Real-valued parameters, e.g.:
\begin{itemize}
\item Minimal error improvement in a tree to accept a split
\item Bandwidths of the kernel density estimates for Naive Bayes 
\end{itemize}
\item Integer parameters, e.g.:
\begin{itemize}
\item Neighborhood size $k$ for $k$-NN
\item $mtry$ in a random forest
\end{itemize}
\item Categorical parameters, e.g.:
\begin{itemize}
\item Which split criterion for classification trees?
\item Which distance measure for $k$-NN?
\end{itemize}
\end{itemize}

Hyperparameters are often \textbf{hierarchically dependent} on each other, e.g., \emph{if} we use 
a kernel-density estimate for Naive Bayes, what is its width? 
  % with polynomials of the features up to a certain maximal degree $d$, then 
% we must specify whether to also include polynomial interaction terms like e.g. $x_j^{d-d'}x_m^{d'}$ or not 
% and up to which degree $d' \leq d$.
\end{vbframe}


\endlecture

\end{document}
