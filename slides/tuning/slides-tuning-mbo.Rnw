<<setup-child, include = FALSE>>=
knitr::set_parent("../../style/preamble.Rnw")
@
% Load all R packages and set up knitr
<<setup, child="../../style/setup.Rnw", include = FALSE>>=
@

%! includes: tuning-tuningtechniques

\lecturechapter{Hyperparameter Tuning - Advanced Tuning Techniques: MBO \& Hyperband}
\lecture{Introduction to Machine Learning}
\sloppy

\begin{vbframe}{Model-based Optimization}

Model-based optimization (MBO) is a sequential optimization procedure. We start with an initial design, i.e. a set of configurations $\lambda_i$ where we have evaluated the corresponding (resampling) performance. 

Repeat:
\begin{enumerate}
\item From the available performance measurements, we build a \textbf{surrogate model} that models the relationship between model-hyperparameters and estimated generalization error. It serves as a cheap approximation of the expensive objective. 
\item Based on information provided by the surrogate model, a new configuration $\lambda^{(\text{new})}$ is proposed: we pick a value for which the surrogate model predicts a large potential improvement over the already evaluated configurations.
\item The resampling performance of the learner with hyperparameter setting $\lambda^{(\text{new})}$ is evaluated and added to the set of design points.  
\end{enumerate}

\framebreak 

<<mbo, echo = FALSE, results = FALSE, fig.height = 4>>=
library(mlrMBO)
library(DiceKriging) #regr.km

test.fun = makeSingleObjectiveFunction(
  fn = function(x) x * sin(14 * x),
  par.set = makeNumericParamSet(lower = 0, upper = 1, len = 1L)
)

ctrl.ei = makeMBOControl()
ctrl.ei = setMBOControlTermination(ctrl.ei, iters = 4L)
ctrl.ei = setMBOControlInfill(ctrl.ei, crit = makeMBOInfillCritEI())

lrn.km = makeLearner("regr.km", predict.type = "se")

set.seed(112)
design = #data.frame(x = c(0.1, 0.3, 0.65, 1))
  data.frame(x = runif(7))

run.ei = exampleRun(test.fun, design = design, learner = lrn.km, control = ctrl.ei, show.info = FALSE)

renderExampleRunPlot(run.ei, 1)
@

\begin{footnotesize}
Upper plot: The surrogate model (black, dashed) models the \emph{unknown} relationship between input and output (black, solid) based on the initial design (red points).\\
Lower plot: Mean and variance of the surrogate model is used to derive the expected improvement (EI) criterion. The point that maximizes the EI is proposed (blue point). 
\end{footnotesize}

\framebreak 

<<mbo-2, echo = FALSE, results = FALSE, fig.height = 4>>=
renderExampleRunPlot(run.ei, 2)
@
\begin{footnotesize}
Upper plot: The surrogate model (black, dashed) models the \emph{unknown} relationship between input and output (black, solid) based on the initial design (red points).\\
Lower plot: Mean and variance of the surrogate model is used to derive the expected improvement (EI) criterion. The point that maximizes the EI is proposed (blue point). 
\end{footnotesize}
\framebreak

<<mbo-3, echo = FALSE, results = FALSE, fig.height = 4>>=
renderExampleRunPlot(run.ei, 3)
@
\begin{footnotesize}
Upper plot: The surrogate model (black, dashed) models the \emph{unknown} relationship between input and output (black, solid) based on the initial design (red points).\\
Lower plot: Mean and variance of the surrogate model is used to derive the expected improvement (EI) criterion. The point that maximizes the EI is proposed (blue point). 
\end{footnotesize}
\framebreak

<<mbo-4, echo = FALSE, results = FALSE, fig.height = 4>>=
renderExampleRunPlot(run.ei, 4)
@
\begin{footnotesize}
Upper plot: The surrogate model (black, dashed) models the \emph{unknown} relationship between input and output (black, solid) based on the initial design (red points).\\
Lower plot: Mean and variance of the surrogate model is used to derive the expected improvement (EI) criterion. The point that maximizes the EI is proposed (blue point). 
\end{footnotesize}
\framebreak

We iteratively perform those steps: (1) fit a surrogate model, (2) propose a new configuration, (3) evaluate the learner's performance and update the surrogate model. 
This guides us to \enquote{interesting} regions of $\Lambda$, and prevents us from searching irrelevant areas:

\vspace*{-0.5cm}

\begin{center}
\begin{figure}
\includegraphics[width=0.7\textwidth]{figure_man/res1.png}
\end{figure}
\end{center}


\end{vbframe}


\begin{vbframe}{Hyperband}

\begin{itemize}
\item It is extremely expensive to train complex models on large datasets
\item For many configurations, it might be clear early on that further training is not likely to significantly improve the performance
\item More importantly, the relative ordering of configurations (for a given dataset) can also become evident early on. 
\item \textbf{Idea:} \enquote{weed out} poor configurations early during training
\item One approach is \textbf{successive halving}: Given an initial set of configurations, all trained for a small initial budget, repeat:
\begin{itemize}
\item Remove the half that performed worst, double the budget
\item Continue until the new budget is exhausted
\end{itemize}  
\item Successful halving is performed several times with different trade-offs between the number of configurations considered and the budget that is spent on them. 
\end{itemize}

\framebreak 

Only the most promising configuration(s) are trained to completion: 

\begin{center}
\begin{figure}
\includegraphics[width=0.8\textwidth]{figure_man/hyperband3.png}
\end{figure}
\end{center}

\end{vbframe}

\begin{frame}{More tuning algorithms:}

Other advanced techniques besides model-based optimization and the hyperband algorithm are: 

\begin{itemize}
\item Stochastic local search, e.g. simulated annealing
\item Genetic algorithms / CMAES
\item Iterated F-Racing
\item many more $\ldots$
\end{itemize}


\end{frame}
\endlecture
