---
output:
  pdf_document: default
  html_document: default
---

# Learner: Representation, Evaluation and Optimization


## Linear Regression  

- Representation: $$ y = \beta^T x $$
- Evaluation: MSE / maximum likelihood
- Optimization: Exact solution or gradient descent


## Logistic regression

- Representation: $$ y = logit(\beta^Tx) $$
- Evaluation: Logistic loss / maximum likelihood
- Optimization: Iteratively weighted least squares 
  
## knn
Since knn has no real training phase, it is a bit different from the other machine learning algorithms.
You can argue that evaluation and optimization only takes place at the prediction step: 

- Representation: Training datapoints (+ distance measure and k)
- Evaluation (only local, in prediction step): mse for regression, local mmce
- Optimization (only local, in prediction step): exact solution for local means or local probability distributions

##  LDA and QDA

- Representation: $$ x | y = k \sim N(\mu_k, \Sigma_k) \text{ (for LDA: } \Sigma_k = \Sigma \text{ } \forall k \in \{1\ldots g\} \text{)}$$
- Evaluation: Maximum Likelihood
- Optimization: Exact solution

## CART
- Representation: Tree with binary splits
- Evaluation (only local): Gini (classification), MSE (regression)
- Optimization: Exhaustive and greedy split variable and split point search

## RandomForest
- Representation: Multiple tree with binary splits
- Evaluation (per tree and node): Gini (classification), MSE (regression)
- Optimization: Per tree and node: Exhaustive and greedy split variable and split point search. Additionally injection of randomness (bootstrap sampling and feature sampling) to decorrelate trees


