\begin{vbframe}{Tuning Pipelines}

\begin{itemize}
\item Many other factors like 
\begin{itemize}
\item optimization control settings, 
\item preprocesssing like scaling or centering of features
\item algorithmic variants in the fitting procedure 
\end{itemize}
can influence model performance in non-trivial ways. It is extremely hard to guess 
the correct choices here.
\item Even \textbf{model selection}, i.e., the choice of the learner itself (e.g. logistic regression, decision tree, random forest) can be seen as a hyperparameter tuning problem. 
\item In general, we might be interested in optimizing an entire ML \enquote{pipeline} (including preprocessing, feature engineering, model selection and all other model-relevant operations). 
\end{itemize}


\vskip 6em
\begin{figure}
%source?
\includegraphics[width = 1\textwidth]{figure_man/automl1.png}
\end{figure}


\end{vbframe}

















\begin{vbframe}{Hyperparameter tuning: definition}
  
  \begin{itemize}
    \item For a learning algorithm $\inducer$ (also inducer) with $d$ hyperparameters, the hyperparameter \textbf{configuration space} is:
      $$\bm{\Lambda}=\Lambda_{1} \times \Lambda_{2} \times \ldots \Lambda_{d}$$
      where $\Lambda_{i}$ is the domain of the $i$-th hyperparameter.
    \item The domains can be continuous, discrete or categorical.
    \item For practical reasons, the domain of a continuous or integer-valued hyperparameter is typically bounded.
    \item A vector in this configuration space is denoted as $\bm{\lambda} \in \bm{\Lambda}$.
    \item A learning algorithm $\inducer$ takes a (training) dataset $\D$ and a hyperparameter configuration $\lambdav \in \Lambda$ and returns a trained model (through risk minimization)

    \vspace*{-0.2cm}
  \begin{eqnarray*}
    \inducer: \left(\Xspace \times \Yspace\right)^n \times \Lambda &\to& \Hspace \\
    (\D, \lambdav) &\mapsto& \inducer(\D, \lambdav) = \hat f_{\D, \lambdav}
  \end{eqnarray*}
    % \item Additionally, some hyperparameters may only need to be specified if another hyperparameter (or combination of hyperparameters) takes on a certain value.
  \end{itemize}
 
  \framebreak 

  

  % We use the subscript $\lambdav$ in the trained model to emphasize that the model was trained with the inducer being configured by $\lambdav$. 

  % \lz 

  The goal in hyperparameter optimization is to minimize the generalization error of the inducer with respect to its hyperparameter $\lambdav$

  $$
    \lambdav^\star = \argmin_{\lambdav} GE_n(\inducer(\D, \lambdav)),
  $$ 

  where 

  $$
  GE_n(\inducer(\D, \lambdav)) = \E_{\D_n \sim \Pxy, (\xv, y) \sim \Pxy} \left(V\left(y, \hat f_{\D_n, \lambdav}(\xv) \right)\right)
  $$ 
  denotes the generalization error of the learning algorithm $\inducer(\D, \lambdav)$ measured by a validation loss $V$. 
 
  \framebreak 

  To calculate the (theoretical) generalization error, we need $\Pxy$. Usually, we do not know the data generating distribution. 

  \lz 

  As an approximation, we optimize its \textbf{empirical} version: 

  $$
    \lambdav^\star = \argmin_{\lambdav} \sum_{(\xi, \yi) \in \Dtest} V(\yi, \underbrace{\hat f_{\Dtrain, \lambdav}}_{= \inducer(\Dtrain, \lambdav)}(\xi)) , 
  $$

  where $\D_n = \Dtrain \cup \Dtest$ denotes a train-test-split$^{(*)}$. 

  \lz 

  Note that is is a \textbf{bi-level} optimization problem: the well-known risk minimization problem (i.e. finding $\hat f_{\Dtrain, \lambdav}$) is nested within the outer hyperparameter optimization problem (also \textbf{second-level} problem).  

  \vfill

  \begin{footnotesize}
  $^{(*)}$ Note that for the estimation of the generalization error, more sophisticated resampling strategies like cross-validation can be used.
  \end{footnotesize}

  % \lz 

  % Note that 

  %   In contrast to the first-level (empirical) risk minimization problem, hyperparameter optimization is also referred to as \textbf{second-level} optimization. The first-level problem can be seen as a subroutine called by the second-level problem: Each evaluation of $\lambdav$ requires to solve the first-level optimization problem.  

  \framebreak 

  Let us compare the first-level (theoretical / empirical) to the second-level hyperparameter optimization problem. In risk minimization, we 

  \begin{itemize}
    \item search for the \textbf{model} parameter $\thetab$
    \item that minimizes the (theoretical / empirical ) \textbf{risk} 
    \begin{footnotesize}  
      $$
      \thetah = \argmin_{\thetab} \risk(\thetab) \qquad \thetah = \argmin_{\thetab} \risket = \argmin_{\thetab} \sum_{(\xi, \yi) \in \Dtrain} L\left(\yi, \fxi\right). 
      $$
      \end{footnotesize}
  \end{itemize}

  In hyperparameter optimization, we

    \begin{itemize}
    \item search for the \textbf{inducer} hyperparameter $\lambdav$
    \item that minimizes the (theoretical / estimated) \textbf{generalization error}
    \begin{footnotesize}
      $$
        \lambdav^\star = \argmin_{\lambdav} GE_n(\inducer) \qquad \lambdav^\star = \argmin_{\lambdav} \sum_{(\xi, \yi) \in \Dtest} V\left(\yi, \inducer(\Dtrain, \lambdav)(\xi)\right). 
      $$
    \end{footnotesize}
  \end{itemize}

%   \framebreak 

%   \begin{itemize}
%     \item search for the \textbf{inducer} hyperparameter $\lambdav$
%     \item that minimizes the \textbf{generalization error}
%       $$
%         \min_{\lambdav} \E_{\D_n \sim \Pxy, (\xv, y) \sim \Pxy} \left(V\left(y, \hat f_{\D, \lambdav}(\xv)\right)\right). 
%       $$
%   \end{itemize}

% We compare: In empirical risk minimization, we 
  
%   \begin{itemize}
%     \item search for the \textbf{model} parameter $\thetab$
%     \item that minimizes the \textbf{empirical risk}
%       $$
%         \min_{\thetab} \sum_{(\xi, \yi) \in \Dtrain} L\left(\yi, \fxi\right). 
%       $$
%   \end{itemize}

%   In hyperparameter optimization, we

%   \begin{itemize}
%     \item search for the \textbf{inducer} hyperparameter $\lambdav$
%     \item that minimizes the \textbf{test error}
%       $$
%         \min_{\lambdav \in \Lambda} \sum_{(\xi, \yi) \in \Dtest} V\left(\inducer(\Dtrain, \lambdav)(\xi), \yi\right). 
%       $$
%   \end{itemize}

%   \framebreak 

    % \framebreak
  
  % The hyperparameter optimization problem is difficult in many ways: 
  \end{vbframe}

