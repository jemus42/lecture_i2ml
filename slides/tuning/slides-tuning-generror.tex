\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R



\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}
\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
  %Define standard arrow tip
  >=stealth',
  %Define style for boxes
  punkt/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=6.5em,
    minimum height=2em,
    text centered},
  % Define arrow style
  pil/.style={
    ->,
    thick,
    shorten <=2pt,
    shorten >=2pt,}
}
\usepackage{subfig}


% Defines macros and environments
\input{../../style/common.tex}
% \input{common.tex}

%\usetheme{lmu-lecture}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}



\begin{document}

% Load all R packages and set up knitr

% This file loads R packages, configures knitr options and sets preamble.Rnw as parent file
% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...




% Defines macros and environments
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-lm.tex}
\input{../../latex-math/ml-automl.tex}

%! includes: tuning-tuning-intro

\lecturechapter{Hyperparameter Tuning - Generalization Error}
\lecture{Introduction to Machine Learning}
\sloppy

\begin{vbframe} {Generalization Error}

Let us take a closer look at the generalization error of a learning algorithm $\inducer_{L, O}$ with associated loss $L$  and optimization strategy $O$. \\
For a training set of size $n$, i.e., $\D_n$, the induced model
$$\fh_{\D_n} := \inducer_{L, O}(\D_n, \hat\lambdav), $$
where the hyperparameter setting $\hat\lambdav$ is found using $O$, s.t. 
$$\hat\lambdav \approx \min_{\lambdav \in \bm{\Lambda}} \GEh{\Dtest}\left(\inducer_{L,O}(\D_n, \lambdav)\right).$$
The \textbf{generalization error} is the expected error of $\fh_{\D_n}$, on training sets $\D_n$, when this is applied to a fresh, random test observation, i.e.,
  $$\GEind := \E_{\D_n \sim \Pxy, (\xv, y) \sim \Pxy}\left( L\left(y, \fh_{\D_n}(\xv)\right) \right) = \E_{\D_n, xy}\left( L\left(y, \fh_{\D_n}(\xv)\right) \right).$$
We therefore need to take the expectation over all training sets of size $n$, as well as the independent test observation.

\lz 

Note:
\begin{itemize}
\item We estimate the generalization error $\GEind$ with the generalization performance $\GEh{\Dtest}$ in the optimization step when we induce $\fh_{\D_n}$. This is valid since we do not violate the principle of the untouched test set.
\item However, now we have used $\Dtest$ to find $\hat\lambdav$, i.e., we can not use $\Dtest$ for the estimate of $\GEind$ of our final model $\fh_{\D_n}$ since it would be too optimistic.
\item Hence, we need a more sophisticated technique, such as nested resampling, to estimate $\GEind$ without violating the principle of the untouched test set.
\end{itemize}

\end{vbframe}

\begin{vbframe} {Bias-Variance decomposition}
If we assume that the data is generated by 
\begin{footnotesize}
$$
y = \ftrue(\xv) + \epsilon\,,
$$
\end{footnotesize}
with error $\epsilon \sim \mathcal{N}(0, \sigma^2)$ independent of $\xv$, it can be shown that 

\begin{footnotesize}

$\GEind =$  
$$
 \underbrace{\sigma^2}_{\text{Variance of the data}} + \E_{xy}\underbrace{\left[\var_{\D_n}\left(\fh_{\D_n}(\xv) ~|~\xv, y\right)\right]}_{\text{Variance of inducer at } (\xv, y)} + \E_{xy}\underbrace{\left[\left(\ftrue(\xv)-\E_{\D_n}\left(\fh_{\D_n}(\xv)\right)^2~|~\xv, y\right)\right]}_{\text{Squared bias of inducer at } (\xv, y)}  
$$
\end{footnotesize}
\begin{enumerate}
  \item The first term expresses the variance of the data. 
    This is \textbf{noise} present in the data.
    Also called Bayes, intrinsic or unavoidable error.
    No matter what we do, we will never get below this error.
  \item The second term expresses how the predictions fluctuate on test-points on average, if we vary the training data. Expresses also the learner's tendency to learn random things irrespective of the real signal (overfitting).
  \item The third term says how much we are "off" on average at test locations (underfitting).
    Models with high capacity\footnote[frame]{Loosely speaking, a model with low capacity can only learn a few simple hypotheses, whereas a model with large capacity can learn many, possibly complex, hypotheses.} tend to have low \textbf{bias} and models with low capacity tend to have high \textbf{bias}.
\end{enumerate}
\framebreak
% So for the squared error loss, the generalized prediction error can be decomposed into

% \begin{itemize}
% \item \textbf{Noise}: Intrinsic error, independent from the learner, cannot be avoided
% \item \textbf{Variance}: Learner's tendency to learn random things irrespective of the real signal (overfitting)
% \item \textbf{Bias}: Learner's tendency to \textbf{consistently} misclassify certain instances (underfitting)
% \end{itemize}




% \framebreak

% For the $k$-NN learning algorithm, which outputs models of the shape $$\fh_{\D}(x) = \frac{1}{k}\sum_{i: \xi \in N_k(x)} \yi,$$
% the generalization error becomes 

% \begin{eqnarray*}
% \GE(\mathcal{I}_{L, O}) &=& \sigma^2 + \var\left(\fh_{\D}(\xv)\right) + \text{Bias}\left(\fh_{\D}(\xv)\right)^2 \\
% &=&\sigma^2 + \frac{\sigma^2}{k} + \E_x \left(f(\xv) - \frac{1}{k}\sum_{\xi \in N_k(\xv)}\fxi\right)^2 \\
% \end{eqnarray*}

% where we assumed for simplicity that training inputs $\xv^{(i)}$ are fixed and the randomness arises only from $y$.




\end{vbframe}
\end{document}

\endlecture
