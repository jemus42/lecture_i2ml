\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure_man/cost-curves-1.png}
\newcommand{\learninggoals}{
  \item \textcolor{blue}{XXX}
  \item \textcolor{blue}{XXX}
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Cost Curves}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{Cost curves}

\begin{itemize}
  \item \textbf{Cost curves} directly plot the relative costs / misclassification error to determine the best classifier (ROC isometrics allow this only indirectly).
  %\item Determining the superiority of a classifier can be difficult using the ROC isometrics.
  \item Cost curves incorporate similar information as ROC curves but are easier, especially in case of different misclassification costs or class distributions. %, or more generally the skew are known.
\end{itemize}
%\tiny{Source for material: Evaluating Learning Algorithms Chapter 4.4.3}
\vspace{-0.1cm}
\begin{minipage}{0.5\textwidth}
\textbf{Example:} Classifier $f_2$ dominates $f_1$ until both ROC curves cross. Then, $f_1$ dominates $f_2$.

\textbf{BUT:} It is hard to tell for which threshold, costs, or class distributions $f_2$ works better than $f_1$.

$\Rightarrow$ Cost curves provide this kind of information.
\end{minipage}
\begin{minipage}{0.49\textwidth}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figure_man/cost-curves-1.png}
    \tiny{\\ Credit: Nathalie Japkowicz  \\}
\end{figure}



%\includegraphics[width=\textwidth]{figure_man/cost-curves-1.png}
\end{minipage}
\vspace{1.5 cm}
{\tiny{Nathalie Japkowicz (2004): Evaluating Learning Algorithms : A Classification Perspective. (p. 125)}}



\end{vbframe}


\begin{vbframe}{Cost curves}
\begin{footnotesize}
\begin{itemize}
  \item Simplifying assumption: equal misclassification costs, i.e., $cost_{FN} = cost_{FP}$.
  \item The misclassification cost (or misclassification error in the case of $cost_{FN} = cost_{FP}$) is plotted as a function of the proportion of positive instances $\rp$.
  \item Cost curves are pointâ€“line duals of ROC curves, i.e., a single classifier is represented by a point in the ROC space and by a line in the cost space.
\end{itemize}
\end{footnotesize}
\vspace{-0.5  cm}
\begin{figure}
    \centering
    \scalebox{0.75}{\includegraphics[width=\textwidth]{figure_man/cost-curves-0.png}}
    \tiny{\\ Credit: Chris Drummond and Robert C. Holte  \\}
\end{figure}
\vspace{-0.25cm}
{\tiny{Chris Drummond and Robert C. Holte (2006): Cost curves: An improved method for visualizing classifier performance. Machine Learning, 65, 95-130. \emph{\url{https://www.semanticscholar.org/paper/Cost-curves\%3A-An-improved-method-for-visualizing-Drummond-Holte/71708ce984e0896e7383435913547e770572410e}}}\par}

\end{vbframe}



\begin{vbframe}{Cost curves}

\begin{itemize}
  %\item The convex hull of the ROC space is the lower envelope created of all classifier lines in the cost space.
  %\item The misclassification error is plotted as a function of the probability of an observation being from the positive class.
  \item Functional form of the cost curve of a classifier:
  $error = (FNR - FPR) \cdot \rp + FPR$ %, \;\;\;$ (Note: $P(+) = \rp$)
\end{itemize}
%\vspace{-15pt}
\begin{center}
\includegraphics[width=0.85\textwidth]{figure_man/cost-curves-3.png}
\end{center}






\end{vbframe}


\begin{vbframe}{Cost curves}
\begin{footnotesize}
\begin{itemize}
  \item Horizontal dashed line: worst classifier, i.e. 100\% error rate for all $\rp$; x-axis as perfect classifier.
  \item Dashed diagonal lines: trivial classifiers, i.e., ascending diagonal always predicts negative instances and vice versa.
  \item Descending/ascending straight lines:
  two families of classifiers $A$ and $B$ (represented by points in their respective ROC curves).
\end{itemize}
\end{footnotesize}
\vspace{-0.4cm}
\begin{center}
  \scalebox{0.65}{\includegraphics{figure_man/cost-curves-2.png}}
  \tiny{\\ Credit: Nathalie Japkowicz  \\}
\end{center}

\end{vbframe}
%
% \begin{vbframe}{Cost curves}
%
% \textbf{Example:} In position (0.4, 0.25) B classifier loses its
% performance to one of the A classifiers (point where ROC curves cross). But
% there is no practical information about when classifier A should be used
% over B. In contrast the cost graph tells us that for $0.26 \leq \rp < 0.4$
% classifier B is preferred and for $0.4 \leq \rp < 0.48$ classifier A1 is
% preferred.
%
% \begin{center}
% \includegraphics[width=0.5\textwidth]{figure_man/cost-curves-2.png}
% \end{center}
%
% \end{vbframe}

\begin{vbframe}{ROC curves vs. cost curves}

\begin{itemize}
  \item In some cases, cost curves provide more practical relevant information
  than ROC curves.
  \item ROC curves can tell us that, sometimes, classifier A is superior to B, but
  we cannot really tell in which settings.
  \item Here, we assumed that the classes had similar classification costs.
  \item However, there is an extension to the cost curve concept which allows for different costs in each class ($\rightarrow$ simple
  modification where the identity of the axes is changed).
  %\item Then, the y-axis represents the normalized expected cost (NEC) or relative expected misclassification cost.
  \end{itemize}



\end{vbframe}

%
% \begin{vbframe}{ROC curves vs. cost curves}
% The general form is as follows:
%
% \vspace{-0.5cm}
% \begin{center}
% \begin{equation*}
% NEC = \text{FNR} \cdot P_C[+] + \text{FPR} \cdot (1 - P_C[+]),
% \end{equation*}
% \end{center}
%
% \begin{itemize}
% \item FNR/FPR are false-negative rate and false-positive rate respectively and
% $P_C[+]$, the probability cost function (modified version of $P[+]$ that takes costs into
% consideration).
%
% \vspace{-0.5cm}
% \begin{center}
% \begin{equation*}
% P_C[+] = \frac{P[+] \cdot C[+|-]}{P[+] \cdot C[+|-] + P[-] \cdot C[-|+]},
% \end{equation*}
% \end{center}
%
% \item $C[+|-]$ and $C[-|+]$ represent the cost of predicting a positive when the
% instance is actually negative and vice versa and $P[-]$ as the probability of
% being in the negative class.
% \end{itemize}
% \end{vbframe}

\endlecture
\end{document}
