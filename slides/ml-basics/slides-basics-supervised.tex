\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R



\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}
\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
  %Define standard arrow tip
  >=stealth',
  %Define style for boxes
  punkt/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=6.5em,
    minimum height=2em,
    text centered},
  % Define arrow style
  pil/.style={
    ->,
    thick,
    shorten <=2pt,
    shorten >=2pt,}
}
\usepackage{subfig}


% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}



\begin{document}
% Set style/preamble.Rnw as parent.

% Load all R packages and set up knitr

% This file loads R packages, configures knitr options and sets preamble.Rnw as parent file
% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...








%! includes: basics-notation, basics-whatisml

\lecturechapter{Introduction: Supervised Learning}
\lecture{Introduction to Machine Learning}

\sloppy

\begin{vbframe}{Functions - From Object to Outcome}

\begin{itemize}
\item In \textbf{supervised ML} we try to find a mapping $f()$ that connects (the description of) an object to a certain aspect of that object.

$$f(\textrm{object}) = \textrm{target}$$

\item So we would like to compute / predict / describe the target outcome, given an object from a certain set of admissible objects.

\item This mapping $f$ must be computable by a machine.
\end{itemize}

\end{vbframe}

\begin{vbframe}{Computable Functions}

\begin{itemize}
\item This seems like a very general and powerful concept. 
  Computable function are also called \emph{algorithms}.
\item For many kinds of problems, the mapping $f$ can be constructed explicitly, either as an actual mathematical mapping or in terms of a finite number of well-defined computable steps, i.e.,  as an algorithm that computes $f$, for example:
\begin{itemize}
\item $f(\textrm{list of elements}) = \textrm{count of distinct elements}$
\item $f(\textrm{polynomial function}) = \textrm{locations of extrem points}$
\item $f(\textrm{map of roads}) = \textrm{shortest route from A to B}$
\end{itemize}
\item Computer scientists study which kinds of functions are computable, how algorithms to compute a given $f$ scale in terms of input sizes, and how to properly construct and implement them.
\item Obviously, such manual or explicit construction of $f$ varies from entirely simple and obvious to nearly or fully impossible.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Learning Functions on a Machine}
\begin{itemize}
\item For many important problems, defining and implementing $f$ \enquote{manually} is difficult to impossible, e.g.:
$$f(\textrm{person}) = \textrm{probability of developing breast cancer}$$
\item \textbf{Supervised ML aims at automatically constructing $f$ from a set of example objects where we already know the target outcome.}  This mapping $f$ is then called a \textbf{model}.
\item Humans are pretty good at this, at least for some domains. We learn that for objects exhibiting certain  patterns or properties, certain outcomes are much more likely.\\
This pattern recognition occurs implicitly and gradually through examples over time, without an explicit construction of an \enquote{algorithm}. Hence, \emph{learning}. 
\item \emph{Supervised} learning because the target is known for all the example objects we use for learning, so we can \enquote{supervise} how well a model $f$ works by comparing its outputs (i.e, its \textbf{predictions}) to the target outcomes we have actually observed.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Feature Vectors}

\begin{itemize}
\item In order to simplify this problem we (usually) assume that the input for the model $f$ 
is not the (often complex) object itself, but rather a list of \textbf{features} that describe it.
\item this list of features must be available for all objects, i.e., measurable or computable.
\item E.g
\begin{align*}
\begin{split}
  f(\textrm{age, sex, gene expression, ...}) =\\
   \textrm{prob. of developing breast cancer}
\end{split}
\end{align*}
\end{itemize}

\end{vbframe}

\begin{vbframe}{Parameters, Statistics and Supervised Machine Learning}

\begin{itemize}
\item Supervised ML additionally assumes that $f$ is of a certain \enquote{form}
or comes from a certain \emph{class of functions}.\\
This is necessary to make the problem of automatically finding a \enquote{good} model feasible at all.
\item The specific behavior of a mapping from this class can then be described by \textbf{parameters} which defines its shape.
\item Statistics also studies how to learn such functions (or, rather: their parameters) from example data and how to perform inference on them and interpret the results.
\item For historical reasons, statistics is mostly focused on fairly simple classes of mappings like (generalized) linear models.
\item Supervised ML also includes more complex kinds of mappings that can often deal with more complicated and high-dimensional inputs.
\end{itemize}
\end{vbframe}


\begin{frame}{Summary}

\textbf{Supervised machine learning} is concerned with learning a function that
predicts a certain \textbf{target} from an object's \textbf{features} from a
set of examples for which both the features and the target are known.\\
The function to be learned is restricted to come from a certain class of functions
and its precise shape is defined in terms of a set of \textbf{parameters}.

\end{frame}


\endlecture
\end{document}
