\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}

\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
  %Define standard arrow tip
  >=stealth',
  %Define style for boxes
  punkt/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=6.5em,
    minimum height=2em,
    text centered},
  % Define arrow style
  pil/.style={
    ->,
    thick,
    shorten <=2pt,
    shorten >=2pt,}
}

\usepackage{subfig}

% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
\newcommand{\titlefigure}{figure/lm_reg2}
\newcommand{\learninggoals}{\item Understand the components of a learner \item Understand the formalization of supervised learning \item Be able to apply the concept of a learner to a supervised learning task}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}

\begin{document}

% Set style/preamble.Rnw as parent.

% Load all R packages and set up knitr

% This file loads R packages, configures knitr options and sets preamble.Rnw as 
% parent file
% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...

% Defines macros and environments

\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\lecturechapter{Introduction: Learners}
\lecture{Introduction to Machine Learning}

% ------------------------------------------------------------------------------

\begin{vbframe}{Components of a Learner}

Summarizing what we have seen before, many supervised learning algorithms 
can be described in terms of three components:

\lz

\begin{center}

  \textbf{Learning = Hypothesis Space + Risk + Optimization}
  
\end{center}

\lz

\begin{itemize}

  \item \textbf{Hypothesis Space:} Defines (and restricts!) what kind of model 
  $f$ can be learned from the data.
  
  \item \textbf{Risk:} Quantifies how well a specific model performs on a given 
  data set. This allows us to rank candidate models in order to choose the best one.
  
  \item \textbf{Optimization:} Defines how to search for the best model in the 
  \textbf{hypothesis space}, i.e., the model with the smallest \textbf{risk}.
  
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Components of a Learner - Advanced}

This concept can be extended by the concept of \textbf{penalization}, where the model complexity is accounted for in the risk:

\lz

%\begin{center}

  \textbf{Learning = Hypothesis Space + \invisible{xxxxxxxx} Risk \invisible{ixxxxx}+ Optim }
  \textbf{Learning = Hypothesis Space + Loss (+ Penalization) + Optim}
  
%\end{center}

\lz

\begin{itemize}

  \item We will not treat penalization in this course, so for the moment you can just think of the risk as sum of the losses.
  
  \item While this a useful framework for most supervised ML problems, it does not cover all special cases, because some ML methods are not defined via risk minimization and for some models, it is not possible (or very hard) to explicitly define the hypothesis space.
  
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Supervised Learning, Formalized}

A \textbf{learner} (or \textbf{inducer}) $\inducer$ is a \emph{program} or 
\emph{algorithm} which

\begin{itemize}

  \item receives a \textbf{training set} $\D \in \Xspace \times \Yspace$, and,
  
  \item for a given \textbf{hypothesis class} $\Hspace$ of \textbf{models} 
  $f:\Xspace \rightarrow \R^g,$ 
  
  \item based on a \textbf{risk} function $\riskef$ that quantifies the 
  performance of $f \in \Hspace$ on $\D,$
  
  \item uses an \textbf{optimization} procedure to find
  $$\fh = \argmin_{f \in \Hspace} \riskef.$$

\end{itemize}

\lz 

As before, we can also adapt this concept to finding $\thetabh$ for parametric
models.

\lz

% {\footnotesize (This does not cover all special cases, but it's a useful framework for most supervised ML problems.)}

\end{vbframe}



% ------------------------------------------------------------------------------

\begin{vbframe}{Example of a Learner}

%So what could a learner look like? 
Let us consider a linear regression task with 
a single feature and a single target variable.

\begin{itemize}
  
  \item The \textbf{hypothesis space} in univariate linear regression is the set 
  of all linear functions, with $\thetab = (\theta_0, \theta)^\top$:
  
  $$\Hspace = \{\fx = \theta_0 + \theta \xv: \theta_0, \theta \in \R \}$$
  
  \begin{center}
    \includegraphics[trim = 1.5cm 1.5cm 1.5cm 1.5cm, width = 0.5\textwidth]{figure/lm_reg3} 
  \end{center}
  
  \framebreak
  
  \item We might use the mean squared error as loss function to our
  \textbf{risk}, punishing larger distances between observations and regression 
  line more severely:
  
  $$\risket = \sumin (\yi - \theta_0 - \theta \xi)^2$$
  
  \begin{center}
    \includegraphics[trim = 1.5cm 1.5cm 1.5cm 1.5cm, width = 0.6\textwidth]{figure/lm_reg1} 
  \end{center}
  
  \framebreak
  
  \item \textbf{Optimization} will usually mean deriving the 
  ordinary-least-squares (OLS) estimator $\thetabh$ analytically. We might,
  however, also use gradient descent or some other optimization procedure.
  
  \begin{center}
    \includegraphics[trim = 1.5cm 1.5cm 1.5cm 1.5cm, width = 0.6\textwidth]{figure/lm_reg2} 
  \end{center}
    
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{frame}[squeeze]{Variety of Learning Components}

% \vskip -.5em

\begin{footnotesize}

$\textbf{Hypothesis Space}: \begin{cases} 

\text{Step functions} \\
\text{Linear functions}\\
\text{Sets of rules}\\
\text{Neural networks}\\
\text{Voronoi tesselations}\\
\text{...}
\end{cases}$

$\phantom{Hypothesis Space RISK } \textbf{Risk / Loss}: \begin{cases}
\text{Mean squared error}\\
\text{Misclassification rate}\\
\text{Negative log-likelihood}\\
\text{Information gain}\\
\text{...}
\end{cases}$

$\phantom{hypothesis space risk RISK RISK} \textbf{Optimization}: \begin{cases}
\text{Analytical solution}\\
\text{Gradient descent}\\
\text{Combinatorial optimization}\\
\text{Genetic algorithms}\\
\text{...}
\end{cases}$

\end{footnotesize}
\end{frame}

% ------------------------------------------------------------------------------

\begin{vbframe}{Learning as Empirical Risk Minimization}

\begin{itemize}

  \item By decomposing learners into these building blocks,
  
  \begin{itemize}

    \item we have a framework to understand how they work,
    
    \item we can more easily evaluate in which settings they may be more or less 
    suitable, and
    
    \item we can tailor learners to specific problems by clever choice of each 
    of the three components.
    
  \end{itemize}
  
  \item There will, for instance, be optimization procedures that work well for
  a certain combination of hypothesis space and risk function but perform poorly 
  on others.
  
  \item In fact, it is a commonly acknowledged problem that no universally best
  learner exists.

\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\endlecture
\end{document}
