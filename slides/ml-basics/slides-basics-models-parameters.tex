\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}
\usepackage{amsmath}

\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
  %Define standard arrow tip
  >=stealth',
  %Define style for boxes
  punkt/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=6.5em,
    minimum height=2em,
    text centered},
  % Define arrow style
  pil/.style={
    ->,
    thick,
    shorten <=2pt,
    shorten >=2pt,}
}

\usepackage{subfig}

% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}

\begin{document}

% Set style/preamble.Rnw as parent.

% Load all R packages and set up knitr

% This file loads R packages, configures knitr options and sets preamble.Rnw as 
% parent file
% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...

% Defines macros and environments

\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-lm.tex}

\lecturechapter{ML-Basics: Models \& Parameters}
\lecture{Introduction to Machine Learning}

% ------------------------------------------------------------------------------

\begin{vbframe}{What is a Model?}

\begin{itemize}

  \item A \textbf{model} (or \textbf{hypothesis}) 
  $$f : \Xspace \rightarrow \R^g$$ 
  is a function that maps feature vectors to predicted target values.
  
  \item Loosely speaking: if $f$ is fed a set of features, it will output the 
  target corresponding to these feature values under our hypothesis.
  
\end{itemize}

% FIGURE SOURCE: https://docs.google.com/presentation/d/1bc6EQSsHEuVnyqFGX9E8oNfwOjAwVglRaIllxnOjLBo  /edit?usp=sharing Page 1
\begin{center}
  \includegraphics[width = 0.8\textwidth]{figure_man/the_model_web} 
\end{center}
  
{\footnotesize In conventional regression we will have $g = 1$; for 
classification $g$ equals the number of classes, and output vectors are scores 
or class probabilities (details later).}

\framebreak

\begin{itemize}

  \item $f$ is meant to capture intrinsic patterns of the data, the
  underlying assumption being that these hold true for \emph{all} data drawn 
  from $\Pxy$.
  
  \item It is easily conceivable how models can range from super simple (e.g.,
  tree stumps) to reasonably complex (e.g., variational autoencoders), and how 
  there is an infinite number of them.
  
  \begin{figure}
  % \centering
  \begin{minipage}{0.4\textwidth}
    \centering
    \includegraphics[width=0.8\linewidth]{figure_man/monotone_trafo1.png}
  \end{minipage}%
  \begin{minipage}{0.6\textwidth}
    \centering
    % https://www.researchgate.net/figure/Structure-of-Stacked-Autoencoders_fig2_325025951
    \includegraphics[width=0.6\linewidth]{figure_man/stacked_autoencoder.png}
  \end{minipage}
  \end{figure}
  
  \vspace{0.2cm}
  
  \item In fact, machine learning requires \textbf{constraining} $f$ to a 
  certain type of functions.

\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Hypothesis Spaces}

\begin{itemize}

  \item Without restrictions on the functional family, the task of finding a 
  \enquote{good} model among all the available ones is impossible to solve.
  
  \item This means: we have to determine the class of our model \emph{a priori}, 
  thereby narrowing down our options considerably.
  
  \item The set of functions defining a specific model class is called a 
  \textbf{hypothesis space} $\Hspace$:
  
  $$\Hspace = \{f: f \text{ belongs to a certain functional family}\}$$

\end{itemize}  

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Parameters of a Model}

\begin{itemize}
  
  \item All models within one hypothesis space share a common functional 
  structure.
  
  \item In fact, the only aspect in which they differ is the values of 
  \textbf{parameters}.
  
  \item We usually subsume all these parameters in a \textbf{parameter vector} 
  $\thetab = (\theta_1, \theta_2, ...)$ from a \textbf{parameter space} 
  $\Theta$.
  
  \item They are our means of configuration: once set, our model is 
  fully determined.
  
  % \item Revisiting the space of linear functions, tweaking $\theta_0$ and
  % $\theta_1$ is what hands us arbitrary straight lines.
  % 
  % \begin{center}
  %   \includegraphics[width = 0.5\textwidth]{figure_man/lines.jpg} 
  % \end{center}
  
  \framebreak
  
  \item This means: finding the optimal model is perfectly equivalent to 
  finding the optimal set of parameter values.
  
  \item The bijective relation between optimization over $f \in \Hspace$ and 
  optimization over $\thetab \in \Theta$ allows us to operationalize our search
  for the best model via the search for the optimal value on a $p$-dimensional
  parameter surface.
  
  \begin{center}
  % https://docs.google.com/presentation/d/1cg7VqEplV8xoBEujxFBDLKNczXdST-EVhgvJHJCHTbo/edit?usp=sharing
    \includegraphics[width = 0.4\textwidth]{figure_man/bijection_f_theta.PNG} 
  \end{center}
  
  \item $\thetab$ might be scalar or comprise thousands of parameters,
  depending on the complexity of our model.
  
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Examples for Hypothesis Spaces}

\textbf{Example 1:} Hypothesis space of univariate linear functions
$$\Hspace = \{f: \fx = \thx =  \textcolor{blue}{\theta_0} + 
\textcolor{orange}{\theta_1} x, \thetab \in \R^2\}$$

\begin{center}
  \includegraphics[width = \textwidth]{figure/hs-lin-functions.pdf}
\end{center}

\framebreak

\textbf{Example 2:} Hypothesis space of bivariate quadratic functions
% $$\Hspace = \{f: \fx =  \theta_0 + P \xv^T + \xv Q \xv^T = 
% \theta_0 + \theta_1 x_1 + \theta_2 x_2 +
% \theta_3 x_1^2 + \theta_4 x_2^2 + \theta_5 x_1 x_2, \thetab \in \R^6\}$$

\begin{equation*}
  \begin{split}
    \Hspace &= \{f: \fx =  \theta_0 + P \xv^T + \xv Q \xv^T =  \\
    &= \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_1^2 + \theta_4 x_2^2 + 
    \theta_5 x_1 x_2, \thetab \in \R^6\}
  \end{split}
\end{equation*}

\framebreak

\textbf{Example 3:} Hypothesis space of radial basis function networks with
Gaussian basis functions
$$\Hspace = \bigg\{ f: \fx =  \sumin \textcolor{blue}{a_i} \rho ( \| \xv - 
\textcolor{orange}{\textbf{c}_i}  \|) 
\bigg\},$$ 

\begin{minipage}{0.55\textwidth}
  \small
  where 
  \begin{itemize}
    \item $\textcolor{blue}{a_i}$ is the weight of the $i$-th neuron, 
    \item $\textcolor{orange}{\textbf{c}_i}$ its center vector, and
    \item $\rho( \| \xv - \textcolor{orange}{\textbf{c}_i} \|) = \exp(- \beta \| 
    \xv - \textcolor{orange}{\textbf{c}_i} \|^2)$ is the $i$-th radial basis
    function with bandwidth $\beta \in \R$.
  \end{itemize}
  Usually, the number of centers, $n$, and the bandwidth $\beta$ need to be set 
  in advance (so-called \emph{hyperparameters}).
  \normalsize
\end{minipage}%
\begin{minipage}{0.45\textwidth}
    \centering
    % https://www.researchgate.net/figure/Structure-of-Stacked-Autoencoders_fig2_325025951
    \includegraphics[width = 0.9\linewidth]{figure_man/rbf_network.png}
\end{minipage}

\framebreak

\begin{center}
  
  \includegraphics[width = \textwidth]{figure/hs-rbf-network-2d.pdf}
  
\centering  

\begin{minipage}{0.33\textwidth}
  \footnotesize
  \begin{itemize}
    \item $a_1 = 0.4$ \\
    $a_2 = 0.2$ \\
    $a_3 = 0.4$
    \item $c_1 = -3$ \\
    $c_2 = -2$ \\
    $c_3 = 1$
  \end{itemize}
\end{minipage}%
\begin{minipage}{0.33\textwidth}
  \footnotesize
  \begin{itemize}
    \item $a_1 = 0.4$ \\
    $a_2 = 0.2$ \\
    $a_1 = 0.4$
    \item $c_1 = -3$ \\
    $c_2 = -2$ \\
    $\textcolor{orange}{c_3 = 4}$  \end{itemize}
\end{minipage}%
\begin{minipage}{0.33\textwidth}
  \footnotesize
  \begin{itemize}
    \item $\textcolor{blue}{a_1 = 0.6}$ \\
    $a_2 = 0.2$ \\
    $\textcolor{blue}{a_3 = 0.2}$
    \item $c_1 = -3$ \\
    $c_2 = -2$ \\
    $c_3 = 1$
  \end{itemize}
\end{minipage}
\normalsize

\end{center}

\end{vbframe}

% ------------------------------------------------------------------------------

\endlecture
\end{document}
