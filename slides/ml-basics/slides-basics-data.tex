
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{figure/sample-dgp-2d.pdf}
\newcommand{\learninggoals}{
\item Understand structure of tabular data in ML 
\item Understand difference between target and features 
\item Understand difference between labeled and unlabeled data 
\item Know concept of data-generating process}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}
\lecturechapter{ML-Basics: Data}
\lecture{Introduction to Machine Learning}

% ------------------------------------------------------------------------------

\begin{vbframe}{Iris data set}
Introduced by the statistician Ronald Fisher and one
of the most frequently used toy examples.
\begin{itemize}
\item Classify iris subspecies based on flower measurements.
\item 150 iris flowers: 50 versicolor, 50 virginica, 50 setosa.
\item Sepal length / width and petal length / width in [cm].
\end{itemize}

\begin{center}
\includegraphics[width=0.9\textwidth]{figure_man/iris_species.png} 

\tiny
Source: \url{https://rpubs.com/vidhividhi/irisdataeda}
\normalsize
\end{center}

Word of warning: "iris" is a small, clean, low-dimensional data set,
which is very easy to classify; this is not necessarily true in the wild. 

\end{vbframe}

\begin{vbframe}{Data in Supervised Learning}

\begin{itemize}

  \item The data we deal with in supervised learning usually consists of 
  observations on different aspects of objects:
  
  \begin{itemize}
  
    \item \textbf{Target}: the output variable / goal of prediction

    \item \textbf{Features}: measurable properties that provide a concise 
    description of the object 
    
%    \item Both features and target variables may be of different data types  (categorical, numeric, ...)

  \end{itemize}
  
  \item We assume some kind of relationship between the features and the target,
  in a sense that the value of the target variable can be explained by a 
  combination of the features.
  
  % picturecreated with google presentation https://docs.google.com/presentation/d/1hoUHBJmXzCyVQXT9uP4sI84N6leQ42YhigFvQiCY0oo/edit?usp=sharing

    \includegraphics[width = 0.4\textwidth]{figure_man/feat_targ_rel.jpg}
     %picture created with google presentation
  %https://docs.google.com/presentation/d/1hnRPmCke8EqBhTrymC_pnuPe17Dk5iMkdBz6AQuIa3A/edit?usp=sharing page 1
    \includegraphics[width = 0.52\textwidth]{figure_man/ml-basic-data-example-iris.png} 

\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Attribute types}

\begin{itemize}

  \item Both features and target variables may be of different data types 
  
  \begin{itemize}
  
    \item \textbf{Numerical} variables can have values in $\R$
    
    \item \textbf{Integer} variables can have values in $\Z$
    
    \item \textbf{Categorical} variables can have values in $\{C_1,...,C_g\}$
    
    \item \textbf{Binary} variables can have values in $\{0, 1\}$
  
  \end{itemize}
  
  \item For the \textbf{target} variable, this results in different tasks of supervised learning: \textit{regression} and \textit{classification}. 
  
  \item Most learning algorithms can only deal with numerical features,
      although there are some exceptions (e.g. decision trees can use integers and categoricals without problems).
      For other feature types, we usually have to pick or create an
      appropriate encoding.
  \item If not stated otherwise, we assume numerical features.

\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Encoding for categorical features}

\begin{itemize}
  \small
  \item \textbf{Encoding} means transforming categorical 
  features to numeric quantities.
  \item We typically choose one of the following encoding schemes for a variable 
  $x$ with $g$ mutually exclusive categories:
  \begin{itemize}
    \small
    \item \textbf{One-hot encoding}: $g$ dummy variables, each assuming values 
    from \setzo, are used to represent $x$ with only one at a time being 
    non-zero. \\
    E.g., $x \in \{ \text{black, white}\} \mapsto x_{\text{black}} 
    \in \setzo, x_{\text{white}} \in \setzo$
    \item \textbf{Dummy encoding}: Like one-hot encoding but more parsimonious, 
    using just $g - 1$ dummies and thus cutting the redundancy inherent to the 
    one-hot representation (which some algorithms that cannot deal with 
    non-singular input matrices, such as linear regression learners, demand). \\
    E.g., $x \in \{ \text{black, white}\} \mapsto x_{\text{black}} \in \setzo$
  \end{itemize}
  \item For features with a natural order in their categories we might resort 
  to different encodings that reflect this ordinality (e.g., a sequence of 
  integer values -- note that equidistant values signify a linear ordering 
  here.)
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Observation Labels}

\begin{itemize}

  \item We call the entries of the target column \textbf{labels}.
  \item We distinguish two basic forms our data may come in:
  
  \begin{itemize}
  
      \item For \textbf{labeled} data we have already observed the target
    
    \item For \textbf{unlabeled} data the target labels are unknown
  
  \end{itemize}
  
%  \item It is easy to see how labeled data are vastly more informative.
  
%  \item In practice, however, we will much more frequently encounter the unlabeled sort.
  
  \begin{center}
    %https://docs.google.com/presentation/d/1hnRPmCke8EqBhTrymC_pnuPe17Dk5iMkdBz6AQuIa3A/edit?usp=sharing page 2
    \includegraphics[width = 0.8\textwidth]{figure_man/ml-basic-data-example-iris-with-new.png} 
  \end{center}

\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Notation for Data}

In formal notation, the data sets we are given are of the following form:
\[
\D = \Dset \in \defAllDatasetsn.
\]

We call

\begin{itemize}

  \item $\Xspace$  the input space with $p = \text{dim}(\Xspace)$ (for now: 
  $\Xspace \subset \R^p$),
  
  \item $\Yspace$ the output / target space,
  
  \item the tuple \(\xyi\) $\in \Xspace\times \Yspace$ the \(i\)-th observation,
  
  \item $\xj = \xjvec$ the j-th feature vector.
  
\end{itemize}

We denote

\begin{itemize}

  \item  $\defAllDatasetsn$, i.e., the set of all data sets of size $n$, as $\allDatasetsn$,
  \item $\defAllDatasets$, i.e., the set of all finite data sets, as $\allDatasets$.
\end{itemize}

\lz

So we have observed $n$ objects, described by $p$ features.

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Data-Generating Process}

\begin{itemize}

  \item We assume the observed data $\D$ to be generated by a process that can
  be characterized by some probability distribution $$\Pxy,$$ defined on 
  $\Xspace \times \Yspace$.
  
  \item We denote the random variables following this 
  distribution by lowercase $\xv$ and $y$.
  
  \item It is important to understand that the true distribution is essentially 
      \textbf{unknown} to us. In a certain sense, learning (part of) its structure is what ML is all about.
  
  \framebreak
  
  \item We assume data to be drawn \emph{i.i.d.} from the joint 
  probability density function (pdf) / probability mass function (pmf) $\pdfxy$.
  
  \begin{itemize}
  
    \item i.i.d. stands for \textbf{i}ndependent and \textbf{i}dentically 
    \textbf{d}istributed.
    
    \item This means: We assume that all samples are drawn from the same distribution 
    and are mutually independent -- the $i$-th realization does not depend on the
    other $n-1$ ones.
    
    \item This is a strong yet crucial assumption that is precondition to most
    theory in (basic) ML.
    % (e.g., the Central Limit Theorem).
    
  
  \end{itemize}

\end{itemize}

\begin{minipage}{0.5\textwidth}
  \centering
  \includegraphics[width = 0.7\linewidth]{figure/sample-dgp-3d.png}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
  % \centering
  \includegraphics[width = 0.9\linewidth]{figure/sample-dgp-2d.pdf}
\end{minipage}

\framebreak

\textbf{Remarks:}

\begin{itemize}

  \item With a slight abuse of notation we write random variables, e.g., $\xv$ 
  and $y$, in lowercase, as normal variables or function arguments. The context 
  will make clear what is meant.
  
  \item Often, distributions are characterized by a parameter vector 
  $\thetab \in \Theta$. We then write $\pdfxyt$.
  
  \item This lecture mostly takes a frequentist perspective. Distribution 
  parameters $\thetab$ appear behind the | for improved legibility, not to imply 
  that we condition on them in a probabilistic Bayesian sense.
  So, strictly speaking, $p(\xv | \thetab)$ should usually be understood to mean 
  $p_{\thetab}(\xv)$ or $p(\xv, \thetab)$ or $p(\xv; \thetab)$.
  On the other hand, this notation makes it very easy to switch to a Bayesian view.

\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\endlecture
\end{document}
