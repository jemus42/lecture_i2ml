\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{figure_man/Network_nutshell_slides.png}
%https://www.forbes.com/sites/jonathonkeats/2017/05/02/tomas-saraceno/?sh=15443dca4626


\newcommand{\learninggoals}{%\item Most important points on: Data, Tasks, Models and Parameters, Learner, Losses and Risk Minimization, Optimization, Components of a Learner.
\item Understand fundamental goal of supervised machine learning
\item Know concepts of task, model, parameter, learner, loss function, and empirical risk minimization
}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
% \institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-% lmu.github.io/lecture\_i2ml}}
\date{}
\begin{document}

\lecturechapter{In a Nutshell: ML-Basics}
\lecture{Introduction to Machine Learning}
\sloppy


\begin{vbframe}{What is ML?}
\begin{itemize}
 \item \small Mathematically well-defined and solves reasonably narrow tasks.
    \item \small ML algorithms usually construct predictive/decision models from data, instead of explicitly programming them.
    \item A computer program is said to learn from experience E with respect to
  some task T and some performance measure P, if its performance on T, as 
  measured by P, improves with experience E. \\
  \begin{footnotesize}
  \emph{Tom Mitchell, Carnegie Mellon University, 1998}
  \end{footnotesize}
\end{itemize}

\small 99 $\%$ of this lecture is about \textbf{supervised learning}! 

\begin{center}
  \includegraphics[width = 0.8\textwidth]{figure_man/CatDog_Learning_example.png} 
\end{center}
\begin{center}
  \includegraphics[width = 0.8\textwidth]{figure_man/CatDog_Prediction_example.png} 
\end{center}
% images taken from Kaggle`s Cats-vs-Dogs Dataset:
% https://www.kaggle.com/datasets/shaunthesheep/microsoft-catsvsdogs-dataset?resource=download

\end{vbframe}

\begin{vbframe}{Data}
\begin{itemize}
\item Two important assumptions regarding the data set:
    \begin{itemize}
  
      \item We assume the observed data $\D$ to be generated by a random process. The true distribution is \textbf{unknown} to us. Learning (part of) its structure is what ML is all about. 
    
    \item We assume data to be drawn \emph{i.i.d.} (\textbf{i}ndependent and \textbf{i}dentically 
    \textbf{d}istributed) from the joint probability density function (pdf) / probability mass function (pmf) $\pdfxy$.
   \end{itemize}

\item We distinguish two basic forms our data may come in:
  
  \begin{itemize}
  
      \item For \textbf{labeled} data we have already observed the target (the output variable).
    
    \item For \textbf{unlabeled} data the target labels are unknown.
   \end{itemize}

\item Sometimes you need to encode the data before running an algorithm:
  
  \begin{itemize}   
    \item  \small Most learning algorithms can only deal with numerical features,
      although there are some exceptions (e.g., decision trees can use integers and categoricals without problems).
      For other feature types, we usually have to pick or create an
      appropriate \textbf{encoding}, for example: One-hot encoding.
  % \item \textbf{Encoding} means transforming categorical features to numeric 
  % quantities.
     \item \small \textbf{One-hot Encoding for categorical features}: For example consider the categorical feature "Fruit" with $k$ different categories (for $k$ = 3 the categories might be "Banana", "Apple" and "Lemon"). For encoding we represent the feature "Fruit" as a vector with $k = 3$ entries. Each entry corresponds to one category. For encoding "Banana" only the corresponding entry will be set to $1$, all other entries will be set to $0$. E.g. "Banana" = $(1,0,0)$.
  
  %one to $\tilde k$ columns in the 
  %design matrix, each element being a \textbf{binary dummy variable} assuming 
  %values from \setzo.
  %\item $x$ is thus represented by 

    \item \small \textbf{Dummy Encoding for categorical features}: Consider above example. For Dummy encoding one category is selected as reference category which is not explicitly encoded. For encoding we represent the feature "Fruit"  as a vector with $k - 1 = 2$ entries. E.g. for encoding "Banana" (in the case of "Banana" being the reference category) we have $(0,0)$ (otherwise $(0,1)$).
 
  %-- note that equidistant values signify a linear ordering here.)
\end{itemize}

\end{itemize}

\end{vbframe}

\begin{vbframe}{Tasks}
\begin{itemize}
    \item Supervised tasks are (labeled) data situations where learning the functional
        relationship between inputs (features) and output (target) is
        useful.
    
    \item The two most basic tasks are 
        \textbf{regression} and \textbf{classification}, depending on whether the 
        target is \textbf{numerical} or \textbf{categorical}. 
\end{itemize}  

\lz

\begin{columns}    
\begin{column}{0.4\textwidth} 
\small \textbf{Regression}: Labels are \textbf{numerical}. E.g., predict days a patient has to stay in hospital.

  \begin{center}
    \includegraphics[width=\textwidth]{figure/ml-basics-supervised-regression-task.png} 
  \end{center}
\end{column}    

\begin{column}{0.4\textwidth} 
\small \textbf{Classification}: Labels are \textbf{categorical}. E.g. predict one of two risk categories for a life insurance customer. 
  
  \begin{center}
    \includegraphics[width=\textwidth]{figure/ml-basics-supervised-classif-task.png} 
  \end{center}
\end{column}    
\end{columns}    

\end{vbframe}


\begin{vbframe}{Models and Parameters}
\small 
\begin{itemize}
    \item A model is a function that maps features to predicted targets.
\end{itemize}

\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure_man/the_model_web.png} 
\end{center}

\small
\begin{itemize}
\item  For finding the model that describes the relation between features and target best, one needs to restrict the set of all possible functions. 
\item This restricted set of functions is called \textbf{hypothesis space}. E.g. one could consider only simple linear functions as hypothesis space.
\item Functions are fully determined by parameters. E.g. in the case of linear functions $y = mx + t$, the parameters $m$ (slope) and $t$ (intercept) determine the relationship between $y$ and $x$.
\item Finding the optimal model is equivalent to finding the optimal set of parameters.
\end{itemize}

\end{vbframe}

\begin{vbframe}{Learners}

\begin{itemize}
\small
\item \small Learners \textbf{automatically} learn the relation between features and target.
\item \small The learner is our means of picking the best element of the hypthesis space.
\end{itemize}

\begin{columns}    
\begin{column}{0.3\textwidth} 
\small \textbf{Regression}: 

  \begin{center}
    \includegraphics[width=\textwidth]{figure_man/Model_Regression_Plot.png} 
  \end{center}
\end{column}    

\begin{column}{0.3\textwidth} 
\small \textbf{Classification}:
  
  \begin{center}
    \includegraphics[width=\textwidth]{slides/ml-basics/figure_man/Model_Classification_Plot.png} 
  \end{center}
\end{column}    
\end{columns}  

\begin{itemize}
\item \small The learner uses labeled training data to learn a model $f$. With this model. 
\end{itemize}

 \begin{center}
    \includegraphics[width = 0.45\textwidth]{figure_man/the_inducer_web.png}
  \end{center}

\end{vbframe}

\begin{vbframe}{Losses and Risk Minimization}

\begin{itemize}
\item Loss: Is measured pointwise for each observation of the data set. 

\item Risk: 

\begin{columns}  
\begin{column}{0.3\textwidth} 
\small Squared \textbf{loss} of one observation.

  \begin{center}
    \includegraphics[width=\textwidth]{slides/ml-basics/figure_man/nutshell_ml_basic_1_loss_sqrd.png} 
  \end{center}
\end{column}  


\begin{column}{0.7\textwidth} 
\small \textbf{Empirical risk} of 

  \begin{center}
    \includegraphics[width=\textwidth]{slides/ml-basics/figure/ml-basic_riskmin-2-risk.png} 
  \end{center}
\end{column} 
\end{columns}
\end{itemize}

%\small
%\begin{itemize}
%\small
%\item By computing the empirical risk of a model (parameter set) one evaluates how well a given function f matches the training data. The goal is to minimize the empirical risk.

%\item The \textbf{empirical risk} computes the average error of a model, applied on a \textbf{specific} data set. To this end the difference between the value of the prediction and the value of the true target is measured by a loss function for each observation of the data set (labeled data). All pointwise losses are summed up to the \textbf{empirical risk score}.

%\ 
%\begin{center}
%\textbf{Empirical Risk = Sum over all pointwise losses}
%\end{center}

%\pagebreak   
%\begin{column}{1\textwidth} 
%\item \small \textbf{Regression example}: The green line represents the model. It differs from the observations (marked as $X$). In this case the difference is quantified by the quadratic loss function (green boxes). All green boxes (losses) are summed up to the empirical risk score ($R_{emp} = 5.88$). 
%\begin{center}
%\minipage{0.49\textwidth}
%\includegraphics[trim=1.5cm 1.5cm 1.5cm 1.5cm, width=\linewidth]{figure/lm_reg2}  
%\endminipage\hfill
%\end{center}
%\end{column} 

%\item In empirical risk minimization (ERM) one wants to find the best parameter set which minimizes the empirical risk. This is usually done by numerical optimization because analytical solutions might not exist.  
%\end{itemize}


\end{vbframe}


\begin{vbframe}{Optimization}

\small
\begin{itemize}
\item It depends on the model and parameter structure which numerical technique should be applied for emprical risk minimization. 
\small
\item One can imagine the empirical risk as an error surface similar to a mountain landscape, where one starts nearby the top and wants to find a way to the valley (minimum of risk).
\small
\item One of the simplest algorithms for reaching the valley is \textbf{Gradient Descent (GD)}:
    \begin{itemize}
    \small
    \item In the case of GD one starts at a point on the surface (can be randomly selected or e.g. by domain knowledge). The algorithm then finds the direction of the surface with the steepest descent and moves towards this direction with a certain step size (learning rate). 
    \small
    \item Moving with the \textbf{right} step size is crucial for the success of GD. If it`s too big the valley might not be reached because of overstepping, if it`s too small the algorithm might be too slow.
    \end{itemize}

\end{itemize}

\end{vbframe}

\begin{vbframe}{Components of a Learner}

\begin{itemize}
\item A learner is basically designed by three components:  
\begin{center}
  \textbf{Learning = Hypothesis Space + Risk + Optimization}
\end{center}
\item It can be extended by a \textbf{regularization} term which controls the complexity of a model.
\item Note that exceptions exist, where the ML method is not defined via \textbf{risk minimization} and defining the \textbf{hypothesis space} can be very hard or impossible.
\end{itemize}


\end{vbframe}


% ------------------------------------------------------------------------------
\endlecture
\end{document}