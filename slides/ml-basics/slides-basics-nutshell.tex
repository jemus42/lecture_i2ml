\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{figure_man/Network_nutshell_slides.png}

\newcommand{\learninggoals}{\item Most important points on: Data, Tasks, Models and Parameters, Learner, Losses and Risk Minimization, Optimization, Components of a Learner.
}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}
\begin{document}

\lecturechapter{In a Nutshell: ML-Basics}
\lecture{Introduction to Machine Learning}

\sloppy

% ------------------------------------------------------------------------------

\begin{vbframe}{Data}
\begin{itemize}
\item Two important assumptions regarding the data set:
    \begin{itemize}
  
      \item We assume the observed data $\D$ to be generated by a random process. The true distribution is \textbf{unknown} to us. Learning (part of) its structure is what ML is all about. 
    
    \item We assume data to be drawn \emph{i.i.d.} (\textbf{i}ndependent and \textbf{i}dentically 
    \textbf{d}istributed) from the joint probability density function (pdf) / probability mass function (pmf) $\pdfxy$.
   \end{itemize}

\item We distinguish two basic forms our data may come in:
  
  \begin{itemize}
  
      \item For \textbf{labeled} data we have already observed the target (the output variable).
    
    \item For \textbf{unlabeled} data the target labels are unknown.
   \end{itemize}

\item Sometimes you need to encode the data before running an algorithm:
  
  \begin{itemize}   
    \item  \small Most learning algorithms can only deal with numerical features,
      although there are some exceptions (e.g., decision trees can use integers and categoricals without problems).
      For other feature types, we usually have to pick or create an
      appropriate \textbf{encoding}, for example: One-hot encoding.
  % \item \textbf{Encoding} means transforming categorical features to numeric 
  % quantities.
     \item \small \textbf{One-hot Encoding for categorical features}: For example consider the categorical feature "Fruit" with $k$ different categories (for $k$ = 3 the categories might be "Banana", "Apple" and "Lemon"). For encoding we represent the feature "Fruit" as a vector with $k = 3$ entries. Each entry corresponds to one category. For encoding "Banana" only the corresponding entry will be set to $1$, all other entries will be set to $0$. E.g. "Banana" = $(1,0,0)$.
  
  %one to $\tilde k$ columns in the 
  %design matrix, each element being a \textbf{binary dummy variable} assuming 
  %values from \setzo.
  %\item $x$ is thus represented by 

    \item \small \textbf{Dummy Encoding for categorical features}: Consider above example. For Dummy encoding one category is selected as reference category which is not explicitly encoded. For encoding we represent the feature "Fruit"  as a vector with $k - 1 = 2$ entries. E.g. for encoding "Banana" (in the case of "Banana" being the reference category) we have $(0,0)$ (otherwise $(0,1)$).
 
  %-- note that equidistant values signify a linear ordering here.)
\end{itemize}

\end{itemize}

\end{vbframe}

\begin{vbframe}{Tasks}
\begin{itemize}
    \item Supervised tasks are (labeled) data situations where learning the functional
        relationship between inputs (features) and output (target) is
        useful.
    
    \item The two most basic tasks are 
        \textbf{regression} and \textbf{classification}, depending on whether the 
        target is \textbf{numerical} or \textbf{categorical}. 
\end{itemize}  

\lz

\begin{columns}    
\begin{column}{0.5\textwidth} 
\small \textbf{Regression}: Our observed labels are \textbf{numerical}. E.g. predict days a patient has to stay in hospital.

  \begin{center}
    \includegraphics[width=\textwidth]{figure/ml-basics-supervised-regression-task.png} 
  \end{center}
\end{column}    

\begin{column}{0.5\textwidth} 
\small \textbf{Classification}: Observations are \textbf{categorical}. E.g. predict one of two risk categories for a life insurance customer. 
  
  \begin{center}
    \includegraphics[width=\textwidth]{figure/ml-basics-supervised-classif-task.png} 
  \end{center}
\end{column}    
\end{columns}    

\end{vbframe}


\begin{vbframe}{Models and Parameters}
\small 
\begin{itemize}
    \item A model is a function that maps feature vectores to predicted target values.
\end{itemize}

\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure_man/the_model_web.png} 
\end{center}

\small
\begin{itemize}
\item  For finding the model that describes the relation between features and target best, one needs to restrict the set of all possible functions. 
\item This restricted set of functions is called \textbf{hypothesis space}. E.g. one could consider only simple linear functions as hypothesis space.
\item Functions are fully determined by parameters. E.g. in the case of linear functions $y = mx + t$, the parameters $m$ (slope) and $t$ (intercept) determine the relationship between $y$ and $x$.
\item This means: finding the optimal model is perfectly equivalent to 
  finding the optimal set of parameter values.
\end{itemize}

\end{vbframe}

\begin{vbframe}{Learners}

\begin{itemize}
\small
\item Instead of trying out different models by hand, learners \textbf{automatically} learn the relation between features and target.
\item E.g. Consider the case of the hypothesis space consisting of simple linear functions. The learner is our means of picking the best element (best parameters $m$ and $t$) from that space for our data set. 
\item To this end it uses labeled data (training data) to learn a model $f$. With this model one computes predictions for \textbf{new} unseen data whose target values are unknown.
\end{itemize}
 \begin{center}
    \includegraphics[width = 0.45\textwidth]{figure_man/the_inducer_web.png}
  \end{center}

\end{vbframe}

\begin{vbframe}{Losses and Risk Minimization}

This means, we are looking for a function, where the 
    predicted output per training point is as close as possible to
    the observed label.

To make this precise, we need to define now how we measure the difference between a prediction and a ground truth label pointwise.

The \textbf{loss function} $\Lxy$ quantifies the "quality" of the prediction $\fx$ of a single observation $\xv$:

 \item The (theoretical) \textbf{risk} associated with a certain hypothesis $\fx$ measured by a loss function $\Lxy$ is the \textbf{expected loss}
  $$ \riskf := \E_{xy} [\Lxy] = \int \Lxy \text{d}\Pxy. $$
  \item This is the average error we incur when we use $f$ on data from $\Pxy$.
  \item Goal in ML: Find a hypothesis $\fx \in \Hspace$ that \textbf{minimizes} risk.

But as we have $n$ i.i.d. data points from $\Pxy$ available we can simply
approximate the expected risk by computing it on $\D$.

To evaluate, how well a given function $f$ matches our training data,
we now simply sum-up all $f$'s pointwise losses.

This gives rise to the \textbf{empirical risk function} which allows us
to associate one quality score with each of our models,
which encodes how well our model fits our training data.

The risk can also be defined as an average loss
  $$
    \riskeb(f) = \frac{1}{n}\sumin \Lxyi. 
  $$
  The factor $\frac{1}{n}$ does not make a difference in optimization, so we will consider $\riske(f)$ most of the time.  
\item Since $f$ is usually defined by \textbf{parameters} $\thetab$, this becomes:
$$\risk : \R^d \to \R$$
\begin{eqnarray*}
\risket & = & \sumin \Lxyit \cr
% \thetabh & = & \argmin_{\thetab \in \Theta} \risket
\end{eqnarray*}




\end{vbframe}


\begin{vbframe}{Optimization}



\end{vbframe}

\begin{vbframe}{Components of a Learner}



\end{vbframe}


% ------------------------------------------------------------------------------

\end{document}