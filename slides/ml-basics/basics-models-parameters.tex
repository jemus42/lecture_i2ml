\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}

\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
  %Define standard arrow tip
  >=stealth',
  %Define style for boxes
  punkt/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=6.5em,
    minimum height=2em,
    text centered},
  % Define arrow style
  pil/.style={
    ->,
    thick,
    shorten <=2pt,
    shorten >=2pt,}
}

\usepackage{subfig}

% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}

\begin{document}

% Set style/preamble.Rnw as parent.

% Load all R packages and set up knitr

% This file loads R packages, configures knitr options and sets preamble.Rnw as 
% parent file
% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...

% Defines macros and environments

\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\lecturechapter{Introduction: Models \& Parameters}
\lecture{Introduction to Machine Learning}

% ------------------------------------------------------------------------------

\begin{vbframe}{What is a Model?}

\begin{itemize}

  \item A \textbf{model} (or \textbf{hypothesis}) 
  $$f : \Xspace \rightarrow \R^g$$ 
  is a function that maps feature vectors to predicted target values.
  
  \item As such, it is (the attempt at) a \textbf{formal representation} of the 
  observed data.
  
  \item In conventional regression we will have $g = 1$, for classification
  see later.
  
  \item $f$ is meant to capture intrinsic patterns of the data, the
  underlying assumption being that these patterns hold true not only for the
  observed sample but for all data drawn from $\Pxy$.
  
  \framebreak
  
  \item It is easily conceivable how models can range from super simple to
  incredibly complex.
  
  \item The ultimate goal is to \emph{generalize} the learned model to new
  data (we already know the outcome for our training data), with as little
  error as possible.
  
  \item This suggests that we might be interested in a certain
  simplifying property: a model is expected to perform complexity reduction.
  
  $\rightarrow$ It needs to be scalable and extendable to new data situations.

    % FIGURE SOURCE: https://docs.google.com/presentation/d/1bc6EQSsHEuVnyqFGX9E8oNfwOjAwVglRaIllxnOjLBo  /edit?usp=sharing Page 1
  \begin{center}
    \includegraphics[width = 0.8\textwidth]{figure_man/the_model_web} 
  \end{center}

\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Hypothesis Spaces}

\begin{itemize}

  \item We have already seen that machine learning typically requires 
  constraining $f$ to a certain class of functions.
  
  \item Otherwise, the task of finding a \enquote{good} model among all the
  available ones is basically impossible to solve.
  
  \item The set of functions defining a specific model class is called a 
  \textbf{hypothesis space} $\Hspace$.
  
  \item For example, the set of all linear functions through $(0|0)$
  $$\Hspace = \{f: \fx = c \xv, c \in \R\}$$
  forms a (rather simple) hypothesis space.

\end{itemize}  

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Parameters of a Model}

\begin{itemize}

  \item Within one hypothesis space, models are \enquote{alike} in a sense: they 
  all share a common structure that makes up the condition in defining 
  $\Hspace$.
  
  $\rightarrow$ In the above example, all models are straight lines passing
  through the origin.
    
  \item Of all models in a class it is the choice of \textbf{parameter} values 
  that singles out a specific representant $f \in \Hspace$.
  
  \item Parameters are our means of configuration: once set, our model is fully
  determined.
  
  $\rightarrow$ Origin-crossing lines are solely determined by their slope, $c$.
  
  \item Parameters are the instrument to tailor the general 
  hypotheses to our data.

    % \item Out of all $f \in \Hspace$ it is the choice of parameter values that
  % hands us a specific model.
  
  \begin{center}
    \includegraphics[width = 0.5\textwidth]{figure_man/lines.jpg} 
  \end{center}
  
  \framebreak
  
  \item We usually subsume all parameters in a \textbf{parameter vector} 
  $\thetab = (\theta_1, \theta_2, ...)$ from a \textbf{parameter space} 
  $\Theta$.
  
  \item $\thetab$ might be one-dimensional or comprise thousands of parameters,
  depending on the complexity of our model.
  
  \item $\thetab$ is what we try to learn during training: finding a 
  \enquote{good} model boils down to finding a suitable combination of 
  parameters.
  
  \item We will see in the next chapter how the \enquote{goodness} of a model 
  can be determined.
  
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\endlecture
\end{document}
