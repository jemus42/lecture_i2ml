\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}

\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
  %Define standard arrow tip
  >=stealth',
  %Define style for boxes
  punkt/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=6.5em,
    minimum height=2em,
    text centered},
  % Define arrow style
  pil/.style={
    ->,
    thick,
    shorten <=2pt,
    shorten >=2pt,}
}

\usepackage{subfig}

% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}

\begin{document}

% Set style/preamble.Rnw as parent.

% Load all R packages and set up knitr

% This file loads R packages, configures knitr options and sets preamble.Rnw as 
% parent file
% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...

% Defines macros and environments

\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-lm.tex}

\lecturechapter{Introduction: Models \& Parameters}
\lecture{Introduction to Machine Learning}

% ------------------------------------------------------------------------------

\begin{vbframe}{What is a Model?}

\begin{itemize}

  \item A \textbf{model} (or \textbf{hypothesis}) 
  $$f : \Xspace \rightarrow \R^g$$ 
  is a function that maps feature vectors to predicted target values.
  
  \item $f$ is meant to capture intrinsic patterns of the data, the
  underlying assumption being that these patterns hold true for \emph{all} data 
  drawn from $\Pxy$.
  
  \item Loosely speaking: if $f$ is fed a set of features, it will output the 
  target corresponding to these feature values under our hypothesis.
  
  % \item As such, it is (the attempt at) a \textbf{formal representation} of the 
  % functional.
  
\end{itemize}
  
\lz  
  
{\footnotesize In conventional regression we will have $g = 1$; for 
classification $g$ equals the number of classes, and output vectors are scores 
or class probabilities (details later).}

\framebreak

% FIGURE SOURCE: https://docs.google.com/presentation/d/1bc6EQSsHEuVnyqFGX9E8oNfwOjAwVglRaIllxnOjLBo  /edit?usp=sharing Page 1
\begin{center}
  \includegraphics[width = 0.8\textwidth]{figure_man/the_model_web} 
\end{center}

\begin{itemize}
  
  \item It is easily conceivable how models can range from super simple (e.g.,
  tree stumps) to reasonably complex (e.g., variational autoencoders), and how 
  there is an infinite number of them.
  
  \item In fact, machine learning requires \textbf{constraining} $f$ to a 
  certain type of functions.

\end{itemize}


% \begin{itemize}
% 
%   \item A \textbf{model} (or \textbf{hypothesis}) 
%   $$f : \Xspace \rightarrow \R^g$$ 
%   is a function that maps feature vectors to predicted target values.
%   
%   \item As such, it is (the attempt at) a \textbf{formal representation} of the 
%   observed data.
%   
%   \item In conventional regression we will have $g = 1$, for classification
%   see later.
%   
%   \item $f$ is meant to capture intrinsic patterns of the data, the
%   underlying assumption being that these patterns hold true not only for the
%   observed sample but for all data drawn from $\Pxy$.
%   
%   \framebreak
%   
%   \item It is easily conceivable how models can range from super simple to
%   incredibly complex.
%   
%   \item The ultimate goal is to \emph{generalize} the learned model to new
%   data (we already know the outcome for our training data), with as little
%   error as possible.
%   
%   \item This suggests that we might be interested in a certain
%   simplifying property: a model is expected to perform complexity reduction.
%   
%   $\rightarrow$ It needs to be scalable and extendable to new data situations.
% 
%     % FIGURE SOURCE: https://docs.google.com/presentation/d/1bc6EQSsHEuVnyqFGX9E8oNfwOjAwVglRaIllxnOjLBo  /edit?usp=sharing Page 1
%   \begin{center}
%     \includegraphics[width = 0.8\textwidth]{figure_man/the_model_web} 
%   \end{center}
% 
% \end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Hypothesis Spaces}

\begin{itemize}

  \item Without restrictions on the functional family, the task of finding a 
  \enquote{good} model among all the available ones is impossible to solve.
  
  \item This means: we have to determine the class of our model \emph{a priori}, 
  thereby narrowing down our options considerably.
  
  \item The set of functions defining a specific model class is called a 
  \textbf{hypothesis space} $\Hspace$:
  
  $$\Hspace = \{f: f \text{ belongs to a certain functional family}\}$$
  
  \framebreak
  
  \item \textbf{Example 1:} Hypothesis space of univariate linear functions
  $$\Hspace = \{f: \fx =  \thx, \thetab \in \R^2\}$$
  \color{red} {FIGURE}
  \color{black}
  
  \item \textbf{Example 2:} Hypothesis space of bivariate quadratic functions
  $$\Hspace = \{f: \fx =  \thx, \thetab \in \R^p\}$$
  \color{red} {FIGURE}
  \color{black}
  
  \item \textbf{Example 3:} Hypothesis space of radial basis function networks
  $$\Hspace = \{f: \fx =  \thx, \thetab \in \R^p\}$$
  \color{red} {FIGURE}
  \color{black}

\end{itemize}  

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Parameters of a Model}

\begin{itemize}

  % \item Within one hypothesis space, models are \enquote{alike} in a sense: they 
  % all share a common structure that makes up the condition in defining 
  % $\Hspace$.
  % 
  % $\rightarrow$ In the above example, all models are straight lines passing
  % through the origin.
  %   
  % \item Of all models in a class it is the choice of \textbf{parameter} values 
  % that singles out a specific representant $f \in \Hspace$.
  % 
  % \item Parameters are our means of configuration: once set, our model is fully
  % determined.
  % 
  % $\rightarrow$ Origin-crossing lines are solely determined by their slope, $c$.
  % 
  % \item Parameters are the instrument to tailor the general 
  % hypotheses to our data.
  
  \item Considering the above examples, we see that all models within a 
  hypothesis space share a common functional structure.
  
  \item In fact, the only aspect in which they differ is the values of 
  \textbf{parameters}.
  
  $\rightarrow$ They are our means of configuration: once set, our model is 
  fully determined.
  
  \item Revisiting the space of linear functions, tweaking $\theta_0$ and
  $\theta_1$ is what hands us arbitrary straight lines.
  
  \begin{center}
    \includegraphics[width = 0.5\textwidth]{figure_man/lines.jpg} 
  \end{center}
  
  \framebreak
  
  \item This means: finding the optimal model is perfectly equivalent to 
  finding the optimal set of parameter values.
  
  \item We usually subsume all parameters in \textbf{parameter vector} 
  $\thetab = (\theta_1, \theta_2, ...)$ from \textbf{parameter space} 
  $\Theta$.
  
  \item The bijective relation between optimization over $f \in \Hspace$ and 
  optimization over $\thetab \in \Theta$ allows us to operationalize our search
  for the best model via the search for the optimal value on a $p$-dimensional
  parameter surface.
  
  \item $\thetab$ might be scalar or comprise thousands of parameters,
  depending on the complexity of our model.
  
  % \item $\thetab$ is what we try to learn during training: finding a 
  % \enquote{good} model boils down to finding a suitable combination of 
  % parameters.
  % 
  % \item We will see in the next chapter how the \enquote{goodness} of a model 
  % can be determined.
  
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Parameters, Statistics and Supervised ML}

\begin{itemize}
  
  \item Statistics, too, studies how to learn functions (or, rather: their 
  parameters) from example data and how to perform inference on them and 
  interpret the results.
  
  \item For historical reasons, though, statistics is mostly focused on fairly 
  simple classes of mappings, like (generalized) linear models.
  
  \item Supervised ML also includes more complex kinds of mappings that can 
  typically deal with more complicated and high-dimensional inputs.

\end{itemize} 

\end{vbframe}

% ------------------------------------------------------------------------------

\endlecture
\end{document}
