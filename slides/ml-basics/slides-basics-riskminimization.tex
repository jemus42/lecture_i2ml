\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi 
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R



\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}
\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
  %Define standard arrow tip
  >=stealth',
  %Define style for boxes
  punkt/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=6.5em,
    minimum height=2em,
    text centered},
  % Define arrow style
  pil/.style={
    ->,
    thick,
    shorten <=2pt,
    shorten >=2pt,}
}
\usepackage{subfig}


% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
\newcommand{\titlefigure}{figure/ml-basic-riskmin-error-surface.png}
\newcommand{\learninggoals}{\item Know the concept of loss \item Understand the relationship between loss and risk \item Understand the relationship between risk minimization and finding the best model}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}



\begin{document}
% Introduction to Machine Learning
% Day 1

% Set style/preamble.Rnw as parent.

% Load all R packages and set up knitr

% This file loads R packages, configures knitr options and sets preamble.Rnw as parent file
% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...








% Defines macros and environments
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

%! includes: basics-learners

\lecturechapter{Introduction: Losses \& Risk Minimization}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{How to Evaluate Models}

\begin{itemize}
\item In the training, we want to optimize $\thetab$. To score $\thetab$, we have to compare the actual output with the predicted output: 
\end{itemize}


% % FIGURE SOURCE: https://docs.google.com/presentation/d/1dTc5act2POjELGuD8wFIUbPxG0QRZlg1PaYdHGU-FJM/edit?usp=sharing
% \begin{center}\includegraphics[width=0.8\textwidth]{figure_man/eval_inducer1_web} \end{center}

%https://docs.google.com/presentation/d/1l3VlmPYZs_ycbAIVet05lBcCGEbQeUCl3CAG56BllvM/edit?usp=sharing page 1


\begin{center}\includegraphics[width=0.8\textwidth]{figure_man/ml-basics-riskmin-eval.png} \end{center}
\vspace{-0.5cm}
\begin{itemize}
    \item We need to define a suitable criterion, e.g.:
    \begin{itemize}
      \item Absolute error $|2588 - 2220| = 368$
      \item Squared error: $(2588 - 2220)^2 = 135,424$\\
    \end{itemize}
    \item The choice of this metric has a major influence on the final model, as it determines what a \emph{good} model is. 
    \item It will determine the ranking of the different models $f \in \Hspace$.
    \item The metric we use is called the \textbf{loss function}. 
  \end{itemize}
  
\end{vbframe}


\begin{vbframe}{Loss}

The \textbf{loss function} $\Lxy$ quantifies the "quality" of the prediction $\fx$ of a single observation $\xv$:

    $$
    L: \Yspace \times \R^g \to \R.
    $$

\lz

How "close" $\fx$ is to y can be quantified e. g. by the absolute loss $\Lxy = |\fx - y|$.


\vfill


%ml-basic-riskmin-1-loss.R first plot
\begin{center}
\begin{figure}[!b]
\includegraphics[width=0.9\textwidth]{figure/ml-basic_riskmin-1-loss_abs.png}
\end{figure}
\end{center}

\framebreak

Often, we use the L2-loss $\Lxy = (y - \fx)^2$:

%ml-basic-riskmin-1-loss.R first plot
\begin{center}
\begin{figure}[!b]
\includegraphics[width=0.9\textwidth]{figure/ml-basic_riskmin-1-loss_sqrd.png}
\end{figure}
\end{center}



\end{vbframe}


\begin{vbframe}{Risk}

The \textbf{risk function} quantifies the "quality" of the whole model. 

\lz

The ability of a model $f$ to reproduce the association between $\xv$ and $y$ that is present in the data $\D$ can be measured by the \textbf{summed loss}, also called \textbf{"empirical risk"}: \\

  $$
  \riskef = \sumin \Lxyi.
  $$
  $$
  \risk:  \Hspace \to \R.
  $$  
        

\vfill

\begin{center}
\begin{figure}[!b]
\includegraphics[width=1\textwidth]{figure/ml-basic_riskmin-2-risk.png}
\end{figure}
\end{center}

\framebreak 
   
\textbf{Notes:}
\begin{itemize}
\item The risk is often denoted as empirical mean over $\Lxy$
  $$
    \riskeb(f) = \frac{1}{n}\sumin \Lxyi. 
  $$
  The factor $\frac{1}{n}$ does not make a difference in optimization, so we will consider $\riske(f)$ most of the time.  
\item Since the model $f$ is usually defined by \textbf{parameters} $\thetab$ in a parameter space $\Theta$, this becomes:
$$\risk : \R^d \to \R.$$
\begin{eqnarray*}
\risket & = & \meanin \Lxyit \cr
\thetabh & = & \argmin_{\thetab \in \Theta} \risket
\end{eqnarray*}
 

\end{itemize}
   
   
\end{vbframe}




\begin{vbframe}{Risk minimization}

The best model is the model with the smallest risk. 

\lz

If we have a finite number of models $f$, we can compare the risk $\risket$ of all models: 



% calculated in ml-basic-riskmin-error-surface.R (theta_risk_df)
\begin{center}
\begin{tabular}{ c | c | c || c }
 Model & \(\displaystyle \thetab_{intercept} \) & \(\displaystyle \thetab_{slope} \) & \(\displaystyle \risket \) \\ 
 \hline
\(\displaystyle f_1 \)   & 2 & 3 & 194.62 \\
\(\displaystyle f_2 \)   & 3 & 2 & 127.12 \\  
\(\displaystyle f_3 \)   & 6 & -1 & 95.81 \\
\rowcolor{lightgray}
\(\displaystyle f_4 \)   & 1 & 1.5 & 57.96 \\  
\end{tabular}
\end{center}


\end{vbframe}

\begin{vbframe}{Risk Minimization}

But: Normally, the hypothesis space $\Hspace$ is infinitely large. 

\lz

As the the mapping of the hypothesis space to its parameters is bijective, we can consider the error surface depending on the parameters $\thetab$:

\begin{table}
\begin{minipage}{0.4\linewidth}
$$\risk : \R^d \to \R.$$
\scalebox{0.8}{
\begin{tabular}{ c | c | c || c }
 Model & \(\displaystyle \thetab_{intercept} \) & \(\displaystyle \thetab_{slope} \) & \(\displaystyle \risket \) \\ 
 \hline
\(\displaystyle f_1 \)   & 2 & 3 & 194.62 \\
\(\displaystyle f_2 \)   & 3 & 2 & 127.12 \\  
\(\displaystyle f_3 \)   & 6 & -1 & 95.81 \\
\rowcolor{lightgray}
\(\displaystyle f_4 \)   & 1 & 1.5 & 57.96 \\  
\end{tabular}
}
\end{minipage}\hfill
	\begin{minipage}{0.55\linewidth}
\includegraphics[width=\textwidth]{figure/ml-basic-riskmin-error-surface.png}
\end{minipage}
\end{table}

\end{vbframe}

\begin{vbframe}{Risk Minimization}


The process of finding the best model is called \textbf{empirical risk minimization} (ERM).

% Learning -- finding the \enquote{best} model -- then amounts to \textbf{empirical risk minimization}:\\    figuring out which model $f$ has the smallest average loss:
$$
\thetah = \argmin_{\thetab \in \Theta} \risket.
$$

\begin{table}
\begin{minipage}{0.4\linewidth}
$$\risk : \R^d \to \R.$$
\scalebox{0.8}{
\begin{tabular}{ c | c | c || c }
 Model & \(\displaystyle \thetab_{intercept} \) & \(\displaystyle \thetab_{slope} \) & \(\displaystyle \risket \) \\ 
 \hline
\(\displaystyle f_1 \)   & 2 & 3 & 194.62 \\
\(\displaystyle f_2 \)   & 3 & 2 & 127.12 \\  
\(\displaystyle f_3 \)   & 6 & -1 & 95.81 \\
\(\displaystyle f_4 \)   & 1 & 1.5 & 57.96 \\ 
\rowcolor{lightgray}
\(\displaystyle f_5 \)   & 1.25 & 0.90 & 23.40 \\ 
\end{tabular}
}
\end{minipage}\hfill
	\begin{minipage}{0.55\linewidth}
\includegraphics[width=\textwidth]{figure/ml-basic-riskmin-error-surface-best.png}
\end{minipage}
\end{table}

Most learners in ML try to solve the above \emph{optimization problem}, which implies a tight connection between ML and optimization.

\end{vbframe}


\begin{vbframe}{Further remarks}

\begin{itemize}
\item For regression tasks, the loss often only depends on the residual $\Lxy = L(y - \fx) = L(\eps)$.
\item The choice of loss implies which kinds of errors are important or not -- requires \emph{domain knowledge}!
\item For learners that correspond to probabilistic models, the loss determines / is equivalent to distributional assumptions.
\item Since learning can be re-phrased as minimizing the loss, the choice of loss strongly affects the computational difficulty of learning:
\begin{itemize}
    \item How smooth is $\risket$ in $\thetab$?
    \item Is $\risket$ differentiable so that we can use gradient-based methods?
    \item Does $\risket$ have multiple local minima or saddlepoints over $\Theta$?\\
\end{itemize}

\end{itemize}
\end{vbframe}



\endlecture

\end{document}