\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R



\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}
\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
  %Define standard arrow tip
  >=stealth',
  %Define style for boxes
  punkt/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=6.5em,
    minimum height=2em,
    text centered},
  % Define arrow style
  pil/.style={
    ->,
    thick,
    shorten <=2pt,
    shorten >=2pt,}
}
\usepackage{subfig}


% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}



\begin{document}
% Set style/preamble.Rnw as parent.

% Load all R packages and set up knitr

% This file loads R packages, configures knitr options and sets preamble.Rnw as parent file
% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...








% Defines macros and environments
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

%! includes: basics-supervised

\lecturechapter{Introduction: Tasks \& Data}

\lecture{Introduction to Machine Learning}

\begin{frame}{Supervised Tasks and Data}

Supervised Learning comes in two flavours:

\begin{itemize}
\item \textbf{Regression}: Given features $x$, predict corresponding output from $\Yspace \in \mathbb{R}^m$.
\item \textbf{Classification}: Assigning an observation with features $x$ to one class of a finite set of classes $\Yspace = \{C_1,...,C_g\}, g \geq 2$. (Details later.)
% \item \textbf{Density estimation}: Given an input $x$, predict the probability distribution $p(y|x)$ on $\Yspace$.
\end{itemize}

\end{frame}

\begin{vbframe}{Regression Task - Income Prediction} 
\begin{center}
  % FIGURE SOURCE: Screnshot from website (https://www.dice.com/salary-calculator)
  \includegraphics[width=\textwidth]{figure_man/salary_prediction.png}
\end{center}
\vspace{-0.5cm}
\begin{flushright}
  \tiny https://www.dice.com/salary-calculator
\end{flushright}

\end{vbframe}

\begin{vbframe}{More Regression Tasks}
\begin{enumerate}
\item \textbf{Predict house prices}
\medskip
\begin{itemize}
\item \textbf{Aim}: Predict the price for a house in a certain area
\item \textbf{Features}: e. g.
\begin{itemize}
\item square footage
\item number of bedrooms
\item swimming pool yes/no
\end{itemize}
\end{itemize}
\item \textbf{Predict the length-of-stay in a hospital at the time of admission}
\begin{itemize}
\item \textbf{Aim}: Predict the number of days a single patient has to stay in hospital
\item \textbf{Features}: e. g.
\begin{itemize}
\item diagnosis category (heart disease, injury,...)
\item admission type (urgent, emergency, newborn,...)
\item age
\item gender
\end{itemize}
\end{itemize}
\end{enumerate}
\end{vbframe}


\begin{vbframe}{Data}

Imagine you want to investigate how salary and workplace conditions
affect productivity of employees. Therefore, you collect data about
their worked minutes per week (productivity), how many people work in the
same office as the employees in question and the employees' salary.

% FIGURE SOURCE: https://docs.google.com/presentation/d/1qIWHJq-iZqfUsLLJD81Z9LhobGTIN3sDHTevnm5dxZ0/edit?usp=sharing
\begin{center}\includegraphics[width=0.6\textwidth]{figure_man/data_table} \end{center}

\end{vbframe}

\begin{frame}{Target and Features Relationship}

\begin{itemize}
\item For our observed data we know which outcome is produced
\item For new employees we can only observe the features, but not the target
\end{itemize}

\vspace{-0.5cm}

\scriptsize

% FIGURE SOURCE: https://drive.google.com/open?id=1WLPubv9vxLL-JIlHAtsvTBBG5pbF4xgRGW_prkOAEnE Page 2
\begin{center}\includegraphics[width=0.9\textwidth]{figure_man/new_data1_web} \end{center}

\normalsize

\vspace{-0.5cm}

\(\implies\) The goal is to predict the target variable for
\textbf{unseen new data} by using a \textbf{model} trained on the
already seen \textbf{training data}.\\

\end{frame}


\begin{vbframe}{Notation for Data}

\scriptsize

% FIGURE SOURCE: https://docs.google.com/presentation/d/1qIWHJq-iZqfUsLLJD81Z9LhobGTIN3sDHTevnm5dxZ0/edit?usp=sharing
\begin{center}\includegraphics[width=0.6\textwidth]{figure_man/data_table} \end{center}

\normalsize

\vspace{-0.5cm}

In supervised machine learning, we are given a dataset
\[
\D = \Dset \subset \left(\Xspace \times \Yspace\right)^n.
\]

We call

\begin{itemize}
  \item $\Xspace$  the input space with $p = \text{dim}(\Xspace)$ (for now: $\Xspace \subset \R^p$),
  \item $\Yspace$ the output / target space (e.g., $\Yspace = \R$ for regression or $\Yspace = \{C_1, ..., C_g\}$, $g \ge 2$, for classification),
  \item the tuple \(\xyi\) $\in \Xspace\times \Yspace$ the \(i\)-th observation,
  \item $\xj = \xjvec$ the j-th feature vector
\end{itemize}

\end{vbframe}


\begin{vbframe}{Data-Generating Process}

\begin{itemize}
\item We assume that a probability distribution
$$
\Pxy
$$
is defined on $\Xspace \times \Yspace$ that characterizes the process that generates the observed data $\D$.
\item Depending on the context, we denote the random variables following this distribution by $\xv$ and $y$.
\item Usually, we assume that the data is drawn i.i.d. from the joint probability density function (pdf) / probability mass function (pmf) $\pdfxy$.
\end{itemize}

\framebreak

\textbf{Remarks:}
\begin{itemize}
\item With a slight abuse of notation we write random variables, e.g., $\xv$ and $y$, in lowercase, as normal
variables or function arguments. The context will make clear what is meant.
\item Often, distributions are characterized by a parameter vector $\thetab \in \Theta$. We then write $\pdfxyt$.
\item This lecture mostly takes a frequentist perspective. Distribution parameters $\thetab$ appear behind the | for improved legibility, not to imply that we condition on them in a probabilistic Bayesian sense.
So, strictly speaking, $p(\xv | \thetab)$ should usually be understood to mean $p_{\thetab}(\xv)$ or $p(\xv, \thetab)$ or $p(\xv; \thetab)$.
\end{itemize}

\end{vbframe}




\endlecture

\end{document}
