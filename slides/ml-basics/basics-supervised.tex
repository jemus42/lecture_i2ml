\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}

\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
  %Define standard arrow tip
  >=stealth',
  %Define style for boxes
  punkt/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=6.5em,
    minimum height=2em,
    text centered},
  % Define arrow style
  pil/.style={
    ->,
    thick,
    shorten <=2pt,
    shorten >=2pt,}
}

\usepackage{subfig}

% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}

\begin{document}

% Set style/preamble.Rnw as parent.

% Load all R packages and set up knitr

% This file loads R packages, configures knitr options and sets preamble.Rnw as 
% parent file
% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...

% Defines macros and environments

\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\lecturechapter{Introduction: Supervised Learning}
\lecture{Introduction to Machine Learning}

% ------------------------------------------------------------------------------

\begin{vbframe}{Idea of Supervised Learning}

\begin{itemize}

  \item \textbf{Goal:} Identify the fundamental relations in the data that map
  the features to the target.
  
  $\rightarrow$ This allows us to make \textbf{predictions} for new data.
  
  \item Ideally, we would have full knowledge about the data-generating process
  and thus be able to specify this mapping function precisely.
  
  \item However, since this is basically impossible, we must try to 
  \textbf{learn} the mapping function: for objects exhibiting certain 
  patterns or properties, certain outcomes are much more likely.
  
  $\rightarrow$ We call such an assumed mapping a \textbf{model} $f$.

\end{itemize}  

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Supervised Learning}

\begin{itemize}

  \item Humans are pretty good at learning, at least for some domains. In 
  machine learning, we rely on computers, which is why the model itself, as well
  as all feature and target values, need to be computable.

  \item In \textbf{supervised} learning we make use of \textbf{labeled}
  data, i.e., observations for which we already know the target outcome.
  
  \item We try to construct $f$ automatically from an example set of such 
  labeled objects.
  
  \item Knowing the \enquote{truth} allows us to test how well we have grasped 
  the nature of the underlying mapping by comparing our predictions to the 
  actually observed values.
  
  
\end{itemize} 

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Tasks in Supervised Learning}

\begin{itemize}

  \item In general, supervised learning comes in two flavors we call 
  \textbf{tasks}:
  
  \begin{itemize}
  
    \item \textbf{Regression}: Given features $\xv$, predict corresponding 
    output from $\Yspace \in \mathbb{R}^m$.
    
    \begin{center}
      \includegraphics[width = 0.3\textwidth]{figure_man/ml-basics-supervised-regression-task.png} 
    \end{center}
    
    \item \textbf{Classification}: Assign an observation with features $\xv$ to 
    one class of a finite set of classes $\Yspace = \{C_1,...,C_g\}, g \geq 2$
    (details later).
    
    \begin{center}
      \includegraphics[width = 0.3\textwidth]{figure_man/ml-basics-supervised-classif-task.png} 
    \end{center}

  \end{itemize}
  
\end{itemize}  

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Regression Tasks: Example}

Imagine you want to investigate how salary and workplace conditions 
(\emph{features}) affect productivity of employees (\emph{target}) -- a standard 
\textbf{regression} task. Therefore, you collect data about their worked minutes 
per week (productivity), how many people work in the same office as the 
employees in question, and the employees' salary.
  
% FIGURE SOURCE: https://docs.google.com/presentation/d/1qIWHJq-iZqfUsLLJD81Z9LhobGTIN3sDHTevnm5dxZ0/edit?usp=sharing
\begin{center}
  \includegraphics[width = 0.8\textwidth]{figure_man/data_table} 
\end{center}

\framebreak

\begin{itemize}

  \item For our observed data we know which outcome is produced.
  
  \item For new employees can only observe the features but not the target.

\end{itemize}

\begin{center}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1WLPubv9vxLL-JIlHAtsvTBBG5pbF4xgRGW_prkOAEnE/edit?usp=sharing Page 4 
  \includegraphics[width=\textwidth]{figure_man/what_is_a_model_web} 
\end{center}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{More Regression Tasks}

\begin{enumerate}

  \item \textbf{Predict house prices}
  
  \medskip
  
  \begin{itemize}
  
    \item \textbf{Aim}: Predict the price for a house in a certain area
    \item \textbf{Features}: e. g.
    
    \begin{itemize}
    
      \item square footage
      \item number of bedrooms
      \item swimming pool yes/no
      
    \end{itemize}
    
  \end{itemize}
  
  \item \textbf{Predict the length-of-stay in a hospital at the time of 
  admission}
  
  \begin{itemize}
  
    \item \textbf{Aim}: Predict the number of days a single patient has to stay 
    in hospital
    \item \textbf{Features}: e. g.
    
    \begin{itemize}
    
      \item diagnosis category (heart disease, injury,...)
      \item admission type (urgent, emergency, newborn,...)
      \item age
      \item gender
      
    \end{itemize}
    
  \end{itemize}
  
\end{enumerate}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Classification Tasks: Example}

\begin{itemize}

  \item Imagine you work for an insurance company which \textbf{classifies} its
  life insurance customers according to five risk categories, depending on which 
  insurance premiums are charged.
  
  \item You might use features such as
  
  \begin{itemize}
  
    \item job type (white collar, carpenter, stuntman, ...)
    \item age
    \item smoking behavior
  
  \end{itemize}
  
  to perform this classification.

\end{itemize}

\begin{center}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1WLPubv9vxLL-JIlHAtsvTBBG5pbF4xgRGW_prkOAEnE/edit?usp=sharing Page 3
  \includegraphics[width = 0.2\textwidth]{figure_man/classif_ex_placeholder.jpg} 
\end{center}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Parameters, Statistics and Supervised ML}

\begin{itemize}

  \item Supervised ML additionally assumes that $f$ is of a certain 
  \enquote{form} or comes from a certain \textbf{class of functions}.\\
  This is necessary to make the problem of automatically finding a 
  \enquote{good} model feasible at all.
  
  \item The specific behavior of a mapping from this class can then be 
  described by \textbf{parameters} that define its shape.
  
  \item Statistics, too, studies how to learn such functions (or, rather: their 
  parameters) from example data and how to perform inference on them and 
  interpret the results.
  
  \item For historical reasons though, statistics is mostly focused on fairly 
  simple classes of mappings, like (generalized) linear models.
  
  \item Supervised ML also includes more complex kinds of mappings that can 
  typically deal with more complicated and high-dimensional inputs.

\end{itemize} 

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Summary}

\medskip

\textbf{Supervised machine learning} is concerned with learning a function 
that predicts a certain \textbf{target} from an object's \textbf{features} 
from a set of examples for which both the features and the target are known.\\
The function to be learned is restricted to come from a certain class of 
functions and its precise shape is defined in terms of a set of 
\textbf{parameters}.

\end{vbframe}

% ------------------------------------------------------------------------------

\endlecture
\end{document}
