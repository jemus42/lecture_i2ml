\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}

\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
  %Define standard arrow tip
  >=stealth',
  %Define style for boxes
  punkt/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=6.5em,
    minimum height=2em,
    text centered},
  % Define arrow style
  pil/.style={
    ->,
    thick,
    shorten <=2pt,
    shorten >=2pt,}
}

\usepackage{subfig}

% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}

\begin{document}

% Set style/preamble.Rnw as parent.

% Load all R packages and set up knitr

% This file loads R packages, configures knitr options and sets preamble.Rnw as 
% parent file
% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...

% Defines macros and environments

\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\lecturechapter{Introduction: Supervised Learning \& Learning Tasks}
\lecture{Introduction to Machine Learning}

% ------------------------------------------------------------------------------

\begin{vbframe}{Idea of Supervised Learning}

\begin{itemize}

  \item \textbf{Goal:} Identify the fundamental functional relation in the data 
  that maps an object's features to the target.
  
  \item Ideally, we would have full knowledge about the data-generating process
  and thus be able to specify this mapping function precisely.
  
  \item However, since this is basically impossible, we must try to 
  \textbf{learn} the mapping function: for objects exhibiting certain 
  patterns or properties, certain outcomes are much more likely.
  
  $\rightarrow$ We call such an assumed mapping a \textbf{model} $f$.
  
  % \item In machine learning, we rely on computers, which is why the model 
  % itself, as well as all feature and target values, need to be 
  % \textbf{computable}.
  
\end{itemize}  

\framebreak

\begin{itemize}

  % \item The set-up in supervised learning will typically look like this:
  % 
  % \begin{itemize}
  % 
  %   \item We restrict our options to a certain class of functions,
  %   
  %   \item choose some metric to evaluate model candidates,
  %   
  %   \item and try to find the best candidate in an efficient way.
  %   
  % \end{itemize} 
  % 
  % \item This procedure is carried out by an algorithm called \textbf{learner} or
  % \textbf{inducer}.
  
  \item \textbf{Supervised} learning means we make use of \emph{labeled}
  data, i.e., observations for which we already know the target outcome.
  
  \item We try to construct $f$ automatically from an example set of such 
  labeled objects.
  
  $\rightarrow$ The algorithm for finding $f$ is called \textbf{learner}.
  
  \item Using the thus learned model, we can make \textbf{predictions} based on
  the features of our data.
  
  \item Knowing the \enquote{truth} allows us to test how well we have grasped 
  the nature of the underlying mapping: we just need to compare our predictions 
  to the actually observed values.
  
  \framebreak
  
  \item Ultimately, we will use our model to compute predictions for 
  \textbf{new} data whose target values are unknown.
  
  \begin{center}
    \includegraphics[width = 0.7\textwidth]{figure_man/the_inducer_web.png}
  \end{center}

\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

% \begin{vbframe}{Supervised Learning}
% 
% \begin{itemize}
% 
%   \item \textbf{Supervised} learning means we make use of \emph{labeled}
%   data, i.e., observations for which we already know the target outcome.
%   
%   \item We try to construct $f$ automatically from an example set of such 
%   labeled objects.
%   
%   \item Using the thus learned model, we can make \textbf{predictions} based on
%   the features of our data.
% 
%   \item Knowing the \enquote{truth} allows us to test how well we have grasped 
%   the nature of the underlying mapping by comparing our predictions to the 
%   actually observed values.
%   
% \end{itemize} 
% 
% \end{vbframe}

% ------------------------------------------------------------------------------


\begin{vbframe}{Tasks in Supervised Learning}

\begin{itemize}

  \item In general, supervised learning comes in two flavors we call 
  \textbf{tasks}:
  
  \begin{itemize}
  
    \item \textbf{Regression}: Given features $\xv$, predict corresponding 
    output from $\Yspace \in \mathbb{R}^m$.
  
    \begin{center}
      \includegraphics[width = 0.3\textwidth]{figure/ml-basics-supervised-regression-task.png} 
    \end{center}

    \item \textbf{Classification}: Assign an observation with features $\xv$ to 
    one class of a finite set of classes $\Yspace = \{C_1,...,C_g\}, g \geq 2$
    (details later).
    \begin{center}
      \includegraphics[width = 0.3\textwidth]{figure/ml-basics-supervised-classif-task.png} 
    \end{center}
  \end{itemize}
\end{itemize}  
\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Regression Tasks: Example}

Imagine you want to investigate how salary and workplace conditions 
(\emph{features}) affect productivity of employees (\emph{target}) -- a standard 
\textbf{regression} task. Therefore, you collect data about their worked minutes 
per week (productivity), how many people work in the same office as the 
employees in question, and the employees' salary.
  
% FIGURE SOURCE: https://docs.google.com/presentation/d/1qIWHJq-iZqfUsLLJD81Z9LhobGTIN3sDHTevnm5dxZ0/edit?usp=sharing
\begin{center}
  \includegraphics[width = 0.8\textwidth]{figure_man/data_table} 
\end{center}

\framebreak

\begin{itemize}

  \item For our observed data we know which outcome is produced.
  
  \item For new employees can only observe the features but not the target.

\end{itemize}

\begin{center}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1WLPubv9vxLL-JIlHAtsvTBBG5pbF4xgRGW_prkOAEnE/edit?usp=sharing Page 4 
  \includegraphics[width=0.8\textwidth]{figure_man/what_is_a_model_web} 
\end{center}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{More Regression Tasks}

\begin{enumerate}

  \item \textbf{House Prices}
  
  % \medskip
  
    \textbf{Aim}: Predict the price for a house in a certain area
    
    \lz
    
    \begin{center}
    %https://docs.google.com/presentation/d/1m86-HK5-iCWbWpc_ssm2sZBQ8yxexIfjKcQXDU6DWss/edit?usp=sharing page 1
    \includegraphics[width=0.6\textwidth]{figure_man/ml-basics-supervised-task-houses-data.png} 
    
    \lz
    
    %https://unsplash.com/photos/sPpe2D7VbpM
    \includegraphics[width=0.35\textwidth]{figure_man/ml-basics-supervised-task-houses-pic.jpg} 
    
    \end{center}
    
  \framebreak
  
  \item \textbf{Length-of-stay in a hospital}
  
  \textbf{Aim}: Predict the number of days a single patient has to stay 
    in hospital at the time of admission
   
    
    \lz
    
    \begin{center}
    %https://docs.google.com/presentation/d/1m86-HK5-iCWbWpc_ssm2sZBQ8yxexIfjKcQXDU6DWss/edit?usp=sharing page 2
    \includegraphics[width=0.6\textwidth]{figure_man/ml-basics-supervised-task-hosp-data.png} 
    
    \lz
    
    %https://unsplash.com/photos/ZCO_5Y29s8k
    \includegraphics[width=0.3\textwidth]{figure_man/ml-basics-supervised-task-hosp-pic.jpg} 
    
    \end{center}
    
  
  
\end{enumerate}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Classification Tasks: Example}

\textbf{Risk category in life insurance}

  \textbf{Aim}: Predict one of five risk categories (\textbf{classification}) for a life insurance customer to determine the insurance premium 
\begin{center}

   %https://docs.google.com/presentation/d/1m86-HK5-iCWbWpc_ssm2sZBQ8yxexIfjKcQXDU6DWss/edit?usp=sharing page 3
    \includegraphics[width=0.6\textwidth]{figure_man/ml-basics-supervised-task-insurance-data.png} 

\lz

% FIGURE SOURCE: https://docs.google.com/presentation/d/1WLPubv9vxLL-JIlHAtsvTBBG5pbF4xgRGW_prkOAEnE/edit?usp=sharing Page 3
  \includegraphics[width = 0.2\textwidth]{figure_man/classif_ex_placeholder.jpg} 
\end{center}

\end{vbframe}

% ------------------------------------------------------------------------------

% \begin{vbframe}{Summary}
% 
% \medskip
% 
% \textbf{Supervised machine learning} is concerned with learning a function 
% that predicts a certain \textbf{target} from an object's \textbf{features} 
% from a set of examples for which both the features and the target are known.\\
% The function to be learned is restricted to come from a certain class of 
% functions and its precise shape is defined in terms of a set of 
% \textbf{parameters}.
% 
% \end{vbframe}

% ------------------------------------------------------------------------------

\endlecture
\end{document}
