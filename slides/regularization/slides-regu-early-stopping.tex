\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure_man/earlystop.png}
\newcommand{\learninggoals}{
  \item \textcolor{blue}{XXX}
  \item \textcolor{blue}{XXX}
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Early Stopping}
\lecture{Introduction to Machine Learning}



\begin{vbframe}{Early Stopping}
  
  \begin{itemize}
    \item When training with an iterative optimizer such as SGD, it is commonly the case that, after a certain number of iterations, generalization error begins to increase even though training error continues to decrease.     
    \item \textbf{Early stopping} refers to stopping the algorithm early before the generalization error increases.
  \end{itemize}
  \begin{figure}
    \centering
      \scalebox{0.5}{\includegraphics{figure_man/earlystop.png}}
      \caption{After a certain number of iterations, the algorithm begins to overfit.}
  \end{figure}
\framebreak
  How early stopping works:
  \begin{enumerate}
    \item Split training data $\Dtrain$ into $\mathcal{D}_{\text{subtrain}}$ and $\mathcal{D}_{\text{val}}$ (e.g. with a ratio of 2:1).
    \item Train on $\mathcal{D}_{\text{subtrain}}$ and evaluate model using the validation set $\mathcal{D}_{\text{val}}$.
    \item Stop training when validation error stops decreasing (after a range of \enquote{patience} steps).
    \item Use parameters of the previous step for the actual model.
  \end{enumerate}
  More sophisticated forms also apply cross-validation.
\framebreak
  \begin{table}
    \begin{tabular}{p{4cm}|p{6cm}}
    Strengths & Weaknesses \\
    \hline
    \hline
    Effective and simple & Periodical evaluation of validation error\\
    \hline
    Applicable to almost any model without adjustment \note{of objective function, parameter space, training procedure} & Temporary copy of $\thetab$ (we have to save the whole model each time validation error improves) \\
    \hline
    Combinable with other regularization methods & Less data for training $\rightarrow$ include $\mathcal{D}_{\text{val}}$ afterwards
    \end{tabular}
  \end{table}
  \begin{itemize}
    \item Relation between optimal early-stopping iteration $T_{\text{stop}}$ and weight-decay penalization parameter $\lambda$ for step-size $\alpha$ (see Goodfellow et al. (2016) page 251-252 for proof):
  \end{itemize}
    \begin{equation*}
      T_{\text{stop}} \approx \frac{1}{\alpha \lambda} 
        \Leftrightarrow \lambda \approx \frac{1}{T_{\text{stop}} \alpha}
    \end{equation*}
  \begin{itemize}
    \item Small $\lambda$ (low penalization) $\Rightarrow$ high $T_{\text{stop}}$ (complex model / lots of updates).
  \end{itemize}
\framebreak
  % \begin{itemize}
  %   \item[]
  % \end{itemize}
  % \begin{figure}
  %   \centering
  %     \includegraphics[width=11cm]{figure_man/early_stopping}
  %     \caption{Optimization path of early stopping (left) and weight decay (right) (Goodfellow et al. (2016))}
  % \end{figure}
  % \framebreak
  \begin{figure}
    \centering
      \scalebox{0.75}{\includegraphics{figure_man/earlystop_int_hat.png}}
      \tiny{\\ Credit: Goodfellow et al. (2016)\\}
  \end{figure}
  
\footnotesize 
\textbf{Figure:} An illustration of the effect of early stopping. \textit{Left:} The solid contour lines indicate the contours of the negative log-likelihood. The dashed line indicates the trajectory taken by SGD beginning from the origin. Rather than stopping at the point $\thetah$ that minimizes the risk, early stopping results in the trajectory stopping at an earlier point $\hat{\thetab}_{\text{Ridge}}$. \textit{Right:} An illustration of the effect of $L_2$ regularization for comparison. The dashed circles indicate the contours of the $L_2$ penalty which causes the minimum of the total cost to lie closer to the origin than the minimum of the unregularized cost.
\end{vbframe}



\endlecture
\end{document}