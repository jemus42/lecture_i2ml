\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure_man/bayes_reg.png}
\newcommand{\learninggoals}{
  \item Understand that regularization and parameter shrinkage can be applied to non-linear models
  \item Know structural risk minimization 
  \item Know how regularization risk minimization is the same as MAP 
      in a Bayesian perspective, where the penalty corresponds to parameter prior.
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Regularization in Non-Linear Models and Bayesian Priors}
\lecture{Introduction to Machine Learning}

%-------------------------------------------------------------------------------

\begin{vbframe}{Summary: Regularized Risk Minimization}

If we should define ML in only one line, this might be it:

$$
\riskrt= \min_{\thetab} \sumin \Lxyit + \lambda \cdot J(\thetab)
$$

We can choose for a task at hand:

\begin{itemize}
  \item the \textbf{hypothesis space} of $f$, which determines how features can 
  influence the predicted $y$
  \item the \textbf{loss} function $L$, which measures how errors should be treated
  \item the \textbf{regularization} $J(\thetab)$, which encodes our inductive 
  bias and preference for certain simpler models
\end{itemize}

\vfill

By varying these choices one can construct a huge number of different ML models. 
Many ML models follow this construction principle or can be interpreted through 
the lens of regularized risk minimization.

\end{vbframe}

%-------------------------------------------------------------------------------

\begin{vbframe}{Regularization in Nonlinear Models}

\begin{itemize}
  \item So far we have mainly considered regularization in LMs.
  \item Can also be applied to non-linear models (with numeric parameters), where it is 
  often important to prevent overfitting.
  \item Here, we typically use $L2$ regularization, which
      still results in parameter shrinkage and weight decay.
  \item By adding regularization, prediction surfaces in regression and 
  classification become smoother. 
  \item Note: In the chapter on non-linear SVMs we will study the effects of
  regularization on a non-linear model in detail. 
\end{itemize}

\end{vbframe}


%-------------------------------------------------------------------------------
%-------------------------------------------------------------------------------
\begin{frame}{Regularization in Nonlinear Models}

\small
\textbf{Setting}: Classification for the \texttt{spirals} data.
Neural network with single hidden layer containing 10 neurons and logistic 
output activation, regularized with $L2$ penalty term for $\lambda > 0$.
Varying $\lambda$ affects smoothness of the decision boundary and magnitude of 
network weights:

\vfill

\only<1>{\begin{center}\includegraphics[width=\textwidth]{figure/fig-regu-nonlin-1.png}\end{center}}
\only<2>{\begin{center}\includegraphics[width=\textwidth]{figure/fig-regu-nonlin-2.png}\end{center}}
\only<3>{\begin{center}\includegraphics[width=\textwidth]{figure/fig-regu-nonlin-3.png}\end{center}}
\only<4>{\begin{center}\includegraphics[width=\textwidth]{figure/fig-regu-nonlin-4.png}\end{center}}

%\only<5>{\includegraphics[width=\textwidth]{figure/fig-regu-nonlin-5.png}}
%\only<6>{\includegraphics[width=\textwidth]{figure/fig-regu-nonlin-6.png}}
\end{frame}

\begin{frame}{Regularization in Nonlinear Models}

The prevention of overfitting can also be seen in CV.
Same settings as before, but each $\lambda$ is evaluated with
repeated CV (10 folds, 5 reps). 

\begin{center}\includegraphics[width=0.7\textwidth]{figure/fig-regu-nonlin-srm-1.png}\end{center}

We see the typical U-shape with the sweet spot between overfitting (LHS, low $\lambda$) and 
underfitting (RHS, high $\lambda$) in the middle.
\end{frame}


%-------------------------------------------------------------------------------
\begin{vbframe} {Structural Risk Minimization}

\begin{itemize}
  % \item Complex models generalize poorly (overfitting) if merely the empirical risk is optimized. 
  \item Thus far, we only considered adding a complexity penalty to empirical risk minimization. 
  \item Instead,  structural risk minimization (SRM) assumes that the hypothesis space $\Hspace$ can be decomposed into increasingly complex hypotheses (size or capacity): $\Hspace = \cup_{k \geq 1 }\Hspace_{k}$. 
  \item Complexity parameters can be the, e.g. the degree of polynomials in linear models or the size of hidden layers in neural networks.  
\end{itemize}

\begin{center}
\includegraphics[width=0.5\textwidth]{figure_man/fig-regu-srm-1}
% FIGURE SOURCE:https://docs.google.com/drawings/d/1qFoFSyuY4glsNvgYgIZ96yRcznOdA5q3oogI5fVBQ1A/edit?usp=sharing
\end{center}

\framebreak


\begin{itemize}

    \item SRM chooses the smallest $k$ such that the optimal model from $\Hspace_k$ found by ERM or RRM cannot significantly
        be outperformed by a model from a $\Hspace_m$ with $m > k$.
  \item By this, the simplest model can be chosen, which minimizes the generalization bound.  
  \item One challenge might be choosing an adequate complexity measure, as for some models, multiple complexity measures exist.
\end{itemize}

\begin{center}
\includegraphics[width=0.6\textwidth]{figure_man/fig-regu-srm-2}
% FIGURE SOURCE: https://docs.google.com/drawings/d/1mk_qVUbfOYwwmuE0AgmnPiNSMoX--pE_nZsWYND0IhQ/edit?usp=sharing
\end{center}

\end{vbframe}

%-------------------------------------------------------------------------------
\begin{frame} {Structural Risk Minimization}

\small

\textbf{Setting}: Classification for the \texttt{spirals} data.
Neural network with single hidden layer containing $k$ neurons and logistic 
output activation, L2 regularized with $\lambda = 0.001$. 
So here SRM and RRM are both used.
Varying the size of the hidden layer affects smoothness of the decision boundary:


\vfill


\only<1>{\begin{center}\includegraphics[width=0.5\textwidth]{figure/fig-regu-nonlin-size-1.png}\end{center}}
\only<2>{\begin{center}\includegraphics[width=0.5\textwidth]{figure/fig-regu-nonlin-size-2.png}\end{center}}
\only<3>{\begin{center}\includegraphics[width=0.5\textwidth]{figure/fig-regu-nonlin-size-3.png}\end{center}}
\only<4>{\begin{center}\includegraphics[width=0.5\textwidth]{figure/fig-regu-nonlin-size-4.png}\end{center}}
\only<5>{\begin{center}\includegraphics[width=0.5\textwidth]{figure/fig-regu-nonlin-size-5.png}\end{center}}
\only<6>{\begin{center}\includegraphics[width=0.5\textwidth]{figure/fig-regu-nonlin-size-6.png}\end{center}}


\end{frame}

\begin{frame} {Structural Risk Minimization}
Again, complexity vs CV score. 

\begin{center}\includegraphics[width=0.7\textwidth]{figure/fig-regu-nonlin-srm-2.png}\end{center}

A minimal model with good generalization seems to have ca. 6-8 hidden neurons.

\end{frame}


\begin{frame} {Structural Risk Minimization and RRM}

Note that normal RRM can also be interpreted through SRM, if we rewrite the penalized ERM as constrained ERM.

\begin{columns}
\begin{column}{0.5\textwidth}
\begin{eqnarray*}
\min_{\thetab} && \sumin \Lxyit  \\
  \text{s.t. } && \|\thetab\|_2^2  \leq t \\
\end{eqnarray*}
\end{column}
\begin{column}{0.5\textwidth}
\begin{figure}
\includegraphics[width=0.6\textwidth]{figure_man/ridge_hat.png}
\end{figure}
\end{column}
\end{columns}

\vspace{0.5cm}

We can interpret going through $\lambda$ from large to small as through $t$ from small to large.
This constructs a series of ERM problems with hypothesis spaces $\Hspace_\lambda$, 
where we constrain the norm of $\thetab$ to unit balls of growing size.
\end{frame}


%-------------------------------------------------------------------------------

% \section{Regularization from a Bayesian Perspective}

\begin{vbframe} {RRM vs. Bayes}

We already created a link between max. likelihood estimation and ERM.

\lz 

Now we will generalize this for RRM.

\lz

Assume we have a parameterized distribution $p(y | \thetab, \xv)$ for our data and 
a prior $q(\thetab)$ over our parameter space, all in the Bayesian framework.

\lz 

From the Bayes theorem we know:

$$
p(\thetab | \xv, y) = \frac{p(y | \thetab, \xv) q(\thetab) }{p(y | \xv)} \propto 
p(y | \thetab, \xv) q(\thetab)
$$

\framebreak

The maximum a posteriori (MAP) estimator of $\thetab$ is now the minimizer of

$$
- \log p\left(y ~|~ \thetab, \xv\right) - \log q(\thetab).
$$

\begin{itemize}
  \item Again, we identify the loss $\Lxyt$ with $-\log(p(y | \thetab, \xv))$.
  \item If $q(\thetab)$ is constant (i.e., we used a uniform, non-informative 
  prior), the second term is irrelevant and we arrive at ERM.
  \item If not, we can identify $J(\thetab) \propto -\log(q(\thetab))$, i.e., 
  the log-prior corresponds to the regularizer, and the additional $\lambda$, which controls the strength of our
  penalty, usually influences the peakedness / inverse variance / strength of our prior.
\end{itemize}

\framebreak

\begin{figure}
  \centering
  \scalebox{1}{\includegraphics[width=\textwidth]{figure/bayes_prior.png}}
\end{figure}

\begin{itemize}
  \item $L2$ regularization corresponds to a zero-mean Gaussian prior with 
  constant variance on our parameters:
  $\theta_j \sim \mathcal{N}(0, \tau^2)$ 
  % ~ \forall j \in \{1, \dots , d\}$, $\tau > 0$.
  \item $L1$ corresponds to a zero-mean Laplace prior: 
  $\theta_j \sim \mathit{Laplace}(0,b)$.
  $\mathit{Laplace}(\mu, b)$ has density $\frac{1}{2b}\exp(-\frac{|\mu-x|}{b})$, 
  with scale parameter $b$, mean $\mu$ and variance $2b^2$.
  \item In both cases, regularization strength increases as the 
  variance of the prior decreases: a prior probability mass more narrowly 
  concentrated around 0 encourages shrinkage.
\end{itemize}
  
\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Example: Bayesian L2 regularization}

\small We can easily see the equivalence of $L2$ regularization and a Gaussian 
prior:

\begin{itemize}
  \small
  \item We define a Gaussian prior with uncorrelated components for $\thetab$:
  \begin{footnotesize}
    $$q(\thetab) = \mathcal{N}_d(\bm{0}, \mathit{diag}(\tau^2)) 
    = \prod_{j = 1}^d  \mathcal{N}(0, \tau^2)  
    = (2\pi\tau^2)^{-\frac{d}{2}} \exp \left( - \frac{1}{2\tau^2} \sum_{j = 1}^d 
    \theta_j^2 \right).$$
  \end{footnotesize} 
  \item With this, the MAP estimator becomes
  \begin{footnotesize}
  \begin{eqnarray*}
    \thetah^{\text{MAP}} &=& \argmin_{\thetab} \left(
    - \log p\left(y ~|~ \thetab, \xv \right) - \log q(\thetab)
    \right) \\
    &=& \argmin_{\thetab} \left(
    - \log p\left(y ~|~ \thetab, \xv \right) + \tfrac{d}{2} \log(2\pi \tau^2) +
    \frac{1}{2\tau^2} \sum_{j = 1}^d \theta_j^2
    \right) \\
    % &=& \argmin_{\thetab} \left(
    % - \log p\left(\xv ~|~ \thetab\right) + \frac{1}{2\tau^2} {\thetab}^T\thetab 
    % \right) \\
    &=& \argmin_{\thetab} \left(
    - \log p\left(y ~|~ \thetab, \xv \right) + \frac{1}{2\tau^2} \| \thetab \|_2^2
    \right).
  \end{eqnarray*}
  \end{footnotesize} 
  % \item $\frac{1}{\tau^2}$ controls prior precision, i.e., inverse variance, 
  % and thus the amount of shrinkage.
  \item We see how the inverse variance (precision) $1/\tau^2$ controls shrinkage.
  % (e.g., for linear Ridge regression with 
  % $\epsilon \sim \mathcal{N}(0, \sigma^2)$ we set 
  % $\lambda = \frac{\sigma^2}{\tau^2}$).
\end{itemize}
  
  % \item The conditional distribution of $\ydat$ in linear regression with 
  % Gaussian errors $\epsilon^{(i)} \sim \mathcal{N}(0, \sigma^2) ~ \forall i \in 
  % \setn$, $\sigma > 0$, is also Gaussian: 
  % \begin{footnotesize}
  % $$- \log p\left(\ydat ~|~ \xv, \thetab\right) = \frac{n}{2} \log (2\pi 
  % \sigma^2) + \frac{1}{2\sigma^2} \sumin \left(\yi - \fxit \right)^2.$$
  % \end{footnotesize}

% &=& \argmin_{\thetab} \left(
% \tfrac{n}{2} \log (2\pi \sigma^2) + \frac{1}{2\sigma^2} \sumin \left(\yi -
% \fxit \right)^2 + \tfrac{p}{2} \log(2\pi \tau^2) +
% \frac{1}{2\tau^2} \sumjp \theta_j^2
% \right) \\
% &=& \argmin_{\thetab} \left( \frac{1}{\sigma^2} \sumin \left(\yi -
% \fxit \right)^2 + \frac{1}{\tau^2} \sumjp \theta_j^2 \right) \\
% &=& \argmin_{\thetab} \left(
% \frac{1}{\sigma^2} \left(\ydat - \Xmat \thetab\right)^\top \left(\ydat - \Xmat
% \thetab\right) + \frac{1}{\tau^2} {\thetab}^T\thetab
% \right)

% \begin{eqnarray*}
% \thetah^{\text{MAP}} &=& argmin_{\thetab} \left(
% - \log p\left(\ydat ~|~ \xv, \thetab\right) - \log q(\thetab)
% \right) \\
% 
% &=& \argmin_{\thetab} \left(
% \tfrac{n}{2} \log (2\pi \sigma^2) + \frac{1}{2\sigma^2} \sumin \left(\yi -
% \fxit \right)^2 + \tfrac{p}{2} \log(2\pi \tau^2) +
% \frac{1}{2\tau^2} \sumjp \theta_j^2
% \right) \\
% &=& \argmin_{\thetab} \left( \frac{1}{\sigma^2} \sumin \left(\yi -
% \fxit \right)^2 + \frac{1}{\tau^2} \sumjp \theta_j^2 \right) \\
% &=& \argmin_{\thetab} \left(
% \frac{1}{\sigma^2} \left(\ydat - \Xmat \thetab\right)^\top \left(\ydat - \Xmat
% \thetab\right) + \frac{1}{\tau^2} {\thetab}^T\thetab
% \right)
% 
% &=& \argmin_{\thetab} \left(
% \tfrac{n}{2} \log (2\pi \sigma^2) + \frac{1}{2\sigma^2} \sumin \left(\yi -
% \fxit \right)^2 + \tfrac{p}{2} \log(2\pi \tau^2) +
% \frac{1}{2\tau^2} \sumjp \theta_j^2
% \right) \\
% &=& \argmin_{\thetab} \left( \frac{1}{\sigma^2} \sumin \left(\yi -
% \fxit \right)^2 + \frac{1}{\tau^2} \sumjp \theta_j^2 \right) \\
% &=& \argmin_{\thetab} \left(
% \frac{1}{\sigma^2} \left(\ydat - \Xmat \thetab\right)^\top \left(\ydat - \Xmat
% \thetab\right) + \frac{1}{\tau^2} {\thetab}^T\thetab
% \right)
% \end{eqnarray*}

% \footnotesize
% Now, define $\lambda := \frac{\sigma^2}{\tau^2}$ -- note how higher prior 
% precision (i.e., lower variance) increases shrinkage! -- and set the derivative 
% to 0:
% 
% \begin{scriptsize}
% \begin{eqnarray*}
% 0 &=& \frac{1}{\lambda \tau^2} \left( - {\Xmat}^T \ydat + \thetab {\Xmat}^T \Xmat
% \right) + \frac{\lambda}{\sigma^2} \thetab
% \quad \Leftrightarrow \quad 0 = \frac{\sigma^2}{\tau^2} \left( - {\Xmat}^T \ydat 
% + \thetab {\Xmat}^T \Xmat \right) + \lambda^2 \thetab \\
% 0 &=&  - {\Xmat}^T \ydat + \thetab {\Xmat}^T \Xmat + \lambda \thetab 
% \quad \Leftrightarrow \quad 
% \thetab(\Xmat^T \Xmat  + \lambda \id) = {\Xmat}^T \ydat
% \end{eqnarray*}
% \end{scriptsize}
% 
% $\Rightarrow \thetah^{\text{MAP}} = 
% (\Xmat^T \Xmat  + \lambda \id)^{-1} \Xmat^T\ydat  = \thetah^{\text{Ridge}}.$

\end{vbframe}

% ------------------------------------------------------------------------------

% \begin{vbframe} {Minimum Description Length}

% MDL principle

% \begin{itemize}
%   \item (Compress data using using fewer symbols than literal)
%   \item (In the MDL framework, learning is seen as data compression)
%   \item (More we compress, more we learn. Therefore, pick the hypothesis which results in the shortest code.)
%   \item (Occam's razor, principle of parsimony)
%   \item (All else being equal, a simpler explanation is better than a complex one.)
% \end{itemize}

% % \begin{equation}
% %     \begin{aligned}
% %     H_{\mathrm{mdl}} & :=\underset{H \in \mathcal{H}}{\arg \min }\left(L(H)+L_{H}(D)\right) \\
% %     L_{\mathrm{mdl}}(D) & :=\min _{H \in \mathcal{H}}\left(L(H)+L_{H}(D)\right)
% %   \end{aligned}
% % \end{equation}

% \framebreak

% \begin{itemize}
%   \item There is a correspondence between the length of a message L(x) and the distribution P(x)
%   $$P(x)=2^{-L(x)}, \quad L(x)=-\log _{2} P(x)$$
%   \item Two-part code : parameter block and data block
%   \item $L(H)$ is the length of a specific hypothesis in the set.
%   \item $L(D|H)$ is the length of the data encoded under H.
%   \item $L(D,H) = L(H) + L(D|H)$
%   \item (Represents a tradeoff between goodness-of-fit and complexity)
% \end{itemize}

% \framebreak

% Regression example : $Y_{i}=f\left(X_{i}\right)+\epsilon_{i} \text { for } i=1, \ldots, n \text { where } \epsilon_{i} \stackrel{i i d}{\sim} \mathcal{N}\left(0, \sigma^{2}\right)$

% For a given dataset, the length of the encoding is :

% $$\log 1 / p_{Y | X}\left(y^{n} | x^{n}\right) = \log \left(\sqrt{2 \pi \sigma^{2}} e^{\frac{1}{2 \sigma^{2}} \sum_{i=1}^{n}\left(y_{i}-f\left(x_{i}\right)\right)^{2}}\right) \propto \sum_{i=1}^{n}\left(y_{i}-f\left(x_{i}\right)\right)^{2}$$

% \lz

% The two-stage MDL procedure to pick the best hypothesis is :

% $$f_{\gamma}=\arg \min _{f \in F_{\gamma}}\left[L(f)+\frac{1}{n} \sum_{i=1}^{n}\left(Y_{i}-f\left(X_{i}\right)\right)^{2}\right]$$

% This is equivalent to regularized least squares.
% \end{vbframe}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%          REFERENCES          %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{vbframe}
% \frametitle{References}
% \footnotesize{
% \begin{thebibliography}{99}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \bibitem[Ian Goodfellow et al., 2016]{5} Ian Goodfellow, Yoshua Bengio and Aaron Courville (2016)
% \newblock Deep Learning
% \newblock \emph{\url{http://www.deeplearningbook.org/}}

% \end{thebibliography}
% }
% \end{vbframe}


\endlecture
\end{document}

