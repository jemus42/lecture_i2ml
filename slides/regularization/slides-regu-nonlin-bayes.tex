\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure_man/bayes_reg.png}
\newcommand{\learninggoals}{
  \item Know how regularization can be motivated from a Bayesian perspective
  \item Understand the correspondence between log-prior and regularization term
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Nonlinear and Bayes}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{Summary: Regularized Risk Minimization}

In $\riskr$ one has extreme flexibility to make appropriate choices

$$
\riskrf= \min_{f \in \Hspace} \sumin \Lxyi + \lambda \cdot J(f)
$$


for a given ML problem:

\begin{itemize}
  \item the \textbf{representation} of $f$, which determines how features can influence the predicted $y$
  \item the \textbf{loss} function, which measures how errors should be treated
  \item the \textbf{regularization} $J(f)$, which encodes our inductive bias and preference for certain simpler models
\end{itemize}

By varying these choices one can construct a huge number of different ML models. Many ML models follow this construction principle or can be interpreted through the lens of regularized risk minimization.

\end{vbframe}

\section{Regularization from a Bayesian Perspective}

\begin{vbframe} {Regularized Risk Minimization vs. Bayes}

We have already created a link between maximum likelihood estimation and empirical risk minimization.

\lz 

Now we will generalize this for regularized risk minimization.

\lz

Assume we have a parameterized distribution $p(\xv | \thetab)$ for our data and a prior $p(\thetab)$ over our
parameter space, all in the Bayesian framework.

\lz 

With Bayes theorem we know:

$$
p(\thetab | \xv) = \frac{p(\xv | \thetab) p(\thetab) }{p(\xv)} \propto p(\xv | \thetab) p(\thetab)
$$

\framebreak

The maximum a posteriori  (MAP) estimator of $\thetab$ is now the minimizer of

$$
- \sumin \log p\left(\xi ~|~ \thetab\right) - \log p(\thetab).
$$

Again, we identify the loss $\Lxyt$ with $-\log(p(\xv | \thetab))$. If $p(\thetab)$ is constant (i.e., we used a
  uniform, non-informative prior), we arrive at empirical risk minimization.

\lz 

If not, we can identify $J(\thetab) \propto -\log(p(\thetab))$, i.e., the log-prior corresponds to the regularizer, and the additional control parameter $\lambda$ corresponds to the relative strength of the prior in regularized risk minimization.

\framebreak

  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{figure_man/bayes_reg.png}}
  \end{figure}
  
  \begin{itemize}
    \item $L_2$ regularization corresponds to a zero-centered Gaussian prior, $\theta_i \sim \mathcal{N}(0,\sigma^2)$.
    \item $L_1$ regularization corresponds to a zero-centered Laplace prior, $\theta_i \sim \text{Laplace}(0,b) = \frac{1}{2b}\exp(-\frac{|\theta_i|}{b})$, where $b$ is a scale parameter.
    \item In both cases, as regularization strength $\lambda$ increases, the variance of the prior decreases, which in turn shrinks the parameters.
  \end{itemize}
\end{vbframe}



% \begin{vbframe} {Minimum Description Length}

% MDL principle

% \begin{itemize}
%   \item (Compress data using using fewer symbols than literal)
%   \item (In the MDL framework, learning is seen as data compression)
%   \item (More we compress, more we learn. Therefore, pick the hypothesis which results in the shortest code.)
%   \item (Occam's razor, principle of parsimony)
%   \item (All else being equal, a simpler explanation is better than a complex one.)
% \end{itemize}

% % \begin{equation}
% %     \begin{aligned}
% %     H_{\mathrm{mdl}} & :=\underset{H \in \mathcal{H}}{\arg \min }\left(L(H)+L_{H}(D)\right) \\
% %     L_{\mathrm{mdl}}(D) & :=\min _{H \in \mathcal{H}}\left(L(H)+L_{H}(D)\right)
% %   \end{aligned}
% % \end{equation}

% \framebreak

% \begin{itemize}
%   \item There is a correspondence between the length of a message L(x) and the distribution P(x)
%   $$P(x)=2^{-L(x)}, \quad L(x)=-\log _{2} P(x)$$
%   \item Two-part code : parameter block and data block
%   \item $L(H)$ is the length of a specific hypothesis in the set.
%   \item $L(D|H)$ is the length of the data encoded under H.
%   \item $L(D,H) = L(H) + L(D|H)$
%   \item (Represents a tradeoff between goodness-of-fit and complexity)
% \end{itemize}

% \framebreak

% Regression example : $Y_{i}=f\left(X_{i}\right)+\epsilon_{i} \text { for } i=1, \ldots, n \text { where } \epsilon_{i} \stackrel{i i d}{\sim} \mathcal{N}\left(0, \sigma^{2}\right)$

% For a given dataset, the length of the encoding is :

% $$\log 1 / p_{Y | X}\left(y^{n} | x^{n}\right) = \log \left(\sqrt{2 \pi \sigma^{2}} e^{\frac{1}{2 \sigma^{2}} \sum_{i=1}^{n}\left(y_{i}-f\left(x_{i}\right)\right)^{2}}\right) \propto \sum_{i=1}^{n}\left(y_{i}-f\left(x_{i}\right)\right)^{2}$$

% \lz

% The two-stage MDL procedure to pick the best hypothesis is :

% $$f_{\gamma}=\arg \min _{f \in F_{\gamma}}\left[L(f)+\frac{1}{n} \sum_{i=1}^{n}\left(Y_{i}-f\left(X_{i}\right)\right)^{2}\right]$$

% This is equivalent to regularized least squares.
% \end{vbframe}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%          REFERENCES          %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{vbframe}
% \frametitle{References}
% \footnotesize{
% \begin{thebibliography}{99}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \bibitem[Ian Goodfellow et al., 2016]{5} Ian Goodfellow, Yoshua Bengio and Aaron Courville (2016)
% \newblock Deep Learning
% \newblock \emph{\url{http://www.deeplearningbook.org/}}

% \end{thebibliography}
% }
% \end{vbframe}


\endlecture
\end{document}

