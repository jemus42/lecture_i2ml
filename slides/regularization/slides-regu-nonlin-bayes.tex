\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure/fig-regu-nonlin-size-4.png}
\newcommand{\learninggoals}{
  \item Know how regularization can be motivated from a Bayesian perspective
  \item Understand the correspondence between log-prior and regularization term
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Non-linear and Bayes Regularization}
\lecture{Introduction to Machine Learning}

%-------------------------------------------------------------------------------

\begin{vbframe}{Summary: Regularized Risk Minimization}

In $\riskr$ one has extreme flexibility to make appropriate choices

$$
\riskrt= \min_{\thetab} \sumin \Lxyit + \lambda \cdot J(\thetab)
$$

for a given ML problem:

\begin{itemize}
  \item the \textbf{representation} of $f$, which determines how features can 
  influence the predicted $y$
  \item the \textbf{loss} function, which measures how errors should be treated
  \item the \textbf{regularization} $J(\thetab)$, which encodes our inductive 
  bias and preference for certain simpler models
\end{itemize}

\vfill

By varying these choices one can construct a huge number of different ML models. 
Many ML models follow this construction principle or can be interpreted through 
the lens of regularized risk minimization.

\end{vbframe}

%-------------------------------------------------------------------------------

\begin{vbframe}{Non-linear Risk Minimization}

\begin{itemize}
  \item So far we have only considered regularization in linear models.
  \item We can equally apply regularization to non-linear models, where it is 
  often particularly important to prevent overfitting.
  \item In non-linear models we typically use $L2$ regularization, which has the 
  same effect as in linear models: parameter shrinkage and weight decay.
  \item By adding regularization, prediction surfaces in regression and 
  classification become smoother. 
  \item Note: In the chapter on non-linear SVMs we will study the effects of
  regularization on a non-linear model in detail. 
\end{itemize}

\end{vbframe}


%-------------------------------------------------------------------------------
%-------------------------------------------------------------------------------
\begin{frame}{Non-linear Risk Minimization: Regularization}

\small
\textbf{Setting}: Classification for the \texttt{spirals} data.
Neural network with single hidden layer containing 10 neurons and logistic 
output activation, regularized with $L2$ penalty term for $\lambda > 0$.
Varying $\lambda$ affects smoothness of the decision boundary and magnitude of 
network weights:

\vfill

\only<1>{\includegraphics[width=\textwidth]{figure/fig-regu-nonlin-1.png}}
\only<2>{\includegraphics[width=\textwidth]{figure/fig-regu-nonlin-2.png}}
\only<3>{\includegraphics[width=\textwidth]{figure/fig-regu-nonlin-3.png}}
\only<4>{\includegraphics[width=\textwidth]{figure/fig-regu-nonlin-4.png}}
\only<5>{\includegraphics[width=\textwidth]{figure/fig-regu-nonlin-5.png}}
\only<6>{\includegraphics[width=\textwidth]{figure/fig-regu-nonlin-6.png}}
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}{Non-linear Risk Minimization: Model Complexity}

\small

\textbf{Setting}: Classification for the \texttt{spirals} data.
Neural network with single hidden layer containing 10 neurons and logistic 
output activation and not regularized. 
Varying the size of the hidden layer affects smoothness of the decision boundary:

\vfill

\begin{center}
\only<1>{\includegraphics[width=0.5\textwidth]{figure/fig-regu-nonlin-size-1.png}}
\only<2>{\includegraphics[width=0.5\textwidth]{figure/fig-regu-nonlin-size-2.png}}
\only<3>{\includegraphics[width=0.5\textwidth]{figure/fig-regu-nonlin-size-3.png}}
\only<4>{\includegraphics[width=0.5\textwidth]{figure/fig-regu-nonlin-size-4.png}}
\only<5>{\includegraphics[width=0.5\textwidth]{figure/fig-regu-nonlin-size-5.png}}
\only<6>{\includegraphics[width=0.5\textwidth]{figure/fig-regu-nonlin-size-6.png}}
\end{center}

\end{frame}

%-------------------------------------------------------------------------------
\begin{vbframe} {Structural Risk Minimization}

\begin{itemize}
  \item Complex classifiers generalize poorly (overfitting) if merely the empirical risk is optimized. 
  \item Thus far, we only considered adding a complexity penalty to empirical risk minimization. 
  \item Instead,  structural risk minimization (SRM) assumes that the hypothesis space $\Hspace$ can be decomposed into increasingly complex hypotheses (size or capacity): $\Hspace = \cup_{k \geq 1 }\Hspace_{k}$. 
  \item Complexity parameters can be the, e.g. the degree of polynomials in linear models or the size of hidden layers in neural networks.  
\end{itemize}

\begin{center}
\includegraphics[width=0.5\textwidth]{figure_man/fig-regu-srm-1}
% FIGURE SOURCE:https://docs.google.com/drawings/d/1qFoFSyuY4glsNvgYgIZ96yRcznOdA5q3oogI5fVBQ1A/edit?usp=sharing
\end{center}

\framebreak


\begin{itemize}

  \item SRM consists of choosing an index $k \geq 1$  and a hypothesis minimizing the empirical risk in $\Hspace_k$, minimizing the generalization error.
  \item By this, the simplest model can be chosen, which minimizes the generalization bound.  
  \item One challenge might be choosing an adequate complexity measure, as for some models, multiple complexity measures exist.
\end{itemize}

\begin{center}
\includegraphics[width=0.7\textwidth]{figure_man/fig-regu-srm-2}
% FIGURE SOURCE: https://docs.google.com/drawings/d/1mk_qVUbfOYwwmuE0AgmnPiNSMoX--pE_nZsWYND0IhQ/edit?usp=sharing
\end{center}

\end{vbframe}
%-------------------------------------------------------------------------------

\section{Regularization from a Bayesian Perspective}

\begin{vbframe} {Regularized Risk Minimization vs. Bayes}

We have already created a link between maximum likelihood estimation and 
empirical risk minimization.

\lz 

Now we will generalize this for regularized risk minimization.

\lz

Assume we have a parameterized distribution $p(\xv | \thetab)$ for our data and 
a prior $q(\thetab)$ over our parameter space, all in the Bayesian framework.

\lz 

With the Bayes theorem we know:

$$
p(\thetab | \xv) = \frac{p(\xv | \thetab) q(\thetab) }{p(\xv)} \propto 
p(\xv | \thetab) q(\thetab)
$$

\framebreak

The maximum a posteriori (MAP) estimator of $\thetab$ is now the minimizer of

$$
- \log p\left(\xv ~|~ \thetab\right) - \log q(\thetab).
$$

\begin{itemize}
  \item Again, we identify the loss $\Lxyt$ with $-\log(p(\xv | \thetab))$.
  \item If $q(\thetab)$ is constant (i.e., we used a uniform, non-informative 
  prior), the second summand becomes irrelevant and we arrive 
  at empirical risk minimization.
  \item If not, we can identify $J(\thetab) \propto -\log(q(\thetab))$, i.e., 
  the log-prior corresponds to the regularizer, and the additional control 
  parameter $\lambda$ corresponds to the relative strength of the prior in 
  regularized risk minimization.
\end{itemize}

\framebreak

\begin{figure}
  \centering
    \scalebox{1}{\includegraphics{figure_man/bayes_reg.png}}
\end{figure}

\begin{itemize}
  \item $L2$ regularization corresponds to a zero-mean Gaussian prior with 
  constant variance:
  $\theta_j \sim \mathcal{N}(0, \tau^2) ~ \forall j \in \{1, \dots , d\}$, 
  $\tau > 0$.
  \item The $L1$ analogue for is a zero-mean Laplace prior: 
  $\theta_j \sim \mathit{Laplace}(0,b) = 
  \frac{1}{2b}\exp(-\frac{|\theta_i|}{b})$, where $b$ is a scale parameter.
  \item In both cases, regularization strength $\lambda$ increases as the 
  variance of the prior decreases: a prior probability mass more narrowly 
  concentrated around 0 encourages shrinkage.
\end{itemize}
  
\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Example: Bayesian L2 regularization}

\small We can easily see the equivalence of $L2$ regularization and Gaussian 
priors considering the example of a \textbf{Ridge} penalty term.

\begin{itemize}
  \small
  \item We define a Gaussian prior with uncorrelated components for $\thetab$:
  \begin{footnotesize}
    $$q(\thetab) = \mathcal{N}_d(\bm{0}, \mathit{diag}(\tau^2)) 
    = \prod_{j = 1}^d  \mathcal{N}(0, \tau^2)  
    = (2\pi\tau^2)^{-\frac{d}{2}} \exp \left( \frac{1}{2\tau^2} \sum_{j = 1}^d 
    \theta_j^2 \right).$$
  \end{footnotesize} 
  \item With this, the MAP estimator becomes
  \begin{footnotesize}
  \begin{eqnarray*}
    \thetah^{\text{MAP}} &=& argmin_{\thetab} \left(
    - \log p\left(\xv ~|~ \thetab\right) - \log q(\thetab)
    \right) \\
    &=& \argmin_{\thetab} \left(
    - \log p\left(\xv ~|~ \thetab\right) + \tfrac{d}{2} \log(2\pi \tau^2) +
    \frac{1}{2\tau^2} \sum_{j = 1}^d \theta_j^2
    \right) \\
    &=& \argmin_{\thetab} \left(
    - \log p\left(\xv ~|~ \thetab\right) + \frac{1}{2\tau^2} {\thetab}^T\thetab 
    \right) \\
    &=& \argmin_{\thetab} \left(
    - \log p\left(\xv ~|~ \thetab\right) + \frac{1}{2\tau^2} \| \thetab \|_2^2
    \right).
  \end{eqnarray*}
  \end{footnotesize} 
  % \item $\frac{1}{\tau^2}$ controls prior precision, i.e., inverse variance, 
  % and thus the amount of shrinkage.
  \item We see how the inverse variance $\tau^2$ controls shrinkage.
  % (e.g., for linear Ridge regression with 
  % $\epsilon \sim \mathcal{N}(0, \sigma^2)$ we set 
  % $\lambda = \frac{\sigma^2}{\tau^2}$).
\end{itemize}
  
  % \item The conditional distribution of $\ydat$ in linear regression with 
  % Gaussian errors $\epsilon^{(i)} \sim \mathcal{N}(0, \sigma^2) ~ \forall i \in 
  % \setn$, $\sigma > 0$, is also Gaussian: 
  % \begin{footnotesize}
  % $$- \log p\left(\ydat ~|~ \xv, \thetab\right) = \frac{n}{2} \log (2\pi 
  % \sigma^2) + \frac{1}{2\sigma^2} \sumin \left(\yi - \fxit \right)^2.$$
  % \end{footnotesize}

% &=& \argmin_{\thetab} \left(
% \tfrac{n}{2} \log (2\pi \sigma^2) + \frac{1}{2\sigma^2} \sumin \left(\yi -
% \fxit \right)^2 + \tfrac{p}{2} \log(2\pi \tau^2) +
% \frac{1}{2\tau^2} \sumjp \theta_j^2
% \right) \\
% &=& \argmin_{\thetab} \left( \frac{1}{\sigma^2} \sumin \left(\yi -
% \fxit \right)^2 + \frac{1}{\tau^2} \sumjp \theta_j^2 \right) \\
% &=& \argmin_{\thetab} \left(
% \frac{1}{\sigma^2} \left(\ydat - \Xmat \thetab\right)^\top \left(\ydat - \Xmat
% \thetab\right) + \frac{1}{\tau^2} {\thetab}^T\thetab
% \right)

% \begin{eqnarray*}
% \thetah^{\text{MAP}} &=& argmin_{\thetab} \left(
% - \log p\left(\ydat ~|~ \xv, \thetab\right) - \log q(\thetab)
% \right) \\
% 
% &=& \argmin_{\thetab} \left(
% \tfrac{n}{2} \log (2\pi \sigma^2) + \frac{1}{2\sigma^2} \sumin \left(\yi -
% \fxit \right)^2 + \tfrac{p}{2} \log(2\pi \tau^2) +
% \frac{1}{2\tau^2} \sumjp \theta_j^2
% \right) \\
% &=& \argmin_{\thetab} \left( \frac{1}{\sigma^2} \sumin \left(\yi -
% \fxit \right)^2 + \frac{1}{\tau^2} \sumjp \theta_j^2 \right) \\
% &=& \argmin_{\thetab} \left(
% \frac{1}{\sigma^2} \left(\ydat - \Xmat \thetab\right)^\top \left(\ydat - \Xmat
% \thetab\right) + \frac{1}{\tau^2} {\thetab}^T\thetab
% \right)
% 
% &=& \argmin_{\thetab} \left(
% \tfrac{n}{2} \log (2\pi \sigma^2) + \frac{1}{2\sigma^2} \sumin \left(\yi -
% \fxit \right)^2 + \tfrac{p}{2} \log(2\pi \tau^2) +
% \frac{1}{2\tau^2} \sumjp \theta_j^2
% \right) \\
% &=& \argmin_{\thetab} \left( \frac{1}{\sigma^2} \sumin \left(\yi -
% \fxit \right)^2 + \frac{1}{\tau^2} \sumjp \theta_j^2 \right) \\
% &=& \argmin_{\thetab} \left(
% \frac{1}{\sigma^2} \left(\ydat - \Xmat \thetab\right)^\top \left(\ydat - \Xmat
% \thetab\right) + \frac{1}{\tau^2} {\thetab}^T\thetab
% \right)
% \end{eqnarray*}

% \footnotesize
% Now, define $\lambda := \frac{\sigma^2}{\tau^2}$ -- note how higher prior 
% precision (i.e., lower variance) increases shrinkage! -- and set the derivative 
% to 0:
% 
% \begin{scriptsize}
% \begin{eqnarray*}
% 0 &=& \frac{1}{\lambda \tau^2} \left( - {\Xmat}^T \ydat + \thetab {\Xmat}^T \Xmat
% \right) + \frac{\lambda}{\sigma^2} \thetab
% \quad \Leftrightarrow \quad 0 = \frac{\sigma^2}{\tau^2} \left( - {\Xmat}^T \ydat 
% + \thetab {\Xmat}^T \Xmat \right) + \lambda^2 \thetab \\
% 0 &=&  - {\Xmat}^T \ydat + \thetab {\Xmat}^T \Xmat + \lambda \thetab 
% \quad \Leftrightarrow \quad 
% \thetab(\Xmat^T \Xmat  + \lambda \id) = {\Xmat}^T \ydat
% \end{eqnarray*}
% \end{scriptsize}
% 
% $\Rightarrow \thetah^{\text{MAP}} = 
% (\Xmat^T \Xmat  + \lambda \id)^{-1} \Xmat^T\ydat  = \thetah^{\text{Ridge}}.$

\end{vbframe}

% ------------------------------------------------------------------------------

% \begin{vbframe} {Minimum Description Length}

% MDL principle

% \begin{itemize}
%   \item (Compress data using using fewer symbols than literal)
%   \item (In the MDL framework, learning is seen as data compression)
%   \item (More we compress, more we learn. Therefore, pick the hypothesis which results in the shortest code.)
%   \item (Occam's razor, principle of parsimony)
%   \item (All else being equal, a simpler explanation is better than a complex one.)
% \end{itemize}

% % \begin{equation}
% %     \begin{aligned}
% %     H_{\mathrm{mdl}} & :=\underset{H \in \mathcal{H}}{\arg \min }\left(L(H)+L_{H}(D)\right) \\
% %     L_{\mathrm{mdl}}(D) & :=\min _{H \in \mathcal{H}}\left(L(H)+L_{H}(D)\right)
% %   \end{aligned}
% % \end{equation}

% \framebreak

% \begin{itemize}
%   \item There is a correspondence between the length of a message L(x) and the distribution P(x)
%   $$P(x)=2^{-L(x)}, \quad L(x)=-\log _{2} P(x)$$
%   \item Two-part code : parameter block and data block
%   \item $L(H)$ is the length of a specific hypothesis in the set.
%   \item $L(D|H)$ is the length of the data encoded under H.
%   \item $L(D,H) = L(H) + L(D|H)$
%   \item (Represents a tradeoff between goodness-of-fit and complexity)
% \end{itemize}

% \framebreak

% Regression example : $Y_{i}=f\left(X_{i}\right)+\epsilon_{i} \text { for } i=1, \ldots, n \text { where } \epsilon_{i} \stackrel{i i d}{\sim} \mathcal{N}\left(0, \sigma^{2}\right)$

% For a given dataset, the length of the encoding is :

% $$\log 1 / p_{Y | X}\left(y^{n} | x^{n}\right) = \log \left(\sqrt{2 \pi \sigma^{2}} e^{\frac{1}{2 \sigma^{2}} \sum_{i=1}^{n}\left(y_{i}-f\left(x_{i}\right)\right)^{2}}\right) \propto \sum_{i=1}^{n}\left(y_{i}-f\left(x_{i}\right)\right)^{2}$$

% \lz

% The two-stage MDL procedure to pick the best hypothesis is :

% $$f_{\gamma}=\arg \min _{f \in F_{\gamma}}\left[L(f)+\frac{1}{n} \sum_{i=1}^{n}\left(Y_{i}-f\left(X_{i}\right)\right)^{2}\right]$$

% This is equivalent to regularized least squares.
% \end{vbframe}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%          REFERENCES          %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{vbframe}
% \frametitle{References}
% \footnotesize{
% \begin{thebibliography}{99}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \bibitem[Ian Goodfellow et al., 2016]{5} Ian Goodfellow, Yoshua Bengio and Aaron Courville (2016)
% \newblock Deep Learning
% \newblock \emph{\url{http://www.deeplearningbook.org/}}

% \end{thebibliography}
% }
% \end{vbframe}


\endlecture
\end{document}

