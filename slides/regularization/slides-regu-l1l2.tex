\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure_man/ridge_hat.png}
\newcommand{\learninggoals}{
  \item Know the regularized linear model
  \item Know Ridge regression ($L2$ penalty)
  \item Know Lasso regression ($L1$ penalty)
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Lasso and Ridge Regression}
\lecture{Introduction to Machine Learning}


\begin{vbframe}{Regularization in the linear model}

  \begin{itemize}
  \item Linear models can also overfit if we operate in a high-dimensional space with not that many observations.    
  \item OLS usually require a full-rank design matrix.
  \item When features are highly correlated, the least-squares estimate becomes highly sensitive to random errors in the observed response, producing a large variance in the fit. 
  \item We now add a complexity penalty to the loss:
  $$
  \riskrt = \sumin \left(\yi - \thetab^\top \xi \right)^2 + \lambda \cdot J(\thetab). 
  $$ 
  \item Intuitive to measure model complexity as deviation from the 0-origin, as the 0-model is empty and contains no effects. Models close to this either have few active features or only weak effects. 
  \item So we measure $J(\thetab)$ through a vector norm.
    This shrinks coefficients closer 0, hence the term \textbf{shrinkage methods}.
  \end{itemize}

\end{vbframe}


% \section{Ridge Regression}

\begin{vbframe}{Ridge Regression}

  \textbf{Ridge regression} uses a simple $L2$ penalty:
  \begin{eqnarray*}  
  \thetah_{\text{Ridge}} &=& \argmin_{\thetab} \sumin \left(\yi - \thetab^T \xi \right)^2 + \lambda \|\thetab\|_2^2 \\
  &=& \argmin_{\thetab} \left(\yv - \Xmat \thetab\right)^\top \left(\yv - \Xmat \thetab\right) + \lambda \thetab^\top \thetab.
  \end{eqnarray*}

Optimization is possible (as in the normal LM) in analytical form:
$$\thetah_{\text{Ridge}} = ({\Xmat}^T \Xmat  + \lambda \id)^{-1} \Xmat^T\yv$$

Name comes from the fact that we add positive entries along the diagonal "ridge" $\Xmat^T \Xmat$.

\framebreak 

We understand the geometry of these 2 mixed components in our regularized risk objective much better, if we formulate the optimization as a constrained problem (see this as Lagrange multipliers in reverse).

\vspace{-0.5cm}

\begin{eqnarray*}
\min_{\thetab} && \sumin \left(\yi - \fxit\right)^2 \\
  \text{s.t. } && \|\thetab\|_2^2  \leq t \\
\end{eqnarray*}

\vspace{-1.0cm}

\begin{figure}
\includegraphics[width=0.3\textwidth]{figure_man/ridge_hat.png}
\end{figure}

\begin{footnotesize} 
NB: Relationship between $\lambda$ and $t$ will be explained later.
\end{footnotesize}

\framebreak
  
\begin{columns}
\begin{column}{0.5\textwidth}
\begin{figure}
\includegraphics[width=\textwidth]{figure_man/ridge_hat.png}
\end{figure}
\end{column}

\begin{column}{0.5\textwidth}
\begin{footnotesize} 
\begin{itemize}
  \item We still optimize the $\risket$, but cannot leave a ball around the origin.
  \item $\risket$ grows monotonically if we move away from $\thetah$.
  \item Inside constraints perspective: From origin, jump from contour line to contour line (better) until you become infeasible, stop before.
\item Outside constraints perspective: From $\thetah$, jump from contour line to contour line (worse) until you become feasible, stop then.
  \item So our new optimum will lie on the boundary of that ball.
\end{itemize}
\end{footnotesize}
\end{column}
\end{columns}


\end{vbframe}



\begin{vbframe}{Example: Polynomial Ridge Regression}

True (unknown) function is \(f(x) = 5 + 2x +10x^2 - 2x^3 + \epsilon\) (in red).

\lz

Let us consider a \(d\)th-order polynomial
\[ f(x) = \theta_0 + \theta_1 x + \cdots + \theta_d x^d = \sum_{j = 0}^{d} \theta_j x^j\text{.} \]
Using model complexity $d = 10$ overfits:

\begin{center}
\includegraphics[width = 10cm ]{figure_man/poly-ridge.png} \\
\end{center}

\framebreak

With an $L2$ penalty we can now select $d$ "too large" but regularize our model by shrinking its coefficients. Otherwise we have to optimize over the discrete $d$.

\vfill

\begin{center}
\includegraphics[width = 11cm ]{figure_man/poly-ridge02.png} \\
\end{center}


\begin{center}
\tiny
\begin{tabular}{ c| c c c c c c c c c c c c}
 $\lambda$ & $\beta_0$ & $\beta_1$ & $\beta_2$ & $\beta_3$ & $\beta_4$ & $\beta_5$ & $\beta_6$ & $\beta_7$ & $\beta_8$ & $\beta_9$ & $\beta_{10}$ \\ 
 \hline
 0.00 & 12.00 & -16.00 & 4.80 & 23.00 & -5.40 & -9.30 & 4.20 & 0.53 & -0.63 & 0.13 & -0.01 \\  
 10.00 & 5.20 &1.30 & 3.70 & 0.69 & 1.90 & -2.00 & 0.47 & 0.20 & -0.14 & 0.03 & -0.00 \\ 
 100.00 & 1.70 & 0.46 & 1.80 & 0.25 & 1.80 & -0.94 & 0.34 & -0.01 & -0.06 & 0.02 & -0.00
\end{tabular}
\end{center}


\end{vbframe}

% \section{Lasso Regression}

\begin{vbframe}{Lasso Regression}

Another shrinkage method is the so-called \textbf{Lasso regression}, which uses an $L1$ penalty on $\thetab$:

\begin{eqnarray*}
\thetah_{\text{Lasso}} &=&  \argmin_{\thetab} \sumin \left(\yi - \thetab^T \xi\right)^2 + \lambda \|\thetab\|_1 \\
  &=& \argmin_{\thetab} \left(\yv - \Xmat \thetab\right)^\top \left(\yv - \Xmat \thetab\right) + \lambda \|\thetab\|_1.
\end{eqnarray*}

Note that optimization now becomes much harder. $\riskrt$ is still convex, but we have moved from an optimization problem with an analytical solution towards a non-differentiable problem.

\lz

Name: least absolute shrinkage and selection operator.

\framebreak 

We can also rewrite this as a constrained optimization problem. The penalty results in the constrained region to look like a diamond shape.

\begin{eqnarray*}
\min_{\thetab} && \sumin \left(\yi - \fxit\right)^2\\
\text{subject to: } && \|\thetab\|_1 \leq t \\
\end{eqnarray*}

\vspace*{-1cm}

  \begin{figure}
\includegraphics[width=0.3\textwidth]{figure_man/lasso_hat.png}\\
\end{figure}

\end{vbframe}


\endlecture
\end{document}

