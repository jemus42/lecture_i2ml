\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R



\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}
\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
  %Define standard arrow tip
  >=stealth',
  %Define style for boxes
  punkt/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=6.5em,
    minimum height=2em,
    text centered},
  % Define arrow style
  pil/.style={
    ->,
    thick,
    shorten <=2pt,
    shorten >=2pt,}
}
\usepackage{subfig}


% Defines macros and environments
\input{../../style/common.tex}
% \input{common.tex}

%\usetheme{lmu-lecture}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}



\begin{document}
% Introduction to Machine Learning
% Day 1

% Set style/preamble.Rnw as parent.

% Load all R packages and set up knitr

% This file loads R packages, configures knitr options and sets preamble.Rnw as parent file
% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...








% Defines macros and environments
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

%! includes: basics-riskminimization

\lecturechapter{Loss Functions for Regression}
\lecture{Introduction to Machine Learning}


\begin{vbframe}{Regression Losses - L2 / Squared Error}
\begin{itemize}
\item $\Lxy = (y-\fx)^2$ or $\Lxy = 0.5 (y-\fx)^2$
\item Convex
\item Differentiable, gradient no problem in loss minimization
\item For latter: $\pd{0.5 (y-\fx)^2}{\fx} = y - \fx = \eps$, derivative is residual
\item Tries to reduce large residuals (if residual is twice as large, loss is 4 times as large), hence
  outliers in $y$ can become problematic
\item Connection to Gaussian distribution (see later)
\end{itemize}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

%source("images/plot_loss_1.R")
{\centering \includegraphics[width=0.85\textwidth]{figure/plot_quad_loss} 
}



\end{knitrout}
\framebreak


What's the optimal constant prediction $c$ (i.e. the same $\yh$ for all $\xv$)?
$$\Lxy = (y-\fx)^2 = (y - c)^2$$
We search for the $c$ that minimizes the empirical risk.
$$  \hat{c} = \argmin_{c \in \R}\riske(c)  =  \argmin_{c \in \R} \meanin  (\yi-c)^2 $$
We set the derivative of the empirical risk to zero and solve for $c$:
\begin{eqnarray*}
 -\meanin 2(\yi - c) &=& 0 \cr
\hat{c} &=& \meanin \yi
\end{eqnarray*}
% $\frac{1}{n} \sumin \yi$ is also the maximum likelihood estimator for $\E[y]$ .

% \framebreak
% What is the optimal prediction if we allow $c$ to depend on $x$? \\
% According to the law of total expectation:
% \begin{displaymath}
%   \E_{xy} [\Lxy] = \E_x
%   \left[\E_{y|x}[(y-\fx)^2|x=x]\right]
% \end{displaymath}
% For the optimal prediction it suffices to minimize the risk pointwise:
% $$
%   \fh(x) = \mbox{argmin}_c \E_{y|x}[(y-c)^2|x=x]=\E (y | x=x)
% $$
% So for squared loss, the best prediction in every point is the conditional mean of $y$ given $x$.
%
% \lz
%
% The last step follows from:
% $$
% E[(y - c)^2] = Var[y - c] + (E[y - c])^2 = Var[y] + (E[y] - c)^2
% $$
% This is minimal for $c=E[y]$ if $c$ constant and for $c = E[y|x = x]$ if $c$ is not constant.
\end{vbframe}

\begin{vbframe}{Regression Losses - L1 / Absolute Error}
\begin{itemize}
\item $\Lxy = |y-\fx|$
\item Convex
\item No derivatives for $ = 0$, $y = \fx$, optimization becomes harder
%\item More robust, outliers in $y$ are less problematic
\item $\fxh = \text{median of } y | \xv$
%\item Connection to Laplace distribution (see later)
\end{itemize}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/plot_abs_loss} 

}



\end{knitrout}

\framebreak

\begin{itemize}
\item $\Lxy = |y-\fx|$
\item Convex
\item No derivatives for $\eps = 0$, $y = \fx$, optimization becomes harder
\item $\fxh = \text{median of } y | \xv$
\item More robust, outliers in $y$ are less influential than for L2
%\item Connection to Laplace distribution (see later)
\end{itemize}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/plot_abs_log_quad_loss} 

}



\end{knitrout}

\end{vbframe}

% \begin{vframe}{Regression losses - Huber loss}
% \begin{itemize}
% \item Huber loss $L_\delta(y, f(x)) =
%   \begin{cases}
%   \frac{1}{2}(y-f(x))^2  & \text{ if } |y-f(x)| \le \delta \\
%   \delta |y-f(x)|-\frac{1}{2}\delta^2 \quad & \text{ otherwise }
%   \end{cases}$
% \item Piecewise combination of of L1 and L2 loss
% \item Convex
% \item Combines advantages of L1 and L2 loss: differentiable, robust
% \end{itemize}
%
% <<echo=FALSE, results='hide', fig.height=3, fig.align='center'>>=
% x = seq(-2, 2, by = 0.01); y = ifelse(abs(x) <= 1, 1 / 2 * x^2, abs(x) - 1 / 2)
% qplot(x, y, geom = "line", xlab = expression(y-f(x)), ylab = expression(L(y-f(x))))
% @
% \end{vframe}

\endlecture
\end{document}
