
\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{figure/reg_l2_residual.pdf}
\newcommand{\learninggoals}{
\item Grasp the overall concept of linear regression
\item Know that $L2$ loss optimization results in SSE-minimal model
\item Be aware that there is $L1$ regression and polynomial regression as an alternative to the classical Linear Model
}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}
\lecturechapter{In a Nutshell: Supervised Regression}
\lecture{Introduction to Machine Learning}

% ------------------------------------------------------------------------------

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% L2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Linear regression}

\begin{itemize}
    \item Setting: Supervised task with labeled data and numerical target
    \item The target is denoted by $y \in \R$ and the features by $\xv \in \R^p$
    \item Idea: predict the target $y$ as a \textbf{linear combination} of 
    features $\xv$:
    $$\yh = \fx = \thx = \theta_0 + \theta_1 x_1 + \dots + \theta_p x_p$$
    $~\rightsquigarrow$ the Hypothesis space is restricted to linear functions
    \item Find loss-optimal parameters $\thetab \in \R^{p+1}$ to describe relation 
    $y | \xv$
    % \item Hypothesis space: $\Hspace = \{\fx = \thx\ |\ \thetab \in \R^{p+1} \}$
    
\end{itemize}
\vfill
\begin{minipage}{0.4\textwidth}
    \includegraphics[width=\textwidth]{figure/reg_l2_basic_lm.pdf} 
\end{minipage}
\hspace{1cm}
\begin{minipage}{0.4\textwidth}
    \includegraphics[width=1.2\textwidth, trim=100 0 0 20, clip]{
    figure/reg_l2_basic_lm_biv.pdf} 
\end{minipage}

\end{vbframe} 

% ------------------------------------------------------------------------------

\begin{vbframe}{Model fit with l2 loss}

\begin{itemize}
    \item How to determine parameters that specify the linear model? $\rightsquigarrow$ define loss function \& optimize risk
    \item Popular loss function for linear regression: \textbf{$L2$ loss} / \textbf{quadratic loss} / 
    \textbf{squared error}
    $$\Lxy = (y - \yh)^2 = (y-\fx)^2 = (y-\thx)^2$$
    
    % \includegraphics[width=0.35\textwidth]{figure/reg_l2_residual.pdf}
    % diese Grafik ist irgendwie verwirrend, weil es so aussieht, als 
    % wäre das Kästchen ein Rechteck und kein Quadrat
    \item The difference between the observed value $y$ and the estimated value $\yh = \fx$ is known as the \textbf{residual} and denoted by $r = y - \fx$
    \item By choosing the $L2$ loss, the residuals are penalized \textbf{quadratically}
    \item Reasons for choosing the $L2$ loss:
    \begin{itemize}
        \item Easy to optimize (convex, differentiable)
        \item Theoretically appealing characteristics 
        \item Connection to classical stats LM
    \end{itemize}
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{l2 loss plots}

We will often visualize loss effects like this:

\vfill

\includegraphics[width=\textwidth]{figure/reg_l2_lossplot_quad.pdf}

\hspace{0.5cm}
\begin{minipage}[t]{0.45\textwidth}
    \footnotesize
    \begin{itemize}
        \item Data as $y \sim x_1$
        % \item Prediction hypersurface \\$\rightsquigarrow$ here: line
        \item Residuals \textcolor{blue}{$r = y - \fx$}
        \\$\rightsquigarrow$ squares to illustrate loss
    \end{itemize}
\end{minipage}
\hfill
\begin{minipage}[t]{0.4\textwidth}
    \footnotesize
    \begin{itemize}
        \item Loss as function of residuals
        %\\$\rightsquigarrow$ strength of penalty? 
        %\\$\rightsquigarrow$ symmetric?
        \item Highlighted: loss for residuals shown on LHS
    \end{itemize}
\end{minipage}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{analytical optimization with l2 loss}

\begin{itemize}
    \item With the L2 loss, the resulting \textbf{risk} is:
    $$\risket = \sumin \left(\yi - \thetab^\top \xi \right)^2$$
    \item This is equivalent to the \textbf{sum of squared errors (SSE)}
    \item By minimizing the sum of the quadratic differences between $y$ and $\yh$, we find the loss-optimal model
    \item Special property of LM with $L2$ loss: \textbf{analytical solution} available
    \begin{align*}
        \thetabh \in 
        \argmin_{\thetab} \risket &=
        \argmin_{\thetab} \sumin \left(\yi - \thetab^\top \xi \right)^2  %\\
        % &= \argmin_{\thetab} \| \yv - \Xmat \thetab \|^2_2
    \end{align*}
    \normalsize
    \item The solution $\thetabh$ is known as the \textbf{ordinary-least-squares (OLS)} estimator and we can find it via \textbf{normal equations}
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Statistical properties l2 loss}

% \footnotesize
\begin{itemize}
    % \small
    % \item In a nutshell: minimize quadratic residuals of form 
    % $\| \yv - \Xmat \thetab \|^2_2$
    \item LM with $L2$ loss intimately related to classical stats LM
    \item Assumptions
    \begin{itemize}
        % \small
        \item $\xi$ \textbf{i}ndependent and \textbf{i}dentically \textbf{d}istributed \textbf{(iid)} for $i \in \nset$
        \item \textbf{Homoskedastic} (equivariant) 
         \textbf{Gaussian} errors
        $$\yv = \Xmat \thetab + \bm{\epsilon}, ~ \bm{\epsilon} \sim 
        \normal(0, \sigma^2 \id)  $$
        $\rightsquigarrow$ $y_i$ conditionally independent \& normally distributed
        % $\yv | \Xmat \sim \normal(\Xmat \thetab, \sigma^2 \id)$
        \item \textbf{Uncorrelated features} \\$\rightsquigarrow$ 
        multicollinearity destabilizes effect estimation 
    \end{itemize}
    \item If assumptions hold: statistical \textbf{inference} applicable
    \begin{itemize}
        % \small
            \item Hypothesis tests on significance of effects
            \item Confidence \& prediction intervals
            \item Meaningful Goodness-of-fit measures 
    \end{itemize}
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------



% ------------------------------------------------------------------------------

\begin{vbframe}{Effect interpretation - ans Ende setzen, ist für l1 und l2 loss gleich}

\begin{itemize}
    \item Big plus of LM: immediately \textbf{interpretable} feature effects
    \item "Marginally increasing $x_j$ by 1 unit increases $y$ by $\theta_j$ 
    units" \\
    $\rightsquigarrow$ \textit{ceteris paribus} assumption: 
    $x_1, \dots, x_{j - 1}, x_{j + 1}, \dots, x_p$ fixed
\end{itemize}

\vfill
\includegraphics[width=0.4\textwidth]{figure/reg_l2_basic_lm_interpreted.pdf} 
\hfill
\includegraphics[width=0.55\textwidth]{figure_man/lm_summary} 

\end{vbframe}

% ------------------------------------------------------------------------------

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% L1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Regression with absolute loss}

\begin{itemize}
    \item $L2$ regression minimizes quadratic residuals -- wouldn't 
    \textbf{absolute} residuals seem more natural? 
    \vspace{0.2cm}
    \begin{center}
    \includegraphics[width=0.55\textwidth]{figure/reg_l1_residual_abs_vs_quad}
    \end{center}
    \item \textbf{$L1$ loss / absolute error / least absolute deviation (LAD)}
    $$\Lxy = |y - \fx|$$
    \begin{center}
    \includegraphics[width=0.55\textwidth]{figure/reg_l1_lossplot_abs}
    \end{center}
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{L1 vs L2 - Pros and Cons}

\begin{itemize}
    \item L1 loss harder to optimize than l2 loss, there is no analytical solution since \textbf{not differentiable} in $y - \fx = 0$
    \item L1 loss slower to optimize
    \item Estimated parameters of $L1$ and $L2$ regression often not that different 
    $\rightsquigarrow$ except in presence of outliers 
    \item In that case, L1 regression is more robust  

\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Polynomial Regression Models}

\begin{itemize}
    \item The assumption that y is a linear function of x is quite restrictive
    \item We can use LM framework much more \textbf{flexibly}
    \item Recall what we previously defined as LM:
    \begin{equation} \label{simple_lm}
      f(x) = \theta_0 + \sumjp \theta_j x_j =
      \theta_0 + \theta_1 x_1 + \dots + \theta_p x_p
    \end{equation}
    \item We can rewrite Eq.~\ref{simple_lm} as this:
    \item \textbf{The linear model} with \textbf{basis functions}
    \textcolor{blue}{$\phi_j$}:
    \begin{equation*} \label{true_lm}
      \fx = \theta_0 + \sumjp \theta_j \textcolor{blue}{\phi_j} (x_j)
      = \theta_0 + \theta_1  \textcolor{blue}{\phi_1} \left( x_1 \right)
      + \dots + \theta_p \textcolor{blue}{\phi_p} \left(x_p \right)
    \end{equation*}
    \item Using the identity transformation \textcolor{blue}{$\phi_j = \text{id}_x: x \mapsto x$}~~ $\forall j$, Eq.~\ref{simple_lm} and Eq.~\ref{true_lm} are equivalent
    \item We can however use other basis functions $\phi_j$ such as \textbf{$d$-polynomials} to increase flexibility

\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------


\endlecture

\end{document}
