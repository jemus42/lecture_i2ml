% das hier ist gerade nur eine Kopie von den l2 slides


\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{figure/reg_l2_residual.pdf}
\newcommand{\learninggoals}{
\item Grasp the overall concept of linear regression
\item Understand how $L2$ loss optimization results in SSE-minimal model
\item Understand this as a general template for ERM in ML
}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}
\lecturechapter{In a Nutshell: Supervised Regression}
\lecture{Introduction to Machine Learning}

% ------------------------------------------------------------------------------

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% L2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Linear regression}

\begin{itemize}
    \item Setting: Supervised task with labeled data and numerical target
    \item The target is denoted by $y \in \R$ and the features by $\xv \in \R^p$
    \item Idea: predict the target $y$ as a \textbf{linear combination} of 
    features $\xv$:
    $$\yh = \fx = \thx = \theta_0 + \theta_1 x_1 + \dots + \theta_p x_p$$
    $~\rightsquigarrow$ the Hypothesis space is restricted to linear functions
    \item Find loss-optimal parameters $\thetab \in \R^{p+1}$ to describe relation 
    $y | \xv$
    % \item Hypothesis space: $\Hspace = \{\fx = \thx\ |\ \thetab \in \R^{p+1} \}$
    
\end{itemize}
\vfill
\begin{minipage}{0.4\textwidth}
    \includegraphics[width=\textwidth]{figure/reg_l2_basic_lm.pdf} 
\end{minipage}
\hspace{1cm}
\begin{minipage}{0.4\textwidth}
    \includegraphics[width=1.2\textwidth, trim=100 0 0 20, clip]{
    figure/reg_l2_basic_lm_biv.pdf} 
\end{minipage}

\end{vbframe} 

% ------------------------------------------------------------------------------

\begin{vbframe}{Model fit with l2 loss}

\begin{itemize}
    \item How to determine parameters that specify the linear model? $\rightsquigarrow$ define loss function \& optimize risk
    \item Popular loss function for linear regression: \textbf{$L2$ loss} / \textbf{quadratic loss} / 
    \textbf{squared error}
    $$\Lxy = (y - \yh)^2 = (y-\fx)^2 = (y-\thx)^2$$
    
    % \includegraphics[width=0.35\textwidth]{figure/reg_l2_residual.pdf}
    % diese Grafik ist irgendwie verwirrend, weil es so aussieht, als 
    % w채re das K채stchen ein Rechteck und kein Quadrat
    \item The difference between the observed value $y$ and the estimated value $\yh = \fx$ is known as a \textbf{residual} and denoted by $r = y - \fx$
    \item By choosing the $L2$ loss, the residuals are penalized \textbf{quadratically}
    \item Reasons for choosing the $L2$ loss:
    \begin{itemize}
        \item Easy to optimize (convex, differentiable)
        \item Theoretically appealing characteristics 
        \item Connection to classical stats LM
    \end{itemize}
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{l2 loss plots}

We will often visualize loss effects like this:

\vfill

\includegraphics[width=\textwidth]{figure/reg_l2_lossplot_quad.pdf}

\hspace{0.5cm}
\begin{minipage}[t]{0.45\textwidth}
    \footnotesize
    \begin{itemize}
        \item Data as $y \sim x_1$
        % \item Prediction hypersurface \\$\rightsquigarrow$ here: line
        \item Residuals \textcolor{blue}{$r = y - \fx$}
        \\$\rightsquigarrow$ squares to illustrate loss
    \end{itemize}
\end{minipage}
\hfill
\begin{minipage}[t]{0.4\textwidth}
    \footnotesize
    \begin{itemize}
        \item Loss as function of residuals
        %\\$\rightsquigarrow$ strength of penalty? 
        %\\$\rightsquigarrow$ symmetric?
        \item Highlighted: loss for residuals shown on LHS
    \end{itemize}
\end{minipage}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{analytical optimization with l2 loss}

\begin{itemize}
    \item With the L2 loss, the resulting \textbf{risk} is:
    $$\risket = \sumin \left(\yi - \thetab^\top \xi \right)^2$$
    \item This is equivalent to the \textbf{sum of squared errors (SSE)}
    \item By minimizing the sum of the quadratic differences between $y$ and $\yh$, we find the loss-optimal model
    \item Special property of LM with $L2$ loss: \textbf{analytical solution} available
    \begin{align*}
        \thetabh \in 
        \argmin_{\thetab} \risket &=
        \argmin_{\thetab} \sumin \left(\yi - \thetab^\top \xi \right)^2  %\\
        % &= \argmin_{\thetab} \| \yv - \Xmat \thetab \|^2_2
    \end{align*}
    \normalsize
    \item The solution $\thetabh$ is known as the \textbf{ordinary-least-squares (OLS)} estimator and we can find it via \textbf{normal equations}
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Statistical properties l2 loss}

% \footnotesize
\begin{itemize}
    % \small
    % \item In a nutshell: minimize quadratic residuals of form 
    % $\| \yv - \Xmat \thetab \|^2_2$
    \item LM with $L2$ loss intimately related to classical stats LM
    \item Assumptions
    \begin{itemize}
        % \small
        \item $\xi$ \textbf{i}ndependent and \textbf{i}dentically \textbf{d}istributed \textbf{(iid)} for $i \in \nset$
        \item \textbf{Homoskedastic} (equivariant) 
         \textbf{Gaussian} errors
        $$\yv = \Xmat \thetab + \bm{\epsilon}, ~ \bm{\epsilon} \sim 
        \normal(0, \sigma^2 \id)  $$
        $\rightsquigarrow$ $y_i$ conditionally independent \& normally ditributed
        % $\yv | \Xmat \sim \normal(\Xmat \thetab, \sigma^2 \id)$
        \item \textbf{Uncorrelated features} \\$\rightsquigarrow$ 
        multicollinearity destabilizes effect estimation 
    \end{itemize}
    \item If assumptions hold: statistical \textbf{inference} applicable
    \begin{itemize}
        % \small
            \item Hypothesis tests on significance of effects, incl. $p$-values
            \item Confidence \& prediction intervals via student-$t$ 
            distribution
        \item Goodness-of-fit measure
        $R^2 = 1 - \text{SSE} ~~ / \underbrace{\text{SST}}_{
        \sumin (\yi - \bar y)^2}$
        
        $\rightsquigarrow$ SSE = part of data variance \textit{not} explained 
        by model
    \end{itemize}
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Effect interpretation - ans Ende setzen, ist f체r l1 und l2 loss gleich}

\begin{itemize}
    \item Big plus of LM: immediately \textbf{interpretable} feature effects
    \item "Marginally increasing $x_j$ by 1 unit increases $y$ by $\theta_j$ 
    units" \\
    $\rightsquigarrow$ \textit{ceteris paribus} assumption: 
    $x_1, \dots, x_{j - 1}, x_{j + 1}, \dots, x_p$ fixed
\end{itemize}

\vfill
\includegraphics[width=0.4\textwidth]{figure/reg_l2_basic_lm_interpreted.pdf} 
\hfill
\includegraphics[width=0.55\textwidth]{figure_man/lm_summary} 

\end{vbframe}

% ------------------------------------------------------------------------------


\begin{vbframe}{Ab hier vollumf채ngliche Folien reinkopiert}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Design matrix}

\begin{itemize}
    \item Mismatch: $\thetab \in \R^{p + 1}$ vs $\xv \in \R^p$ due to intercept  
    term
    \item Trick: pad feature vectors with leading 1, s.t. 
    \begin{itemize}
        \item $\xv \mapsto \xv = (1, x_1, \dots, x_p)^\top$, and 
        \item $\thx = \theta_0 \cdot 1 + \theta_1 x_1 + \dots + \theta_p x_p$
    \end{itemize}
    \item Collect all observations in \textbf{design matrix} 
    $\Xmat \in \R^{n \times (p + 1)}$ \\
    $\rightsquigarrow$ more compact: single param vector incl. intercept
    \item Resulting linear model:
    \begin{align*}
    \hat \yv = \Xmat \thetab = 
        \left(
        \begin{smallmatrix}
            1 & x^{(1)}_1 & \ldots & x^{(1)}_p \\
            1 & x^{(2)}_1 & \ldots & x^{(2)}_p \\
            \vdots & \vdots & & \vdots \\
            1 & x^{(n)}_1 & \ldots & x^{(n)}_p \\
        \end{smallmatrix}
        \right)
        \left(
        \begin{smallmatrix}
            \theta_0 \\ \theta_1 \\ \vdots \\ \theta_p
        \end{smallmatrix}
        \right)
        &=
        \left(
        \begin{smallmatrix}
            \theta_0 + \theta_1 x_1^{(1)} + \dots + \theta_p x_p^{(1)} \\
            \theta_0 + \theta_1 x_1^{(2)} + \dots + \theta_p x_p^{(2)} \\
            \vdots \\
            \theta_0 + \theta_1 x_1^{(n)} + \dots + \theta_p x_p^{(n)} \\
        \end{smallmatrix}
        \right)
    \end{align*}
    \item We will make use of this notation in other contexts
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Effect interpretation}

\begin{itemize}
    \item Big plus of LM: immediately \textbf{interpretable} feature effects
    \item "Marginally increasing $x_j$ by 1 unit increases $y$ by $\theta_j$ 
    units" \\
    $\rightsquigarrow$ \textit{ceteris paribus} assumption: 
    $x_1, \dots, x_{j - 1}, x_{j + 1}, \dots, x_p$ fixed
\end{itemize}

\vfill
\includegraphics[width=0.4\textwidth]{figure/reg_l2_basic_lm_interpreted.pdf} 
\hfill
\includegraphics[width=0.55\textwidth]{figure_man/lm_summary} 

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{frame}{Model fit}

\begin{itemize}
    \item How to determine LM fit? $\rightsquigarrow$ define risk \& optimize
    \item Popular: \textbf{$L2$ loss} / \textbf{quadratic loss} / 
    \textbf{squared error}
    $$\Lxy = (y-\fx)^2 ~~ \text{or} ~~ \Lxy = 0.5 \cdot (y-\fx)^2$$
    
    \includegraphics[width=0.35\textwidth]{figure/reg_l2_residual.pdf}
    \item Why penalize \textbf{residuals} $r = y - \fx$ quadratically?
    \begin{itemize}
        \item Easy to optimize (convex, differentiable)
        \item Theoretically appealing (connection to classical stats LM)
    \end{itemize}
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{loss plots}

We will often visualize loss effects like this:

\vfill

\includegraphics[width=\textwidth]{figure/reg_l2_lossplot_quad.pdf}

\hspace{0.5cm}
\begin{minipage}[t]{0.45\textwidth}
    \footnotesize
    \begin{itemize}
        \item Data as $y \sim x_1$
        \item Prediction hypersurface \\$\rightsquigarrow$ here: line
        \item Residuals \textcolor{blue}{$r = y - \fx$}
        \\$\rightsquigarrow$ squares to illustrate loss
    \end{itemize}
\end{minipage}
\hfill
\begin{minipage}[t]{0.4\textwidth}
    \footnotesize
    \begin{itemize}
        \item Loss as function of residuals
        \\$\rightsquigarrow$ strength of penalty? 
        \\$\rightsquigarrow$ symmetric?
        \item Highlighted: loss for residuals shown on LHS
    \end{itemize}
\end{minipage}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{optimization}

\begin{itemize}
    \item Resulting risk equivalent to 
    \textbf{sum of squared errors (SSE)}:
    $$\risket = \sumin \left(\yi - \thetab^\top \xi \right)^2$$
    \item Consider example with $n = 5$ $\rightsquigarrow$ 
    different models with varying SSE
\end{itemize}

\vfill
\only<1>{
    \phantom{\includegraphics[width=0.25\textwidth]{
    figure/reg_l2_sse_1.pdf}}
}
\only<2>{
    \includegraphics[width=0.3\textwidth]{figure/reg_l2_sse_1.pdf}
}
\only<3>{
    \includegraphics[width=0.3\textwidth]{figure/reg_l2_sse_1.pdf}
    \includegraphics[width=0.3\textwidth]{figure/reg_l2_sse_2.pdf}
}
\only<4>{
    \includegraphics[width=0.3\textwidth]{figure/reg_l2_sse_1.pdf}
    \includegraphics[width=0.3\textwidth]{figure/reg_l2_sse_2.pdf}
    \includegraphics[width=0.3\textwidth]{figure/reg_l2_sse_3.pdf}
}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{optimization}

% Only stuff below super tedious, but when leaving out the repetitions (defining
% table globally and only adding lines in only statements), there's weird
% indenting...

\vspace{0.2cm}
\begin{minipage}[c]{0.5\textwidth}
    % \only<1>{
    % \includegraphics[width=0.4\textwidth]{figure/reg_l2_sse_1.pdf} \\
    % \phantom{
    %     \includegraphics[width=0.4\textwidth]{figure/reg_l2_sse_2.pdf} \\
    % }
    % \phantom{\includegraphics[width=0.4\textwidth]{figure/reg_l2_sse_3.pdf}}
    % \vfill \vspace{0.5cm} \scriptsize
    % \begin{tabular}{r|r|r}
    %     Intercept $\theta_0$ & Slope $\theta_1$ & SSE
    %     \\ \hline 1.80 & 0.30 & 16.86\\ \hline && \\ \hline && \\ \hline &&
    % \end{tabular}
    % }
    % \only<2>{
    % \includegraphics[width=0.4\textwidth]{figure/reg_l2_sse_1.pdf}
    % \includegraphics[width=0.4\textwidth]{figure/reg_l2_sse_2.pdf} \\
    % \phantom{\includegraphics[width=0.4\textwidth]{figure/reg_l2_sse_3.pdf}}
    % \vfill \vspace{0.5cm} \scriptsize
    % \begin{tabular}{r|r|r}
    %     Intercept $\theta_0$ & Slope $\theta_1$ & SSE
    %     \\ \hline 1.80 & 0.30 & 16.86\\ \hline 1.00 & 0.10 & 24.29 \\ 
    %     \hline && \\ \hline &&
    % \end{tabular}
    % }
    \only<1>{
    \includegraphics[width=0.4\textwidth]{figure/reg_l2_sse_1.pdf}
    \includegraphics[width=0.4\textwidth]{figure/reg_l2_sse_2.pdf} \\
    \includegraphics[width=0.4\textwidth]{figure/reg_l2_sse_3.pdf}
    \vfill \vspace{0.5cm} \scriptsize
    \begin{tabular}{r|r|r}
        Intercept $\theta_0$ & Slope $\theta_1$ & SSE
        \\ \hline 1.80 & 0.30 & 16.86\\ \hline 1.00 & 0.10 & 24.29 \\
        \hline 0.50 & 0.80 & 10.61 \\ \hline &&
    \end{tabular}
    }
    \only<2>{
    \includegraphics[width=0.4\textwidth]{figure/reg_l2_sse_1.pdf}
    \includegraphics[width=0.4\textwidth]{figure/reg_l2_sse_2.pdf} \\
    \includegraphics[width=0.4\textwidth]{figure/reg_l2_sse_3.pdf}
    \includegraphics[width=0.4\textwidth]{figure/reg_l2_sse_4.pdf}
    \vfill \vspace{0.5cm} \scriptsize
    \begin{tabular}{r|r|r}
        Intercept $\theta_0$ & Slope $\theta_1$ & SSE
        \\ \hline 1.80 & 0.30 & 16.86\\ \hline 1.00 & 0.10 & 24.29 
        \\ \hline 0.50 & 0.80 & 10.61 \\ \hline
        \textcolor{blue}{-1.65} & \textcolor{blue}{1.29} &
        \textcolor{blue}{5.88}
    \end{tabular}
    }
\end{minipage}
\begin{minipage}[c]{0.45\textwidth}
    % \only<1>{\includegraphics[width=1.2\textwidth, trim=130 30 80 20, clip]{
    % figure/reg_l2_sse_optim_1.pdf}}
    % \only<2>{\includegraphics[width=1.2\textwidth, trim=130 30 80 20, clip]{
    % figure/reg_l2_sse_optim_2.pdf}}
    \only<1>{\includegraphics[width=1.2\textwidth, trim=130 30 80 20, clip]{
    figure/reg_l2_sse_optim_3.pdf}}
    \only<2>{
    \includegraphics[width=1.2\textwidth, trim=130 10 80 20, clip]{
    figure/reg_l2_sse_optim_4.pdf}}
\end{minipage}

\only<2>{
\vfill

Instead of guessing, of course, use \textbf{optimization}!
}

\end{frame}

% ------------------------------------------------------------------------------

\begin{vbframe}{analytical optimization}

\begin{itemize}
    \item Special property of LM with $L2$ loss: \textbf{analytical solution}
    available
    \begin{align*}
        \thetabh \in 
        \argmin_{\thetab} \risket &=
        \argmin_{\thetab} \sumin \left(\yi - \thetab^\top \xi \right)^2  \\
        &= \argmin_{\thetab} \| \yv - \Xmat \thetab \|^2_2
    \end{align*}
    \normalsize
    \item Find via \textbf{normal equations}
    $$\pd{\risket}{\thetab} = 0$$
    \item Solution: \textbf{ordinary-least-squares (OLS)} estimator
    $$\thetabh = \olsest$$
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Statistical properties}

% \footnotesize
\begin{itemize}
    % \small
    % \item In a nutshell: minimize quadratic residuals of form 
    % $\| \yv - \Xmat \thetab \|^2_2$
    \item LM with $L2$ loss intimately related to classical stats LM
    \item Assumptions
    \begin{itemize}
        % \small
        \item $\xi$ \textbf{iid} for $i \in \nset$
        \item \textbf{Homoskedastic} (equivariant) 
         \textbf{Gaussian} errors
        $$\yv = \Xmat \thetab + \bm{\epsilon}, ~ \bm{\epsilon} \sim 
        \normal(0, \sigma^2 \id)  $$
        $\rightsquigarrow$ $y_i$ conditionally independent \& normal:
        $\yv | \Xmat \sim \normal(\Xmat \thetab, \sigma^2 \id)$
        \item Uncorrelated features \\$\rightsquigarrow$ 
        multicollinearity destabilizes effect estimation 
    \end{itemize}
    \item If assumptions hold: statistical \textbf{inference} applicable
    \begin{itemize}
        % \small
            \item Hypothesis tests on significance of effects, incl. $p$-values
            \item Confidence \& prediction intervals via student-$t$ 
            distribution
        \item Goodness-of-fit measure
        $R^2 = 1 - \text{SSE} ~~ / \underbrace{\text{SST}}_{
        \sumin (\yi - \bar y)^2}$
        
        $\rightsquigarrow$ SSE = part of data variance \textit{not} explained 
        by model
    \end{itemize}
\end{itemize}

\end{vbframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% deep dive OLS regression raus

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% L1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Absolute loss}

\begin{itemize}
    \item $L2$ regression minimizes quadratic residuals -- wouldn't 
    \textbf{absolute} residuals seem more natural? 
    \vspace{0.2cm}
    \begin{center}
    \includegraphics[width=0.55\textwidth]{figure/reg_l1_residual_abs_vs_quad}
    \end{center}
    \item \textbf{$L1$ loss / absolute error / least absolute deviation (LAD)}
    $$\Lxy = |y - \fx|$$
    \begin{center}
    \includegraphics[width=0.55\textwidth]{figure/reg_l1_lossplot_abs}
    \end{center}
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{$L1$ vs $L2$ -- loss surface}

\includegraphics[width=0.49\textwidth, trim=100 30 100 0, clip]{
figure/reg_l1_surface_abs.pdf}
\includegraphics[width=0.49\textwidth, trim=100 30 100 0, clip]{
figure/reg_l1_surface_quad.pdf}
\vfill

$L1$ loss (left) harder to optimize than $L2$ loss (right)
\begin{itemize}
    \item Convex but \textbf{not differentiable} in
    $y - \fx = 0$
    \item No analytical solution
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{$L1$ vs $L2$ -- estimated parameters}

\begin{itemize}
    \item Results of $L1$ and $L2$ regression often not that different
    \item Simulated data: $\yi = 1 + 0.5 x^{(i)}_1 + \epsi$, ~~ $\epsi \iid
    \normal(0, 0.01)$
    % \item Coefficients:
\end{itemize}
    
\vfill

\begin{minipage}[b]{0.65\textwidth}
    \hspace{0.7cm}
    \footnotesize
    \begin{tabular}{r|r|r}
        & intercept & slope \\ \hline
        \textcolor{blue}{$L1$} & 0.91 & 0.53 \\ \hline
        \textcolor{orange}{$L2$} & 0.91 & 0.57 
    \end{tabular}

    \vspace{0.5cm}
    \includegraphics[width=0.8\textwidth]{figure/reg_l1_comparison.pdf}
\end{minipage}
\begin{minipage}[b]{0.34\textwidth}
    \includegraphics[width=\textwidth, trim=80 0 100 80, clip]{
    figure/reg_l1_comparison_optim_abs.pdf}
    
    \vfill
    
    \includegraphics[width=\textwidth, trim=80 0 100 80, clip]{
    figure/reg_l1_comparison_optim_quad.pdf}
\end{minipage}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{$L1$ vs $L2$ -- robustness}

\begin{itemize}
    \item $L2$ quadratic in residuals $\rightsquigarrow$ outlying points 
    carry lots of weight
    \item E.g., $3 \times$ residual $\Rightarrow$ $9 \times$ loss contribution
    \item $L1$ more \textbf{robust} in presence of outliers (example ctd.):
\end{itemize}

\vfill
\includegraphics[width=\textwidth]{figure/reg_l1_comparison_outlier.pdf}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{$L1$ vs $L2$ -- optimization cost}

\begin{itemize}
    \item Real-world \texttt{weather} problem $\rightsquigarrow$ 
    predict mean temperature
    \item Compare \textbf{time} to fit $L1$ (\texttt{quantreg::rq()}) vs 
    $L2$ (\texttt{lm::lm()})
    for different dataset proportions (repeat $50\times$)
\end{itemize}

\vfill

\begin{minipage}[c]{0.54\textwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{figure/reg_l1_benchmark.pdf}
\end{minipage}
\scriptsize
\begin{minipage}[c]{0.45\textwidth}

    Loss
    
    \begin{tabular}{l|r|r}
        & Fitted: \textcolor{blue}{$L1$}  &
        Fitted: \textcolor{orange}{$L2$} \\ \hline
        Total \textcolor{blue}{$L1$} loss & $8.98 \times 10^4$ & 
        $8.99 \times 10^4$ \\
        Total \textcolor{orange}{$L2$} loss & $5.83 \times 10^6$ & 
        $5.81 \times 10^6$ \\
    \end{tabular}
    
    \vspace{0.5cm}
    
    Estimated coefficients \\
    
    \begin{tabular}{l|r|r}
        $x_j$ & \textcolor{blue}{$L1$: $\hat \theta_j$}  &
        \textcolor{orange}{$L2$: $\hat \theta_j$} \\ \hline
        \texttt{Max\_temperature} & 0.553 & 0.563 \\
        \texttt{Min\_temperature} & 0.441 & 0.427 \\
        \texttt{Visibility} & 0.026 & 0.041 \\
        \texttt{Wind\_speed} & 0.002 & 0.010 \\
        \texttt{Max\_wind\_speed} & $-$0.026 & $-$0.039 \\
        \texttt{(Intercept)} & $-$0.380 & $-$0.102 \\
    \end{tabular}
    
    \vspace{0.5cm}
    \normalsize
    $L1$ \textbf{slower} to optimize!
\end{minipage}

\end{vbframe}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% POLYNOMIAL REGRESSION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Eher raus denke ich

\lecturechapter{Supervised Regression:\\Polynomial Regression Models}
\lecture{Introduction to Machine Learning}

% ------------------------------------------------------------------------------

\begin{vbframe}{Increasing flexibility}

\begin{itemize}
    \item Recall our definition of LM: model $y$ as linear combo of features
    \item But: isn't that pretty \textbf{inflexible}?
    \item E.g., here, $y$ does not seem to be a linear function of $x$...

    \vspace{0.5cm}
    \begin{center}
        \includegraphics[width=0.6\textwidth]{figure/reg_poly_yx3.pdf}
    \end{center}

    ... but relation to $x^3$ looks pretty linear!

    \item Many other trafos conceivable, e.g.,
    $\sin(x_1), ~ \max(0, x_2), ~ \sqrt{x_3}, \dots$
    \item Turns out we can use LM much more
    \textbf{flexibly} (and: it's still linear) \\
    $\rightsquigarrow$ interpretation might get less straightforward, though
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{the linear model}

\begin{itemize}
    \item Recall what we previously defined as LM:
    \begin{equation} \label{simple_lm}
      f(x) = \theta_0 + \sumjp \theta_j x_j =
      \theta_0 + \theta_1 x_1 + \dots + \theta_p x_p
    \end{equation}
    \item Actually, just special case of "true" LM
    \item \textbf{The linear model} with \textbf{basis functions}
    \textcolor{blue}{$\phi_j$}:
    \begin{equation*} \label{true_lm}
      \fx = \theta_0 + \sumjp \theta_j \textcolor{blue}{\phi_j} (x_j)
      = \theta_0 + \theta_1  \textcolor{blue}{\phi_1} \left( x_1 \right)
      + \dots + \theta_p \textcolor{blue}{\phi_p} \left(x_p \right)
    \end{equation*}
    \item In Eq.~\ref{simple_lm}, we implicitly use identity trafo:
    \textcolor{blue}{$\phi_j = \text{id}_x: x \mapsto x$}~~ $\forall j$ \\
    $\rightsquigarrow$ we often say LM and imply
    $\phi_j = \text{id}_x$
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{the linear model}

\begin{itemize}
\item Are models like $\fx = \theta_0 + \theta_1 x^2$
  \textbf{really linear}?
\begin{minipage}[b]{0.7\textwidth}
  \vspace{0.3cm}
  \begin{itemize}
    \item Certainly not in covariates:
    \scriptsize
    \begin{align*}
      a \cdot f(x, \thetab) + b \cdot f(x_\ast, \thetab)
      &= \theta_0 (a + b) + \theta_1(a x^2 + b x_\ast^2) \\
      &\textcolor{red}{\neq} \theta_0 + \theta_1 (a x + b  x_\ast)^2\\
      &= f(a  x + b x_\ast, \thetab)
    \end{align*}
    \normalsize
    \item Crucially, however, \textbf{linear in params}:
    \scriptsize
    \begin{align*}
      a \cdot f(x, \textcolor{blue}{\thetab}) +
      b \cdot f(x, \textcolor{orange}{\thetab^\ast})
      &= a \theta_0 + b \theta_0^\ast + (a \theta_1 + b \theta_1^\ast) x^2 \\
      &= f(x, \textcolor{magenta}{a \thetab + b \thetab^\ast})
    \end{align*}
  \end{itemize}
\end{minipage}
\begin{minipage}[b]{0.2\textwidth}
  \includegraphics[width=\textwidth]{figure/reg_poly_linearity}
  \tiny \raggedleft
  \textcolor{blue}{$\thetab = (0.5, 0.4)^\top$} \\
  \textcolor{orange}{$\thetab = (1.0, 0.8)^\top$} \\
  \textcolor{magenta}{$\thetab = (1.5, 1.2)^\top$} \\
  \vspace{0.1cm}
\end{minipage}

\vfill

\item NB: we still call design matrix $\Xmat$, incorporating possible trafos:
\begin{align*}
  \Xmat =
  \left(
    \begin{smallmatrix}
        1 & \textcolor{black}{\phi_1} (x^{(1)}_1) & \ldots &
        \textcolor{black}{\phi_p} (x^{(1)}_p) \\
        % 1 & \textcolor{black}{\phi_2} (x^{(2)}_1) & \ldots &
        % \textcolor{black}{\phi_2} (x^{(2)}_p) \\
        \vdots & \vdots & & \vdots \\
        1 & \textcolor{black}{\phi_1} (x^{(n)}_1) & \ldots &
        \textcolor{black}{\phi_p} (x^{(n)}_p) \\
    \end{smallmatrix}
    \right)
\end{align*}
$\rightsquigarrow$ solution via normal equations as usual
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{frame}{polynomial regression}

\begin{itemize}
    \item Simple \& flexible choice for basis funs: \textbf{$d$-polynomials}
    \item Idea: map $x_j$ to (weighted) sum of its monomials up to order
    $d \in \N$
    $$ \phi^{(d)}: \R \rightarrow \R, ~~
    x_j \mapsto \sum_{k = 1}^d \beta_k x_j^k$$
    \includegraphics[width=0.9\textwidth]{figure/reg_poly_basis}
    \item How to estimate coefficients $\beta_k$?
    \begin{itemize}
      \item Both LM \& polynomials \textbf{linear} in their params
      $\rightsquigarrow$ merge
      \item E.g.,
      $\fx = \theta_0 + \theta_1 \phi^{(d)}(x)  =
      \theta_0 + \sum_{k = 1}^d \theta_{1, k} x^k$
      $$\rightsquigarrow \Xmat = \left(
      \begin{smallmatrix}
          1 & x^{(1)} & (x^{(1)})^2 & \hdots & (x^{(1)})^d \\
          \vdots & \vdots & \vdots & & \vdots \\
          1 & x^{(n)} & (x^{(n)})^2 & \hdots & (x^{(n)})^d \\
      \end{smallmatrix}
      \right),
      ~~ \thetab \in \R^{d + 1}
      $$
    \end{itemize}
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{polynomial regression -- examples}

Univariate regression, $d \in \{1, 5\}$

\begin{minipage}[c]{0.5\textwidth}
  \includegraphics[width=\textwidth]{figure/reg_poly_univ_2}
\end{minipage}
\begin{minipage}[c]{0.45\textwidth}
  \begin{itemize}
    \footnotesize
    \item
    Data-generating process:
    \begin{align*}
      y &= 0.5 \sin(x) + \epsilon, \\
      \epsilon &\sim \normal(0, 0.3^2)
    \end{align*}
    \item Model: $$f(x) = \theta_0 + \sum_{k = 1}^d \theta_{1, k} x^k$$
  \end{itemize}
\end{minipage}

\vfill

Bivariate regression, $d = 7$

\begin{minipage}[b]{0.5\textwidth}
  \includegraphics[width=0.7\textwidth, trim=80 0 60 80, clip]{
  figure/reg_poly_biv}
\end{minipage}
\begin{minipage}[b]{0.45\textwidth}
  \begin{itemize}
    \footnotesize
    \item
    Data-generating process: 
    \begin{align*}
      y &= 1 + 2 x_1 + x_2^3 + \epsilon, \\
      \epsilon &\sim \normal(0, 0.5^2)
    \end{align*}
    \item Model: $$f(x) = \theta_0 + \theta_1 x_1 + 
    \sum_{k = 1}^7 \theta_{2, k} x_2^k$$
  \end{itemize}
\end{minipage}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{complexity of polynomials}

\begin{itemize}
  \item Higher $d$ allows to learn more complex functions 
  
  $\rightsquigarrow$ richer hyp space / higher \textbf{capacity}
  
  \vfill
  \includegraphics[width=0.8\textwidth]{figure/reg_poly_univ_4}
  \item Should we then simply let $d \rightarrow \infty$?
  \begin{itemize}
    \item \textbf{No}: data contains random \textbf{noise} -- not part of true
    DGP
    \item Model with overly high capacity learns all those spurious patterns
    $\rightsquigarrow$ poor generalization to new data
    \item Also, higher $d$ can lead to oscillation esp. at bounds 
    
    (Runge's phenomenon\footnote[frame]{\scriptsize
    Interpolation of $m$ equidistant points with $d$-polynomial only 
    well-conditioned for $d < 2 \sqrt{m}$. Plot: 50 points, models with $d \geq
    14$ instable (under equidistance assumption).
    })
    \vfill
  \end{itemize}
\end{itemize}

\end{frame}


% ------------------------------------------------------------------------------

\begin{frame}{bike rental example}

\begin{itemize}
    \footnotesize
    \item OpenML task \href{https://www.openml.org/search?type=data&sort=runs&id=45103&status=active}{
    \texttt{dailybike}}: predict \texttt{rentals} from weather conditions
    \item Hunch: non-linear effect of \texttt{temperature} $\rightsquigarrow$
    include with polynomial:
    \[
        \fx = \sum_{k=1}^d \theta_{\text{temperature}, k}
        x_{\text{temperature}}^k + \theta_{\text{season}} x_{\text{season}} +
        \theta_{\text{humidity}} x_{\text{humidity}}
    \]
    \item Test error\footnote[frame]{
    \scriptsize
    Reliable insights about model performance only via separate test
    dataset not used during training (here computed via 10-fold
    \textit{cross validation}). Much more on this in Evaluation chapter.
    } confirms suspicion $\rightsquigarrow$ minimal
    for $d = 3$
    \vfill
    \includegraphics[width=0.9\textwidth]{figure/reg_poly_bike}
    \tiny
    \begin{tabular}{rrrrrrrrrr}
        $d$ & $\theta_0$ & $\theta_{\text{temp}, 1}$ & $\theta_{\text{temp}, 2}$
        & $\theta_{\text{temp}, 3}$ & $\theta_{\text{sSPRING}}$ &
        $\theta_{\text{sSUMMER}}$ & $\theta_{\text{sFALL}}$ &
        $\theta_{\text{hum}}$ & \textbf{test error} \\ \hline
        1 & 377.3 & 2778.2 & & & 101.0 & 57.0 & 80.1 & -230.0 & \textbf{121.9}\\
        \rowcolor{black!10}
        3 & 419.3 & 2645.8 & -963.1 & -430.9 & 71.9 & 75.8 & 56.6 & -283.8 &
        \textbf{117.6}
    \end{tabular}
    \vfill
    \footnotesize
    \item Conclusion: flexible effects can improve fit/performance
\end{itemize}

\end{frame}



\endlecture

\end{document}
