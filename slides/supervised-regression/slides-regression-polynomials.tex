\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{amsmath}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{figure/reg_poly_title}
\newcommand{\learninggoals}{
\item Learn about general form of linear model
\item See how to add flexibility by using polynomials
\item Understand that more flexibility is not necessarily better}


\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}

\lecturechapter{Polynomial Regression Models}
\lecture{Introduction to Machine Learning}

% ------------------------------------------------------------------------------

\begin{vbframe}{Increasing flexibility}

\begin{itemize}
    \item Recall our definition of LM: model $y$ as linear combo of covariates
    \item But: isn't that pretty \textbf{inflexible}?
    \item E.g., here, $y$ does not seem to be linear function of $x$... 

    \vspace{0.5cm}
    \includegraphics[width=0.6\textwidth]{figure/reg_poly_yx3.pdf} 
    
    ... but relation to $x^3$ looks pretty linear!
    
    \item Many other trafos conceivable, e.g., 
    $\sin(x_1), ~ \max(0, x_2), ~ \sqrt{x_3}, \dots$
    \item Turns out we can use LM much more 
    \textbf{flexibly} (and: it's still linear) \\
    $\rightsquigarrow$ interpretation might get less straightforward, though
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{the linear model}

\begin{itemize}
    \item Recall what we previously defined as LM:
    \begin{equation} \label{simple_lm}
      f(x) = \theta_0 + \sumjp \theta_j x_j =
      \theta_0 + \theta_1 x_1 + \dots + \theta_p x_p
    \end{equation}
    \item Actually, just special case of "true" LM 
    \item \textbf{The linear model} with \textbf{basis functions}
    \textcolor{blue}{$\phi_j$}:
    \begin{equation*} \label{true_lm}
      \fx = \theta_0 + \sumjp \theta_j \textcolor{blue}{\phi_j} (x_j)
      = \theta_0 + \theta_1  \textcolor{blue}{\phi_1} \left( x_1 \right)
      + \dots + \textcolor{blue}{\phi_p} \left(x_p \right)
    \end{equation*}
    \item In Eq.~\ref{simple_lm}, we implicitly use identity trafo:
    \textcolor{blue}{$\phi_j = \text{id}_x: x \mapsto x$}~~ $\forall j$ \\
    $\rightsquigarrow$ we often say LM and imply
    $\phi_j = \text{id}_x$
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{the linear model}

\begin{itemize}
\item Are models like $\fx = \theta_0 + \theta_1 x^2$ 
  \textbf{really linear}?
\begin{minipage}[b]{0.7\textwidth}
  \vspace{0.3cm}
  \begin{itemize}
    \item Certainly not in covariates: 
    \scriptsize
    \begin{align*}
      a \cdot f(x, \thetab) + b \cdot f(x_\ast, \thetab) 
      &= \theta_0 (a + b) + \theta_1(a x^2 + b x_\ast^2) \\
      &\textcolor{red}{\neq} \theta_0 + \theta_1 (a x + b  x_\ast)^2\\
      &= f(a  x + b x_\ast)
    \end{align*}
    \normalsize
    \item Crucially, however, \textbf{linear in params}:
    \scriptsize
    \begin{align*}
      a \cdot f(x, \textcolor{blue}{\thetab}) + 
      b \cdot f(x, \textcolor{orange}{\thetab^\ast}) 
      &= a \theta_0 + b \theta_0^\ast + (a \theta_1 + b \theta_1^\ast) x^2 \\
      &= f(x, \textcolor{magenta}{a \thetab + b \thetab^\ast})
    \end{align*}
  \end{itemize}
\end{minipage}
\begin{minipage}[b]{0.2\textwidth}
  \includegraphics[width=\textwidth]{figure/reg_poly_linearity}
  \tiny \raggedleft
  \textcolor{blue}{$\thetab = (0.5, 0.4)^\top$} \\
  \textcolor{orange}{$\thetab = (1.0, 0.8)^\top$} \\
  \textcolor{magenta}{$\thetab = (1.5, 1.2)^\top$} \\
  \vspace{0.1cm}
\end{minipage}

\vfill

\item NB: we still call design matrix $\Xmat$, incorporating possible trafos:
\begin{align*}
  \Xmat = 
  \left(
    \begin{smallmatrix}
        1 & \textcolor{black}{\phi_1} (x^{(1)}_1) & \ldots & 
        \textcolor{black}{\phi_1} (x^{(1)}_p) \\
        % 1 & \textcolor{black}{\phi_2} (x^{(2)}_1) & \ldots & 
        % \textcolor{black}{\phi_2} (x^{(2)}_p) \\
        \vdots & \vdots & & \vdots \\
        1 & \textcolor{black}{\phi_p} (x^{(n)}_1) & \ldots & 
        \textcolor{black}{\phi_p} (x^{(n)}_p) \\
    \end{smallmatrix}
    \right)
\end{align*}
$\rightsquigarrow$ solution via normal equations as usual
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{frame}{polynomial regression}

\begin{itemize}
    \item Simple \& flexible choice for basis funs: \textbf{$d$-polynomials}
    \item Idea: map $x_j$ to (weighted) sum of its monomials up to order 
    $d \in \N$
    $$ \phi^{(d)}: \R^d \times \R \rightarrow \R, ~~
    x_j \mapsto \sum_{k = 1}^d \beta_k x_j^k$$
    \includegraphics[width=0.45\textwidth]{figure/reg_poly_basis}
    \includegraphics[width=0.45\textwidth]{figure/reg_poly_basis_weighted}
    \item How to estimate additional coefficients $\beta_k$? 
    \begin{itemize}
      \item Both LM \& polynomials \textbf{linear} in their params
      $\rightsquigarrow$ merge
      \item E.g.,
      $\fx = \theta_0 + \theta_1 \phi^{(d)}(x_1)  =
      \theta_0 + \sum_{k = 1}^d \theta_{1, k} x_1^k$
      $$\rightsquigarrow \Xmat = \left(
      \begin{smallmatrix}
          1 & x_{(1)} & x_{(1)}^2 & \hdots & x_{(1)}^d \\
          \vdots & \vdots & \vdots & & \vdots \\
          1 & x_{(n)} & x_{(n)}^2 & \hdots & x_{(n)}^d \\
      \end{smallmatrix}
      \right),
      ~~ \thetab \in \R^{d + 1}
      $$
    \end{itemize}
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{polynomial regression -- examples}

Univariate regression, $d \in \{1, 5\}$

\begin{minipage}[c]{0.5\textwidth}
  \includegraphics[width=\textwidth]{figure/reg_poly_univ_2}
\end{minipage}
\begin{minipage}[c]{0.45\textwidth}
  \begin{itemize}
    \footnotesize
    \item
    Data-generating process: 
    \begin{align*}
      y &= 0.5 \sin(x) + \epsilon, \\
      \epsilon &\sim \normal(0, 0.3^2)
    \end{align*}
    \item Model: $$f(x) = \theta_0 + \sum_{k = 1}^d \theta_{1, k} x^k$$
  \end{itemize}
\end{minipage}

\vfill

Bivariate regression, $d = 7$

\begin{minipage}[b]{0.5\textwidth}
  \includegraphics[width=0.9\textwidth, trim=80 80 80 80, clip]{
  figure/reg_poly_biv}
\end{minipage}
\begin{minipage}[b]{0.45\textwidth}
  \begin{itemize}
    \footnotesize
    \item
    Data-generating process: 
    \begin{align*}
      y &= 1 + 2 x_1 + x_2^3 + \epsilon, \\
      \epsilon &\sim \normal(0, 0.5^2)
    \end{align*}
    \item Model: $$f(x) = \theta_0 + \theta_1 x_1 + 
    \sum_{k = 1}^7 \theta_{2, k} x_2^k$$
  \end{itemize}
\end{minipage}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{complexity of polynomials}

\begin{itemize}
  \item Higher $d$ allows to learn more complex functions 
  
  $\rightsquigarrow$ richer hyp space / higher \textbf{capacity}
  
  \vfill
  \includegraphics[width=0.8\textwidth]{figure/reg_poly_univ_4}
  \item Should we then simply let $d \rightarrow \infty$?
  \begin{itemize}
    \item \textbf{No}: data contains random \textbf{noise} -- not part of true
    DGP
    \item Model with overly high capacity learns all those spurious patterns
    $\rightsquigarrow$ poor generalization to new data
    \item Also, higher $d$ can lead to oscillation esp. at bounds 
    
    (Runge's phenomenon\footnote[frame]{\scriptsize
    Interpolation of $m$ equidistant points with $d$-polynomial only 
    well-conditioned for $d < 2 \sqrt{m}$. Plot: 50 points, models with $d \geq
    14$ instable (under equidistance assumption).
    })
    \vfill
  \end{itemize}
\end{itemize}

\end{frame}


% ------------------------------------------------------------------------------

\begin{frame}{bike rental example}

\begin{itemize}
    \footnotesize
    \item OpenML task \href{https://www.openml.org/search?type=data&sort=runs&id=45103&status=active}{
    \texttt{dailybike}}: predict \texttt{rentals} from weather conditions
    \item Hunch: non-linear effect of \texttt{temperature} $\rightsquigarrow$
    include with polynomial:
    \[
        \fx = \sum_{k=1}^d \theta_{\text{temperature}, k}
        x_{\text{temperature}}^k + \theta_{\text{season}} x_{\text{season}} +
        \theta_{\text{humidity}} x_{\text{humidity}}
    \]
    \item Test error\footnote[frame]{
    \scriptsize
    Reliable insights about model performance only via separate test
    dataset not used during training (here computed via 10-fold
    \textit{cross validation}). Much more on this in Evaluation chapter.
    } confirms suspicion $\rightsquigarrow$ minimal
    for $d = 3$
    \vfill
    \includegraphics[width=0.9\textwidth]{figure/reg_poly_bike}
    \tiny
    \begin{tabular}{rrrrrrrrrr}
        $d$ & $\theta_0$ & $\theta_{\text{temp}, 1}$ & $\theta_{\text{temp}, 2}$
        & $\theta_{\text{temp}, 3}$ & $\theta_{\text{sSPRING}}$ &
        $\theta_{\text{sSUMMER}}$ & $\theta_{\text{sFALL}}$ &
        $\theta_{\text{hum}}$ & \textbf{test error} \\ \hline
        1 & 377.3 & 2778.2 & & & 101.0 & 57.0 & 80.1 & -230.0 & \textbf{121.9}\\
        \rowcolor{black!10}
        3 & 419.3 & 2645.8 & -963.1 & -430.9 & 71.9 & 75.8 & 56.6 & -283.8 &
        \textbf{117.6}
    \end{tabular}
    \vfill
    \footnotesize
    \item Conclusion: flexible effects can improve fit/performance
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------

\endlecture
\end{document}
