\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{figure/reg_poly_title}
\newcommand{\learninggoals}{
\item Learn about general form of linear model
\item See how to add flexibility by using polynomials
\item Understand that more flexibility is not necessarily better}


\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}

\lecturechapter{Polynomial Regression Models}
\lecture{Introduction to Machine Learning}

% ------------------------------------------------------------------------------

\begin{vbframe}{Increasing flexibility}

\begin{itemize}
    \item Recall our definition of LM: model $y$ as linear combo of covariates
    \item But: isn't that pretty \textbf{inflexible}?
    \item E.g., here, $y$ does not seem to be linear function of $x$... 

    \vspace{0.5cm}
    \includegraphics[width=0.6\textwidth]{figure/reg_poly_yx3.pdf} 
    
    ... but relation to $x^3$ looks pretty linear!
    
    \item Many other trafos conceivable, e.g., 
    $\sin(x_1), ~ \max(0, x_2), ~ \sqrt{x_3}, \dots$
    \item Turns out we can use LM much more 
    \textbf{flexibly} (and: it's still linear)
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{the linear model}

\begin{itemize}
    \item Recall what we previously defined as LM:
    \begin{equation} \label{simple_lm}
      f(x) = \theta_0 + \sumjp \theta_j x_j =
      \theta_0 + \theta_1 x_1 + \dots + \theta_p x_p
    \end{equation}
    \item Actually, just special case of "true" LM 
    \item \textbf{The linear model} with \textbf{basis functions}
    \textcolor{blue}{$\phi_j$}:
    \begin{equation*} \label{true_lm}
      \fx = \theta_0 + \sumjp \theta_j \textcolor{blue}{\phi_j} (x_j)
      = \theta_0 + \theta_1  \textcolor{blue}{\phi_1} \left( x_1 \right)
      + \dots + \textcolor{blue}{\phi_p} \left(x_p \right)
    \end{equation*}
    \item In Eq.~\ref{simple_lm}, we implicitly use identity trafo:
    \textcolor{blue}{$\phi_j = \text{id}_x: x \mapsto x$}~~ $\forall j$ \\
    $\rightsquigarrow$ we often say LM and imply
    $\phi_j = \text{id}_x$
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{the linear model}

\begin{itemize}
\item Are models like $\fx = \theta_0 + \theta_1 x^2$ 
  \textbf{really linear}?
\begin{minipage}[b]{0.7\textwidth}
  \vspace{0.3cm}
  \begin{itemize}
    \item Certainly not in covariates: 
    \scriptsize
    \begin{align*}
      a \cdot f(x, \thetab) + b \cdot f(x_\ast, \thetab) 
      &= \theta_0 (a + b) + \theta_1(a x^2 + b x_\ast^2) \\
      &\textcolor{red}{\neq} \theta_0 + \theta_1 (a x + b  x_\ast)^2\\
      &= f(a  x + b x_\ast)
    \end{align*}
    \normalsize
    \item Crucially, however, \textbf{linear in params}:
    \scriptsize
    \begin{align*}
      a \cdot f(x, \textcolor{blue}{\thetab}) + 
      b \cdot f(x, \textcolor{orange}{\thetab^\ast}) 
      &= a \theta_0 + b \theta_0^\ast + (a \theta_1 + b \theta_1^\ast) x^2 \\
      &= f(x, \textcolor{magenta}{a \thetab + b \thetab^\ast})
    \end{align*}
  \end{itemize}
\end{minipage}
\begin{minipage}[b]{0.2\textwidth}
  \includegraphics[width=\textwidth]{figure/reg_poly_linearity}
  \tiny \raggedleft
  \textcolor{blue}{$\thetab = (0.5, 0.4)^\top$} \\
  \textcolor{orange}{$\thetab = (1.0, 0.8)^\top$} \\
  \textcolor{magenta}{$\thetab = (1.5, 1.2)^\top$} \\
  \vspace{0.1cm}
\end{minipage}

\vfill

\item NB: we still call design matrix $\Xmat$ $\rightsquigarrow$ 
incorporating possible trafos:
\begin{align*}
  \Xmat = 
  \left(
    \begin{smallmatrix}
        1 & \textcolor{black}{\phi_1} (x^{(1)}_1) & \ldots & 
        \textcolor{black}{\phi_1} (x^{(1)}_p) \\
        % 1 & \textcolor{black}{\phi_2} (x^{(2)}_1) & \ldots & 
        % \textcolor{black}{\phi_2} (x^{(2)}_p) \\
        \vdots & \vdots & & \vdots \\
        1 & \textcolor{black}{\phi_p} (x^{(n)}_1) & \ldots & 
        \textcolor{black}{\phi_p} (x^{(n)}_p) \\
    \end{smallmatrix}
    \right)
\end{align*}
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{frame}{polynomial regression}

\begin{itemize}
    \item Simple \& flexible choice for basis funs: \textbf{$d$-polynomials}
    \item Idea: map $x_j$ to (weighted) sum of its monomials up to order 
    $d \in \N$
    $$ \phi^{(d)}: \R^d \times \R \rightarrow \R, ~~
    x_j \mapsto \sum_{k = 1}^d \beta_k x_j^k$$
    \includegraphics[width=0.45\textwidth]{figure/reg_poly_basis}
    \textcolor{red}{TODO: plot weighted sum }
    \item How to estimate additional coefficients $\beta_k$? 
    \begin{itemize}
      \item Both LM \& polynomials \textbf{linear} in their params
      $\rightsquigarrow$ merge
      \item E.g.,
      $\fx = \theta_0 + \theta_1 \phi^{(d)}(x_1)  =
      \theta_0 + \sum_{k = 1}^d \theta_{1, k} x_1^k$
      $$\rightsquigarrow \Xmat = \left(
      \begin{smallmatrix}
          1 & x_{(1)} & x_{(1)}^2 & \hdots & x_{(1)}^d \\
          \vdots & \vdots & \vdots & & \vdots \\
          1 & x_{(n)} & x_{(n)}^2 & \hdots & x_{(n)}^d \\
      \end{smallmatrix}
      \right),
      ~~ \thetab \in \R^{d + 1}
      $$
    \end{itemize}
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{polynomial regression -- examples}

Univariate regression, $d \in \{1, 5\}$

\includegraphics[width=0.7\textwidth]{figure/reg_poly_univ_2}

Bivariate regression, $d = 7$

\includegraphics[width=0.4\textwidth, trim=80 80 80 80, clip]{
figure/reg_poly_biv}

\textcolor{red}{TODO: add text}

\end{frame}

% ------------------------------------------------------------------------------

\begin{vbframe}{complexity of polynomials}

\begin{itemize}
  \item Higher $d$ allows to learn more complex functions 
  
  $\rightsquigarrow$ richer hyp space / higher \textbf{capacity}
  
  \vfill
  \includegraphics[width=0.9\textwidth]{figure/reg_poly_univ_4}
  \item Should we then simply let $d \rightarrow \infty$?
  \begin{itemize}
    \item \textbf{No}: data contains random \textbf{noise} -- not part of true
    data-generating process
    \item Model with overly high capacity learns all those spurious patterns
    $\rightsquigarrow$ poor generalization to new data
  \end{itemize}
\end{itemize}

\end{vbframe}


% ------------------------------------------------------------------------------

\begin{frame}{polynomial regression}

\textcolor{red}{TODO: zusatzfolie mit datenbeispiel, das zeigt, dass polynomials 
besseren fit bringen können. zeigen: fit, test error via CV (auch wenn 
foreshadowing, fußnote mit verweis auf eval). auch effektplots zeigen 
(--> IML-folien, z. b, bike beispiel von dort)}

\end{frame}

% ------------------------------------------------------------------------------

\endlecture
\end{document}
