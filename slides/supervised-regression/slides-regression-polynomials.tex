\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{figure/reg_pol_4}
\newcommand{\learninggoals}{
\item Understand how to add flexibility to the linear model by using polynomials
\item Understand that this only affects the hypothesis space, not risk or optimization
\item Understand that more flexibility is not equivalent to a better model}


\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}

\lecturechapter{Polynomial Regression Models}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{Regression: Polynomials}

We can make linear regression models much more flexible by using \emph{polynomials} $\xj^d$ -- or any other \emph{derived features} like $\sin(\xj)$ or $(\xj \cdot \xv_k)$ -- as additional features.\\

\lz

The optimization and risk of the learner remain the same.\\
\lz

Only the hypothesis space of the learner changes:\\ 
instead of linear functions 
\begin{align*}
\fxit &= \theta_0 + \theta_1 \xi_1 + \theta_2 \xi_2 + \dots\\
\intertext{of only the original features,}
\intertext{it now includes linear functions of the derived features as well, e.g.} 
\fxit &= \theta_0 + \sum^d_{k=1} \theta_{1k} \left(\xi_1\right)^k + \sum^d_{k=1} \theta_{2k} \left(\xi_2\right)^k + \dots
\end{align*}
\framebreak

\textbf{Polynomial regression example}

 \lz

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.9\textwidth]{figure/reg_pol_1} 

}



\end{knitrout}

\framebreak

\textbf{Polynomial regression example}

Models of different \emph{complexity}, i.e., of different polynomial order $d$, are fitted to the data:

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.9\textwidth]{figure/reg_pol_2} 

}



\end{knitrout}

\framebreak

\textbf{Polynomial regression example}

Models of different \emph{complexity}, i.e., of different polynomial order $d$, are fitted to the data:

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.9\textwidth]{figure/reg_pol_3} 

}



\end{knitrout}

\framebreak

\textbf{Polynomial regression example}

Models of different \emph{complexity}, i.e., of different polynomial order $d$, are fitted to the data:

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.9\textwidth]{figure/reg_pol_4} 

}



\end{knitrout}

\framebreak

The higher $d$ is, the more \textbf{capacity} the learner has to learn complicated functions of $\xv$, but
this also increases the danger of \textbf{overfitting}:\\
\lz

The model space $\Hspace$ contains so many complex functions that we are able to find one that approximates
the training data arbitrarily well.
\lz

However, predictions on new data are not as successful because our model has learnt spurious \enquote{wiggles} from the random noise in the training data (much, much more on this later).

\framebreak

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/reg_pol_5} 

}



\end{knitrout}

\end{vbframe}
\endlecture
\end{document}
