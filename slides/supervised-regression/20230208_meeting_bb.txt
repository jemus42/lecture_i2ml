vision for regression chapter (BB, PK, LW)

- envisioned chunks:
  1. lin reg with L2 loss 
     --> intro, intuition
     --> first part of "linear models", L2-related part of "regr losses"
     1.a deep dive: proof OLS estimator --> new
  2. lin reg with L1 loss 
     --> comparison to L2
     --> second part of "linear models", L2-related part of "regr losses"
  3. polynomial regression --> as-is
  4. splines & GAMs --> new
- in the end: rename chapter to "Supervised Regression: LMs & GAMs"
  
- general remarks
  - clean up globally (refs, rsrc, overleaf comments, ...)
  - steno-style language, no complete sentences
  - avoid single words after line breaks etc.
  - **always** integrate text & viz --> if too much text, make multiple slides 
    for viz and let text run via \only
  
chunk 1 -- standard LM
- we want to fit a line through our data
  - visualize 2D and 3D
  - always with accompanying math
- why? for instance, we can interpret feat effects --> visualize with triangle
- so how do we find that LM?
  - disclaimer: design matrix w intercept col --> single theta vector
  - let's start with SSE: emp risk formula, pictures with colored rectangles
    --> message: we see some lines fit better than others
  - then: ofc we use proper optim --> cube viz
    - more slowly: show step by step with optim path
    - green optimal model results in the end
    - add tables with values for intercept/slope (= cube points)
    - check with chapter 1 (probably lots of redundancy)
- lastly: for L2 we have an analytical option too --> deep dive chunk 1a

chunk 2 -- L1 version
- we could have used L1 loss --> perhaps more intuitive 
- also, more robust toward outliers
- however, harder to optimize: no analytical solution, only subgradients
- also: standard LM has some nice interpretations (gaussian errors, 
  minimizing non-explained part of variance (SSE), ...)
  
chunk 3 -- polynomial
- add multivariate viz
- to be discussed

chunk 4 -- splines & GAMs
- must be very beginner-friendly
- to be discussed

