\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{figure/reg_lm_sse}
\newcommand{\learninggoals}{
\item Grasp the overall concept of linear regression
\item Understand how $L2$ loss optimization results in SSE-minimal model
\item See how linear regression with $L2$ loss can be solved numerically \& 
analytically
}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}
\lecturechapter{Linear Models with $L2$ Loss}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{Linear regression}

\begin{itemize}
    \item Idea: predict $y \in \R$ as \textbf{linear} combination of features
    (plus intercept term):
    $$y = \fx = \thx = \theta_0 + \theta_1 x_1 + \dots + \theta_p x_p$$
    $~\rightsquigarrow$ find loss-optimal hyperplane to describe relation 
    $y | \xv$
    \item Hypothesis space: $\Hspace = \{\fx = \thx\ |\ \thetab \in \R^{p+1} \}$
    \item Key advantage: interpretable feature effects \\ $~\rightsquigarrow$
    "marginally increasing $x_j$ by 1 unit increases $y$ by \dots"
    \vfill
    \begin{minipage}{0.45\textwidth}
        \includegraphics[width=\textwidth]{figure/reg_lm_plot} 
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \framebox(120, 70){\textcolor{red}{TODO: bivariate example}}
    \end{minipage}
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Design matrix}

\begin{itemize}
    \item Mismatch: $\thetab \in \R^{p + 1}$ vs $\xv \in \R^p$ due to intercept  
    term
    \item Trick: pad feature vectors with leading 1, s.t. 
    \begin{itemize}
        \item $\xv \mapsto \xv = (1, x_1, \dots, x_p)^\top$, and 
        \item $\thx = \theta_0 + \theta_1 x_1 + \dots + \theta_p x_p$
    \end{itemize}
    \item Collect all observations in \textbf{design matrix} 
    $\Xmat \in \R^{n \times (p + 1)}$
    $$\Xmat = \left(
    \begin{smallmatrix}
        1 & x^{(1)}_1 & \ldots & x^{(1)}_p \\
        1 & x^{(2)}_1 & \ldots & x^{(2)}_p \\
        \vdots & \vdots & & \vdots \\
        1 & x^{(n)}_1 & \ldots & x^{(n)}_p \\
    \end{smallmatrix}
    \right)
    ~~ \rightsquigarrow \Xmat \thetab =
    \left(
    \begin{smallmatrix}
        \theta_0 + \theta_1 x_1^{(1)} + \dots + \theta_p x_p^{(1)} \\
        \theta_0 + \theta_1 x_1^{(2)} + \dots + \theta_p x_p^{(2)} \\
        \vdots \\
        \theta_0 + \theta_1 x_1^{(n)} + \dots + \theta_p x_p^{(n)} \\
    \end{smallmatrix}
    \right)
    =
    \left(
    \begin{smallmatrix}
        \thetab^\top \xv^{(1)} \\
        \thetab^\top \xv^{(2)} \\
        \vdots \\
        \thetab^\top \xv^{(n)} \\
    \end{smallmatrix}
    \right)
    $$
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{frame}{Model fit}

\begin{itemize}
    \item How to determine LM fit?
    \item E.g., \textbf{SSE}: part of total 
    variance model fails to explain
    \begin{minipage}[b]{0.6\textwidth}
        \footnotesize
        \begin{align*}
            \underbrace{\sumin \left(\yi - \bar y\right)^2}_{\text{sum of 
            squares total (SST)}} &= 
            \underbrace{\sumin \left(\fxi - \bar y\right)^2}_{\text{
            sum of squares regression (SSR)}} \\
            &+ \underbrace{\sumin \left(\yi - \fxi\right)^2}_{\color{blue} 
            \text{sum of squared errors (SSE)} ~\rightarrow ~ \text{min}}
        \end{align*}
    \end{minipage}
    \begin{minipage}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figure/reg_lm_sse}
    \end{minipage}
    \item Consider example with $n = 5$ $\rightsquigarrow$ varying SSE for 
    different models:
    \vfill
    \only<1>{
        \phantom{\includegraphics[width=0.25\textwidth]{
        figure/reg_lm_sse_11.pdf}}
    }
    \only<2>{
        \includegraphics[width=0.25\textwidth]{figure/reg_lm_sse_11.pdf}
    }
    \only<3>{
        \includegraphics[width=0.25\textwidth]{figure/reg_lm_sse_11.pdf}
        \includegraphics[width=0.25\textwidth]{figure/reg_lm_sse_12.pdf}
    }
    \only<4>{
        \includegraphics[width=0.25\textwidth]{figure/reg_lm_sse_11.pdf}
        \includegraphics[width=0.25\textwidth]{figure/reg_lm_sse_12.pdf}
        \includegraphics[width=0.25\textwidth]{figure/reg_lm_sse_13.pdf}
    }
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------

\begin{vbframe}{Linear model: optimization}

\begin{itemize}
    \item Instead of guessing, use \textbf{optimization}
    \item Revisiting SSE reveals: minimizing SSE = minimizing 
    \textbf{$L2$ loss} 
    \footnotesize
    \begin{align*}
        \min_{\thetab} \risket = \min_{\thetab} \sumin \Lxyit = 
        \underbrace{\min_{\thetab} \sumin \left(\yi - \fxit\right)^2}_{
        \text{SSE}}
    \end{align*}
    \normalsize
    \item $L2$ loss surface $\rightsquigarrow$ numerical optimization, e.g.,
    gradient descent
    
    \includegraphics[width=0.25\textwidth]{figure/reg_lm_plot_35.pdf}
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Linear model: optimization}

\begin{itemize}
    \item Special property of LM with $L2$ loss: \textbf{analytical solution}
    available
    % \footnotesize
    \begin{align*}
        \thetabh \in 
        \argmin_{\thetab} \risket &=
        \argmin_{\thetab} \sumin \left(\yi - \thetab^\top \xi \right)^2  \\
        &= \argmin_{\thetab} \| \yv - \Xmat \thetab \|^2_2
    \end{align*}
    \normalsize
    \item Find via \textbf{normal equations}
    $$
    \pd{\risket}{\thetab} = 0 \quad\implies\quad \thetabh = \olsest
    $$
    \item Matrix inversion possibly expensive (for large $n$)
\end{itemize}

\end{vbframe}


\endlecture
\end{document}
