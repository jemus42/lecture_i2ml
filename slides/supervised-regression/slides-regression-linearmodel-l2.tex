\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{figure/reg_lm_sse}
\newcommand{\learninggoals}{
\item Grasp the overall concept of linear regression
\item Understand how $L2$ loss optimization results in SSE-minimal model
\item See how linear regression with $L2$ loss can be solved numerically \& 
analytically
}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}
\lecturechapter{Linear Models with $L2$ Loss}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{Linear regression}

\begin{itemize}
    \item Idea: predict $y \in \R$ as \textbf{linear} combination of features
    (plus intercept term):
    $$y = \fx = \thx = \theta_0 + \theta_1 x_1 + \dots + \theta_p x_p$$
    $~\rightsquigarrow$ find loss-optimal hyperplane to describe relation 
    $y | \xv$
    \item Hypothesis space: $\Hspace = \{\fx = \thx\ |\ \thetab \in \R^{p+1} \}$
    
\end{itemize}
\vfill
    \begin{minipage}{0.45\textwidth}
        \includegraphics[width=\textwidth]{figure/reg_lm_plot} 
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \includegraphics[width=0.8\textwidth]{figure/ss_regr_bivariate} 
        
        \textcolor{red}{TODO: visualize properly}
    \end{minipage}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Design matrix}

\begin{itemize}
    \item Mismatch: $\thetab \in \R^{p + 1}$ vs $\xv \in \R^p$ due to intercept  
    term
    \item Trick: pad feature vectors with leading 1, s.t. 
    \begin{itemize}
        \item $\xv \mapsto \xv = (1, x_1, \dots, x_p)^\top$, and 
        \item $\thx = \theta_0 \cdot 1 + \theta_1 x_1 + \dots + \theta_p x_p$
    \end{itemize}
    \item Collect all observations in \textbf{design matrix} 
    $\Xmat \in \R^{n \times (p + 1)}$
    \item Linear model:


% $$
% \Xmat = \left(
% \begin{smallmatrix}
%     1 & x^{(1)}_1 & \ldots & x^{(1)}_p \\
%     1 & x^{(2)}_1 & \ldots & x^{(2)}_p \\
%     \vdots & \vdots & & \vdots \\
%     1 & x^{(n)}_1 & \ldots & x^{(n)}_p \\
% \end{smallmatrix}
% \right)
% $$

\begin{align*}
\Xmat \thetab = 
    % \underbrace{
    \left(
    \begin{smallmatrix}
        1 & x^{(1)}_1 & \ldots & x^{(1)}_p \\
        1 & x^{(2)}_1 & \ldots & x^{(2)}_p \\
        \vdots & \vdots & & \vdots \\
        1 & x^{(n)}_1 & \ldots & x^{(n)}_p \\
    \end{smallmatrix}
    \right)
    % }_{\Xmat}
    % \underbrace{
    \left(
    \begin{smallmatrix}
        \theta_0 \\ \theta_1 \\ \vdots \\ \theta_p
    \end{smallmatrix}
    \right)
    % }_{\thetab}
    &=
    \left(
    \begin{smallmatrix}
        \theta_0 + \theta_1 x_1^{(1)} + \dots + \theta_p x_p^{(1)} \\
        \theta_0 + \theta_1 x_1^{(2)} + \dots + \theta_p x_p^{(2)} \\
        \vdots \\
        \theta_0 + \theta_1 x_1^{(n)} + \dots + \theta_p x_p^{(n)} \\
    \end{smallmatrix}
    \right) \\[2ex]
    &=
    \left(
    \begin{smallmatrix}
        \thetab^\top \xv^{(1)} \\
        \thetab^\top \xv^{(2)} \\
        \vdots \\
        \thetab^\top \xv^{(n)} \\
    \end{smallmatrix}
    \right)
\end{align*}

\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Effect interpretation}

\begin{itemize}
    \item Big plus of LM: immediately \textbf{interpretable} feature effects
    \item "Marginally increasing $x_j$ by 1 unit increases $y$ by $\theta_j$ 
    units" \\
    $\rightsquigarrow$ \textit{ceteris paribus} assumption: 
    $x_1, \dots, x_{j - 1}, x_{j + 1}, \dots, x_p$ fixed
\end{itemize}

\includegraphics[width=0.6\textwidth]{figure/reg_lm_plot_interpreted} 

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{frame}{Model fit}

\begin{itemize}
    \item How to determine LM fit? $\rightsquigarrow$ define risk \& optimize
    \item Popular: \textbf{$L2$ loss} / \textbf{quadratic loss} / 
    \textbf{squared error}
    $$\Lxy = (y-\fx)^2 ~~ \text{or} ~~ \Lxy = \textcolor{blue}{0.5} (y-\fx)^2$$
    \begin{minipage}{0.5\textwidth}
        \includegraphics[width=0.9\textwidth]{figure/plot_quad_loss} 
    \end{minipage}
    \begin{minipage}{0.3\textwidth}
        \footnotesize \raggedright
        \textcolor{red}{\footnotesize TODO: adapt style}
    \end{minipage}
    \item Why penalize \textbf{quadratic} differences between $y$ and $\hat y$?
    \begin{itemize}
        \item Easy to optimize (convex, differentiable)
        \item Theoretically appealing (connection to classic stats LM)
    \end{itemize}
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Linear model: optimization}

\begin{itemize}
    \item Resulting risk equivalent to 
    \textbf{sum of squared errors (SSE)}:
    $$\risket = \sumin \left(\yi - \thetab^\top \xi \right)^2$$
    \item Consider example with $n = 5$ $\rightsquigarrow$ 
    different models with varying SSE
\end{itemize}

\vfill
\only<1>{
    \phantom{\includegraphics[width=0.25\textwidth]{
    figure/reg_lm_sse_11.pdf}}
}
\only<2>{
    \includegraphics[width=0.3\textwidth]{figure/reg_lm_sse_11.pdf}
}
\only<3>{
    \includegraphics[width=0.3\textwidth]{figure/reg_lm_sse_11.pdf}
    \includegraphics[width=0.3\textwidth]{figure/reg_lm_sse_12.pdf}
}
\only<4>{
    \includegraphics[width=0.3\textwidth]{figure/reg_lm_sse_11.pdf}
    \includegraphics[width=0.3\textwidth]{figure/reg_lm_sse_12.pdf}
    \includegraphics[width=0.3\textwidth]{figure/reg_lm_sse_13.pdf}
}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Linear model: optimization}

% Example revisited: optimization on $L2$ loss surface
Instead of guessing, of course, use \textbf{optimization}

\vspace{0.2cm}
\begin{minipage}{0.45\textwidth}
    \only<1>{
    \includegraphics[width=0.4\textwidth]{figure/reg_lm_sse_11.pdf} \\
    \phantom{\includegraphics[width=0.4\textwidth]{figure/reg_lm_sse_12.pdf} \\}
    \phantom{\includegraphics[width=0.4\textwidth]{figure/reg_lm_sse_13.pdf}}
    }
    \only<2>{
    \includegraphics[width=0.4\textwidth]{figure/reg_lm_sse_11.pdf}
    \includegraphics[width=0.4\textwidth]{figure/reg_lm_sse_12.pdf} \\
    \phantom{\includegraphics[width=0.4\textwidth]{figure/reg_lm_sse_13.pdf}}
    }
    \only<3>{
    \includegraphics[width=0.4\textwidth]{figure/reg_lm_sse_11.pdf}
    \includegraphics[width=0.4\textwidth]{figure/reg_lm_sse_12.pdf} \\
    \includegraphics[width=0.4\textwidth]{figure/reg_lm_sse_13.pdf}
    }
    \vfill
    \scriptsize
        \begin{tabular}{l|l|l}
            Intercept $\theta_0$ & Slope $\theta_1$ & SSE
            \\ \hline 1.8 & 0.3 & 16.86
            \only<1>{\\ \hline & & \\ \hline &&}
            \only<2>{\\ \hline 1.0 & 0.10 & 24.29 \\ \hline &&}
            \only<3>{\\ \hline 1.0 & 0.10 & 24.29 \\ \hline
            0.5 & 0.8 & 10.61
            }
        \end{tabular}
\end{minipage}
\begin{minipage}{0.5\textwidth}
    \includegraphics[width=\textwidth]{figure/ss_surf.png}
\end{minipage}

\vfill
\textcolor{red}{TODO: make proper visualization corresponding to example; check 
for redundancy w chap 1}

\end{frame}

% ------------------------------------------------------------------------------

\begin{vbframe}{Linear model: optimization}

\begin{itemize}
    \item Special property of LM with $L2$ loss: \textbf{analytical solution}
    available
    % \footnotesize
    \begin{align*}
        \thetabh \in 
        \argmin_{\thetab} \risket &=
        \argmin_{\thetab} \sumin \left(\yi - \thetab^\top \xi \right)^2  \\
        &= \argmin_{\thetab} \| \yv - \Xmat \thetab \|^2_2
    \end{align*}
    \normalsize
    \item Direct connection to assumption of 
    Gaussian errors: $$\yv = \Xmat \thetab + \bm{\epsilon}, ~ \bm{\epsilon} \sim 
    \normal(0, \id)$$
    \item Find via \textbf{normal equations}
    $$
    \pd{\risket}{\thetab} = 0 \quad\implies\quad \thetabh = \olsest
    $$
    % \item Matrix inversion possibly expensive (for large $n$)
\end{itemize}

\end{vbframe}


\endlecture
\end{document}

