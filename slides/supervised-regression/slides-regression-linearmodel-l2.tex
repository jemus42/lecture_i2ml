\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{figure/plot_loss}
\newcommand{\learninggoals}{
\item Grasp the overall concept of linear regression
\item Understand how $L2$ loss minimization results in SSE-minimal model
\item See how linear regression with $L2$ loss can be solved numerically \& 
analytically
}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}
\lecturechapter{Linear Models with $L2$ Loss}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{Linear regression}

\begin{itemize}
    \item Idea: predict $y \in \R$ as \textbf{linear} combination of features
    (plus intercept term):
    $$y = \fx = \thx = \theta_0 + \theta_1 x_1 + \dots + \theta_p x_p$$
    \item Hypothesis space: $\Hspace = \{\fx = \thx\ |\ \thetab \in \R^{p+1} \}$
    \item Key advantage: interpretable feature effects \\ $~\rightsquigarrow$
    "marginal 1-unit increase of $x_j$ increases $y$ by \dots"
    \vfill
    \begin{minipage}{0.45\textwidth}
        \includegraphics[width=\textwidth]{figure/reg_lm_plot} 
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \textcolor{red}{TODO: bivariate example}
    \end{minipage}
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Design matrix}

\begin{itemize}
    \item Mismatch: $\thetab \in \R^{p + 1}$ vs $\xv \in \R^p$ due to intercept  
    term
    \item Trick: pad feature vectors with leading 1, s.t. 
    \begin{itemize}
        \item $\xv \mapsto \xv = (1, x_1, \dots, x_p)^\top$, and 
        \item $\thx = \theta_0 + \theta_1 x_1 + \dots + \theta_p x_p$
    \end{itemize}
    \item Collect all observations in \textbf{design matrix} 
    $\Xmat \in \R^{n \times (p + 1)}$
    $$\Xmat = \left(
    \begin{smallmatrix}
        1 & x^{(1)}_1 & \ldots & x^{(1)}_p \\
        1 & x^{(2)}_1 & \ldots & x^{(2)}_p \\
        \vdots & \vdots & & \vdots \\
        1 & x^{(n)}_1 & \ldots & x^{(n)}_p \\
    \end{smallmatrix}
    \right)
    ~~ \rightsquigarrow \Xmat \thetab =
    \left(
    \begin{smallmatrix}
        \theta_0 + \theta_1 x_1^{(1)} + \dots + \theta_p x_p^{(1)} \\
        \theta_0 + \theta_1 x_1^{(2)} + \dots + \theta_p x_p^{(2)} \\
        \vdots \\
        \theta_0 + \theta_1 x_1^{(n)} + \dots + \theta_p x_p^{(n)} \\
    \end{smallmatrix}
    \right)
    =
    \left(
    \begin{smallmatrix}
        \thetab^\top \xv^{(1)} \\
        \thetab^\top \xv^{(2)} \\
        \vdots \\
        \thetab^\top \xv^{(n)} \\
    \end{smallmatrix}
    \right)
    $$
\end{itemize}

\end{vbframe}

\endlecture
\end{document}
