\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{figure/reg_lm_sse}
\newcommand{\learninggoals}{
\item Grasp the overall concept of linear regression
\item Understand how $L2$ loss optimization results in SSE-minimal model
\item See how linear regression with $L2$ loss can be solved numerically \& 
analytically
}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}
\lecturechapter{Linear Models with $L2$ Loss}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{Linear regression}

\begin{itemize}
    \item Idea: predict $y \in \R$ as \textbf{linear} combination of features
    (plus intercept term):
    $$y = \fx = \thx = \theta_0 + \theta_1 x_1 + \dots + \theta_p x_p$$
    $~\rightsquigarrow$ find loss-optimal hyperplane to describe relation 
    $y | \xv$
    \item Hypothesis space: $\Hspace = \{\fx = \thx\ |\ \thetab \in \R^{p+1} \}$
    \item Key advantage: interpretable feature effects \\ $~\rightsquigarrow$
    "marginally increasing $x_j$ by 1 unit increases $y$ by \dots"
    
\end{itemize}
\vfill
    \begin{minipage}{0.45\textwidth}
        \includegraphics[width=\textwidth]{figure/reg_lm_plot} 
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \includegraphics[width=0.8\textwidth]{figure/ss_regr_bivariate} 
        
        \textcolor{red}{TODO: visualize properly}
    \end{minipage}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Design matrix}

\begin{itemize}
    \item Mismatch: $\thetab \in \R^{p + 1}$ vs $\xv \in \R^p$ due to intercept  
    term
    \item Trick: pad feature vectors with leading 1, s.t. 
    \begin{itemize}
        \item $\xv \mapsto \xv = (1, x_1, \dots, x_p)^\top$, and 
        \item $\thx = \theta_0 + \theta_1 x_1 + \dots + \theta_p x_p$
    \end{itemize}
    \item Collect all observations in \textbf{design matrix} 
    $\Xmat \in \R^{n \times (p + 1)}$
    $$\Xmat = \left(
    \begin{smallmatrix}
        1 & x^{(1)}_1 & \ldots & x^{(1)}_p \\
        1 & x^{(2)}_1 & \ldots & x^{(2)}_p \\
        \vdots & \vdots & & \vdots \\
        1 & x^{(n)}_1 & \ldots & x^{(n)}_p \\
    \end{smallmatrix}
    \right)
    ~~ \rightsquigarrow \Xmat \thetab =
    \left(
    \begin{smallmatrix}
        \theta_0 + \theta_1 x_1^{(1)} + \dots + \theta_p x_p^{(1)} \\
        \theta_0 + \theta_1 x_1^{(2)} + \dots + \theta_p x_p^{(2)} \\
        \vdots \\
        \theta_0 + \theta_1 x_1^{(n)} + \dots + \theta_p x_p^{(n)} \\
    \end{smallmatrix}
    \right)
    =
    \left(
    \begin{smallmatrix}
        \thetab^\top \xv^{(1)} \\
        \thetab^\top \xv^{(2)} \\
        \vdots \\
        \thetab^\top \xv^{(n)} \\
    \end{smallmatrix}
    \right)
    $$
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{frame}{Model fit}

\begin{itemize}
    \item How to determine LM fit?
    \item E.g., \textbf{SSE}: part of total 
    variance model fails to explain
    \begin{minipage}[b]{0.6\textwidth}
        \footnotesize
        \begin{align*}
            \underbrace{\sumin \left(\yi - \bar y\right)^2}_{\text{sum of 
            squares total (SST)}} &= 
            \underbrace{\sumin \left(\fxi - \bar y\right)^2}_{\text{
            sum of squares regression (SSR)}} \\
            &+ \underbrace{\sumin \left(\yi - \fxi\right)^2}_{\color{blue} 
            \text{sum of squared errors (SSE)} ~\rightarrow ~ \text{min}}
        \end{align*}
    \end{minipage}
    \begin{minipage}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figure/reg_lm_sse}
    \end{minipage}
    \item Consider example with $n = 5$ $\rightsquigarrow$ varying SSE for 
    different models:
    \vfill
    \only<1>{
        \phantom{\includegraphics[width=0.25\textwidth]{
        figure/reg_lm_sse_11.pdf}}
    }
    \only<2>{
        \includegraphics[width=0.25\textwidth]{figure/reg_lm_sse_11.pdf}
    }
    \only<3>{
        \includegraphics[width=0.25\textwidth]{figure/reg_lm_sse_11.pdf}
        \includegraphics[width=0.25\textwidth]{figure/reg_lm_sse_12.pdf}
    }
    \only<4>{
        \includegraphics[width=0.25\textwidth]{figure/reg_lm_sse_11.pdf}
        \includegraphics[width=0.25\textwidth]{figure/reg_lm_sse_12.pdf}
        \includegraphics[width=0.25\textwidth]{figure/reg_lm_sse_13.pdf}
    }
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------

\begin{vbframe}{Linear model: optimization}

\begin{itemize}
    \item Instead of guessing, use \textbf{optimization}
    \item Minimizing SSE equivalent to minimizing popular $L2$ loss
    \item \textbf{$L2$ loss} / \textbf{quadratic loss} / \textbf{squared error}
    $$\Lxy = (y-\fx)^2 ~~ \text{or} ~~ \Lxy = 0.5 (y-\fx)^2$$
    \item \textbf{Convex \& differentiable} $\rightsquigarrow$ first-order 
    numerical optimization
    % \footnotesize
    \begin{align*}
        \min_{\fx} \Lxy ~&\Longleftrightarrow ~ 0 = \pd{0.5 (y-\fx)^2}{\fx} 
        = -(\underbrace{y - \fx}_{=: \epsilon \text{ (residual)}}) \\
        &\Longleftrightarrow ~ y = \fx
    \end{align*}
    \normalsize
\end{itemize}

\begin{minipage}{0.65\textwidth}
    \includegraphics[width=0.9\textwidth]{figure/plot_quad_loss} 
\end{minipage}
\begin{minipage}{0.3\textwidth}
    \footnotesize \raggedright
    \textcolor{red}{\footnotesize TODO: adapt style}
    Quadratic relationship between $\epsilon$ and loss $\rightsquigarrow$ 
    $L2$ focuses on reducing large residuals
\end{minipage}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{frame}{Linear model: optimization}

Example revisited: optimization on $L2$ loss surface
\vspace{0.2cm}
\begin{minipage}{0.45\textwidth}
    \only<1>{
    \includegraphics[width=0.4\textwidth]{figure/reg_lm_sse_11.pdf} \\
    \phantom{\includegraphics[width=0.4\textwidth]{figure/reg_lm_sse_12.pdf} \\}
    \phantom{\includegraphics[width=0.4\textwidth]{figure/reg_lm_sse_13.pdf}}
    }
    \only<2>{
    \includegraphics[width=0.4\textwidth]{figure/reg_lm_sse_11.pdf}
    \includegraphics[width=0.4\textwidth]{figure/reg_lm_sse_12.pdf} \\
    \phantom{\includegraphics[width=0.4\textwidth]{figure/reg_lm_sse_13.pdf}}
    }
    \only<3>{
    \includegraphics[width=0.4\textwidth]{figure/reg_lm_sse_11.pdf}
    \includegraphics[width=0.4\textwidth]{figure/reg_lm_sse_12.pdf} \\
    \includegraphics[width=0.4\textwidth]{figure/reg_lm_sse_13.pdf}
    }
    \vfill
    \scriptsize
        \begin{tabular}{l|l|l}
            Intercept $\theta_0$ & Slope $\theta_1$ & SSE
            \\ \hline 1.8 & 0.3 & 16.86
            \only<1>{\\ \hline & & \\ \hline &&}
            \only<2>{\\ \hline 1.0 & 0.10 & 24.29 \\ \hline &&}
            \only<3>{\\ \hline 1.0 & 0.10 & 24.29 \\ \hline
            0.5 & 0.8 & 10.61
            }
        \end{tabular}
\end{minipage}
\begin{minipage}{0.5\textwidth}
    \includegraphics[width=\textwidth]{figure/ss_surf.png}
\end{minipage}

\vfill
\textcolor{red}{TODO: make proper visualization corresponding to example}

\end{frame}

% ------------------------------------------------------------------------------

\begin{vbframe}{Linear model: optimization}

\begin{itemize}
    \item Special property of LM with $L2$ loss: \textbf{analytical solution}
    available
    % \footnotesize
    \begin{align*}
        \thetabh \in 
        \argmin_{\thetab} \risket &=
        \argmin_{\thetab} \sumin \left(\yi - \thetab^\top \xi \right)^2  \\
        &= \argmin_{\thetab} \| \yv - \Xmat \thetab \|^2_2
    \end{align*}
    \normalsize
    \item Direct connection to assumption of 
    Gaussian errors: $$\yv = \Xmat \thetab + \bm{\epsilon}, ~ \bm{\epsilon} \sim 
    \normal(0, \id)$$
    \item Find via \textbf{normal equations}
    $$
    \pd{\risket}{\thetab} = 0 \quad\implies\quad \thetabh = \olsest
    $$
    \item Matrix inversion possibly expensive (for large $n$)
\end{itemize}

\end{vbframe}


\endlecture
\end{document}

