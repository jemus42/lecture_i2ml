\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{slides/supervised-regression/figure/nutshell-regression-derivative-L2.pdf}


\newcommand{\learninggoals}{
\item Understand basic concept of regressors
\item Understand difference between L1 and L2 Loss
\item Know basic idea of OLS estimator
}

\title{Introduction to Machine Learning}
\date{}
\begin{document}

%\lecturechapter{In a Nutshell: ML-Basics}
\lecturechapter{Supervised Regression: In a Nutshell}
\lecture{Introduction to Machine Learning}
\sloppy

\begin{vbframe}{Linear Regression Tasks}
\begin{itemize}
\item \small Learn linear combination of features for predicting the target variable
\item \small Find best parameters of the model by training w.r.t. a loss function $Credit Balance = \textcolor{blue}{\theta_0} + \textcolor{blue}{\theta_1}Rating + \textcolor{blue}{\theta_2}Income + \textcolor{blue}{\theta_3}Credit Limit$

\end{itemize}

\begin{columns}  
\begin{column}{0.1\textwidth} 
\begin{center}
Training
\end{center}
\end{column}
\begin{column}{0.9\textwidth} 
\begin{center}
%https://docs.google.com/presentation/d/172tBPXPO2B0whGwUaVZyrI4YHbh8QvS7/edit#slide=id.p1
  \includegraphics[width = 0.8\textwidth]{slides/supervised-regression/figure_man/nutshell-regression-learning-task.png}
\end{center}
\end{column}
\end{columns}
%\end{center}
%\begin{center}
\begin{columns}
\begin{column}{0.1\textwidth} 
\begin{center}
Prediction
\end{center}
\end{column}
\begin{column}{0.9\textwidth} 
\begin{center}
%https://docs.google.com/presentation/d/1cCEtDDxFkLOBv0psld-6-bMtPyjC4yVI/edit#slide=id.p1
  \includegraphics[width = 0.8\textwidth]{slides/supervised-regression/figure_man/nutshell-regression-prediction-task.png} 
\end{center}
\end{column}
\end{columns}

\end{vbframe}



\begin{vbframe}{Linear Models: L1 vs L2 Loss}

\small Loss can be characterized as a function of residuals \textbf{$r = y - \thx$}

\begin{columns}  
\begin{column}{0.4\textwidth} 
\begin{center}
  \includegraphics[width = \textwidth]{slides/supervised-regression/figure/nutshell-regression-L1.pdf}
\end{center}
\end{column}
\begin{column}{0.4\textwidth} 
\begin{center}
  \includegraphics[width = \textwidth]{slides/supervised-regression/figure/nutshell-regression-L2.pdf}
\end{center}
\end{column}
\end{columns}

\begin{minipage}[t]{0.45\textwidth}
    \footnotesize
    \begin{itemize}
        \item \small \textbf{L1} penalizes the \textbf{absolute} value of residuals 
        \item \textbf{$L(r) = |r|$}
        \item \small Robust to outliers

    \end{itemize}
\end{minipage}
\hfill
\begin{minipage}[t]{0.45\textwidth}
    \footnotesize
    \begin{itemize}
        \item \small \textbf{L2} penalizes the \textbf{quadratic} value of residuals
        \item \textbf{$L(r) = r^2$}
        \item \small Faster to optimize
    \end{itemize}
\end{minipage}


\end{vbframe}

\begin{vbframe}{Linear Models: L1 vs L2 Loss}

\begin{minipage}[t]{0.45\textwidth}
    \footnotesize
    \begin{itemize}
        \item \small \textbf{L1} Loss is not differentiable in \textbf{$r = 0$}
        \item \small Optimal parameters are computed numerically 

    \end{itemize}
\end{minipage}
\hfill
\begin{minipage}[t]{0.45\textwidth}
    \footnotesize
    \begin{itemize}
        \item \small \textbf{L2} is a smooth function hence it is differentiable everywhere 
        \item \small Optimal parameters can be computed analytically 
    \end{itemize}
\end{minipage}

\end{vbframe}

\begin{vbframe}{Linear Models: L1 vs L2 Loss}

\begin{itemize}
\item \small The parameter values of the best model depend on the loss type
\end{itemize}
\begin{columns}  
\begin{column}{0.5\textwidth} 
\begin{center}
  \includegraphics[width = \textwidth]{slides/supervised-regression/figure/nutshell-regression-L1-regr-line.pdf}
\end{center}
\end{column}
\begin{column}{0.5\textwidth} 
\begin{center}
  \includegraphics[width = \textwidth]{slides/supervised-regression/figure/nutshell-regression-L2-regr-line.pdf}
\end{center}
\end{column}
\end{columns}
\begin{minipage}[t]{0.5\textwidth}
    \footnotesize
    \begin{itemize}
        \item \small $\thetah_{L_1} = 0.14$ $\rightarrow$ if the Credit Limit increases by 1\$ the Credit Balance increases by 14 Cents 
    \end{itemize}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}
    \footnotesize
    \begin{itemize}
        \item \small $\thetah_{L_2} = 0.19$ $\rightarrow$ if the Credit Limit increases by 1\$ the Credit Balance increases by 19 Cents
    \end{itemize}
\end{minipage}


\end{vbframe}

\begin{vbframe}{OLS estimator}
\small \textbf{ordinary-least-square (OLS)} estimator:  Analytical solution for linear models with L2 loss
\begin{itemize}
    \item \small Best parameters can be computed by derivation of the empirical risk
\end{itemize}
\small $$\pd{\risket}{\thetab} = \pd{\sumin (\yi - \theta^T \xi)^2}{\thetab} = 0$$ % 

\begin{columns}  
\begin{column}{0.3\textwidth} 
\vspace{1.5cm}
 \item \small Solution:  $\thetabh = \olsest$

\end{column}
\begin{column}{0.5\textwidth} 
\begin{center}
 \includegraphics[width =0.8\textwidth]{slides/supervised-regression/figure/nutshell-regression-derivative-L2.pdf}
\end{center}
\end{column}
\end{columns}
\end{vbframe}
   




\begin{vbframe}{OLS estimator}
Components of \textbf{OLS} estimator:
\begin{itemize}
    \item \Xmat: Features + extra column for intercept
    \item \yv: Label vector
\end{itemize}
\vspace{0.7 cm}
%https://docs.google.com/presentation/d/1hYu69Z5dmnHEq05qgMkT1-zQE3rE4D2q/edit#slide=id.p1
  \includegraphics[width = 0.6\textwidth]{slides/supervised-regression/figure_man/nutshell-regression-design-matrix.png}


\end{vbframe}

\begin{vbframe}{Polynomial Regression}

\begin{itemize}
\small \item Adding polynomial terms to the linear combination leads to more flexible regression lines 
\small \item Too high degrees can lead to overfitting
\end{itemize}

\vspace{1cm}

\begin{columns}  
\begin{column}{0.3\textwidth} 
\tiny \text{$Balance = \textcolor{blue}{\theta_0} + \textcolor{blue}{\theta_1}Limit$}
 \includegraphics[width = \textwidth]{slides/supervised-regression/figure/nutshell-regression-poly-plot-1.pdf}
\end{column}
\begin{column}{0.3\textwidth} 
\tiny \text{$Balance = \textcolor{blue}{\theta_0} + \textcolor{blue}{\theta_1}Limit + \textcolor{blue}{\theta_2}Limit^2$}
  \includegraphics[width = \textwidth]{slides/supervised-regression/figure/nutshell-regression-poly-plot-2.pdf}
\end{column}
\begin{column}{0.4\textwidth} 
\tiny \text{$Balance = \textcolor{blue}{\theta_0} + \textcolor{blue}{\theta_1}Limit + \textcolor{blue}{\theta_2}Limit^2 
+ \textcolor{blue}{\theta_3}Limit^3$}
   \includegraphics[width = 0.75\textwidth]{slides/supervised-regression/figure/nutshell-regression-poly-plot-3.pdf}
\end{column}
\end{columns}


\end{vbframe}

\endlecture
\end{document}