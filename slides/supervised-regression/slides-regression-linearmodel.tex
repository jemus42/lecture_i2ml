\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R



\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}
\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
  %Define standard arrow tip
  >=stealth',
  %Define style for boxes
  punkt/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=6.5em,
    minimum height=2em,
    text centered},
  % Define arrow style
  pil/.style={
    ->,
    thick,
    shorten <=2pt,
    shorten >=2pt,}
}
\usepackage{subfig}


% Defines macros and environments
\input{../../style/common.tex}
% \input{common.tex}

%\usetheme{lmu-lecture}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}



\begin{document}
% Introduction to Machine Learning
% Day 1

% Set style/preamble.Rnw as parent.

% Load all R packages and set up knitr

% This file loads R packages, configures knitr options and sets preamble.Rnw as parent file
% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...








% Defines macros and environments
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-lm.tex}

%! includes: basics-supervised, basics-learnercomponents-hro, regression-losses

\lecturechapter{Linear Regression Models}
\lecture{Introduction to Machine Learning}

% slides adapted from rcourses/chapters/ml/ml_simple_models/slides_sm_lmXXX.Rmd @ fbce91e4277

\begin{vbframe}{Linear Regression: Hypothesis Space}
\lz

We want to predict a numerical target variable by a \emph{linear transformation} of the features $\xv \in \R^p$.

\lz

So with $\thetab \in \R^p$ this mapping can be written as:
\begin{align*}
y &= \fx = \theta_0 + \thx \\
  &= \theta_0 + \theta_1 x_1 + \dots + \theta_p x_p
\end{align*}

\lz

This defines the hypothesis space $\Hspace$ as the set of all linear functions in $\thetab$:
\[
\Hspace = \{ \theta_0 + \thx\ |\ (\theta_0, \thetab) \in \R^{p+1} \}
\]

\framebreak

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.7\textwidth]{figure/reg_lm_plot} 

}



\end{knitrout}

\[
y = \theta_0 + \theta \cdot x
\]

\framebreak

\lz

Given observed labeled data $\D$, how to find $(\theta_0, \thetab)$?\\

\lz

This is \textbf{learning} or parameter estimation, the learner does exactly this by \textbf{empirical risk minimization}.

\lz
\lz

NB: We assume from now on that $\theta_0$ is included in $\thetab$.

\end{vbframe}

\begin{frame}{Linear Regression: Risk}

We could measure training error as the sum of squared prediction errors (SSE). This is the risk that corresponds to \textbf{L2 loss}:
\[
\risket = \operatorname{SSE}(\thetab) = \sumin \Lxyit = \sumin \left(\yi - \thetab^T \xi\right)^2
\]

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.7\textwidth]{figure/reg_lm_plot_2} 

}



\end{knitrout}

Minimizing the squared error is computationally much simpler than minimizing the absolute differences (\textbf{L1 loss}).

\end{frame}

\begin{vbframe}{Linear Model: Optimization}

We want to find the parameters $\thetab$ of the linear model, i.e., an element of the hypothesis space $\Hspace$ that fits the data optimally.\\

So we evaluate different candidates for $\thetab$.\\

A first (random) try yields a rather large SSE: (\textbf{Evaluation}).
\lz

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\textwidth]{figure/reg_lm_plot_31} 

}



\end{knitrout}

\framebreak

We want to find the parameters of the LM / an element of the hypothesis space $\Hspace$ that best suits the data.
So we evaluate different candidates for $\thetab$.\\

Another line yields an even bigger SSE (\textbf{Evaluation}). Therefore, this one is even worse in
terms of empirical risk.

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\textwidth]{figure/reg_lm_plot_32} 

}



\end{knitrout}

\framebreak

We want to find the parameters of the LM / an element of the hypothesis space $\Hspace$ that best suits the data.
So we evaluate different candidates for $\thetab$.\\

Another line yields an even bigger SSE (\textbf{Evaluation}). Therefore, this one is even worse in
terms of empirical risk. Let's try again:

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\textwidth]{figure/reg_lm_plot_33} 

}



\end{knitrout}


\framebreak


Since every $\thetab$ results in a specific value of $\risket$, and we try
to find $\argmin_{\thetab} \risket$, let's look at what we have so far:

\lz

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=1.12\textwidth]{figure/reg_lm_plot_34} 

}



\end{knitrout}

\framebreak

Instead of guessing, we use \textbf{optimization} to find the best $\thetab$:
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.5\textwidth]{figure/reg_lm_plot_35} 

}



\end{knitrout}

\framebreak

Instead of guessing, we use \textbf{optimization} to find the best $\thetab$:
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.5\textwidth]{figure/reg_lm_plot_36} 

}



\end{knitrout}

\framebreak

Instead of guessing, we use \textbf{optimization} to find the best $\thetab$:

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=1.1\textwidth]{figure/reg_lm_plot_37} 

}



\end{knitrout}


\framebreak

For L2 regression, we can find this optimal value analytically:
\begin{align*}
\thetabh &= \argmin_{\thetab} \risket = \sumin \left(\yi - \thetab^T \xi\right)^2\\
             &=\argmin_{\thetab} \|\yv - X\thetab\|^2_2\
\end{align*}
where $\Xmat = \left(\begin{smallmatrix}1 & x^{(1)}_1 & \ldots & x^{(1)}_p \\
                              1 & x^{(2)}_1 & \ldots & x^{(2)}_p \\
                              \vdots & \vdots & & \vdots \\
                              1 & x^{(n)}_1 & \ldots & x^{(n)}_p \end{smallmatrix}\right)$
is the $n \times (p+1)$-\textbf{design matrix}.

\lz

This yields the so-called normal equations for the LM:
\[
\frac{\partial}{\partial\thetab} \risket = 0 \quad\implies\quad \thetabh = \left(\Xmat^T \Xmat\right)^{-1}\Xmat^T\yv
\]

\end{vbframe}

\begin{vbframe}{Example: Regression With L1 vs L2 Loss}

We could also minimize the L1 loss. This changes the risk and optimization steps:
\[
\risket = \sumin \Lxyit = \sumin \left|\yi - \thetab^T \xi\right| \qquad \textsf{(Risk)}
\]

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.75\textwidth]{figure/reg_lm_plot_38}

}



\end{knitrout}

L1 loss is harder to optimize, but the model is less sensitive to outliers.

\framebreak

\lz
\lz

% FIGURE SOURCE: No source
{\centering \includegraphics{figure_man/l2-vs-l1-1.pdf}}

\framebreak

Adding an outlier (highlighted red) pulls the line fitted with L2 into the direction of the outlier:

\lz

% FIGURE SOURCE: No source
{\centering \includegraphics{figure_man/l2-vs-l1-2.pdf}}

\end{vbframe}

\begin{frame}{Linear Regression}
\lz

\textbf{Hypothesis Space:} Linear functions $\xv^T\thetab$ of features $\in \Xspace$.\\

\lz

\textbf{Risk:} Any regression loss function.

\lz

\textbf{Optimization:} Direct analytical solution for L2 loss, numerical optimization for L1 and others.

\end{frame}

\endlecture
\end{document}
