\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}


\newcommand{\titlefigure}{figure/reg_lm_plot}
\newcommand{\learninggoals}{
\item Know the hypothesis space of the linear model
\item Understand the risk function that follows with L2 loss
\item Understand how optimization works for the linear model
\item Understand how outliers affect the estimated model differently when using L1 or L2 loss}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}
\lecturechapter{Linear Regression Models}
\lecture{Introduction to Machine Learning}

% slides adapted from rcourses/chapters/ml/ml_simple_models/slides_sm_lmXXX.Rmd @ fbce91e4277

\begin{vbframe}{Linear Regression: Hypothesis Space}
\lz

We want to predict a numerical target variable by a \emph{linear transformation} of the features $\xv \in \R^p$.

\lz

So with $\thetab \in \R^p$ this mapping can be written as:
\begin{align*}
y &= \fx = \theta_0 + \thx \\
  &= \theta_0 + \theta_1 x_1 + \dots + \theta_p x_p
\end{align*}

\lz

This defines the hypothesis space $\Hspace$ as the set of all linear functions in $\thetab$:
\[
\Hspace = \{ \theta_0 + \thx\ |\ (\theta_0, \thetab) \in \R^{p+1} \}
\]

\framebreak

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.7\textwidth]{figure/reg_lm_plot} 

}



\end{knitrout}

\[
y = \theta_0 + \theta_1 \cdot x
\]

\framebreak

\lz

Given observed labeled data $\D$, how to find $(\theta_0, \thetab)$?\\

\lz

This is \textbf{learning} or parameter estimation, the learner does exactly this by \textbf{empirical risk minimization}.

\lz
\lz

NB: We assume from now on that $\theta_0$ is included in $\thetab$.

\end{vbframe}

\begin{frame}{Linear Regression: Risk}

We could measure training error as the sum of squared prediction errors (SSE). This is the risk that corresponds to \textbf{L2 loss}:
\[
\risket = \operatorname{SSE}(\thetab) = \sumin \Lxyit = \sumin \left(\yi - \thetab^T \xi\right)^2
\]

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.7\textwidth]{figure/reg_lm_plot_2} 

}



\end{knitrout}

Minimizing the squared error is computationally much simpler than minimizing the absolute differences (\textbf{L1 loss}).

\end{frame}

\begin{vbframe}{Linear Model: Optimization}

We want to find the parameters $\thetab$ of the linear model, i.e., an element of the hypothesis space $\Hspace$ that fits the data optimally.\\

So we evaluate different candidates for $\thetab$.\\

A first (random) try yields a rather large SSE: (\textbf{Evaluation}).
\lz

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\textwidth]{figure/reg_lm_plot_31} 

}



\end{knitrout}

\framebreak

We want to find the parameters $\thetab$ of the linear model, i.e., an element of the hypothesis space $\Hspace$ that fits the data optimally.\\

So we evaluate different candidates for $\thetab$.\\

Another line yields an even bigger SSE (\textbf{Evaluation}). Therefore, this one is even worse in
terms of empirical risk.

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\textwidth]{figure/reg_lm_plot_32} 

}



\end{knitrout}

\framebreak

We want to find the parameters $\thetab$ of the linear model, i.e., an element of the hypothesis space $\Hspace$ that fits the data optimally.\\

So we evaluate different candidates for $\thetab$.\\

Another line yields an even bigger SSE (\textbf{Evaluation}). Therefore, this one is even worse in
terms of empirical risk. Let's try again:

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\textwidth]{figure/reg_lm_plot_33} 

}



\end{knitrout}


\framebreak


Since every $\thetab$ results in a specific value of $\risket$, and we try
to find $\argmin_{\thetab} \risket$, let's look at what we have so far:

\lz

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=1.12\textwidth]{figure/reg_lm_plot_34} 

}



\end{knitrout}

\framebreak

Instead of guessing, we use \textbf{optimization} to find the best $\thetab$:
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.5\textwidth]{figure/reg_lm_plot_35} 

}



\end{knitrout}

\framebreak

Instead of guessing, we use \textbf{optimization} to find the best $\thetab$:
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.5\textwidth]{figure/reg_lm_plot_36} 

}



\end{knitrout}

\framebreak

Instead of guessing, we use \textbf{optimization} to find the best $\thetab$:

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=1.1\textwidth]{figure/reg_lm_plot_37} 

}



\end{knitrout}


\framebreak

For L2 regression, we can find this optimal value analytically:
\begin{align*}
\thetabh &= \argmin_{\thetab} \risket = \argmin_{\thetab} \sumin \left(\yi - \thetab^T \xi\right)^2\\
             &=\argmin_{\thetab} \|\yv - \Xmat\thetab\|^2_2\
\end{align*}
where $\Xmat = \left(\begin{smallmatrix}1 & x^{(1)}_1 & \ldots & x^{(1)}_p \\
                              1 & x^{(2)}_1 & \ldots & x^{(2)}_p \\
                              \vdots & \vdots & & \vdots \\
                              1 & x^{(n)}_1 & \ldots & x^{(n)}_p \end{smallmatrix}\right)$
is the $n \times (p+1)$-\textbf{design matrix}.

\lz

This yields the so-called normal equations for the LM:
\[
\frac{\partial}{\partial\thetab} \risket = 0 \quad\implies\quad \thetabh = \left(\Xmat^T \Xmat\right)^{-1}\Xmat^T\yv
\]

\end{vbframe}

\begin{vbframe}{Example: Regression With L1 vs L2 Loss}

We could also minimize the L1 loss. This changes the risk and optimization steps:
\[
\risket = \sumin \Lxyit = \sumin \left|\yi - \thetab^T \xi\right| \qquad \textsf{(Risk)}
\]

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.75\textwidth]{figure/reg_lm_plot_38}

}



\end{knitrout}

L1 loss is harder to optimize, but the model is less sensitive to outliers.

\framebreak

\lz
\lz

% FIGURE SOURCE: No source
{\centering \includegraphics{figure_man/l2-vs-l1-1.pdf}}

\framebreak

Adding an outlier (highlighted red) pulls the line fitted with L2 into the direction of the outlier:

\lz

% FIGURE SOURCE: No source
{\centering \includegraphics{figure_man/l2-vs-l1-2.pdf}}

\end{vbframe}

\begin{frame}{Linear Regression}
\lz

\textbf{Hypothesis Space:} Linear functions $\xv^T\thetab$ of features $\in \Xspace$.\\

\lz

\textbf{Risk:} Any regression loss function.

\lz

\textbf{Optimization:} Direct analytical solution for L2 loss, numerical optimization for L1 and others.

\end{frame}

\endlecture
\end{document}
