\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{figure/plot_loss}
\newcommand{\learninggoals}{
\item Understand difference between $L1$- and $L2$-regression
\item See how outliers affect LM
}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}
\lecturechapter{Linear Models with $L1$ Loss}
\lecture{Introduction to Machine Learning}

% ------------------------------------------------------------------------------

\begin{vbframe}{Absolute loss}

\begin{itemize}
    \item $L2$ regression minimizes quadratic residuals -- wouldn't
    \textbf{absolute} residuals seem more natural?

    \vspace{0.2cm}
    \includegraphics[width=0.25\textwidth]{figure/reg_lm_sse}
    \includegraphics[width=0.25\textwidth]{figure/reg_lm_sse}

    \textcolor{red}{TODO: make right plot w abs res, both more highlighted
    points
    }
    \item \textbf{$L1$ loss / absolute error / least absolute deviation (LAD)}
    $$
    \Lxy = |y-\fx|
    $$
    \includegraphics[width=0.4\textwidth]{figure/plot_abs_loss}
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{$L1$ vs $L2$}

\textcolor{red}{TODO: plot 3D loss surfaces of $L1$ vs $L2$}

\vfill

$L1$ loss harder to optimize
\begin{itemize}
    \item Convex but \textbf{not differentiable} in
    $y - \fx = 0$
    \item No analytical solution
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{$L1$ vs $L2$}

\begin{itemize}
    \item Quadratic relation to residuals means $L2$ gives lots of weight to
    outlying points: point with $3 \times$ the residual of another will have
    $9 \times$ loss contribution
    \item $L1$ more \textbf{robust}
\end{itemize}

\vfill
\includegraphics[width=0.45\textwidth]{figure_man/l2-vs-l1-1.pdf}
\includegraphics[width=0.45\textwidth]{figure_man/l2-vs-l1-2.pdf}

\textcolor{red}{TODO: re-do figures}

\end{vbframe}

% ------------------------------------------------------------------------------
\begin{vbframe}{$L1$ vs $L2$}
Consider this real world regression problem with mean temperature as target and further weather data as dependent variables. \\
A regression was once fitted using $L1$ and once fitted using $L2$ loss:
\begin{table}[!htbp] \centering
\begin{tabular}{@{\extracolsep{1pt}}lcc}
\\[-1.8ex]\hline
\hline \\[-1.8ex]
 & \multicolumn{2}{c}{\textit{Dependent variable:}} \\
\cline{2-3}
\\[-1.8ex] & \multicolumn{2}{c}{Mean\_temperature} \\
\\[-1.8ex] & \textit{L1-loss} & \textit{L2-loss} \\
\hline \\[-1.8ex]
 Max\_temperature & 0.553 & 0.563 \\
 Min\_temperature & 0.441 & 0.427 \\
 Visibility & 0.026 & 0.041 \\
 Wind\_speed & 0.002 & 0.010 \\
 Max\_wind\_speed & $-$0.026 & $-$0.039 \\
 Constant & $-$0.380 & $-$0.102 \\
\hline \\[-1.8ex]
Observations & 1,461 & 1,461 \\
\hline
\end{tabular}
\end{table}
\end{vbframe}

\begin{vbframe}{$L1$ vs $L2$}
As $L1$ is harder to optimize it usually takes longer to optimize. Using different proportions of the weather dataset and repeating the estimations 10-times illustrates this:
\vfill
\includegraphics[width=0.9\textwidth]{figure/l1_vs_l2_benchmark_plot.pdf}

\end{vbframe}

\endlecture
\end{document}
