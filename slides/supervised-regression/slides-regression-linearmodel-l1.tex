\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{figure/reg_lm_l1_l2.pdf}
\newcommand{\learninggoals}{
\item Understand difference between $L1$- and $L2$-regression
\item See how outliers affect LM
}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}
\lecturechapter{Linear Models with $L1$ Loss}
\lecture{Introduction to Machine Learning}

% ------------------------------------------------------------------------------

\begin{vbframe}{Absolute loss}

\begin{itemize}
    \item $L2$ regression minimizes quadratic residuals -- wouldn't 
    \textbf{absolute} residuals seem more natural? 
    
    \vspace{0.2cm}
    \includegraphics[width=0.55\textwidth]{figure/reg_lm_residual_l2_l1.pdf}
    \item \textbf{$L1$ loss / absolute error / least absolute deviation (LAD)}
    $$\Lxy = |y - \fx|$$
    \includegraphics[width=0.55\textwidth]{figure/l1} 
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{$L1$ loss surface}

\includegraphics[width=0.49\textwidth, trim=100 30 100 0, clip]{
figure/reg_lm_surface_l2.pdf}
\includegraphics[width=0.49\textwidth, trim=100 30 100 0, clip]{
figure/reg_lm_surface_l1.pdf}
\vfill

$L1$ loss harder to optimize
\begin{itemize}
    \item Convex but \textbf{not differentiable} in 
    $y - \fx = 0$
    \item No analytical solution
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{$L1$ vs $L2$}

\textcolor{red}{TODO: datenbeispiel ohne outlier, surfaces mit optim traces 
sowie param vectors zeigen. message: nicht krass unterschiedlich}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{$L1$ vs $L2$ -- robustness}

\begin{itemize}
    \item $L2$ quadratic in residuals $\rightsquigarrow$ outlying points 
    carry lots of weight
    \item E.g., $3 \times$ residual $\Rightarrow$ $9 \times$ loss contribution
    \item $L1$ more \textbf{robust} in presence of outliers (red):
\end{itemize}

\vfill
\includegraphics[width=\textwidth]{figure/reg_lm_l1_l2_outlier.pdf}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{$L1$ vs $L2$ -- optimization cost}

\textcolor{red}{TODO: optim effort L1 vs L2. openml datensatz nehmen, 4-5 sample 
sizes nehmen und wall clock time L1 vs L2 hinschreiben}

\end{vbframe}

\endlecture
\end{document}