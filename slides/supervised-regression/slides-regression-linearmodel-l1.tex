\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{figure/reg_l1_lossplot_abs_vs_quad.pdf}
\newcommand{\learninggoals}{
\item Understand difference between $L1$ and $L2$ regression
\item See how choice of loss affects optimization \& robustness
}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}
\lecturechapter{Linear Models with $L1$ Loss}
\lecture{Introduction to Machine Learning}

% ------------------------------------------------------------------------------

\begin{vbframe}{Absolute loss}

\begin{itemize}
    \item $L2$ regression minimizes quadratic residuals -- wouldn't 
    \textbf{absolute} residuals seem more natural? 
    
    \vspace{0.2cm}
    \includegraphics[width=0.55\textwidth]{figure/reg_l1_residual_abs_vs_quad}
    \item \textbf{$L1$ loss / absolute error / least absolute deviation (LAD)}
    $$\Lxy = |y - \fx|$$
    \includegraphics[width=0.55\textwidth]{figure/reg_l1_lossplot_abs} 
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{$L1$ loss surface}

\includegraphics[width=0.49\textwidth, trim=100 30 100 0, clip]{
figure/reg_l1_surface_abs.pdf}
\includegraphics[width=0.49\textwidth, trim=100 30 100 0, clip]{
figure/reg_l1_surface_quad.pdf}
\vfill

$L1$ loss (left) harder to optimize
\begin{itemize}
    \item Convex but \textbf{not differentiable} in 
    $y - \fx = 0$
    \item No analytical solution
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{$L1$ vs $L2$}

\begin{itemize}
    \item Results of $L1$ and $L2$ regression often not that different
    \item Example: $y = 1 + 0.5 x_1 + \epsilon$, ~~ $\epsilon_i \sim 
    \normal(0, 0.01) ~ \forall i$
    % \item Coefficients:
\end{itemize}
    
\vfill

\begin{minipage}[b]{0.65\textwidth}
    \hspace{0.7cm}
    \footnotesize
    \begin{tabular}{r|r|r}
        & intercept & slope \\ \hline
        \textcolor{blue}{$L1$} & 0.91 & 0.53 \\ \hline
        \textcolor{orange}{$L2$} & 0.91 & 0.57 
    \end{tabular}

    \vspace{0.5cm}
    \includegraphics[width=0.8\textwidth]{figure/reg_l1_comparison.pdf}
\end{minipage}
\begin{minipage}[b]{0.34\textwidth}
    \includegraphics[width=\textwidth, trim=80 0 100 80, clip]{
    figure/reg_l1_comparison_optim_abs.pdf}
    
    \vfill
    
    \includegraphics[width=\textwidth, trim=80 0 100 80, clip]{
    figure/reg_l1_comparison_optim_quad.pdf}
\end{minipage}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{$L1$ vs $L2$ -- robustness}

\begin{itemize}
    \item $L2$ quadratic in residuals $\rightsquigarrow$ outlying points 
    carry lots of weight
    \item E.g., $3 \times$ residual $\Rightarrow$ $9 \times$ loss contribution
    \item $L1$ more \textbf{robust} in presence of outliers (example ctd.):
\end{itemize}

\vfill
\includegraphics[width=\textwidth]{figure/reg_l1_comparison_outlier.pdf}

\end{vbframe}
% ------------------------------------------------------------------------------

\begin{vbframe}{$L1$ vs $L2$ -- optimization cost}

\textcolor{red}{TODO: re-do figures}
\textcolor{red}{TODO: optim effort L1 vs L2. openml datensatz nehmen, 4-5 sample 
sizes nehmen und wall clock time L1 vs L2 hinschreiben}

\end{vbframe}

\endlecture
\end{document}