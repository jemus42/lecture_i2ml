% Introduction to Machine Learning
% Day 1

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../../style/preamble.Rnw")
@
% Load all R packages and set up knitr
<<setup-r, child="../../style/setup.Rnw", include = FALSE>>=
@

%! includes: basics-riskminimization

\lecturechapter{Loss Functions for Regression}
\lecture{Introduction to Machine Learning}


\begin{vbframe}{Regression Losses - L2 / Squared Error}
\begin{itemize}
\item $\Lxy = (y-\fx)^2$ or $\Lxy = 0.5 (y-\fx)^2$
\item Convex
\item Differentiable, gradient no problem in loss minimization
\item For latter: $\pd{0.5 (y-\fx)^2}{\fx} = y - \fx = \eps$, derivative is residual
\item Tries to reduce large residuals (if residual is twice as large, loss is 4 times as large), hence
  outliers in $y$ can become problematic
\item Connection to Gaussian distribution (see later)
\end{itemize}

<<echo=FALSE, results='hide', fig.height=3>>=
source("figure_man/plot_loss.R")

set.seed(31415)

x = 1:5
y = 2 + 0.5 * x + rnorm(length(x), 0, 1.5)
data = data.frame(x = x, y = y)
model = lm(y ~ x)

plotModQuadraticLoss(data = data, model = model, pt_idx = c(1,4))
@
\framebreak

What's the optimal constant prediction $c$ (i.e. the same $\hat{y}$ for all $x$)?
$$\Lxy = (y-\fx)^2 = (y - c)^2$$
We search the $c$ that minimizes the empirical risk.
$$  \hat{c} = \argmin_{c \in \R}\riske(c)  =  \argmin_{c \in \R} \frac{1}{n} \sumin  (\yi-c)^2 $$
We set the derivative of the empirical risk to zero and solve for $c$:
\begin{eqnarray*}
 -\frac{1}{n}\sumin 2(\yi - c) &=& 0 \cr
\hat{c} &=& \frac{1}{n} \sumin \yi
\end{eqnarray*}
% $\frac{1}{n} \sumin \yi$ is also the maximum likelihood estimator for $\E[y]$ .

% \framebreak
% What is the optimal prediction if we allow $c$ to depend on $x$? \\
% According to the law of total expectation:
% \begin{displaymath}
%   \E_{xy} [\Lxy] = \E_x
%   \left[\E_{y|x}[(y-\fx)^2|x=x]\right]
% \end{displaymath}
% For the optimal prediction it suffices to minimize the risk pointwise:
% $$
%   \fh(x) = \mbox{argmin}_c \E_{y|x}[(y-c)^2|x=x]=\E (y | x=x)
% $$
% So for squared loss, the best prediction in every point is the conditional mean of $y$ given $x$.
%
% \lz
%
% The last step follows from:
% $$
% E[(y - c)^2] = Var[y - c] + (E[y - c])^2 = Var[y] + (E[y] - c)^2
% $$
% This is minimal for $c=E[y]$ if $c$ constant and for $c = E[y|x = x]$ if $c$ is not constant.
\end{vbframe}

\begin{vbframe}{Regression Losses - L1 / Absolute Error}
\begin{itemize}
\item $\Lxy = |y-f(x)|$
\item Convex
\item No derivatives for $ = 0$, $y = f(x)$, optimization becomes harder
%\item More robust, outliers in $y$ are less problematic
\item $\fh(x) = \text{median of } y | x$
%\item Connection to Laplace distribution (see later)
\end{itemize}

<<echo=FALSE, results='hide', fig.height=3, fig.align='center'>>=
plotModAbsoluteLoss(data, model = model, pt_idx = c(1,4), add_quadratic = FALSE) +
  ylim(c(0, 16))
@

\framebreak

\begin{itemize}
\item $\Lxy = |y-f(x)|$
\item Convex
\item No derivatives for $\eps = 0$, $y = f(x)$, optimization becomes harder
\item $\fh(x) = \text{median of } y | x$
\item More robust, outliers in $y$ are less influential than for L2
%\item Connection to Laplace distribution (see later)
\end{itemize}

<<echo=FALSE, results='hide', fig.height=3, fig.align='center'>>=
plotModAbsoluteLoss(data, model = model, pt_idx = c(1,4))
@

\end{vbframe}

% \begin{vframe}{Regression losses - Huber loss}
% \begin{itemize}
% \item Huber loss $L_\delta(y, f(x)) =
%   \begin{cases}
%   \frac{1}{2}(y-f(x))^2  & \text{ if } |y-f(x)| \le \delta \\
%   \delta |y-f(x)|-\frac{1}{2}\delta^2 \quad & \text{ otherwise }
%   \end{cases}$
% \item Piecewise combination of of L1 and L2 loss
% \item Convex
% \item Combines advantages of L1 and L2 loss: differentiable, robust
% \end{itemize}
%
% <<echo=FALSE, results='hide', fig.height=3, fig.align='center'>>=
% x = seq(-2, 2, by = 0.01); y = ifelse(abs(x) <= 1, 1 / 2 * x^2, abs(x) - 1 / 2)
% qplot(x, y, geom = "line", xlab = expression(y-f(x)), ylab = expression(L(y-f(x))))
% @
% \end{vframe}

\endlecture
