% Introduction to Machine Learning
% Day 1

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../../style/preamble.Rnw")
@
% Load all R packages and set up knitr
<<setup-r, child="../../style/setup.Rnw", include = FALSE>>=
@

%! includes:regression-linearmodel

\lecturechapter{Polynomial Regression Models}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{Regression: Polynomials}

We can make linear regression models much more flexible by using \emph{polynomials} $x_j^d$ -- or any other \emph{derived features} like $\sin(x_j)$ or $(x_j \cdot x_k)$ -- as additional features.\\

\lz

The optimization and risk of the learner remains the same.\\
\lz

Only the hypothesis space of the learner changes:\\ 
instead of linear functions 
\begin{align*}
f(\xi) &= \theta_0 + \theta_1 \xi_1 + \theta_2 \xi_2 + \dots\\
\intertext{of only the original features,}
\intertext{it now includes linear functions of the derived features as well, e.g.} 
f(\xi) &= \theta_0 + \sum^d_{k=1} \theta_{1k} \left(\xi_1\right)^k + \sum^d_{k=1} \theta_{2k} \left(\xi_2\right)^k + \dots
\end{align*}
\framebreak

\textbf{Polynomial regression example}

 \lz

<<echo=FALSE, out.width="0.9\\textwidth", fig.height = 4.5>>=
.h = function(x) 0.6 + 0.4 * sin(2 * pi * x)
h = function(x) .h(x) + .1 * arima.sim(list(ar = .7, ma = 0), length(x)) + rnorm(length(x), sd = .5)

x = seq(0.4, 2, length = 31L)
y = h(x)


line.palette = viridisLite::viridis(4)
baseplot = function() {
  # par(mar = c(2, 2, 1, 1))
  plot(x, y, pch = 19L, ylim = c(-1, 2))
}

p1 = lm(y ~ poly(x, 1, raw = TRUE))
p3 = lm(y ~ poly(x, 5, raw = TRUE))
p10 = lm(y ~ poly(x, 25, raw = TRUE))
mods = list(p1, p3, p10)
x.plot = seq(min(x), max(x), length = 500L)
baseplot()
@

\framebreak

\textbf{Polynomial regression example}

Models of different \emph{complexity}, i.e. of different polynomial order $d$, are fitted to the data:

<<echo=FALSE, out.width="0.9\\textwidth", fig.height=4.5>>=
baseplot()
for (i in 1) {
  lines(x.plot, predict(mods[[i]], newdata = data.frame(x = x.plot)),
    col = line.palette[i], lwd = 2L)
}
legend("topright", paste(sprintf("f(x) for d = %s", c(1, 5, 25)), c("(linear)", "", "")),
  col = line.palette, lwd = 2L)
@

\framebreak

\textbf{Polynomial regression example}

Models of different \emph{complexity}, i.e. of different polynomial order $d$, are fitted to the data:

<<echo=FALSE, out.width="0.9\\textwidth", fig.height=4.5>>=
baseplot()
for (i in 1:2) {
  lines(x.plot, predict(mods[[i]], newdata = data.frame(x = x.plot)),
    col = line.palette[i], lwd = 2L)
}
legend("topright", paste(sprintf("f(x) for d = %s", c(1, 5, 25)), c("(linear)", "", "")),
  col = line.palette, lwd = 2L)
@

\framebreak

\textbf{Polynomial regression example}

Models of different \emph{complexity}, i.e. of different polynomial order $d$, are fitted to the data:

<<echo=FALSE, out.width="0.9\\textwidth", fig.height=4.5>>=
baseplot()
for (i in 1:3) {
  lines(x.plot, predict(mods[[i]], newdata = data.frame(x = x.plot)),
    col = line.palette[i], lwd = 2L)
}
legend("topright", paste(sprintf("f(x) with d = %s", c(1, 5, 25)), c("(linear)", "", "")),
  col = line.palette, lwd = 2L)
@

\framebreak

The higher $d$ is, the more \textbf{capacity} the learner has to learn complicated functions of $x$, but
this also increases the danger of \textbf{overfitting}:\\
\lz

The model space $\Hspace$ contains so many complex functions that we are able to find one that approximates
the training data arbitrarily well.
\lz

However, predictions on new data are not as successful because our model has learnt spurious \enquote{wiggles} from the random noise in the training data (much, much more on this later).

\framebreak

<<echo=FALSE, fig.height = 6.5>>=
layout((1:2))
baseplot()
title(main = "Training Data")
for (i in 2:3) {
  lines(x.plot, predict(mods[[i]], newdata = data.frame(x = x.plot)),
    col = line.palette[i], lwd = 2L)
}

plot(x.plot,


     .h(x.plot) + .3 * rnorm(length(x.plot)), pch = 19, col = rgb(0,0,0,.5),

     cex = .8,
     xlab = "x", ylab = "y",
     ylim = c(-1, 2))
title(main = "Test Data")
for (i in 2:3) {
  lines(x.plot, predict(mods[[i]], newdata = data.frame(x = x.plot)),
    col = line.palette[i], lwd = 2L)
}
@

\end{vbframe}
\endlecture
