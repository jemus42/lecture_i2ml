
% Introduction to Machine Learning
% Day 1

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../../style/preamble.Rnw")
@
% Load all R packages and set up knitr
<<setup-r, child="../../style/setup.Rnw", include = FALSE>>=
@

<<bin-task>>=
n = 30
target = factor(c(rep(1L, times = n), rep(2L, times = n)))
feat1 = c(rnorm(n, sd = 1.5), rnorm(n, mean = 2, sd = 1.5))
feat2 = c(rnorm(n, sd = 1.5), rnorm(n, mean = 2, sd = 1.5))
bin.data = data.frame(feat1, feat2, target)
bin.tsk = makeClassifTask(data = bin.data, target = "target")
@

\lecturechapter{Logistic Regression}
\lecture{Introduction to Machine Learning}
%
%   \begin{vbframe}{Bin. classif. losses - Logistic loss}
%   \begin{itemize}
%     \item $\Lxy = \ln(1+\exp(-y\fx))$, used in logistic regression
%     \item $y \in \{-1, 1\}$
%     \item Also called Bernoulli loss
%   \item Convex, differentiable, not robust
%   \end{itemize}
%
%   <<fig.height=3>>=
%   x = seq(-2, 2, by = 0.01); y = log(1 + exp(-x))
%   qplot(x, y, geom = "line", xlab = expression(yf(x)), ylab = expression(L(yf(x))))
%   @
%   % the minimizer of $\risk(f)$ for the logistic loss function is
%
%   % \begin{eqnarray*}
%   % \fh(x) &=&  \ln \biggl(\frac{\pix}{1-\pix}\biggr)
%   % \end{eqnarray*}
%
%   % The function is undefined when $\pix = 1$ or $\pix = 0$, but predicts a smooth curve which grows when $\pix$ increases and equals $0$ when $\pix = 0.5$
%
%
%   \end{vbframe}

\begin{vbframe}{Logistic regression}

A \emph{discriminant} approach for directly modeling the posterior probabilities $\pix$ of the labels is \textbf{logistic regression}.\\

For now, let's focus on the binary case $y \in \{0, 1\}$.

A naive approach would be to model
\[
\pix = \post = \theta^T x .
\]
Obviously this could result in predicted probabilities $\pix \not\in [0,1]$.

\framebreak

To avoid this, logistic regression \enquote{squashes} the estimated linear scores $\theta^T x$ to $[0,1]$ through the \textbf{logistic function} $s$:
\[
\pix = \post = \frac{\exp\left(\theta^Tx\right)}{1+\exp\left(\theta^Tx\right)} = \frac{1}{1+\exp\left(-\theta^Tx\right)} = s\left(\theta^T x\right)
\]
\lz

Note that we will again usually suppress the intercept in notation, i.e., $\theta^Tx \equiv \theta_0 + \theta^Tx$.


\end{vbframe}

\begin{vbframe}{Logistic function}

The logistic function $s(t) = \frac{exp(t)}{1 + exp(t)}$ which we use to model the probability
$\post = s(\theta^T x)  = \frac{\exp(\theta^Tx)}{1+\exp(\theta^Tx)} $
\lz
<<logistic-function, echo=FALSE, fig.height = 3.5>>=
n = 100
df = data.frame(x = seq(from = -10, to  = 10, length.out = n))
df$y = exp(df$x) / (1 + exp(df$x))
ggplot(df) + geom_line(aes(x = x, y = y)) + scale_x_continuous('t')  + scale_y_continuous('s(t)')
@

\framebreak
Changing the intercept shifts the logistic curve in x-axis direction.
Let's assume $\theta_1 = 1$ for simplicity, so that
$\post =  \frac{\exp(\theta_0 + x)}{1+\exp(\theta_0 + x)}$
\lz
<<logistic-function-par, echo=FALSE, fig.height = 3.5>>=
n = 100
df = data.frame(x = rep(seq(from = -10, to  = 10, length.out = n ), times = 3),
                        intercept = rep(c(-3,0,3), each = n))
df$y = exp(df$intercept + df$x) / (1 + exp(df$intercept + df$x))

ggplot(df) + geom_line(aes(x = x, y = y, group = intercept, color = factor(intercept))) +
scale_x_continuous('x')  + scale_y_continuous(expression(s(theta^T*x))) +
  scale_color_viridis_d(expression(theta[0]))
@

\framebreak
Assuming a single feature and no intercept: $\post =  \frac{\exp(\theta_1 x_1)}{1+\exp(\theta_1 x_1)}$:
Parameter $\theta_1$ controls the slope and direction of the logistic curve.
\lz
<<logistic-function-par2, echo=FALSE, fig.height = 3.5>>=
n = 100
df = data.frame(x = rep(seq(from = -10, to  = 10, length.out = n ), times = 4),
                        theta1 = rep(c(-2,-0.3, 1, 6), each = n))
df$y = exp(df$x * df$theta1) / (1 + exp(df$theta1 * df$x))

ggplot(df) + geom_line(aes(x = x, y = y, group = theta1, color=factor(theta1))) +
scale_x_continuous('x')  + scale_y_continuous(expression(s(theta^T*x))) +
  scale_color_viridis_d(expression(theta[1]))
@

\end{vbframe}

\begin{vbframe}{Classification loss}

In order to find the \enquote{optimal} model represented by $\theta$, we need to define a \emph{loss function}.\lz

For a single observation, it makes sense to simply compare the probability implied by the model to the actually observed target variable:\\
\begin{align*}
\text{\enquote{accuracy}}^{(i)} &:=
  \begin{cases} \pi(x^{(i)}, \theta) & \text{if } \yi = 1 \\
              1-\pi(x^{(i)}, \theta) & \text{if } \yi = 0 \end{cases} \\
  & = \pi(\xi, \theta)^{\yi} (1-\pi(\xi, \theta))^{1-\yi}
\intertext{For the entire data set, we combine these predicted probabilites into a joint probability of observing the
target vector given the model:}
  \text{\enquote{global accuracy}} &:= \prod^n_{i=1} \pi(\xi, \theta)^{\yi} (1-\pi(\xi, \theta))^{1-\yi}
\intertext{We want a \emph{loss} function, so we actually need the inverse of that:}
  \text{\enquote{global inacccuracy}} &:= \frac{1}{\prod^n_{i=1} \pi(\xi, \theta)^{\yi} (1-\pi(\xi, \theta))^{1-\yi}}
\intertext{Finally, we want the empirical risk to be a \emph{sum} of loss function values, not a \emph{product}}
\text{recall: } \riske &= \sum^n_{i=1} \Lxyi
\intertext{so we turn the product into a sum by taking its log -- the same parameters minimize this, which is all we care about, and we end up with the \textbf{logistic} or \textbf{cross entropy loss function}:}
 \Lxy &= - y \log[\pix] - (1-y) \log [1 - \pix] \\
      & = y \theta^T x - \log[1 + \exp(\theta^T x)]
\end{align*}

\framebreak

Remember that $\log[\pix] = - \log[1 + \exp(-\theta^Tx)]$.
\lz

For $y=0$ and $y=1$ and $f(x) = \theta^Tx$, this yields:
\begin{eqnarray*}
y=0 &:& \log[1 + \exp(\fx] \\
y=1 &:& \log[1 + \exp(-\fx)]
\end{eqnarray*}


If we encode the labels with $\Yspace = \{-1,+1\}$ instead, we can simplify this as:

$$\Lxy = \log[1 + \exp(-\yf] $$

This is called \textbf{Bernoulli loss}.

Logistic regression minimizes this, and we can use these loss
functions for any other discriminative classification model which directly models
$\fx$.

\framebreak

<<bernoulli-loss-plot, fig.height=4, fig.width=6, message=FALSE, echo=FALSE, results = 'none'>>=
n = 100
df = data.frame(fx = rep(seq(from = -4, to  = 4, length.out = n), times =2), y = rep(c(-1, 1), each=n))
df$l = log(1 + exp(- (df$y * df$fx)))
ggplot(df) + geom_line(aes(x = fx, y = l, col = as.factor(y))) +
  scale_x_continuous(expression(f(x)))  + scale_y_continuous(expression(L(y, f(x)))) +
  ggtitle("Bernoulli / Logistic Loss:") + scale_color_viridis_d("", labels = c("y = -1 (or 0)", "y = 1"))
@

\framebreak

In order to minimize the loss (misclassification), we should predict $y=1$ if

$$
\pi(x, \theta) = \P(y = 1 | x, \theta) = \frac{\exp(\theta^T x)}{1+\exp(\theta^Tx)} \ge 0.5,
$$

which is equivalent to
$$
\theta^T x \ge 0 \implies y = 1.
$$

\end{vbframe}

\begin{vbframe}{Estimating Probabilities - Multinomial Regression and Softmax}

<<multi-classification-task-plot, fig.height=4, fig.width=6, message=FALSE, echo=FALSE, results = 'none'>>=
p = plotLearnerPrediction(makeLearner("classif.multinom", trace = FALSE),
                          iris.task, c("Petal.Length", "Petal.Width")) +
  ggtitle("")
p + scale_fill_viridis_d()
@


\framebreak
For a categorical response variable $y \in \gset$ with $g>2$ the model extends to

$$
\pikx = \P(y = k | x) = \frac{\exp(\theta_k^Tx)}{\sum_{j=1}^g\exp(\theta_j^Tx)}.
$$

The latter function is called the \emph{softmax}, and defined on a numerical vector $z$:
$$
s(z)_k = \frac{\exp(z_k)}{\sum_{j}\exp(z_j)}
$$


This is a generalization of the logistic function (check for g=2).\\
It \enquote{squashes} a g-dimensional real-valued vector $z$ to a vector of the same dimension,
with every entry in the range [0, 1] and all entries adding up to 1.

\framebreak

By comparing the posterior probabilities of two categories $k$ and $l$ we end up in a linear function (in $x$),

$$
\log\frac{\pi_k(x)}{\pi_l(x)} = (\theta_k-\theta_l)^Tx.
$$

The class boundaries lie where these (linear) functions are zero, i.e., where the predicted class probabilities are equal.

\textbf{Remark:}
\begin{itemize}
\item $\theta_j$ are vectors here.
\item Well-definedness: $\pi_k(x) \in [0, 1]$ and $\sum_k \pi_k(x) = 1$

\end{itemize}

\framebreak

This approach can be extended in exactly the same fashion for other score based models.
For discrimnating each class $k$ from all others,
we define a binary score model $f_k(x)$ with parameter vector $\theta_k$.
We then combine these models through the softmax function

$$
\postk = \pikx = s_k(f_1(x), \ldots f_g(x))
$$

and optimize all parameter vectors of the $f_k$ jointly.

\framebreak

Further comments:

\begin{itemize}
%\item We can now, e.g., calculate gradients and optimize this with standard numerical optimization software.
\item For linear $\fxt = \theta^T x$, this is also called \emph{softmax regression}. (Note that $x$ can include derived features like polynomials or interactions as well.)
\item One set of parameters is  \enquote{redundant}: If we subtract any fixed vector
  from all $\theta_k$, the predictions do not change. The model is \enquote{overparameterized}, the minimizer of $\risket$ is not unique. Hence, we set $\theta_g = (0, \dots, 0)$ and only optimize the other $\theta_k$, $k=1,\dots,g-1$.\\ (Compare: logistic regression for binary classification also has only \emph{one} parameter vector for discriminating between two classes...).
\item A similar approach can be used for many ML models: multiclass LDA, naive Bayes, neural networks and boosting.

\end{itemize}

\end{vbframe}

\begin{frame}{Logistic and Softmax Regression}
\lz

\textbf{Representation:} Design matrix $X$, coefficients $\theta$.\\

\lz

\textbf{Evaluation:} Logistic/Bernoulli loss function.

\lz

\textbf{Optimization:} Numerical optimization, typically gradient descent based methods.

\end{frame}

% \framebreak
%
% Logistic regression is usually fitted by maximum likelihood.
% \begin{eqnarray*}
% \LLt &=& \prod_{i=1}^n \P(y = y^{(i)} | x^{(i)}, \theta) \\
% % &=& \prod_{i=1}^n (\P(y = 1 | x^{(i)}, \theta)^{y^{(i)}}[1 - \P(y = 1 | x^{(i)}, \theta)]^{1 - y^{(i)}} \\
% &=& \prod_{i=1}^n \pi(x^{(i)}, \theta)^{y^{(i)}} [1 - \pi(x^{(i)}, \theta)]^{1 - y^{(i)}}.
% \end{eqnarray*}
%
% % \framebreak
%
% % Consequently, the log-likelihood is derived by
%
% \small
% \begin{eqnarray*}
% \llt
% &=& \sum_{i=1}^n \yi \log[\pi(\xi, \theta)] + (1-\yi) \log [1 - \pi(\xi, \theta)]\\
% &=& \sum_{i=1}^n \yi \log [\exp(\theta^T \xi)] - \yi \log[1 + \exp(\theta^T \xi)] \\
% &\quad& + \quad (1 - \yi) \log \biggl[1 - \frac{\exp(\theta^T \xi)}{1 + \exp(\theta^T \xi)}\biggr]\\
% &=& \sum_{i=1}^n \yi \theta^T \xi - \log[1 + \exp(\theta^T \xi)]
% \end{eqnarray*}
%
% \framebreak
%
% \normalsize
% We can minimize our loss, by maximizing $\llt$.
% % Therefore, we need to differentiate w.r.t. $\theta$, which gives us the score function:\\
% % $$
% % s(\theta) = \fp{\llt}{\theta} = \sum_{i=1}^n \xi \cdot \left( \yi - \frac{\exp(\theta^T \xi)}{1 + \exp(\theta^T \xi)}\right) = 0
% % $$
%
% % \lz
%
% This now cannot be solved analytically, but is at least concave, so we have to refer to
% numerical optimization, e.g., BFGS.
%
% \lz
%
% In order to minimize the loss (misclassification), we should predict $y=1$, if
%
% $$
% \pi(x, \thetah) = \P(y = 1 | x, \thetah) = \frac{\exp(\thetah^T x)}{1+\exp(\thetah^Tx)} \ge 0.5,
% $$
%
% which is equal to
% $$
% \hat \theta^T x \ge 0.
% $$
%
% So logistic regression gives us a \emph{linear classifier}:
% $$
% \yh = h(\thetah^T x) =
% \begin{cases}
% 1 & \text{ for } x^T\thetah \ge 0 \\
% 0 & \text{ otherwise}
% \end{cases}
% $$
%
%
% \framebreak
%
% <<echo=FALSE>>=
% plotLearnerPrediction("classif.logreg", bin.tsk)
% @
%
% \end{vbframe}
%
%
% \begin{vbframe}{Error distributions and losses}
%
% Let us generalize what we have observed from the comparison between the maximum-likelihood
% and the risk-based approach linear regression.
%
% The maximum-likelihood principle is to maximize
% $$ \LLt = \prod_{i=1}^n \pdfyigxit $$
% or to minimize the neg. log-likelihood:
% $$ -\llt = -\sumin \lpdfyigxit $$
% Now let us define a new loss as:
% $$ \Lxyt = - \lpdfygxt $$
% And consider the empirical risk
% $$\risket = \sumin \Lxyt$$
%
% Then the maximum-likelihood estimator $\thetah$, which we obtain by optimizing $\LLt$ is identical
% to the loss-minimal $\thetah$ we obtain by minimizing $\risket$.
% This implies that we can identify for every error distribution and its pdf $\pdfygxt$ an
% equivalent loss function which leads to the same point estimator for the parameter vector $\theta$.
% We can even disregard multiplicative or additive constants in the loss,
% as they do not change the minimizer.
%
% The other way around does not always work: We cannot identify for every loss function and associated
% pdf for the distribution - the hinge loss is a prominent example.
%
% \framebreak
%
% Let us reconsider the logistic regression maximum likelihood fit. The neg. log-likelihood for the pdf is:
%
% $$
% -\lpdfygxt = - y \log[\pix] - (1-y) \log [1 - \pix]
% $$
%
% This is the cross-entropy loss. Logistic regression minimizes this, and we could use this loss
% for any other model with directly models $\pix$.
%
% \lz
%
% Now lets assume we have a score function $\fx$ instead of $\pix$. We can transform the score to a probability
% via the logistic transformation:
%
% \begin{eqnarray*}
% \pix     &=& \frac{\exp(\fx)}{1 + \exp(\fx)}   =  \frac{1}{1 + \exp(-\fx)}\\
% 1- \pix  &=& \frac{\exp(-\fx)}{1 + \exp(-\fx)} =  \frac{1}{1 + \exp(\fx)}\\
% \end{eqnarray*}
%
% \framebreak
%
% The loss now becomes
% \begin{eqnarray*}
% -\lpdfygxt &=& -y \log[\pix] - (1-y) \log [1 - \pix] \\
%            &=& y \log[1 + \exp(-\fx)] + (1-y) \log[1 + \exp(\fx] \\
% \end{eqnarray*}
% For y=0 and y=1 this is:
% \begin{eqnarray*}
% y=0 &:& \log[1 + \exp(\fx] \\
% y=1 &:& \log[1 + \exp(-\fx)]
% \end{eqnarray*}
% If we would encode now with $\Yspace = \{-1,+1\}$, we can unify this like this:
% $$\Lxy = \log[1 + \exp(-\yf] $$
% So we have recovered the Bernoulli loss. LR minimizes this, and we could use this loss
% for any other model with directly models $\fx$.
%
% \end{vbframe}
%
% \begin{vbframe}{Estimating Probabilities - Multinomial Regression and Softmax}
%
% <<multi-classification-task-plot, fig.height=4, fig.width=6, message=FALSE, echo=FALSE, results = 'none'>>=
%   p = plotLearnerPrediction(makeLearner("classif.multinom", trace=FALSE), iris.task, c("Petal.Length", "Petal.Width")) +
%     ggtitle("")
%     plot(p)
% @
%
%
% \framebreak
% For a categorical response variable $y \in \gset$ with $g>2$ the model extends to
%
% $$
% \pikx = \P(y = k | x) = \frac{\exp(\theta_k^Tx)}{\sum_{j=1}^g\exp(\theta_j^Tx)}.
% $$
%
% The latter function is called the \emph{softmax}, and defined on a numerical vector $z$:
% $$
% s(z)_k = \frac{\exp(z_k)}{\sum_{j}\exp(z_j)}
% $$
%
% It is a generalization of the logistic function (check for g=2). It \enquote{squashes} a g-dim. real-valued vector $z$ to a vector of the same dimension,
% with every entry in the range [0, 1] and all entries adding up to 1.
%
% \framebreak
%
% By comparing the posterior probabilities of two categories $k$ and $l$ we end up in a linear function (in $x$),
%
% $$
% \log\frac{\pi_k(x)}{\pi_l(x)} = (\theta_k-\theta_l)^Tx.
% $$
%
% \textbf{Remark:}
% \begin{itemize}
% \item $\theta_j$ are vectors here.
% \item Well-definedness: $\pi_k(x) \in [0, 1]$ and $\sum_k \pi_k(x) = 1$
%
% \end{itemize}
%
% \framebreak
%
% This approach can be extended in exactly the same fashion for other score based models.
% For each class $k$ we define a binary score model $f_k(x)$ with parameter vector $\theta_k$.
% We then combine these models through the softmax function
%
% $$
% \postk = \pikx = s_k(f_1(x), \ldots f_g(x))
% $$
%
% and optimize all parameter vectors of the $f_k$ jointly. Maximum likelihood:
%
% $$
% \LLt = \prodin \prodkg \pikxt^{[y = k]}
% $$
%
% Negative log-likelihood turns this into empirical risk minimization:
%
% $$
% \risket = \sumin \sumkg 1_{[y = k]} \log s_k(f_1(x | \theta_k), \ldots f_g(x | \theta_g))
% $$
%
% \framebreak
%
% Further comments:
%
% \begin{itemize}
%
% \item We can now, e.g., calculate gradients and optimize this with standard numerical optimization software.
%
% \item For linear $\fxt = \theta^T x$, this is also called \emph{softmax regression}.
%
% \item Has an unusual property that it has a \enquote{redundant} set of parameters. If we subtract a fixed vector
%   from all $\theta_k$, the predictions do not change at all.
%   I.e.,  our model is \enquote{overparameterized}, and for any hypothesis we might fit,
%   there are multiple parameter vectors that give rise to exactly the same hypothesis function.
%   This also implies that the minimizer of $\risket$ above is not unique (but $\risket$ is convex)!
%   Hence, a numerical trick is to set $\theta_g = 0$ and only optimize the other $\theta_k$.
%
% \item A similar approach is used in many ML models: multiclass LDA, naive Bayes, neural networks and boosting.
%
% \end{itemize}
%
% \end{vbframe}
% We ended up maximizing

% $$ \sumin \yi \theta^T \xi - \log[1 + \exp(\theta^T \xi)] $$

% \framebreak

% Regression

% \lz

% \begin{tabular}{ l  l | l l }
%   loss name & loss formula  & pdf name   & pdf formula \\
%   \hline
%   squared   & $(y - f)^2$      & Gaussian & $exp()$       \\
%   absolute  & $|y - f|$        & Laplace  & $...$         \\
% \end{tabular}

% \lz

% Classification

% \lz

% \begin{tabular}{ l  l | l l }
%   loss name & loss formula  & pdf name   & pdf formula \\
%   \hline
%   Bernoulli / cross entropy   & $(y - f)^2$      & Gaussian & $exp()$       \\
% \end{tabular}



% \begin{vbframe}{Entropy}
%
% In some situations we might be interested in alternative definitions of uncertainty of a random variable $x$,
% as in traditional statistics. \emph{Information theory} establishes the so-called \emph{entropy}.
%
% \small
% \textbf{Thought experiment:}
%
% Level of uncertainty of rolling a die should be influenced by three factors:
%   \begin{enumerate}
%   \item The more sides, the harder to predict the outcome.
%
%   $\rightarrow$ Uncertainty grows \textit{monotonically} with number of potential outcomes.
%   \item The relative likelihood of each outcome determines the uncertainty.
%   The result of an \enquote{unfair} die is easier to predict than the result of a fair one.
%
%   $\rightarrow$ The uncertainty depends on the outcome probabilities $\{p_x(1),\ldots,p_x(K)\}$
%   \item The weighted uncertainty of independent events must sum.
%
%   The total uncertainty of rolling two independent dices must equal the sum of the uncertainties for each die alone.
%   For a composite event, its uncertainty should equal the sum of the single uncertainties weighted by the probabilities of their occurrence.
%   \end{enumerate}
%
% \framebreak
%
% \normalsize
% Those postulates lead to a unique mathematical expression for the entropy $H(x)$ for a RV $x$ with outcome probabilities $p_x(k):=\P(x=k)$ for $k \in \Xspace$.
%
% $$
% H(x)=-\sum_{k \in \Xspace} p_x(k) \log_2(p_x(k)) \quad \text{(in bits)}
% $$
% (Actually the base of the log is not uniquely determined)
%
% \lz
%
% More intuitively, the entropy describes how much information is received when we observe a specific value for a random variable $x$.
%
% \lz
%
% The term $-\log_2(p_x(k))$ is called the \emph{information} $I$ an observation of $x=k$ gives us about $x$.
%
% \lz
%
% Base $2$ means the information is measured in bits, but you can use any number $>1$ as base of the logarithm.
%
% \framebreak
%
% \textbf{Example 1:} flipping an (un-)fair coin
%
% \lz
%
% Let $p_x(1) = p$ and $p_x(0) = 1 - p$ .
%
% \lz
%
% The entropy is then given by
% $$
% H(x)= -p \cdot \log_2(p)-(1-p)\cdot \log_2(1-p).
% $$
%
% The entropy (uncertainty) is maximal for fair coin ($p=1/2$).
%
% <<echo=FALSE>>=
% x <- seq(0.1, 0.9, by = 0.01)
% y = - x * log2(x) - (1 - x) * log2(1 - x)
% plot(x, y, type = 'l', xlab = "p", ylab = "H(x)")
% box()
% @
%
% \framebreak
%
% \begin{columns}[T,onlytextwidth]
% \column{0.3\textwidth}
% \textbf{Example 2:} \\
% \lz
% \begin{center}
% <<echo=FALSE, size = "footnotesize">>=
% ex1 <- cbind(class=c("+","+","-","+","-","-", "-"), attr_1 = c(T,T,T,F,F,F,F), attr_2 = c(T,T,F,F,T,T,T))
% kable(ex1)
% @
% \end{center}
% \column{0.65\textwidth}
% \begin{itemize}
% \item How big is the uncertainty/entropy in \textit{class} (in bits)?
% \small
% \begin{eqnarray*}
% H(\text{class}) &=& - \sum_{k=+,\, -} P(k) log_2(P(k)) \\
% &=& - \frac{3}{7} log_2\left(\frac{3}{7}\right)  - \frac{4}{7} log_2\left(\frac{4}{7}\right) \\
% &=& 0.985
% \end{eqnarray*}
% \normalsize
% \item How much can it be reduced by knowing the other attributes?
% \end{itemize}
% \end{columns}
%
% \end{vbframe}
%
% \begin{vbframe}{Conditional entropy}
%
% The \emph{conditional entropy} $H(y|x)$ quantifies the uncertainty of $y$ that remains if the outcome of $x$ is given
% $$
% H(y|x) = \sum_{x \in \Xspace} p_x(x) H(y|x=x) \overset{(*)}{=}H(y|x) = H(y, x) - H(x).
% $$
%
% \textbf{Remark:}
% \begin{itemize}
% \item $H(y|x) = 0$ if (and only if) $y$ is completely determined by $x$
% \item $H(y|x) = H(y)$ if (and only if) $x$ and $y$ are independent
% \end{itemize}
%
% \framebreak
%
% \quad
%
% \vspace{.5cm}
%
% $\quad ^{(*)}$ Proof: \\
% \footnotesize
% \begin{eqnarray*}
% H(y|x) &=& \sum_{x \in \Xspace} p_x(x) H(y|x=x) = -\sum_{x \in \Xspace} p_x(x) \sum_{y \in \Yspace}p(y|x)\log_2 p(y|x) \\
% &=& - \sum_{x \in \Xspace}  \sum_{y \in \Yspace} p_x(x) \frac{p_{xy}(x, y)}{p_x(x)}\log_2 \frac{p_{xy}(x, y)}{p_x(x)}\\
% &=& - \sum_{x \in \Xspace, y \in \Yspace} p_{xy}(x, y)\log_2 \frac{p_{xy}(x, y)}{p_x(x)}\\
% &=& - \sum_{x \in \Xspace, y \in \Yspace} p_{xy}(x, y)\log_2 p_{xy}(x, y)+\sum_{x \in \Xspace, y \in \Yspace} p_{xy}(x, y)\log_2 p_x(x)\\
% &=& H(x, y) + \sum_{x\in \Xspace}p_x(x)\log_2 p_x(x) = H(x, y) - H(x)
% \end{eqnarray*}
%
% \framebreak
% \begin{columns}[T,onlytextwidth]
% \column{0.3\textwidth}
% \textbf{Example:} \\
% \lz
% \begin{center}
% <<echo=FALSE, size = "footnotesize">>=
% kable(ex1)
% @
% \end{center}
% \column{0.65\textwidth}
% \scriptsize
%
% \vspace*{1.5cm}
%
% $H(\text{class}|\text{attr}_1 = T) = - \frac{2}{3} log_2(\frac{2}{3}) - \frac{1}{3} log_2(\frac{1}{3}) = 0.92$ \\
% $H(\text{class}|\text{attr}_1 = F) = - \frac{1}{4} log_2(\frac{1}{4}) - \frac{3}{4} log_2(\frac{3}{4}) = 0.81$ \\
% $H(\text{class}|\text{attr}_2 = T) = - \frac{2}{5} log_2(\frac{2}{5}) - \frac{3}{5} log_2(\frac{3}{5}) = 0.97$ \\
% $H(\text{class}|\text{attr}_2 = F) = - \frac{1}{2} log_2(\frac{1}{2}) - \frac{1}{2} log_2(\frac{1}{2}) = 1$ \\
% \lz
% $H(\text{class}|\text{attr}_1) = \frac{3}{7} 0.92 + \frac{4}{7} 0.81 = 0.86$ \\
% $H(\text{class}|\text{attr}_2) = \frac{5}{7} 0.97 + \frac{2}{7} 1 = 0.98$
%
% \normalsize
%
% \end{columns}
%
% \end{vbframe}
%
%
% \begin{vbframe}{Mutual information}
%
% The reduction of entropy after \textit{learning} $x$ is called \emph{mutual information}
%
% $$
% I(y;x) := H(y) - H(y|x) = H(y) + H(x) - H(x, y).
% $$
%
% \textbf{Remark:}
% \begin{itemize}
% \item The mutual information is symmetric, i. e. $I(y;x) = I(x;y)$.
% \item It describes the amount of information about one RV obtained through the other one (\emph{information gain}).
% \end{itemize}
%
% \begin{figure}
%  \includegraphics{figure_man/mutualinformation.pdf}
% \end{figure}
%
% \framebreak
%
% \begin{columns}[T,onlytextwidth]
% \column{0.3\textwidth}
% \begin{center}
% <<echo=FALSE, size = "footnotesize">>=
% kable(ex1)
% @
% \end{center}
% \column{0.65\textwidth}
% \begin{itemize}
% \item For our example we can compute the information gains:
% \footnotesize
% \begin{eqnarray*}
% I(\text{class}; \text{attr}_1) &=& H(\text{class}) - H(\text{class}|\text{attr}_1) \\
% &=& 0.985 - 0.86 = 0.125
% \end{eqnarray*}
%
% \begin{eqnarray*}
% I(\text{class}; \text{attr}_2) &=& H(\text{class}) - H(\text{class}|\text{attr}_2) \\
% &=& 0.985 - 0.98 = 0.005
% \end{eqnarray*}
% \normalsize
% \lz
% \item $\text{attr}_1$ tells us more about $\text{class}$.
% \end{itemize}
%
% \end{columns}
%
% \end{vbframe}
%
% \begin{vbframe}{Kullback-Leibler-Divergence}
%
% Suppose that data is being generated from an unknown distribution $p(x)$. Suppose we modeled $p(x)$ using an approximating distribution $q(x)$. A "good" approximation $q(x)$ should minimize the difference to $p(x)$.
%
% \lz
%
% A measure for the difference between $p$ and $q$ is the \emph{Kullback-Leibler-Divergence}
% \begin{eqnarray*}
% D_{KL}(p||q) := & \int_{-\infty}^{\infty} p(z) \cdot \log \frac{p(z)}{q(z)} dz &\quad \text{continuous case} \\
% D_{KL}(p||q) := & \sum_{k} p(k) \cdot \log \frac{p(k)}{q(k)} &\quad \text{discrete case}
% \end{eqnarray*}
%
% \textbf{Remark:}
% \begin{itemize}
% \item In general: $D_{KL}(p||q) \ne D_{KL}(p||q)$ (no symmetry)
% \item $D_{KL}$ is often called the \emph{information gain} achieved if $q$ is used instead of $p$.
% \end{itemize}
%
% \end{vbframe}
%
%
% \begin{vbframe}{Mutual information \& Kullback-Leibler-Divergence}
%
% $I(x;y)$ is the \emph{information gain} achieved if the product of marginal distributions $p_x(x) p_y(y)$ is used instead of the joint distribution $p_{xy}(x,y)$:
%
% \begin{eqnarray*}
% I(x;y) &=& D_{KL}(p_{xy}||p_x p_y) = \sum_{x \in \Xspace} \sum_{y \in \Yspace} p_{xy}(x, y) \cdot \log \biggl(\frac{p_{xy}(x, y)}{p_x(x)p_y(y)}\biggr)
% \end{eqnarray*}
%
% \framebreak
%
% \footnotesize
% Proof:
%
% \begin{eqnarray*}
% I(x;y) &=& H(y) + H(x) - H(x, y)\\
% &=& -\sum_{y \in \Yspace} p_y(y) \log_2(p_y(y)) -\sum_{x \in \Xspace} p_x(x) \log_2(p_x(x)) \\
% && -\sum_{x \in \Xspace, y \in \Yspace} p_{xy}(x, y) \log_2(p_{xy}(x, y))\\
% &=& -\sum_{x \in \Xspace, y \in \Yspace}p_{xy}(x, y) \log_2(p_y(y)) -\sum_{x \in \Xspace, y \in \Yspace} p_{xy}(x, y) \log_2(p_x(x)) \\
% && \quad+ \sum_{x \in \Xspace, y \in \Yspace} p_{xy}(x, y) \log_2(p_{xy}(x, y)) \\
% &=& \sum_{x \in \Xspace} \sum_{y \in \Yspace} p_{xy}(x, y) \cdot \log \biggl(\frac{p_{xy}(x, y)}{p_x(x)p_y(y)}\biggr) = D_{KL}(p_{xy}||p_x p_y)
% \end{eqnarray*}
%
%
% \end{vbframe}

% \begin{vbframe}{Cross-Entropy}

% A related measure to compare two probability measures $p$ and $q$ is the \emph{cross entropy} (deviance):
% $$
% H(p, q) = - \sum_{x} p(x) \log q(x)
% $$

% The cross entropy defines a loss function.

% Remember the log-Likelihood function derived in the example of logistic regression with binary response $y\in \{0, 1\}$.

% \small
% \begin{eqnarray*}
% -\llt &=&   -\sum_{i=1}^n y^{(i)} \log[\P(y = 1 | x^{(i)} , \theta)] + (1-y^{(i)}) \log [1 - \P(y = 1 | x^{(i)} , \theta)]\\
% &=& \sum_{i=1}^n H(p_i, q_i)
% \end{eqnarray*}

% \normalsize
% with $p_i := y^{(i)}$ are our true labels and $q_i:=\P(y = 1 | x^{(i)} , \theta)$ the posterior probabilites.

% \end{vbframe}

% old slides about entropy and mutual information

%\begin{vbframe}{Entropy}
% Entnommen aus A Light Discussion and Derivation of Entropy, Jonathon Shlens, Google Research 09.04.14 in Appendix auch Beweis enthalten
%\begin{itemize}
%\item The uncertainty $H$ of a discrete random %variable $X$ is the entropy.

%(\textit{A Light Discussion and Derivation of Entropy}, Jonathon Shlens, Google Research, \url{http://arxiv.org/pdf/1404.1998.pdf})
%\item Thought experiment: The level of uncertainty of rolling a die should intuitively be influenced by three factors:
%  \begin{enumerate}
%  \item The more sides a die has, the harder to predict the outcome, the greater the uncertainty.

%  $\rightarrow$ Uncertainty/entropy grows \textit{monotonically} with the numbers of potential outcomes.
%  \item The relative likelihood of each outcome determines the uncertainty.
%  The result of an \enquote{unfair} die is easier to predict than the result of a fair one.

%  $\rightarrow$ $H$ can be written as a function of the outcome probabilities $\{P_1,\ldots,P_K\}$
%  \item The weighted uncertainty of independent events must sum.

%  The total uncertainty of rolling two independent dice must equal the sum of the uncertainties for each die alone.
%  For a composite event, its uncertainty should equal the sum of the single uncertainties weighted by the probabilities of their occurrence.
%  \end{enumerate}
% \item The first postulate gives us that $\frac{\partial H}{\partial K} > 0$, since the uncertainty grows monotonically with the number of outcomes.
% To be positive, the derivative must exist, thus $H$ is assumed to be \textit{continuous}.
% \item Those postulates lead to a unique mathematical expression for the entropy $H$ of a discrete random variable $X$ with prob. mass distr. $P(X)$:
% $$
% H(X)=-\sum_{k=1}^g P_k log_2(P_k)
% $$
% (Shannon and Weaver, The mathematical theory of communication, 1949)

% \item The term $-log_2(P_k)$ is called \textit{information} $I$ and is the information that an observation of $x=k$ gives us about $X$.
% \item Observations of rare events give more information (sometimes also called \textit{surprise}) about the random variable $X$.
% \item In general the base of the logarithm can be any fixed real number greater than 1, for binary logarithm the unit of the information is \textit{bit}.
% \item The entropy $H(X)=-\sum_{k=1}^g P_k log_2(P_k)$ equals the average (or expected) amount of information obtained by observing $x$ (in bits) $\E(I(X))$.
% \end{itemize}

% \framebreak


% \begin{vbframe}{Conditional entropy}
% \begin{itemize}
% \item $H(Y|X)$ is the remaining entropy of $Y$ given $X$ or the expected entropy of $P(Y|X)$
% $$ H(Y|X) = -\sum_x P(X = x) H(Y|X=x) $$

% \item $H(Y|X=x)$ is the entropy of $Y$ for all observations with a specific value of (rule regarding) feature $X$
% \end{itemize}

% \end{vbframe}

% \begin{vbframe}{Mutual Information}
% \begin{itemize}
% \item Entropy of a discrete RV $H(Y)$: a measure of uncertainty about $Y$
% \item Conditional entropy $H(Y|X)$: remaining entropy of $Y$ knowing $X$
% \item After \textit{learning} $X$ it is easy to define the reduction of entropy.
% \item The \textit{mutual information} between $Y$ and $X$ is
% $$
% I(Y;X) = H(Y) - H(Y|X) \; (= H(X) - H(X|Y))
% $$
% \item Describes the amount of information obtained about one random variable, through the other.
% \item Can also be derived as the Kullback-Leibler divergence of the product of the two marginal distributions from their joint distribution.
% \item In tree learning $I(Y;X)$ is also called \textit{information gain}.
% \end{itemize}
% \framebreak

% \end{vbframe}
\endlecture
