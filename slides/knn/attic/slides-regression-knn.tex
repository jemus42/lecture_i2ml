\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R



\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}
\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
  %Define standard arrow tip
  >=stealth',
  %Define style for boxes
  punkt/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=6.5em,
    minimum height=2em,
    text centered},
  % Define arrow style
  pil/.style={
    ->,
    thick,
    shorten <=2pt,
    shorten >=2pt,}
}
\usepackage{subfig}


% Defines macros and environments
\input{../../style/common.tex}
% \input{common.tex}

%\usetheme{lmu-lecture}
\newcommand{\titlefigure}{figure/reg_knn_0}
\newcommand{\learninggoals}{
\item Understand the basic idea of k-NN
\item Know different distance measures for different scales of feature variables
\item Understand that k-NN has no optimization step}
\usepackage{../../style/lmu-lecture}


\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}



\begin{document}
% Set style/preamble.Rnw as parent.

% Load all R packages and set up knitr

% This file loads R packages, configures knitr options and sets preamble.Rnw as parent file
% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...








% Defines macros and environments
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

%! includes: basics-supervised, basics-learnercomponents-hro, regression-losses

\lecturechapter{$k$-Nearest Neighbors Regression}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{Nearest Neighbors: Intuition}

\begin{itemize}
 \item Say we know locations of cities in 2 different countries.
 \item Say we know which city is in which country.
 \item Say we don't know where the countries' border is.
 \item For a given location, we want to figure out which country it belongs to.
 \item Nearest neighbor rule: every location belongs to the same country as the closest city.
 \item $k$-nearest neighbor rule: vote over the $k$ closest cities (smoother)
\end{itemize}
%\begin{center}
%  \includegraphics[width=.5\textwidth]{figure_man/nn.png}
%\end{center}
\end{vbframe}



\begin{vbframe}{$k$-Nearest-Neighbors}

\begin{itemize}
\item $k$-\textbf{NN} can be used for regression and classification
\item It generates predictions $\yh$ for a given $\xv$ by comparing the $k$ observations that are closest to $\xv$
\item "Closeness" requires a distance or similarity measure (usually: Euclidean).
\item The set containing the $k$ closest points $\xi$ to $\xv$ in the training data is called  the $k$-\textbf{neighborhood} $N_k(\xv)$ of $\xv$.
\end{itemize}


\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/reg_knn_1} 

}



\end{knitrout}

\end{vbframe}

\begin{vbframe}{Distance Measures}

\textbf{How to calculate distances?}
  \begin{itemize}
    \item Most popular distance measure for numerical features: \textbf{Euclidean distance}
    \item Imagine two data points $\xv = (x_1, ..., x_p)$ and $\tilde{\xv} = (\tilde{x}_1, ..., \tilde{x}_p)$ with $p$ features $\in \R$
    \item The Euclidean distance:
    \begin{equation*}
      d_{Euclidean}\left(\xv, \tilde{\xv}\right) = \sqrt{\sumjp(x_j- \tilde{x}_j)^2}
    \end{equation*}
    %\item It is based on a special case of the $L_p$-norm: $||x||_p = (|x_1|^p + ... + |x_q|^p)^\frac{1}{p}$ with $p = 2$
\framebreak

\item Example:
\begin{itemize}
  \item Three data points with two metric features each: $a = (1, 3),
  b = (4, 5)$ and $c = (7, 8)$
  \item Which is the nearest neighbor of $b$ in terms of the Euclidean distance?
  \item $d(b, a) = \sqrt{(4 - 1)^2 + (5 - 3)^2} = 3.61$
  \item $d(b, c) = \sqrt{(4 - 7)^2 + (5 - 8)^2} = 4.24$
  \item $\Rightarrow a$ is the nearest neighbor for $b$.
\end{itemize}
    \item Alternative distance measures are:
    \begin{itemize}
      \item Manhattan distance% based on the $L_1$-norm:
        \begin{equation*}
          d_{manhattan}(\xv, \tilde{\xv}) = \sumjp |x_j - \tilde{x}_j|
        \end{equation*}
      \item Mahalanobis distance (takes covariances in $\Xspace$ into account)
    \end{itemize}
  \end{itemize}

\framebreak

Comparison between Euclidean and Manhattan distance measures:\\
\vskip 1em
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.955\textwidth,height=.75\textheight]{figure/reg_knn_2} 

}



\end{knitrout}

\framebreak

\textbf{Categorical variables, missing data and mixed space:}

The Gower distance \(d_{gower}(\xv, \tilde{\xv})\) is a weighted mean of \(d_{gower}(x_j,\tilde{x}_j)\):

$$d_{gower}(\xv, \tilde{\xv}) = \frac{\sumjp \delta_{x_j,\tilde{x}_j} \cdot d_{gower}(x_j,\tilde{x}_j)}{
\sumjp \delta_{x_j,\tilde{x}_j}}.
$$

\begin{itemize}

  \item \(\delta_{x_j,\tilde{x}_j}\) is 0 or 1. It becomes 0
  when the $j$-th variable is \textbf{\textit{missing}} in at least one of the observations ($\xv$ or $\tilde{\xv}$),
  or when the variable is asymmetric binary (where \enquote{1} is more
  important/distinctive than \enquote{0}, e.~g., \enquote{1} means \enquote{color-blind}) and both values are zero. Otherwise it is 1.
  \item \(d_{gower}(x_j,\tilde{x}_j)\), the $j$-th variable contribution to the
  total distance, is a distance between the values of $x_j$ and $\tilde{x}_j$.
  For nominal variables the distance is 0 if both values are equal and 1 otherwise.
  The contribution of other variables is the absolute difference of
  both values, divided by the total range of that variable.

\end{itemize}

\framebreak

Example of Gower distance with data on sex and income:

\begin{columns}[T]
  \begin{column}{0.4\textwidth}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{table}[H]
\centering\begingroup\fontsize{10}{12}\selectfont

\begin{tabular}{>{\leavevmode\color{black}}l|>{\leavevmode\color{black}}l|>{\leavevmode\color{black}}r}
\hline
index & sex & salary\\
\hline
1 & m & 2340\\
\hline
2 & w & 2100\\
\hline
3 & NA & 2680\\
\hline
\end{tabular}
\endgroup{}
\end{table}


\end{knitrout}
  \end{column}
  \begin{column}{0.5\textwidth}
    \vspace{0.6cm}
    \fbox{
      $d_{gower}(\xv,\tilde{\xv}) = \frac{\sumjp \delta_{x_j,\tilde{x}_j} \cdot d_{gower}(x_j,\tilde{x}_j)}{
      \sumjp \delta_{x_j,\tilde{x}_j}}
      $}
  \end{column}
\end{columns}

\vfill

$d_{gower}(\xv^{(1)},\xv^{(2)}) = \frac{ 1 \cdot 1 + 1 \cdot \frac{\left| 2340 - 2100 \right|}
{\left| 2680 - 2100 \right|}}{1 + 1} = \frac{1 + \frac{240}{580}}{2} = \frac{1 + 0.414}{2} = 0.707
$

\vfill

$d_{gower}(\xv^{(1)},\xv^{(3)}) = \frac{ 0 \cdot 1 + 1 \cdot \frac{\left| 2340 - 2680 \right|}
{\left| 2680 - 2100 \right|}}{0 + 1} = \frac{0 + \frac{340}{580}}{1} = \frac{0 + 0.586}{1} = 0.586
$

\vfill

$d_{gower}(\xv^{(2)},\xv^{(3)}) = \frac{0 \cdot 1 + 1 \cdot \frac{\left| 2100 - 2680 \right|}
{\left| 2680 - 2100 \right|}}{0 + 1} = \frac{0 + \frac{580}{580}}{1} = \frac{0 + 1.000}{1} = 1
$

\vfill

%For categorical variables distances are often not easily comparible
%(Is male closer to female than 2340\$ to 2100\$?). Hence, adding weights for variables
%can be especially useful here.
\framebreak

\textbf{Weights:}
\vfill

Weights can be used to address two problems in distance calculation:

\begin{itemize}
  \item \textbf{Standardization:} Two features may have values with a different scale. Many distance formulas (not Gower) would place a higher importance on a feature with higher values, leading to an imbalance. Assigning a higher weight to the lower-valued feature can combat this effect.
  \item \textbf{Importance:} Sometimes one feature has a higher importance (e.~g., more recent measurement). Assigning weights according to the importance of the feature can align the distance measure with known feature importance.
\end{itemize}

\begin{columns}[T]
  \begin{column}{0.2\textwidth}
    For example:
  \end{column}
  \begin{column}{0.8\textwidth}
     $d^{weighted}_{Euclidean}\left(\xv, \tilde{\xv}\right) = \sqrt{\sumjp w_{j}(x_j- \tilde{x}_j)^2}$
  \end{column}
\end{columns}

\vfill

\end{vbframe}

\begin{frame}{$k$-NN Regression}

Predictions for regression:
\begin{align*}
\yh &= \frac{1}{k} \sum_{i: \xi \in N_k(\xv)} \yi\\
\yh &= \frac{1}{\sum_{i: \xi \in N_k(\xv)} w_i} \sum_{i: \xi \in N_k(\xv)} w_i \yi
\end{align*}
with neighbors weighted according to their distance to $\xv$: $w_i = \frac{1}{d(\xi, \xv)}$

\end{frame}

\begin{vbframe}{$k$-NN Summary}

\begin{itemize}
\item $k$-NN has no optimization step and is a very local model.
\item We cannot simply use least-squares loss on the training data for picking $k$,
  because we would always pick $k=1$.
\item $k$-NN makes no assumptions about the underlying data distribution.
\item The smaller $k$, the less stable, less smooth and more \enquote{wiggly} the decision
  boundary becomes.
\item Accuracy of $k$-NN can be severely degraded by the presence of noisy or irrelevant features,
  or when the feature scales are not consistent with their importance.
%\item In binary classification, we might choose an odd k to avoid ties.
\end{itemize}


\framebreak

\lz

\textbf{Hypothesis Space:} Step functions over tesselations of $\Xspace$.\\
Hyperparameters: distance measure $d(\cdot,\cdot)$ on $\Xspace$; size of neighborhood $k$.

\lz

\textbf{Risk:} Use any loss function for regression or classification.

\lz

\textbf{Optimization:} Not applicable/necessary.\\ But: clever look-up methods \& data structures to avoid computing all $n$ distances for generating predictions.

\end{vbframe}

\endlecture
\end{document}
