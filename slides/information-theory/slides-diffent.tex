\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure_man/diffent-quant.png}
\newcommand{\learninggoals}{
  \item \textcolor{blue}{XXX}
  \item \textcolor{blue}{XXX}
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Differential Entropy}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{Differential Entropy} 
  \begin{itemize}
    \item For a continuous random variable $X$ with density function $f(x)$ and support $\Xspace$, the analogue of entropy is \textbf{differential entropy}:
    $$ h(X) := h(f) := - \int_{\Xspace} f(x) \log(f(x)) dx $$
    \item The base of the log is again somewhat arbitrary, and we could either use 2 (and measure in bits) or e (to measure in nats).
    \item The integral above does not necessarily exist for all densities.
    \item Differential entropy lacks some properties of discrete entropy.
    \item $h(X) < 0$ is possible because $f(x) > 1$ is possible.
    \end{itemize}
\end{vbframe}

\begin{vbframe}{Diff. Entropy of Uniform Distribution}
Let $X$ be a uniform random variable on $[0, a]$.
  \begin{equation*}
    \begin{aligned} 
      h(X) &= - \int_0^a f(x) \log(f(x)) dx \\
           &= - \int_0^a \frac{1}{a} \log\left(\frac{1}{a}\right) dx = \log(a) 
    \end{aligned}
  \end{equation*}
  \begin{itemize}
    \item For $a < 1$, $h(X) < 0$.
    \end{itemize}
\end{vbframe}


\begin{vbframe}{Diff. Entropy of Gaussian}
Let $X$ be a Gaussian random variable $X \sim \normal(\mu, \sigma^2)$ and let us measure in nats:
% The differential entropy is then given by
  \begin{equation*}
    \begin{aligned} 
     h(X) &= - \int_{\R} f(x) \ln(f(x)) dx \\
          &=  - \int_{\R} f(x) \ln\left(\frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}} \right) dx \\
          &= - \int_{\R} f(x) \ln\left(\frac{1}{\sqrt{2 \pi \sigma^{2}}}\right) dx + \int_{\R} f(x) \frac{(x-\mu)^{2}}{2 \sigma^{2}} dx \\
          &= - \ln \left(\frac{1}{\sqrt{2 \pi \sigma^{2}}}\right) \underbrace{\int_{\R} f(x) dx}_{= 1} + \frac{1}{2\sigma^2} \underbrace{\int_{\R} f(x) (x-\mu)^{2}\,dx}_{=: \sigma^2} \\
          &= \frac{1}{2} \ln \left(2 \pi \sigma^{2}\right) + \frac{1}{2} =  \ln(\sigma \sqrt{2\pi e})
    \end{aligned}
  \end{equation*}
\framebreak

  \begin{itemize}
    \item $h(X)$ is not a function of $\mu$ (see translation invariance).
    \item As $\sigma^2$ increases, the differential entropy also increases.
    \item For $\sigma^2 < \frac{1}{2\pi e}$, it is negative.
    
  \end{itemize}
\end{vbframe}


\begin{vbframe}{Diff. Entropy vs. Discrete}
It is not so simple as to characterize $h(X)$ as 
a straightforward generalization of $H(X)$ of a limiting process.
Consider the quantized random variable $X^\Delta$, which is defined by
$$X^\Delta = x_i \qquad \text{ if } \qquad i \Delta \leq X < (i + 1) \Delta$$  
\begin{center}
\includegraphics[width = 7cm ]{figure_man/diffent-quant.png} \\
\end{center}

If the density $f(x)$ of the random variable $X$ is Riemann-integrable, then
$$H(X^\Delta) + \log(\Delta) \rightarrow h(X) \text{ as } \Delta \rightarrow 0.$$
Thus, the entropy of an n-bit quantization of a continuous random variable 
$X$ is approximately $h(X) + n$.
\end{vbframe}

\begin{vbframe}{Joint Differential Entropy} 
\begin{itemize}
\item For a continuous random vector $X$ with density function $f(x)$ and support $\Xspace$, 
  differential entropy is also defined as:
$$ h(X) = h(X_1, \ldots, X_n) = h(f) = - \int_{\Xspace} f(x) \log(f(x)) dx $$
\item Hence this also defines the joint differential entropy for a set of continuous RVs.
\end{itemize}

\lz
  
Entropy of a multivariate normal distribution: 
If $X \sim N(\mu, \Sigma)$ is multivariate Gaussian, then
  $$h(X) = \frac{1}{2} \ln(2 \pi e)^n |\Sigma| \qquad \text{(nats)}$$
\end{vbframe}

\begin{vbframe}{Properties of Differential Entropy} 
\begin{enumerate}
  \item $h(f)$ can be negative.
  \item $h(f)$ is additive for independent RVs.
  \item $h(f)$ is maximized by the multivariate normal, if we restrict 
    to all distributions with the same (co)variance, so
    $h(X) \leq \frac{1}{2} \ln(2 \pi e)^n |\Sigma|.$
    % We postpone the proof to a later chapter, as it is based on Kullback-Leibler divergence.
    % $H(X) \leq -g\frac{1}{g} \log_2(\frac{1}{g}) = log_2(g)$.
\item Translation-invariant, $ h(X+a) = h(X)$. 
\item $h(aX) = h(X) + \log |a|$.
\item $h(AX) = h(X) + \log |A|$ for random vectors and matrix A.
\end{enumerate}
\end{vbframe}

\endlecture
\end{document}
