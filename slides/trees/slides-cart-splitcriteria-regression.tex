\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-trees.tex}

\newcommand{\titlefigure}{figure/splitcrit_optimal-constant.pdf}
\newcommand{\learninggoals}{
\item Know definition of a tree's node
\item Understand how split points are derived via empirical risk minimization
\item Learn how we perform ERM for regression tasks}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}
\lecturechapter{CART: Splitting Criteria for Regression}
\lecture{Introduction to Machine Learning}
\sloppy


\begin{frame}{Splitting criteria}

 \begin{figure}
    \centering
      % FIGURE SOURCE: No source
      \includegraphics[height = 5.0cm, keepaspectratio]{figure/cart_intro_annotated-tree.pdf}
    \end{figure}

How to find good splitting rules to define the tree?
\lz

$\implies$ \textbf{Empirical Risk Minimization}

\end{frame}

\begin{vbframe}{Splitting criteria: Motivation}
Idea: A split is good if each leaf's point predictor is good itself. \\

For each leaf $\Np$ we pick the optimal constant for a given loss function, e.g. the mean $c = \frac{1}{|\Np|}\sum\limits_{(\xv, y) \in \Np}$ for the L2 loss $\risk(\Np) = \sum\limits_{(\xv, y) \in \Np} (y - c)^2$

\begin{figure}
\includegraphics[width=0.45\textwidth]{figure/splitcrit_optimal-constant.pdf} 
\end{figure}



\end{vbframe}



\begin{vbframe}{Splitting criteria: Excurse}
When splitting the data into leaves, we can use the idea of the "optimal constant model" in each leaf. E.g., consider these two splits:

\begin{columns}
\begin{column}{0.5\textwidth}

\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

\begin{figure}
\includegraphics[width=0.9\textwidth]{figure/splitcrit_optimal-constant-sub1.pdf} 
\end{figure}

 
\end{column}
\begin{column}{0.5\textwidth}

\begin{figure}
\includegraphics[width=0.9\textwidth]{figure/splitcrit_optimal-constant-sub2.pdf} 
\end{figure}

\end{column}
\end{columns}

Each prediction is optimal with respect to its leaves. But which split is better?

\end{vbframe}

\begin{vbframe}{Splitting criteria: Heuristics}

\begin{columns}
\begin{column}{0.5\textwidth}

%\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

\begin{figure}
\includegraphics[width=0.7\textwidth]{figure/splitcrit_optimal-constant-sub1.pdf} 
\end{figure}

$\risk(\Np_1) = 23.4$, $\risk(\Np_2) = 72.4$ 
 
\end{column}
\begin{column}{0.5\textwidth}

\begin{figure}
\includegraphics[width=0.7\textwidth]{figure/splitcrit_optimal-constant-sub2.pdf} 
\end{figure}

$\risk(\Np_1) = 78.1$, $\risk(\Np_2) = 46.1$

\end{column}
\end{columns}
\vspace{0.1in}

The total sum of individual risks, or sum of squared errors (SSE), is:
\vspace{0.1in}

\begin{columns}
\begin{column}{0.5\textwidth}
$23.4 + 72.4 = 95.8$
\end{column}

\begin{column}{0.5\textwidth}
$78.0 + 46.1 = 124.1$ 
\end{column}
\end{columns}

\vspace{0.1in}
Based on the SSE, we would prefer the first split.

\end{vbframe}

\begin{vbframe}{Splitting criteria: Heuristics}

Now, we want to find the split with the lowest SSE out of all possible ones.
As a second-best solution, we search the domain of $x_j$ to do so.
We start with the instances below:

\begin{figure}
\includegraphics[width=0.75\textwidth]{figure/splitcrit_optimal-constant-grid.pdf} 
\end{figure}


\end{vbframe}

\begin{vbframe}{Splitting criteria: Heuristics}

If we extend this grid (quantile-wise or to every single possible split), we can choose the split with the lowest risk. 
Here we investigated 40 distinct splits:

\begin{figure}
\includegraphics[width=0.68\textwidth]{figure/splitcrit_optimal-constant-grid2.pdf} 
\end{figure}


\end{vbframe}

\begin{vbframe}{Splitting criteria: Formalization}

\begin{itemize}
\item $\Np \subseteq \D$ is the data that is assigned to a terminal node $\Np$ of a tree.
\item Let $c$ be the predicted constant value for the data assigned to $\Np$: $\yh \equiv c$ for all $\left(\xv, y\right) \in \Np$.
\item Then, the risk $\risk(\Np)$ for a leaf is the sum of the individual losses for the data assigned to that leaf under a given loss function $L$:
  $$\risk(\Np) = \sum\limits_{(\xv, y) \in \Np} L(y, c)$$
\item The prediction is given by the optimal constant $c = \argmin_c \risk(\Np)$
\item A binary split results in the two regions $\Nl$ and $\Nr$
\end{itemize}

\framebreak

\begin{itemize}
\item A split w.r.t. \textbf{feature $x_j$ at split point $t$} divides a parent node $\Np$ into 
  \begin{align*}
    \Nl &= \{ (\xv, y) \in \Np: x_j \leq t \} \text{ and } \Nr = \{ (\xv, y) \in \Np: x_j > t \}.
  \end{align*}
\item   
  In order to evaluate how good a split is, we compute the empirical risks
  in both child nodes and sum them up.
     \begin{align*}
      \risk(\Np, j, t) &=  \risk(\Nl) +  \risk(\Nr) \\
                  &= \left(\sum\limits_{(\xv,y) \in \Nl} L(y, c_1) + \sum\limits_{(\xv,y) \in \Nr} L(y, c_2)\right)
      \end{align*}
  \item Finding the best way to split $\Np$ into $\Nl, \Nr$ means solving
  $$\argmin_{j, t} \risk(\Np, j, t)$$
\end{itemize}

\framebreak
\begin{itemize}

\item In many cases, you will see that ERM is carried out with respect to the mean empirical risk
\item While optimizing the mean vs. the sum ERM is equivalent, when computing means, we need to account for the number of each observation in each leaf
\item Then, we reweight the losses w.r.t. the number of observations in each node:
     \begin{align*}
      \risk(\Np, j, t) &= \frac{|\Nl|}{|\Np|} \risk(\Nl) + \frac{|\Nr|}{|\Np|} \risk(\Nr) \\
                  &= \frac{1}{|\Np|}\left(\sum\limits_{(\xv,y) \in \Nl} L(y, c_1) + \sum\limits_{(\xv,y) \in \Nr} L(y, c_2)\right)
      \end{align*}

\end{itemize}
\end{vbframe}



\endlecture
\end{document}
