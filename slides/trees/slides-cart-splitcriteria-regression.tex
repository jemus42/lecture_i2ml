\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-trees.tex}

\newcommand{\titlefigure}{figure/split_point.pdf}
\newcommand{\learninggoals}{
\item Know definition of a tree's node
\item Understand how split points are derived via empirical risk minimization
\item Learn how we perform ERM for regression tasks}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}
\lecturechapter{CART: Splitting Criteria}
\lecture{Introduction to Machine Learning}
\sloppy


\begin{frame}{Splitting criteria}

 \begin{figure}
    \centering
      % FIGURE SOURCE: No source
      \includegraphics[height = 5.0cm, keepaspectratio]{figure/cart_intro_annotated-tree.pdf}
    \end{figure}

How to find good splitting rules to define the tree?
\lz

$\implies$ \textbf{empirical risk minimization}

\end{frame}

\begin{vbframe}{Splitting criteria: Motivation}
Idea: A split is good if each leaf's point predictor is good itself. \\

When minimizing risk this means we select the point predictor with lowest risk per leaf.
This predictor depends on the risk function.
For example, for regression and L2 loss, the mean in each leaf is optimal.
These are proofable properties covered in the Graduate level lecture.

\begin{figure}
\includegraphics[width=0.45\textwidth]{figure/splitcrit_optimal-constant.pdf} 
\end{figure}



\end{vbframe}



\begin{vbframe}{Splitting criteria: Excurse}
When splitting the data into leaves, we can use the idea of the "optimal constant model" in each leaf. E.g. consider these two splits:

\begin{columns}
\begin{column}{0.5\textwidth}

\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

\begin{figure}
\includegraphics[width=0.9\textwidth]{figure/splitcrit_optimal-constant-sub1.pdf} 
\end{figure}

 
\end{column}
\begin{column}{0.5\textwidth}

\begin{figure}
\includegraphics[width=0.9\textwidth]{figure/splitcrit_optimal-constant-sub2.pdf} 
\end{figure}

\end{column}
\end{columns}

Each prediction is optimal with respect to its leaves. But which split is better?

\end{vbframe}

\begin{vbframe}{Splitting criteria: Heuristics}

\begin{columns}
\begin{column}{0.5\textwidth}

%\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

\begin{figure}
\includegraphics[width=0.7\textwidth]{figure/splitcrit_optimal-constant-sub1.pdf} 
\end{figure}

$\risk(\Np_1) = 23.4$, $\risk(\Np_2) = 72.4$ 
 
\end{column}
\begin{column}{0.5\textwidth}

\begin{figure}
\includegraphics[width=0.7\textwidth]{figure/splitcrit_optimal-constant-sub2.pdf} 
\end{figure}

$\risk(\Np_1) = 78.1$, $\risk(\Np_2) = 46.1$

\end{column}
\end{columns}
\vspace{0.1in}

The total sum of individual risks, or sum of squared errors (SSE), is:
\vspace{0.1in}

\begin{columns}
\begin{column}{0.5\textwidth}
$23.4 + 72.4 = 95.8$
\end{column}

\begin{column}{0.5\textwidth}
$78.0 + 46.1 = 124.1$ 
\end{column}
\end{columns}

\end{vbframe}

\begin{vbframe}{Splitting criteria: Heuristics}

Based on the SSE, we would prefer the first split.
If we want to find the split with the lowest SSE, we need to search the domain of $x_j$.

\begin{figure}
\includegraphics[width=0.75\textwidth]{figure/splitcrit_optimal-constant-grid.pdf} 
\end{figure}


\end{vbframe}

\begin{vbframe}{Splitting criteria: Heuristics}

If we extend this grid (quantile-wise or to every single possible split), we can choose the split with the lowest risk:

\begin{figure}
\includegraphics[width=0.75\textwidth]{figure/splitcrit_optimal-constant-grid2.pdf} 
\end{figure}


\end{vbframe}

\begin{vbframe}{Splitting criteria: Formalization}

\begin{itemize}
\item $\Np \subseteq \D$ is the data that is assigned to a terminal node $\Np$ of a tree.
\item Let $c$ be the predicted constant value for the data assigned to $\Np$: $\yh \equiv c$ for all $\left(\xv, y\right) \in \Np$.
\item Then, the risk $\risk(\Np)$ for a leaf is the sum of the individual losses for the data assigned to that leaf under a given loss function $L$:
  $$\risk(\Np) = \sum\limits_{(\xv, y) \in \Np} L(y, c)$$
\item The prediction is given by the optimal constant $c = \argmin_c \risk(\Np)$
\end{itemize}

\framebreak

\begin{itemize}
\item A split w.r.t. \textbf{feature $x_j$ at split point $t$} divides a parent node $\Np$ into 
  \begin{align*}
    \Nl &= \{ (\xv, y) \in \Np: x_j \leq t \} \text{ and } \Nr = \{ (\xv, y) \in \Np: x_j > t \}.
  \end{align*}
\item   
  In order to evaluate how good a split is, we compute the empirical risks
  in both child nodes and sum them up.
  Here, we reweight the losses w.r.t. the number of observations in each node:
     \begin{align*}
      \risk(\Np, j, t) &= \frac{|\Nl|}{|\Np|} \risk(\Nl) + \frac{|\Nr|}{|\Np|} \risk(\Nr) \\
                  &= \frac{1}{|\Np|}\left(\sum\limits_{(\xv,y) \in \Nl} L(y, c_1) + \sum\limits_{(\xv,y) \in \Nr} L(y, c_2)\right)
      \end{align*}
  \item Finding the best way to split $\Np$ into $\Nl, \Nr$ means solving
  $$\argmin_{j, t} \risk(\Np, j, t)$$
\end{itemize}
\end{vbframe}




\endlecture
\end{document}
