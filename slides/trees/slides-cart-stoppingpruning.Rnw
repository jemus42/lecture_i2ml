% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../../style/preamble.Rnw")
library(rpart)
library(rpart.plot)
library(randomForest)
library(rattle)
library(smoof)
@

% Load all R packages and set up knitr
<<setup, child="../../style/setup.Rnw", include = FALSE>>=
@

<<results='hide', code = readLines("rsrc/draw-cart-iris.R")>>=
@

%! includes: cart-intro, cart-splitcriteria

\lecturechapter{CART: Stopping Criteria \& Pruning}
\lecture{Introduction to Machine Learning}
\sloppy

\begin{frame}{Overfitting Trees}

The \textbf{recursive partitioning} procedure used to grow a CART would run until every leaf only contains a single observation. 
\begin{itemize}
\item Problem 1: This would take a very long time, as the amount of splits we have to try \emph{grows exponentially} with the number of leaves in the trees.
\item Problem 2: At some point before that we should stop splitting nodes into ever smaller child nodes: very complex trees with lots of branches and leaves will \emph{overfit the training data.}
\item Problem 3: However, it is very hard to tell where we should stop while we're growing the tree: Before we actually try all possible additional splits further down a branch, we can't know whether any one of them will be able to reduce the risk by a lot (\emph{horizon effect}).
\end{itemize}
\end{frame}

\begin{vbframe}{Stopping Criteria}
Problems 1 and 2 can be \enquote{solved} by defining different \textbf{stopping criteria}:
  \begin{itemize}
    \item Stop once the tree has reached a certain number of leaves.
    \item Don't try to split a node further if it contains too few observations.
    \item Don't perform a split that results in child nodes with too few observations.
    \item Don't perform a split unless it achieves a certain minimal improvement of the empirical risk in the child nodes compared to the empirical risk in the parent node.
    \item Obviously: Stop once all observations in a node have the same target value (\textbf{pure node}) or identical values for all features. 
  \end{itemize}
\end{vbframe}

\begin{vbframe}{Pruning}

We try to solve problem 3 by \textbf{pruning}:

\begin{itemize}
\item a method to select the optimal size of a tree
\item Finding a combination of suitable strict stopping criteria (\enquote{pre-pruning}) is a hard problem: there are many different stopping criteria and it's hard to find the best combination (see chapter on \textbf{tuning})
\item Better: Grow a large tree, then remove branches so that the resulting smaller tree has optimal cross-validation risk
\item Feasible without cross-validation: Grow a large tree, then remove branches so that the resulting smaller tree has a good balance between training set performance (risk) and complexity (i.e., number of terminal nodes). The trade-off between complexity and accuracy is governed by a \textbf{complexity parameter}.
\end{itemize}

\framebreak

<<size='footnotesize'>>=
lrn = makeLearner("regr.rpart", maxdepth = 2)
mod = train(lrn, bh.task)
mod = mod$learner.model
@

\framebreak

<<results='hide', caption = "Pruning a CART on the Boston Housing dataset">>=
# Pruning with every cp taken from the cptable
cps = rev(mod$cptable[-4, "CP"])
rattle::fancyRpartPlot(mod, sub = "Full tree")

lapply(cps, function(x) {
    p = rpart::prune(mod, cp = x)
    sub_title = sprintf("Pruning with complexity parameter = %.3f.", x)
    rattle::fancyRpartPlot(p, sub = sub_title)
})
@
\end{vbframe}



\endlecture
