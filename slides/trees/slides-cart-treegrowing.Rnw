% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../../style/preamble.Rnw")
library(rpart)
library(rpart.plot)
library(randomForest)
library(rattle)
library(smoof)
@

% Load all R packages and set up knitr
<<setup, child="../../style/setup.Rnw", include = FALSE>>=
@

<<results='hide', code = readLines("rsrc/draw-cart-iris.R")>>=
@

%! includes: cart-intro, cart-splittingcriteria

\lecturechapter{CART: Growing a Tree}
\lecture{Introduction to Machine Learning}
\sloppy

\begin{vbframe}{Tree Growing}

We start with an empty tree, a root node that contains all the data.\\
Trees are then grown by recursively applying \emph{greedy} optimization to each node $\Np$.\\
\lz

Greedy means we do an \textbf{exhaustive search}: All possible splits of $\Np$ on all possible points $t$ for all features $x_j$ are compared in terms of their empirical risk $\risk(\Np, j, t)$. 

The training data is then distributed to child nodes according to the optimal split and the procedure is repeated in the child nodes.

\framebreak

Start with a root node of all data, then search for a feature and split-point that minimizes the empirical risk in the child nodes. 
<<result='hide', fig.height=2.2>>=
draw_cart_on_iris(depth = 1, with_tree_plot = TRUE)
@
Nodes display their current label distribution here for illustration.


\framebreak

We then proceed recursively for each child node:
Iterate over all features, and for each feature over all possible split points. Select the best split an divide data in parent node into left and right child nodes:
<<result='hide', fig.height=2.2>>=
draw_cart_on_iris(depth = 2, with_tree_plot = TRUE)
@

\framebreak

We then proceed recursively for each child node:
Iterate over all features, and for each feature over all split points. Select the \enquote{best} split and divide data in parent node into left and right child nodes:
<<result='hide', fig.height=2.2>>=
draw_cart_on_iris(depth = 3, with_tree_plot = TRUE)
@

\end{vbframe}

\begin{vbframe}{Split placement}
<<fig.height=4>>=
task = subsetTask(iris.task, seq(1, 150, by = 20))
lrn = makeLearner("classif.rpart", cp = 0, minbucket = 1, maxdepth = 1)
pl = plotLearnerPrediction(lrn, task, gridsize = 100,
  cv = 0, prob.alpha = FALSE, err.mark = "none")
pl = pl + theme(legend.position="none")
print(pl)
@
\lz
Splits are usually placed at the mid-point of the observations they split: the large margin to the next closest observations makes better generalization on new, unseen data more likely.
\end{vbframe}

% \begin{vbframe}{Regression Example}
% \begin{columns}[T,onlytextwidth]
% \column{0.2\textwidth}
% <<out.width='\\textwidth'>>=
% modForrester = makeSingleObjectiveFunction(
%   name = "Modification Forrester et. all function",
%   fn = function(x) (sin(4*x - 4)) * ((2*x - 2)^2) * (sin(20*x - 4)),
%   par.set = makeNumericParamSet(lower = 0, upper = 1, len = 1L),
%   noisy = TRUE
% )
% design = generateDesign(7L, getParamSet(modForrester), fun = lhs::maximinLHS)
% design$y = modForrester(design)
% ordered.design = design[order(design$x),]
% rownames(ordered.design) = NULL
% kable(ordered.design, digits = 3)
% @
% 
% \hspace{0.5cm}
% \column{0.7\textwidth}
% % FIGURE SOURCE: No source
% \includegraphics[height = 0.55\textheight]{figure_man/regression_tree}
% \end{columns}
% \vspace{0.5cm}
% Data points (red) were generated from the underlying function (black):
% 
% $ sin(4x - 4) * (2x - 2)^2 * sin(20x -4) $
% 
% % \framebreak
% 
% % BB: doesnt seem too useful to show this, nothing really new in here
% % <<fig.height=5>>=
% % regr.task = makeRegrTask(data = design, target = "y")
% % regr.rpart = makeLearner("regr.rpart", par.vals = list(minsplit=1, minbucket = 1))
% % regr.model = train(regr.rpart, regr.task)
% % fancyRpartPlot(regr.model$learner.model, sub="")
% % @
% \end{vbframe}


\begin{vbframe}{Stopping Criteria}

The recursive procedure defined above would run until every leaf only contains a single observation. At some point before that we have to stop splitting nodes into ever smaller child nodes to avoid overfitting the training data.\\
Different stopping criteria can be defined:
  \begin{itemize}
    \item Stop splitting once the tree has reached a certain number of leafs.
    \item Don't split a node further if it contains too few observations.
    \item Don't perform a split that results in child nodes with too few observations.
    \item Don't perform a split unless it achieves a certain minimal improvement of the empirical risk compared to the empirical risk in the parent node.
  \end{itemize}
\end{vbframe}

\begin{vbframe}{Pruning}

\begin{itemize}
\item A method to select the optimal size of a tree
\item Problem 1: while growing the tree, it is hard to tell when we should stop, as we don't know if the addition of further splits down the line will dramatically decrease error (\enquote{horizon effect}).
\item Problem 2: very complex trees with many branches and leafs will suffer from overfitting
\item We could use strict stopping criteria (\enquote{pre-pruning}), but there are many different ones and it's hard to find the best combination (see chapter on \textbf{tuning})
  and you would have to tune over their settings
\item Better: Grow a large tree, then remove branches so that the resulting smaller tree is optimal w.r.t. cross-validated error
\item Here: Grow a large tree, then remove the parts so that the resulting smaller tree has a good balance between training error and complexity (as counted by the number of terminal nodes).
\end{itemize}

\framebreak

<<size='footnotesize'>>=
lrn = makeLearner("regr.rpart", maxdepth = 2)
mod = train(lrn, bh.task)
mod = mod$learner.model
@

\framebreak

<<results='hide', caption = "Pruning a CART on the Boston Housing dataset">>=
# Pruning with every cp taken from the cptable
cps = rev(mod$cptable[, "CP"])
lapply(cps, function(x) {
    p = rpart::prune(mod, cp = x)
    sub_title = sprintf("Pruning with complexity parameter = %.3f.", x)
    rattle::fancyRpartPlot(p, sub = sub_title)})
@
\end{vbframe}



\endlecture
