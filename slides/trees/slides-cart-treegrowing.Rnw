% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../../style/preamble.Rnw")
library(rpart)
library(rpart.plot)
library(randomForest)
library(rattle)
library(smoof)
@

% Load all R packages and set up knitr
<<setup, child="../../style/setup.Rnw", include = FALSE>>=
@

<<results='hide', code = readLines("rsrc/draw-cart-iris.R")>>=
@

% Defines macros and environments
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-trees.tex}
%! includes: cart-intro, cart-splitcriteria

\lecturechapter{CART: Growing a Tree}
\lecture{Introduction to Machine Learning}
\sloppy

\begin{vbframe}{Tree Growing}

We start with an empty tree, a root node that contains all the data.\\
Trees are then grown by recursively applying \emph{greedy} optimization to each node $\Np$.\\
\lz

Greedy means we do an \textbf{exhaustive search}: All possible splits of $\Np$ on all possible points $t$ for all features $x_j$ are compared in terms of their empirical risk $\risk(\Np, j, t)$. 

The training data is then distributed to child nodes according to the optimal split and the procedure is repeated in the child nodes.

\framebreak

Start with a root node of all data, then search for a feature and split-point that minimizes the empirical risk in the child nodes. 
<<result='hide', fig.height=2.2>>=
draw_cart_on_iris(depth = 1, with_tree_plot = TRUE)
@
Nodes display their current label distribution here for illustration.


\framebreak

We then proceed recursively for each child node:
Iterate over all features, and for each feature over all possible split points. Select the best split and divide data in parent node into left and right child nodes:
<<result='hide', fig.height=2.2>>=
draw_cart_on_iris(depth = 2, with_tree_plot = TRUE)
@

\framebreak

We then proceed recursively for each child node:
Iterate over all features, and for each feature over all split points. Select the \enquote{best} split and divide data in parent node into left and right child nodes:
<<result='hide', fig.height=2.2>>=
draw_cart_on_iris(depth = 3, with_tree_plot = TRUE)
@

\end{vbframe}

\begin{vbframe}{Split placement}
<<fig.height=4>>=
task = subsetTask(iris.task, seq(1, 150, by = 20))
lrn = makeLearner("classif.rpart", cp = 0, minbucket = 1, maxdepth = 1)
pl = plotLearnerPrediction(lrn, task, gridsize = 100,
  cv = 0, prob.alpha = FALSE, err.mark = "none")
pl = pl + theme(legend.position="none")
print(pl)
@
\lz
Splits are usually placed at the mid-point of the observations they split: the large margin to the next closest observations makes better generalization on new, unseen data more likely.
\end{vbframe}

% \begin{vbframe}{Regression Example}
% \begin{columns}[T,onlytextwidth]
% \column{0.2\textwidth}
% <<out.width='\\textwidth'>>=
% modForrester = makeSingleObjectiveFunction(
%   name = "Modification Forrester et. all function",
%   fn = function(x) (sin(4*x - 4)) * ((2*x - 2)^2) * (sin(20*x - 4)),
%   par.set = makeNumericParamSet(lower = 0, upper = 1, len = 1L),
%   noisy = TRUE
% )
% design = generateDesign(7L, getParamSet(modForrester), fun = lhs::maximinLHS)
% design$y = modForrester(design)
% ordered.design = design[order(design$x),]
% rownames(ordered.design) = NULL
% kable(ordered.design, digits = 3)
% @
% 
% \hspace{0.5cm}
% \column{0.7\textwidth}
% % FIGURE SOURCE: No source
% \includegraphics[height = 0.55\textheight]{figure_man/regression_tree}
% \end{columns}
% \vspace{0.5cm}
% Data points (red) were generated from the underlying function (black):
% 
% $ sin(4x - 4) * (2x - 2)^2 * sin(20x -4) $
% 
% % \framebreak
% 
% % BB: doesnt seem too useful to show this, nothing really new in here
% % <<fig.height=5>>=
% % regr.task = makeRegrTask(data = design, target = "y")
% % regr.rpart = makeLearner("regr.rpart", par.vals = list(minsplit=1, minbucket = 1))
% % regr.model = train(regr.rpart, regr.task)
% % fancyRpartPlot(regr.model$learner.model, sub="")
% % @
% \end{vbframe}






\endlecture
