\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-trees.tex}
\newcommand{\titlefigure}{figure/horizon.pdf}
\newcommand{\learninggoals}{
\item Understand which problems arise when growing the tree until the end
\item Know different stopping criteria
\item Understand the idea of pruning}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}
\lecturechapter{CART: Stopping Criteria \& Pruning}
\lecture{Introduction to Machine Learning}
\sloppy

\begin{vbframe}{Computational Complexity}

The \textbf{recursive partitioning} procedure used to grow a CART would run until every leaf only contains a single observation. 
\begin{itemize}
\item Problem 1: This can become computationally expensive as the number of splits may \emph{grows exponentially} with the number of leaves in the trees.
\end{itemize}

While growing trees fully is computationally not efficient, in typical applications on modern computers, this does usually not imply infeasibly large computational costs.
Single trees are almost always very fast to compute.
However, high computational costs can be undesirable if many trees need to be fit (e.g. ensembles).

\end{vbframe}

\begin{vbframe}{Overfitting Trees}

The \textbf{recursive partitioning} procedure used to grow a CART would run until every leaf only contains a single observation. 
\begin{itemize}
\item Problem 2: At some point before that we should stop splitting nodes into ever smaller child nodes: very complex trees with lots of branches and leaves will \emph{overfit the training data.}

\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{figure/tree-overfitting-prediction.pdf}
\end{figure}

\end{itemize}

\end{vbframe}

\begin{vbframe}{Overfitting Trees}

We can reduce overfitting to some extent with \emph{stopping criteria}:
\vspace{0.25cm}

\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{figure/tree-less-overfitting-prediction.pdf}
\end{figure}

\end{vbframe}

\begin{vbframe}{Horizon Effect}

The \textbf{recursive partitioning} procedure used to grow a CART would run until every leaf only contains a single observation. 
\begin{itemize}
\item Problem 3: However, it is very hard to tell where we should stop while we're growing the tree: Before we have actually tried all possible additional splits further down a branch, we can't know whether any one of them will be able to reduce the risk by a lot (\emph{horizon effect}).
\end{itemize}

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{figure/horizon.pdf}
\end{figure}


\end{vbframe}

\begin{vbframe}{Problems with growing large trees}
The \textbf{recursive partitioning} procedure used to grow a CART would run until every leaf only contains a single observation. 
\begin{itemize}
\item Problem 1: This can become computationally expensive as the number of splits may \emph{grows exponentially} with the number of leaves in the trees.
\item Problem 2: At some point before that we should stop splitting nodes into ever smaller child nodes: very complex trees with lots of branches and leaves will \emph{overfit the training data.}
\item Problem 3: However, it is very hard to tell where we should stop while we're growing the tree: Before we have actually tried all possible additional splits further down a branch, we can't know whether any one of them will be able to reduce the risk by a lot (\emph{horizon effect}).
\end{itemize}
\end{vbframe}

\begin{vbframe}{Stopping Criteria}
Problems 1 and 2 can be \enquote{solved} by defining different \textbf{stopping criteria}:
  \begin{itemize}
    \item Stop once the tree has reached a certain number of leaves.
    \item Don't try to split a node further if it contains too few observations.
    \item Don't perform a split that results in child nodes with too few observations.
    \item Don't perform a split unless it achieves a certain minimal improvement of the empirical risk in the child nodes, compared to the empirical risk in the parent node.
    \item Obviously: Stop once all observations in a node have the same target value (\textbf{pure node}) or identical values for all features. 
    \item There are heuristics for selecting these criteria and software packages usually have considerate defaults that work well out-of-the-box.
  \end{itemize}
\end{vbframe}

\begin{vbframe}{Pruning}

We try to solve problem 3 by \textbf{pruning}:

\begin{itemize}
\item A method to select the optimal size of a tree.
\item Finding a combination of suitable strict stopping criteria (\enquote{pre-pruning}) is a hard problem: there are many different stopping criteria and it's hard to find the best combination (see chapter on \textbf{tuning}).
\item Alternative: Grow a large tree, then remove branches so that the resulting smaller tree has lower risk (\enquote{post-pruning}).
\item Oftentimes, only post-pruning is meant when referring to pruning in general.

\end{itemize}
\end{vbframe}

\begin{vbframe}{Post-Pruning: CCP}
\begin{itemize}
\item Most prominent example of post-pruning: cost-complexity \\ pruning (CCP)
\item When doing CCP, we grow a large tree and remove the least informative leaves later on
\item CCP is steered with a regularization parameter $\alpha$ that penalizes the number of leaves in a subtree (complexity) and adds this penalization onto the risk of the subtree:
\end{itemize}

$$\risk_{T \subset T_0}(y^{(i)}, c_m) = \sum_{m=1}^{|T|} \sum_{x^{(i)} \in Q_m} L(y^{(i)}, c_m) + \alpha |T|,$$

where $|T|$ refers to the number of leaves of subtree $T$ and $Q_m$ to the subset of the predictor space of the $m$-th terminal node, with its prediction $c_m$ and the complete tree $T_0$.

\end{vbframe}

\begin{vbframe}{CCP}
\begin{itemize}
\item CCP performs a greedy backward search:
\item It computes $\risk_{T \subset T_0}(y^{(i)}, c_m)$ with a fixed $\alpha$ for all possible sub trees that can be created by removing exactly one node.
\item This does not need to be a terminal node.
\item We select the sub tree with the lowest risk and repeat the procedure.
\item We stop if pruning does not further reduce the risk.
\item This is proven to result in the pruned tree with the lowest risk.
\item For $\alpha = 0$, we would obviously select $T_0$.
\item The $\alpha$ parameter is typically selected via cross-validation.
\item Other prominent post-pruning methods next to CCP include e.g. reduced error pruning (REP) or pessimistic error pruning (PEP).
\end{itemize}
\end{vbframe}


\begin{frame}{CCP}

We run the CCP algorithm step-by-step with $\alpha = 1.2$:
\vspace{0.25cm}

\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/ccp_1.pdf} 

}

\end{frame}

\begin{frame}[noframenumbering]{CCP}

There are three possible nodes that we can eliminate to prune the tree.
We take the one the elimination that results in the lowest risk.

\vspace{0.25cm}

\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/ccp_1_notes.pdf} 

}

\end{frame}

\begin{frame}[noframenumbering]{CCP}

The first pruned sub tree has a lower risk than the full tree.
\vspace{0.25cm}

{\centering \includegraphics[width=0.95\textwidth]{figure/ccp_2.pdf} 

}

\end{frame}

\begin{frame}[noframenumbering]{CCP}

From here on, the risk increases.
\vspace{0.25cm}


{\centering \includegraphics[width=0.95\textwidth]{figure/ccp_3.pdf} 

}

\end{frame}

\begin{frame}[noframenumbering]{CCP}


{\centering \includegraphics[width=0.95\textwidth]{figure/ccp_4.pdf} 

}


\end{frame}

\begin{frame}[noframenumbering]{CCP}

We select the first sub tree as it results in the lowest risk.
\vspace{0.25cm}


{\centering \includegraphics[width=0.99\textwidth]{figure/ccp_5.pdf} 

}


\end{frame}



\endlecture
\end{document}
