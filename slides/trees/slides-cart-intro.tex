\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}


\newcommand{\titlefigure}{figure_man/labelling_of_tree.png}
\newcommand{\learninggoals}{
\item Understand the basic structure of a tree model
\item Understand that the basic idea of a tree model is the same for classification and regression 
\item Know how the label of a new observation is predicted via CART
\item Know hypothesis space of CART}


\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}

\lecturechapter{Classification and Regression Trees (CART): Basics}
\lecture{Introduction to Machine Learning}
\sloppy

\begin{vbframe}{Binary Trees}
    \begin{figure}
    \centering
      % FIGURE SOURCE: No source
      \includegraphics[height = 4.5cm, keepaspectratio]{figure/cart_intro_binary-tree_1.pdf}
    \end{figure}
  \begin{itemize}
    \item Binary trees are a common data structure
    \item They repesent a top-down hierarcy facilitated by binary splits
    \item They can be used in many application, including machine learning
    \item A tree is split into different nodes: a) the root node, b) internal nodes and c) terminal nodes (also leaves).
  \end{itemize}
\end{vbframe}

\begin{vbframe}{Binary Trees}
    \begin{figure}
    \centering
      % FIGURE SOURCE: No source
      \includegraphics[height = 4.5cm, keepaspectratio]{figure/cart_intro_binary-tree_2.pdf}
    \end{figure}
  \begin{itemize}
    \item Nodes have relative relationships, they are either:
    \begin{itemize}
    \item Parent nodes
    \item Children nodes
    \end{itemize}
    \item Root nodes do not have parents and terminal nodes do not have children
  \end{itemize}
\end{vbframe}

\begin{vbframe}{Classification Trees}
    \begin{figure}
    \centering
      % FIGURE SOURCE: No source
      \includegraphics[height = 4.5cm, keepaspectratio]{figure/cart_intro_annotated-tree.pdf}
    \end{figure}
  \begin{itemize}
    \item Classification Trees use the data structure of a binary tree
    \item Binary splits are constructed top-down in an \emph{optimal} way
    \item A splitting rule decides what is optimal and coincides with the lowest possible loss
  \end{itemize}
\end{vbframe}



\begin{vbframe}{Classification Tree Model and Prediction}
  \begin{itemize}
    \item For predictions, observations are passed down the tree, according to the splitting rules in each node
    \item An observation will end up in exactly one leaf node
    \item All observations in a leaf node are assigned the same prediction for the target
  \end{itemize}

\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
{\centering \includegraphics[width=0.8\textwidth, keepaspectratio]{figure/tree-classif-depth3.pdf}

}

\end{vbframe}

\begin{vbframe}{Regression Tree Model and Prediction}
  \begin{itemize}
    \item For predictions, observations are passed down the tree, according to the splitting rules
      in each node
    \item An observation will end up in exactly one terminal node
    \item All observations in a leaf node are assigned the same prediction for the target
  \end{itemize}

\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
{\centering \includegraphics[width=0.8\textwidth, keepaspectratio]{figure/tree-regr-depth3.pdf}

}

\end{vbframe}

\begin{vbframe}{Building a tree}
\begin{itemize}
\item A tree iteratively selects the single best* split of the data
\item The resulting leaves' predictions depend on the observations being assigned to them
\item For example, consider this stump on the left with the resulting predictions on the right:
for Sepal.Length < 5.5, we predict setosa (red area), else virginica (green area)
\end{itemize}

\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
{\centering \includegraphics[width=0.8\textwidth, keepaspectratio]{figure/tree-classif-depth1.pdf}

}

\end{vbframe}

\begin{vbframe}{Building a tree}
\begin{itemize}
\item The best split is determined w.r.t. some loss
\item Each resulting leaf can be split again, resulting in 2 additional splits in this case
\item This way we end up with a more complex decision boundary
\end{itemize}

\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
{\centering \includegraphics[width=0.85\textwidth, keepaspectratio]{figure/tree-classif-depth2.pdf}

}

\end{vbframe}

\begin{vbframe}{Building a tree}
\begin{itemize}
\item We stop splitting until we reach a stopping criterion or each leaf is pure
\item The splitting rules are stored for predicting out-of-sample
\end{itemize}

\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
{\centering \includegraphics[width=0.89\textwidth, keepaspectratio]{figure/tree-classif-depth3.pdf}

}

\end{vbframe}

\begin{vbframe}{Predicting new data}
\begin{itemize}
\item When predicting new data we use the learnt split points and pass an observation through the tree
\item Classification trees can make hard-label predictions (here: "setosa") or predict scores / probabilities (here: 0.98 setosa, 0.02 versicolor, 0.00 virginica)
\item Regression trees always predict numeric outcomes
\end{itemize}

\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
{\centering \includegraphics[width=0.82\textwidth, keepaspectratio]{figure/no-points-classif-depth3.pdf}

}
\end{vbframe}


\begin{vbframe}{Tree as an additive model}
Each point in $\Xspace$ is assigned to exactly one leaf, and each leaf has a set of input points leading to it through axis-parallel splits.\\
Hence, trees divide the feature space $\Xspace$ into \textbf{rectangular regions}: 
  \begin{align*}
    \fx = \sum_{m=1}^M c_m \I(\xv \in Q_m),
  \end{align*}
  where a tree with $M$ leaf nodes defines $M$ \enquote{rectangles} $Q_m$.\\
  $c_m$ is the predicted numerical response, class label or class
  distribution in the respective leaf node.

\end{vbframe}

\begin{vbframe}{Tree as an additive model}
Trees divide the feature space $\Xspace$ into \textbf{rectangular regions}: 
  \begin{align*}
    \fx = \sum_{m=1}^M c_m \I(\xv \in Q_m),
  \end{align*}


\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

\begin{figure}
\includegraphics[width=0.8\textwidth, keepaspectratio]{figure/cart_intro_classification_tree_wide.pdf}
\end{figure}
\end{vbframe}

\begin{vbframe}{Tree as an additive model}

The hypothesis space of a CART is the set of all step functions over rectangular partitions of $\Xspace$:
\begin{align*}
    \fx = \sum_{m=1}^M c_m \I(\xv \in Q_m)
\end{align*}

The steps are learnt sequentially:


\begin{figure} 
\includegraphics[width=0.75\textwidth, keepaspectratio]{figure/cart_intro_regression_tree_wide.pdf}
\end{figure}

\end{vbframe}








% % BB: as we are not really talking too much about impurity anymore, I took this out
% % <<splitcriteria-plot, results='hide', fig.height=5>>=
% % Colors = pal_3
% % par(mar = c(5.1, 4.1, 0.1, 0.1))
% % p = seq(1e-6, 1-1e-6, length.out = 200)
% % entropy = function(p) (p * log(p) + (1 - p) * log(1 - p))/(2 * log(0.5))
% % gini = function(p) 2 * p * (1 - p)
% % missclassification = function(p) (1 - max(p, 1 - p))
% % plot(p, entropy(p), type = "l", col = Colors[1], lwd = 1.5, ylab = "",
% %   ylim = c(0, 0.6), xlab = expression(hat(pi)[Nk]))
% % lines(p, gini(p), col = Colors[2], lwd = 1.5)
% % lines(p, sapply(p, missclassification), col = Colors[3], lwd = 1.5)
% % legend("topright", c("Gini Index", "Entropy", "Misclassification Error"),
% %        col = Colors[1:3], lty = 1)
% % @





\endlecture
\end{document}
