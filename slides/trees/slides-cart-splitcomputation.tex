\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/ml-trees.tex}

\newcommand{\titlefigure}{figure/cart_splitcomp_3_part.png}
\newcommand{\learninggoals}{
\item Know how monotone feature transformations affect the tree
\item Understand how nominal features can be treated effectively while growing a CART
\item Understand how missing values can be treated in a CART}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}
\lecturechapter{CART: Computational Aspects of Finding Splits}
\lecture{Introduction to Machine Learning}

\sloppy

\begin{vbframe}{Monotone feature transformations}

Monotone transformations of one or several features will neither change the value of the splitting criterion nor the structure of the tree,  only the numerical value of the split point.
\vspace{0.5cm}
\begin{columns}[T]
\column{0.49\textwidth}
Original data
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\begin{tabular}{l|r|r|r|r|r}
\hline
x & 1 & 2 & 7.0 & 10 & 20\\
\hline
y & 1 & 1 & 0.5 & 10 & 11\\
\hline
\end{tabular}


\end{knitrout}
% FIGURE SOURCE: Use picture created in rsrc/monotone_trafo.R
\includegraphics[width = \textwidth]{figure/cart_splitcomp_1}
\column{0.49\textwidth}
Data with log-transformed $x$
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\begin{tabular}{l|r|r|r|r|r}
\hline
log(x) & 0 & 0.7 & 1.9 & 2.3 & 3\\
\hline
y & 1 & 1.0 & 0.5 & 10.0 & 11\\
\hline
\end{tabular}


\end{knitrout}
% FIGURE SOURCE: Use picture created in rsrc/monotone_trafo.R
\includegraphics[width = \textwidth]{figure/cart_splitcomp_2}
\end{columns}
\vspace{0.5cm}
\centering
\end{vbframe}

\begin{vbframe}{CART: Nominal Features}
  \begin{itemize}
  \item A split on a nominal feature partitions the feature levels:
    $$x_j \in \{a,c,e\} \leftarrow \Np \rightarrow x_j \in \{b,d\} $$
  \item For a feature with $m$ levels,
  there are about $2^m$ different possible partitions of the $m$ values into two groups\\ ($2^{m-1} - 1$ because of symmetry and empty groups).
  \item Searching over all these becomes prohibitive for larger values of $m$.
  \item For regression with squared loss and binary classification, we can define clever shortcuts.
  \end{itemize}

  \framebreak

For $0-1$ responses, in each node:
  \begin{enumerate}
  \item Calculate the proportion of $1$-outcomes for each category of the feature in $\Np$.
  \item Sort the categories according to these proportions.
  \item The feature can then be treated as if it was ordinal, so we only have to investigate at most $m-1$ splits.
  \end{enumerate}

  \vspace{0.3cm}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/cart_splitcomp_3} 

}



\end{knitrout}

\pagebreak

  \begin{itemize}
  \item This procedure finds the optimal split.
  \item This result also holds for regression trees (with squared error loss) if the levels of the feature are ordered by increasing mean of the target
  \item The proofs are not trivial and can be found here:
    \begin{itemize}
    \item for 0-1 responses:
      \begin{itemize}
      \item Breiman, 1984, Classification and Regression Trees.
      \item Ripley, 1996, Pattern Recognition and Neural Networks.
      \end{itemize}
    \item for continuous responses:
      \begin{itemize}
      \item Fisher, 1958, On grouping for maximum homogeneity.
      \end{itemize}
    \end{itemize}
  \item Such simplifications are not known for multiclass problems.
  %\item The Algorithm prefers categorical variables with a large value
  %of categories $Q$
  \end{itemize}

\pagebreak

For continuous responses, in each node:
  \begin{enumerate}
  \item Calculate the mean of the outcome in each category
  \item Sort the categories by increasing mean of the outcome
  \end{enumerate}

\vspace{0.3cm}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/cart_splitcomp_4} 

}



\end{knitrout}
\end{vbframe}

\begin{vbframe}{CART: Missing feature values}
  \begin{itemize}
    \item When splits are evaluated, only observations for which the used feature is not missing are used. (This can actually bias splits towards using features with lots of missing values.) 
  \item CART often use the so-called \textbf{surrogate split} principle to automatically deal with missing values in features used for splits during prediction.
  \item Surrogate splits are created during training. They define replacement splitting rules, using a different feature, that result in almost the same child nodes as the original split.
   \item When observations are passed down the tree (in training or prediction), and the feature value used in a split is missing, we use a "surrogate split" instead to decide to which branch of the tree the data should be assigned. 
  \end{itemize}
\end{vbframe}

\endlecture
\end{document}
