\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R



\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}
\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
  %Define standard arrow tip
  >=stealth',
  %Define style for boxes
  punkt/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=6.5em,
    minimum height=2em,
    text centered},
  % Define arrow style
  pil/.style={
    ->,
    thick,
    shorten <=2pt,
    shorten >=2pt,}
}
\usepackage{subfig}


% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}



\begin{document}
% Set style/preamble.Rnw as parent.

% Load all R packages and set up knitr

% This file loads R packages, configures knitr options and sets preamble.Rnw as parent file
% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...








% Defines macros and environments
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-lm.tex}

%! includes: basics-riskminimization

\lecturechapter{Classification: Basic Definitions}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{Classification Tasks}

In classification, we aim at predicting a discrete output 

$$
y \in \Yspace = \{C_1, ..., C_g\}
$$

with $2 \le g < \infty$, given data $\D$.  

\lz 

In this course, we assume the classes to be encoded as

\begin{itemize}
  \item $\Yspace = \setzo$ or $\Yspace = \setmp$ (in the binary case $g = 2$)
  \item $\Yspace = \setg$  (in the multiclass case $g \ge 3$)
\end{itemize}

\vfill

% $^1$\textbf{Remark}: We will see later, that for probability / likelihood-based model derivations a 0/1-Coding and for geometric / loss-based models the -1/+1-coding is preffered. 

\end{vbframe}


\begin{vbframe}{Classification Models} 
We defined models $f: \Xspace \to \R^g$ as functions that output (continuous) \textbf{scores} / \textbf{probabilities} and \textbf{not} (discrete) classes. Why? 

\begin{itemize}
  \item From an optimization perspective, it is \textbf{much} (!) easier to optimize costs for continuous-valued functions 
  \item Scores / probabilities (for classes) contain more information than the class labels alone
  \item As we will see later, scores can easily be transformed into class labels; but class labels cannot be transformed into scores
\end{itemize}

We distinguish \textbf{scoring} and \textbf{probabilistic} classifiers.
\end{vbframe}


\begin{vbframe}{Scoring Classifiers}
\begin{itemize}
% \item Scoring classifiers assume the output variable to be -1/+1-encoded, i. e. $\Yspace = \{-1, 1\}$
\item Construct $g$ \textbf{discriminant} / \textbf{scoring functions} $f_1, ..., f_g: \Xspace \to \R$
\item Scores $f_1(\xv), \ldots, \fgx$ are transformed into classes by choosing the class with the maximum score 
$$
h(\xv) = \argmax_{k \in \setg} \fkx. 
$$ 

\item For $g = 2$, a single discriminant function $\fx = f_{1}(\xv) - f_{-1}(\xv)$ is sufficient (note that it would be natural here to label the classes with $\setmp$)  

% \begin{eqnarray*}
 % \qquad h(\bm{x}) &=& 1 \\
 % f_1(\xv) &>&  f_{-1}(\xb) \\
 % f_1(\xb) - f_{-1}(\xb) &>& 0 \\
% f(\xb) &>& 0 
% \end{eqnarray*}

\item Class labels are constructed by $\hx = \text{sgn}(\fx)$
\item $|\fx|$ is called \enquote{confidence}
\end{itemize}
\end{vbframe}

\begin{vbframe}{Probabilistic Classifiers}
\begin{itemize}
% \item Probabilistic classifiers assume the output variable to be 0/1-encoded, i. e. $\Yspace = \{0, 1\}$
\item Construct $g$ \textbf{probability functions} $\pi_1, ..., \pi_g: \Xspace \to [0, 1],~\sum_i \pi_i = 1$ 
\item Probabilities $\pi_1(\xv), \ldots, \pigx$ are transformed into labels by predicting the class with the maximum probability
$$
\hx = \argmax_{k \in \setg} \pikx
$$ 
\item For $g = 2$ one $\pix$ is constructed (note that it would be natural here to label the classes with $\setzo$)

\item Probabilistic classifiers can also be seen as scoring classifiers

\item If we want to emphasize that our model outputs probabilities, we denote the model as $\pix: \Xspace \to [0, 1]^g$; if we are talking about models in a general sense, we write $f$, comprising both probabilistic and scoring classifiers (context will make this clear!) 
\end{itemize}

\framebreak 


\begin{itemize}
\item Both scoring and probabilistic classifiers can output classes by thresholding (binary case) / selecting the class with the maximum score (multiclass)
\item Thresholding: $\hx:= [\pix \ge c]$ or $\hx = [\fx \ge c]$ for some threshold $c$.
\item Usually $c = 0.5$ for probabilistic, $c = 0$ for scoring classifiers.
\item There are also versions of thresholding for the multiclass case

\end{itemize}

\begin{center}
  \includegraphics{figure_man/classifiers.png}
\end{center}

\end{vbframe} 

\begin{vbframe}{Decision regions and boundaries}
\begin{itemize}
  \item A \textbf{decision region} for class $k$ is the set of input points $\xv$ where class $k$ is assigned as prediction of our model:
$$
\Xspace_k = \{\xv \in \Xspace : \hx = k\}
$$

\item Points in space where the classes with maximal score are tied and the corresponding hypersurfaces are called \textbf{decision boundaries}
\begin{eqnarray*}  
\{ \xv \in \Xspace: && \exists~i \ne j \text{ s.t. } \fix = \fjx \\ && \text{and } \fix, \fjx \ge \fkx ~ \forall k \ne i, j\}
\end{eqnarray*}  

In the binary case we can simplify and generalize to the decision boundary for general threshold $c$:

$$
    \{ \xv \in \Xspace : \fx = c \}
$$

If we set $c=0$ for scores and $c=0.5$ for probabilities, this is consistent with the definition above.

\end{itemize}
\end{vbframe} 


\begin{vbframe}{Decision boundary examples}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/reg_class_bdefs.pdf} 

}



\end{knitrout}

\end{vbframe}

\begin{vbframe}{Classification Approaches}

  Two fundamental approaches exist to construct classifiers:\\
  The \textbf{generative approach} and the \textbf{discriminant approach}.

\lz
They tackle the classification problem from different angles:

\begin{itemize}
\item \textbf{Generative} classification approaches assume a data-generating process in which the distribution of the features $\xv$ is different for the various classes of the output $y$, and try to learn these conditional distributions:\\ \enquote{Which $y$ tends to have $\xv$ like these?}
\lz
\item \textbf{Discriminant} approaches use \textbf{empirical risk minimization} based on a suitable loss function:\\ \enquote{What is the best prediction for $y$ given these $\xv$?}
\end{itemize}
\end{vbframe}

\begin{vbframe}{Generative approach}

  The \textbf{generative approach}
  models $\pdfxyk$, usually by making some assumptions about the structure of these distributions, and employs the Bayes theorem:
  $$\pikx = \postk = \frac{\P(x | y = k) \P(y = k)}{\P(x)} = \frac{\pdfxyk \pik}{\sumjg \pdfxyj \pi_j}$$

  % The discriminant functions are then $\pikx$ or $\lpdfxyk + \lpik$.\\

  Prior class probabilities $\pik$ are easy to estimate from the training data.

  \lz

  Examples:
  \begin{itemize}
  \item Naive Bayes classifier
  \item Linear discriminant analysis (generative, linear)
  \item Quadratic discriminant analysis (generative, not linear)
  \end{itemize}

{\small Note: LDA and QDA have 'discriminant' in their name, but are generative models! (\dots sorry.)}
\end{vbframe}

\begin{vbframe}{Discriminant approach}
  The \textbf{discriminant approach} tries to optimize the discriminant functions directly, usually via empirical
  risk minimization.
  $$ \fh = \argmin_{f \in \Hspace} \riskef = \argmin_{f \in \Hspace} \sumin \Lxyi.$$
  \lz
  Examples:
  \begin{itemize}
  \item Logistic regression (discriminant, linear)
  \item Neural networks 
  \item Support vector machines
  \end{itemize}

% \lz
% Representation and optimization depend on the specific learner.\\
% Evaluation via classification loss functions.

\end{vbframe}

\endlecture


\end{document}
