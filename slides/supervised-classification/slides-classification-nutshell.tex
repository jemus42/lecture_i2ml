\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

%\newcommand{\titlefigure}{figure_man/nutshell-titlefigure.png}
%Stock free image from pexels.com

\newcommand{\learninggoals}{
\item Understand basic concept of classifiers
\item Know basic definitions
\item Know concepts of discriminant approach, generative approach, logistic regression, naive bayes
}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
% \institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-% lmu.github.io/lecture\_i2ml}}
\date{}
\begin{document}

%\lecturechapter{In a Nutshell: ML-Basics}
\lecturechapter{Supervised Classification: In a Nutshell}
\lecture{Introduction to Machine Learning}
\sloppy

\begin{vbframe}{Classification Tasks}
\begin{itemize}
\item \small Learn functions that assign class labels to observation
\item \small Each observation belongs to exactly one class
\item \small The task can contain 2 (binary) or multiple (multiclass) classes
\end{itemize}

%\end{center} \hspace{0.1cm}

\begin{columns}  
\begin{column}{0.1\textwidth} 
\begin{center}
Training
\end{center}
\end{column}
\begin{column}{0.9\textwidth} 
\begin{center}
  \includegraphics[width = 0.75\textwidth]{slides/supervised-classification/figure_man/nutshell-classification-training-task.png}
\end{center}
\end{column}
\end{columns}
%\end{center}
%\begin{center}
\begin{columns}
\begin{column}{0.1\textwidth} 
\begin{center}
Prediction
\end{center}
\end{column}
\begin{column}{0.9\textwidth} 
\begin{center}
  \includegraphics[width = 0.75\textwidth]{slides/supervised-classification/figure_man/nutshell-classification-prediction-task.png} 
\end{center}
\end{column}
\end{columns}
\end{vbframe}

\begin{vbframe}{Basic Definitions}
\begin{itemize}
\item \small For every observation a model outputs the probability (probabilistic classifier) or score (scoring classifier) of each class
\item \small In the multiclass case the class label is usually assigned by choosing the class with the maximum score or probability
\item \small In the binary case a class label is assigned by choosing the class which probability or score exceeds a threshold value c
\end{itemize}

\vspace{5mm}

\begin{center}
  \includegraphics[width = \textwidth]{slides/supervised-classification/figure_man/nutshell-classification-label-assignment.png}
\end{center}

Two fundamental approaches exist to construct a classifier:
\begin{itemize}
\item \small The \textbf{generative approach} asks "Which class tends to have data like these?" 
\item \small The \textbf{discirminant approach} asks "What is the best prediction for the class given these data?"
\end{itemize}

\begin{center}
  \includegraphics[width = 0.85\textwidth]{slides/supervised-classification/figure_man/nutshell_classif_binary_task.png}
\end{center}
\end{vbframe}


%\begin{vbframe}{Linear Classifiers}
%\end{vbframe}

\begin{vbframe}{Logistic Regression}

\begin{itemize}
\item \small Logistic Regression is a \textbf{discriminant approach} for binary classification. It turns scores into probabilities with the logistic function.
\item \small You just need to compute the probability for \textbf{one} class. Usually class 1
\item \small If the probability exceeds a threshold value \textbf{c} class 1 is assigned
\end{itemize}

\begin{center}
  \includegraphics[width = 1\textwidth]{slides/supervised-classification/figure_man/nutshell-classif-logistic-regression.png}
\end{center}
\begin{columns}
\begin{column}{0.35\textwidth} 
\begin{center}
\includegraphics[width=\textwidth]{slides/supervised-classification/figure_man/nutshell-classification-text-box-logisticreg.png}
\end{center}
\end{column}
\begin{column}{0.55\textwidth} 
\begin{center}
  \includegraphics[width=1\textwidth]{slides/supervised-classification/figure/nutshell_classif_logistic_function.png}
\end{center}
\end{column}
\end{columns}



\end{vbframe}



%\begin{vbframe}{Discriminant Analysis}
%\end{vbframe}

\begin{vbframe}{Naive Bayes}
\begin{itemize}
\item \small Naive Bayes is a \textbf{generative multiclass approach}. It computes the class probability for each class based on the distribution of the data.
\item \small Example: Class probability of "not too happy" given the observation "fair" health:
\end{itemize}

\begin{columns}
\begin{column}{0.1\textwidth}
\begin{center}
Learning
\end{center}
\end{column}
\begin{column}{0.7\textwidth} 

  \includegraphics[width=1\textwidth]{slides/supervised-classification/figure_man/nutshell-classification-naive-bayes-learning.png}
\end{column}
\end{columns}

\newpage

\begin{columns}
\begin{column}{0.08\textwidth}
\begin{center}
Prediction
\end{center}
\end{column}
\begin{column}{0.75\textwidth} 

  \includegraphics[width=1\textwidth]{slides/supervised-classification/figure_man/nutshell-classification-naive-bayes-prediction.png}

\end{column}
\end{columns}





% TODO: include intuitive computing example for posterior class probability (excercises).

\end{vbframe}

\endlecture
\end{document}