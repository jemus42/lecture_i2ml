% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../../style/preamble.Rnw")
@
% Load all R packages and set up knitr
<<setup-r, child="../../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{Classification: - Linear Classifiers}
\lecture{Introduction to Machine Learning}
\framebreak

<<>>=
iris_petal <- makeClassifTask(data = iris[,-(1:2)], target = "Species")
iris_sepal <- makeClassifTask(data = iris[,-(3:4)], target = "Species")
iris_sepal_bin <- makeClassifTask(data = subset(iris[,-(3:4)], Species != "setosa"),
                                  target = "Species")
# plotLearnerPrediction(makeLearner("classif.kknn"), iris_petal, cv=0, prob.alpha = FALSE, gridsize = 100)
# plotLearnerPrediction(makeLearner("classif.kknn"), iris_sepal, cv=0, prob.alpha = FALSE, gridsize = 100)
plotLearnerPrediction(makeLearner("classif.kknn", k = 25),
                      iris_sepal_bin, cv = 0, prob.alpha = FALSE, gridsize = 400) +
  scale_fill_viridis_d()
@

\begin{vbframe}{Linear classifier}

  If these functions $f_k(x)$ can be specified as linear functions,
  we will call the classifier a \emph{linear classifier}. We can then write a
  decision boundary as $x^T\theta = 0$, which is a hyperplane separating two classes.

    \lz

  If only 2 classes exist (\textbf{binary classification}), we can simply use a single discriminant function $f(x) = f_1(x) - f_2(x)$
  (note that it would be more natural here to label the classes with \{+1, -1\} or \{0, 1\}).

  \lz
  Note that all linear classifiers can represent non-linear decision boundaries in our original input space if we include \emph{derived features} like higher order interactions, polynomials or other transformations of $x$ in the model.


  \framebreak

<<>>=
 plotLearnerPrediction(makeLearner("classif.logreg"),
                      iris_sepal_bin, cv = 0, prob.alpha = FALSE, gridsize = 400) +
  scale_fill_viridis_d()
@

\end{vbframe}

\endlecture

