% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../../style/preamble.Rnw")
@
% Load all R packages and set up knitr
<<setup-r, child="../../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{Classification: Linear Classifiers}
\lecture{Introduction to Machine Learning}
\framebreak


\begin{vbframe}{Linear Classifiers}

Linear classifiers are an important subclass of classification models. 
W

INSERT PIC here

Be careful:

f1(x)

f2(x) = 1 - f(x)

f1(x) >= f2(x)

f1(x) >= 1 - f1(x)


\begin{itemize}
\item Linear model in statistics: The model is linear in its parameters
\item Linear classifier: See this setcion.
\end{itemize}

\framebreak

Linear classifiers are an important subclass of classification models. 
If the discriminant function(s) $\fkx$ can be specified as linear function(s) (possibly through a rank-preserving,
monotone transformation $g: \R \to \R$), i. e. 

$$
  g(\fkx) = \bm{w}_k^\top \bm{x} + b_k,
$$

we will call the classifier a \textbf{linear classifier}. 
  
  
Note that this is different from the definition of a linear model in statistics:

\framebreak
  
VISUALIZE the score
SHOW THAT THRESHOLD CUT IS LINEAR

  
We can also easily show that the decision boundary between too classes $i$ and $j$ isa hyperplane. For every $x$ where there is a tie in scores: 

\begin{eqnarray*}
  \fix &=& \fjx \\
  g(\fix)) &=& g(\fjx) \\
  \bm{w}_i^\top \xv + b_i &=& \bm{w}_j^\top \xv + b_j \\
  \left(\bm{w}_i - \bm{w}_j\right)^\top \xv + \left(b_i - b_j\right) &=& 0 
  % \bm{w}_{ij}^\top \xv + b_{ij} &=& 0 
\end{eqnarray*}

This is a \textbf{hyperplane} separating two classes. 

% \lz

% Note that linear classifiers can represent non-linear decision boundaries in the original input space if we use derived features like higher order interactions, polynomial features, basis function expansions, etc.


\end{vbframe}

\begin{vbframe}{Linear classifier}

  If these functions $f_k(x)$ can be specified as linear functions,
  we will call the classifier a \emph{linear classifier}. We can then write a
  decision boundary as $x^T\theta = 0$, which is a hyperplane separating two classes.

    \lz

  If only 2 classes exist (\textbf{binary classification}), we can simply use a single discriminant function $f(x) = f_1(x) - f_2(x)$
  (note that it would be more natural here to label the classes with \{+1, -1\} or \{0, 1\}).

  \lz
  Note that all linear classifiers can represent non-linear decision boundaries in our original input space if we include \emph{derived features} like higher order interactions, polynomials or other transformations of $x$ in the model.


  \framebreak


\end{vbframe}

\begin{vbframe}{Linear Classifiers}
<<>>=
iris_petal <- makeClassifTask(data = iris[,-(1:2)], target = "Species")
iris_sepal <- makeClassifTask(data = iris[,-(3:4)], target = "Species")
iris_sepal_bin <- makeClassifTask(data = subset(iris[,-(3:4)], Species != "setosa"),
                                  target = "Species")
# plotLearnerPrediction(makeLearner("classif.kknn"), iris_petal, cv=0, prob.alpha = FALSE, gridsize = 100)
# plotLearnerPrediction(makeLearner("classif.kknn"), iris_sepal, cv=0, prob.alpha = FALSE, gridsize = 100)
plotLearnerPrediction(makeLearner("classif.kknn", k = 25),
                      iris_sepal_bin, cv = 0, prob.alpha = FALSE, gridsize = 400) +
  scale_fill_viridis_d()
@

\framebreak


<<>>=
 plotLearnerPrediction(makeLearner("classif.logreg"),
                      iris_sepal_bin, cv = 0, prob.alpha = FALSE, gridsize = 400) +
  scale_fill_viridis_d()
@

\end{vbframe}

\endlecture

