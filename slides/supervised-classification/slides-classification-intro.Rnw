% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../../style/preamble.Rnw")
@
% Load all R packages and set up knitr
<<setup-r, child="../../style/setup.Rnw", include = FALSE>>=
@

%! includes: basics-supervised, basics-riskminimization

\lecturechapter{ML Basics for Classification - Basic Terminology and Tasks}
\lecture{Introduction to Machine Learning}
\framebreak

% <<include=FALSE>>=
%   library(datasets)
% df <- as.data.frame(Titanic)
% titanic.raw <- NULL
% for (i in 1:4)
% {
%   titanic.raw <- cbind(titanic.raw,
%                        rep(as.character(df[,i]),df$Freq))
% }
% titanic.raw <- as.data.frame(titanic.raw)
% names(titanic.raw) <- names(df)[1:4]
% @
%
\begin{vbframe}{Classification}
% The Titanic dataset is a famous beginner's problem for binary classification (see for example \href{https://www.kaggle.com/c/titanic}{the Titanic competition on kaggle.com}).
% The goal is to classify the passengers of the Titanic into Survived $\in$ $\{Yes, No\}$ given information about the class they traveled in, the sex and the age.
%
%
%  % \column{0.5\textwidth}
% \begin{center}
% \textbf{Titanic Passengers} \\
% \vspace{0.25cm}
% <<>>=
%   kable(unique(titanic.raw)[c(1:8,12:15),], row.names = FALSE)
% @
%   \end{center}
%
%  \framebreak

 % \begin{vbframe}{Classification}
We want to assign new observations to known categories according to criteria learned from a training set.
{\centering \includegraphics[height = .7\textheight, width = .8\textwidth]{figure_man/classifier.pdf}}

% \end{vbframe}
%
%   \begin{vbframe}{Classification}

  Assume we are given a \emph{classification problem}:
  \begin{eqnarray*}
  & x \in \Xspace \quad & \text{feature vector}\\
  & y \in \Yspace = \gset \quad & \text{\emph{categorical} output variable (label)}\\
  &\D = \Dset & \text{observations of $x$ and $y$}
  \end{eqnarray*}


  Classification usually means to construct $g$ discriminant functions $f_1(x), \ldots f_g(x)$,
  so that we choose our class as
  $$h(x) = \argmax_k f_k(x)$$ for $k = 1, 2,\ldots, g$.

  \lz

  This divides the feature space into $g$ \emph{decision regions} $\{x \in \Xspace | h(x) = k\}$.
  These regions are separated by the \emph{decision boundaries} where ties occur between these
  regions.

\framebreak

<<>>=
iris_petal <- makeClassifTask(data = iris[,-(1:2)], target = "Species")
iris_sepal <- makeClassifTask(data = iris[,-(3:4)], target = "Species")
iris_sepal_bin <- makeClassifTask(data = subset(iris[,-(3:4)], Species != "setosa"),
                                  target = "Species")
# plotLearnerPrediction(makeLearner("classif.kknn"), iris_petal, cv=0, prob.alpha = FALSE, gridsize = 100)
# plotLearnerPrediction(makeLearner("classif.kknn"), iris_sepal, cv=0, prob.alpha = FALSE, gridsize = 100)
plotLearnerPrediction(makeLearner("classif.kknn", k = 25),
                      iris_sepal_bin, cv = 0, prob.alpha = FALSE, gridsize = 400) +
  scale_fill_viridis_d()
@


<<echo=FALSE, warning=FALSE, message=FALSE>>=
# n = 100
# library(kknn)
# bin.class = data.frame(x1 = runif(n=n, 0, 5),
#                         x2 = runif(n=n, -2, 4))
#
# bin.class $class = (bin.class $x1 + 0.5*bin.class $x2 + rnorm(n, sd = 0.3)) > 3
# bin.class $class = factor(bin.class $class, levels = c(TRUE,FALSE), labels = c(1,2))
# bin.class.task =  makeClassifTask('bin.class.task', bin.class , 'class')
# lrn1 = makeLearner("classif.kknn", predict.type='prob')
# plotLearnerPrediction(lrn1, bin.class.task, cv=0, prob.alpha = FALSE, gridsize = 400)

@
\end{vbframe}

\textbf{Binary Classification Task}
\lz
\begin{columns}[T]
  \begin{column}{0.5\textwidth}
    \structure{Goal}: Predict a class (or membership probabilities)
    \begin{itemize}
      \item $y$ is a categorical variable with two possible values
      \item Each observation belongs to exactly one class
    \end{itemize}
  \end{column}
  \begin{column}{0.5\textwidth}
<<classification-task-plot, fig.height=6, fig.width=6>>=
set.seed(1)
df2 = data.frame(x1 = c(rnorm(10, mean = 3), rnorm(10, mean = 5)), x2 = runif(10), class = rep(c("a", "b"), each = 10))
ggplot(df2, aes(x = x1, y = x2, shape = class, color = class)) +
  geom_point(size = 3) + geom_abline(slope = -.22, intercept = 1.7, linetype = "longdash") +
  scale_color_viridis_d()
@
  \end{column}
\end{columns}

\framebreak

\textbf{Binary Classification Task - Examples}

\begin{itemize}
  \item \textbf{Credits:} Predicting credit fraud or default risk based on transactions
  \item \textbf{Medical Diagnosis:} Medically testing whether a patient has a specific illness or not
  \item \textbf{Software:} Detecting whether an e-mail is spam or not by using its content
  \item \textbf{Lie Detection:} Determine truthfulness of statements from physiological cues
\end{itemize}

\framebreak

\textbf{Binary Classification Task - Lie Detection}
\vspace{-0.3cm}

\begin{center}
  % FIGURE SOURCE: https://www.bendbulletin.com/localstate/deschutescounty/3430324-151/fact-or-fiction-polygraphs-just-an-investigative-tool
  \includegraphics[width=0.72\textwidth]{figure_man/lie-detector-polygraph.jpg}
\end{center}
\vspace{-0.6cm}
\begin{flushright}
  \tiny https://www.bendbulletin.com/localstate/deschutescounty/3430324-151/fact-or-fiction-polygraphs-just-an-investigative-tool
\end{flushright}

\framebreak

\textbf{Multiclass Classification Task}
\lz
\begin{columns}[T]
  \begin{column}{0.5\textwidth}
    \structure{Goal}: Predict a class (or membership probabilities)
    \begin{itemize}
      \item $y$ is a categorical variable with more than two different unordered discrete values
      \item Each observation belongs to exactly one class
    \end{itemize}
  \end{column}
  \begin{column}{0.5\textwidth}
<<multi-classification-task-plot, fig.height=6, fig.width=6>>=
plotLearnerPrediction(makeLearner("classif.svm"), iris.task, c("Petal.Length", "Petal.Width")) +
  ggtitle("") +  scale_fill_viridis_d()
@
  \end{column}
\end{columns}

\framebreak

\textbf{Multiclass Classification Task - Examples}

\begin{itemize}
  \item \textbf{Image Recognition:} Deciding what animal (for example) a picture is showing
  \item \textbf{Stock Trading:} Identifying the best strategy for a specific stock (buy, sell, or wait) based on past prices
  \item \textbf{Biology:} Classifying plants and animals based on their exterior characteristics (e. g. iris flowers)
  \item \textbf{Medical Diagnosis:} Predicting a patients illness using the their symptoms
\end{itemize}

\framebreak

\textbf{Multiclass Classification Task - Medical Diagnosis}

\begin{center}
  % FIGURE SOURCE: https://symptoms.webmd.com
  \includegraphics[width=0.8\textwidth]{figure_man/webmd.png}
\end{center}
\vspace{-0.5cm}
\begin{flushright}
  \tiny https://symptoms.webmd.com
\end{flushright}

\framebreak

\lz
\textbf{Classification Models}

\begin{itemize}
  \item Most classification models yield scoring functions for each of the $g$ classes: $\fx = (f_1(x), \dots, f_g(x)) \in \R^g$.
  \item These are often called \textbf{discriminant functions}, their outputs are class scores or class probabilities.
  \item The actual classification rule is usually defined as: $h(x) = \displaystyle \argmax_{k \in \{1, \dots ,g\}} f_k(x)$
  % \item This classification rule also defines the \textbf{decision boundaries} in the feature space $\mathcal X.$
\end{itemize}

\framebreak



\endlecture

