% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../../style/preamble.Rnw")
@
% Load all R packages and set up knitr
<<setup-r, child="../../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{ML Basic for Classification - Discriminant Analysis}
\lecture{Introduction to Machine Learning}
\framebreak



\begin{vbframe}{Linear discriminant analysis (LDA)}

% Linear discriminant follows a similar idea. As before, we want to classify a categorical target $y \in \Yspace = \gset$ on basis of $x$.

% \lz

LDA follows a generative approach, each class density is modeled as a \emph{multivariate Gaussian}
with equal covariance, i. e. $\Sigma_k = \Sigma \quad \forall k$. \\
$$
\pdfxyk = \frac{1}{(2\pi)^{\frac{p}{2}}|\Sigma|^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(x - \mu_k)^T \Sigma^{-1} (x - \mu_k)\right)
$$

\lz
% \framebreak

Parameters $\theta$ are estimated in a straight-forward manner by estimating
\begin{eqnarray*}
\hat{\pi}_k &=& n_k / n,\text{ where $n_k$ is the number of class $k$ observations} \\
\hat{\mu}_k &=& \frac{1}{n_k}\sum_{i: \yi = k} \xi \\
\hat{\Sigma} &=& \frac{1}{n - g} \sum_{k=1}^g \sum_{i: \yi = k} (\xi - \hat{\mu}_k) (\xi - \hat{\mu}_k) ^T
\end{eqnarray*}

\framebreak

\begin{itemize}
  \item Each class fit as a Gaussian distribution over the feature space
  \item Different means but same covariance for all classes
  \item Rather restrictive model assumption.
\end{itemize}

<<echo = FALSE, warning=FALSE, message=FALSE, fig.height=4>>=
library(ggplot2)
library(MASS)
library(car)

# fake data
n = 300
classa = data.frame(mvrnorm(n = n, mu = c(2,2), Sigma = matrix(c(2, 0, 0, 2), ncol = 2, byrow = TRUE)))
classb = data.frame(mvrnorm(n = n, mu = c(10,7), Sigma = matrix(c(8, -6, -6, 8), ncol = 2, byrow = TRUE)))

# Target data frame
df = cbind(classa, rep("a", ncol(classa)))
colnames(df) = c("x1", "x2", "y")
foo = cbind(classb, rep("b", ncol(classb)))
colnames(foo) = c("x1", "x2", "y")
df = rbind(df, foo)

####### LDA
## all from scratch:
mu_a = c(mean(df[which(df$y == "a"), 1]), mean(df[which(df$y == "a"), 2]))
mu_b = c(mean(df[which(df$y == "b"), 1]), mean(df[which(df$y == "b"), 2]))

foo = matrix(c(0, 0, 0, 0), ncol = 2)
for (i in 1:nrow(df)) {
  if (df[i, "y"] == "a") {
    # i = 1
    foo = foo +  as.matrix(t(df[i, c(1, 2)] - mu_a)) %*% as.matrix(df[i, c(1, 2)] - mu_a)
  }
  else {
    foo = foo +  as.matrix(t(df[i, c(1, 2)] - mu_b)) %*% as.matrix(df[i, c(1, 2)] - mu_b)
  }
}
var_lda = 1 / (nrow(df) - 2) * foo

# create ellipsoids
a_ell = as.data.frame(ellipse(center = mu_a, shape = var_lda, radius = 2, draw = FALSE))
colnames(a_ell) = c("x1", "x2")
b_ell = as.data.frame(ellipse(center = mu_b, shape = var_lda, radius = 2, draw = FALSE))
colnames(b_ell) = c("x1", "x2")

dens_a =
  dens_b = expand.grid(
    X1 = seq(min(c(classa$X1, classb$X1)), max(c(classa$X1, classb$X1)), l = 50),
    X2 = seq(min(c(classa$X2, classb$X2)), max(c(classa$X2, classb$X2)), l = 50))
dens_a$dens = sqrt(mvtnorm::dmvnorm(dens_a[,1:2], m = mu_a, sigma = var_lda))
dens_b$dens = sqrt(mvtnorm::dmvnorm(dens_b[,1:2], m = mu_b, sigma = var_lda))

## ggplot
plot1 = ggplot(data = classa, aes(x = X1, y = X2, alpha = 0.2)) +
  geom_contour(data = dens_a, aes(z = dens), col = "blue", alpha = .2, lwd = 1, bins = 5) +
  geom_contour(data = dens_b, aes(z = dens), col = "red", alpha = .2, lwd = 1, bins = 5) +
  geom_point(col = "blue", show.legend = FALSE) +
  geom_point(data = classb, aes(x = X1, y = X2, alpha = 0.2), col = "red", show.legend = FALSE) +
  coord_fixed()
plot1

@


\framebreak

For the posterior probability of class $k$ it follows:
\begin{eqnarray*}
\pikx &\propto& \pi_k \cdot \pdfxyk \\
&\propto& \pi_k \exp\left(- \frac{1}{2} x^T\Sigma^{-1}x - \frac{1}{2} \mu_k^T \Sigma^{-1} \mu_k + x^T \Sigma^{-1} \mu_k \right) \\
&=& \exp\left(\log \pi_k  - \frac{1}{2} \mu_k^T \Sigma^{-1} \mu_k + x^T \Sigma^{-1} \mu_k \right) \exp\left(- \frac{1}{2} x^T\Sigma^{-1}x\right) \\
&=& \exp\left(\theta_{0k} + x^T \theta_k\right) \exp\left(- \frac{1}{2} x^T\Sigma^{-1}x\right)
\end{eqnarray*}

by defining
$\theta_{0k} := \log \pi_k  - \frac{1}{2} \mu_k^T \Sigma^{-1} \mu_k$ and $\theta_k := \Sigma^{-1} \mu_k$.

\framebreak

Finally, the posterior probability becomes

$$
\pikx = \frac{\pi_k \cdot \pdfxyk }{\pdfx} = \frac{\exp(\theta_{0k} + x^T \theta_k)}{\sum_j \exp(\theta_{0j} + x^T \theta_j)}
$$

(the term $\exp(- \frac{1}{2} x^T\Sigma^{-1}x)$ will cancel out in numerator and denominator).

\lz

And (simplified) discriminant functions can be defined as
$$ f_k(x) =  \theta_{0k} + x^T \theta_k $$
Hence, LDA defines a \emph{linear classifier} with linear decision boundaries.

% \lz

% \framebreak

% If we compare the likelihood of two classes $k, l \in \gset$ via the log-odds, we end up in a linear function of x

% \small
% \begin{eqnarray*}
% \log \frac{\pikx}{\pi_l(x)}&=& \log\exp(\theta_{0k} + x^T \theta_k)-\log\exp(\theta_{0l} + x^T \theta_l) \\
% &=& (\theta_{0k} - \theta_{0l}) + x^T(\theta_k-\theta_l)
% \end{eqnarray*}

% \normalsize
% This equation is linear in $x$, so the decision boundary between the classes can
% only be linear. Linear discriminant analysis provides a linear classifier.



% Finally we will predict the most probable category given $x$

% $$
% \yh = h(x) = \argmax_{k \in \gset} \pikx.
% $$

\framebreak

<<echo = FALSE, warning=FALSE, message=FALSE>>=
plotLearnerPrediction("classif.lda",
                      makeClassifTask(data = iris[,-(1:2)], target = "Species"),
                      cv = 0) +
  scale_fill_viridis_d()
@

\end{vbframe}


\begin{vbframe}{Quadratic discriminant analysis (QDA)}

QDA is a direct generalization of LDA, where the class densities are now Gaussians with unequal covariances $\Sigma_k$.
$$
\pdfxyk = \frac{1}{(2\pi)^{\frac{p}{2}}|\Sigma_k|^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k)\right)
$$

\lz

Parameters are estimated in a straight-forward manner by:\\
\begin{eqnarray*}
\hat{\pi}_k &=& \frac{n_k}{n},\text{ where $n_k$ is the number of class $k$ observations} \\
\hat{\mu}_k &=& \frac{1}{n_k}\sum_{i: \yi = k} \xi \\
\hat{\Sigma_k} &=& \frac{1}{n_k - 1} \sum_{i: \yi = k} (\xi - \hat{\mu}_k) (\xi - \hat{\mu}_k)^T \\
\end{eqnarray*}

\framebreak

\begin{itemize}
  \item Covariance matrices can differ over classes.
  \item Yields better data fit but also requires estimation of more parameters.
\end{itemize}

<<echo = FALSE, warning=FALSE, message=FALSE, fig.height = 4.5>>=
########### QDA
mu_a = c(mean(df[which(df$y == "a"), 1]), mean(df[which(df$y == "a"), 2]))
mu_b = c(mean(df[which(df$y == "b"), 1]), mean(df[which(df$y == "b"), 2]))

foo_a = matrix(c(0, 0, 0, 0), ncol = 2)
for (i in 1:nrow(df[which(df$y == "a"), ])) {
    foo_a = foo_a +  as.matrix(t(df[which(df$y == "a"), ][i, c(1, 2)] - mu_a)) %*% as.matrix(df[which(df$y == "a"), ][i, c(1, 2)] - mu_a)
}
var_qda_a = 1 / (nrow(classa) - 1) * foo_a

foo_b = matrix(c(0, 0, 0, 0), ncol = 2)
for (i in 1:nrow(df[which(df$y == "b"), ])) {
  foo_b = foo_b +  as.matrix(t(df[which(df$y == "b"), ][i, c(1, 2)] - mu_b)) %*% as.matrix(df[which(df$y == "b"), ][i, c(1, 2)] - mu_b)
}
var_qda_b = (1 / (nrow(classb) - 1) ) * foo_b

# create ellipsoids
a_ell = as.data.frame(ellipse(center = mu_a, shape = var_qda_a, radius = 2, draw = FALSE))
colnames(a_ell) = c("x1", "x2")
b_ell = as.data.frame(ellipse(center = mu_b, shape = var_qda_b, radius = 2, draw = FALSE))
colnames(b_ell) = c("x1", "x2")

dens_a =
  dens_b = expand.grid(
    X1 = seq(min(c(classa$X1, classb$X1)), max(c(classa$X1, classb$X1)), l = 50),
    X2 = seq(min(c(classa$X2, classb$X2)), max(c(classa$X2, classb$X2)), l = 50))
dens_a$dens = sqrt(mvtnorm::dmvnorm(dens_a[,1:2], m = mu_a, sigma = var_qda_a))
dens_b$dens = sqrt(mvtnorm::dmvnorm(dens_b[,1:2], m = mu_b, sigma = var_qda_b))

## ggplot
plot1 = ggplot(data = classa, aes(x = X1, y = X2, alpha = 0.2)) +
  geom_contour(data = dens_a, aes(z = dens), col = "blue", alpha = .2, lwd = 1, bins = 5) +
  geom_contour(data = dens_b, aes(z = dens), col = "red", alpha = .2, lwd = 1, bins = 5) +
  geom_point(col = "blue", show.legend = FALSE) +
  geom_point(data = classb, aes(x = X1, y = X2, alpha = 0.2), col = "red", show.legend = FALSE) +
  coord_fixed()
plot1
@

\framebreak



\begin{eqnarray*}
\pikx &\propto& \pi_k \cdot \pdfxyk \\
&=& \pi_k |\Sigma_k|^{-\frac{1}{2}}\exp(- \frac{1}{2} x^T\Sigma_k^{-1}x - \frac{1}{2} \mu_k^T \Sigma_k^{-1} \mu_k + x^T \Sigma_k^{-1} \mu_k )
\end{eqnarray*}

Taking the log of the above, we can define a discriminant function that is quadratic in $x$.

$$
\log \pi_k - \frac{1}{2} \log |\Sigma_k| - \frac{1}{2} x^T\Sigma_k^{-1}x - \frac{1}{2} \mu_k^T \Sigma_k^{-1} \mu_k + x^T \Sigma_k^{-1} \mu_k
$$


% Let's look at the log-odds now. \\

% \begin{eqnarray*}
% \log \frac{\pikx}{\pi_g(x)}&=& \log \frac{\pi_k}{\pi_g}
% - \frac{1}{2} \log \frac{|\Sigma_k|}{|\Sigma_g|}
% + x^T(\Sigma_k^{-1}\mu_k - \Sigma_g^{-1}\mu_g) \\
% &-& \frac{1}{2} x^T (\Sigma_k^{-1} - \Sigma_g^{-1})x
% - \frac{1}{2} x^T (\mu_k^T\Sigma_k^{-1}\mu_k - \mu_g^T\Sigma_g^{-1}\mu_g)
% \end{eqnarray*}

% We see that this function is quadratic in $x$, hence we obtain a quadratic decision boundary.

% \framebreak


% Finally we will predict again the most probable category given $x$

% $$
% \yh = h(x) = \argmax_{k \in \gset} \pikx.
% $$

\framebreak

<<fig.height=5>>=
plotLearnerPrediction("classif.qda",
                      makeClassifTask(data = iris[,-(1:2)], target = "Species"),
                      cv = 0) +
  scale_fill_viridis_d()

@

\end{vbframe}
\endlecture

