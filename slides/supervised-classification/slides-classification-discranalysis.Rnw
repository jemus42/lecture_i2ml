% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../../style/preamble.Rnw")
@
% Load all R packages and set up knitr
<<setup-r, child="../../style/setup.Rnw", include = FALSE>>=
@

% Defines macros and environments
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/probmodel.tex}

%! includes: classification-basicdefs

\lecturechapter{Classification: Discriminant Analysis}
\lecture{Introduction to Machine Learning}
\framebreak



\begin{vbframe}{Linear discriminant analysis (LDA)}

% Linear discriminant follows a similar idea. As before, we want to classify a categorical target $y \in \Yspace = \gset$ on basis of $x$.

% \lz
LDA follows a generative approach

$$\pikx = \postk = \frac{\P(x | y = k) \P(y = k)}{\P(x)} = \frac{\pdfxyk \pik}{\sumjg \pdfxyj \pi_j}$$
  
where we now have to pick a distributional form for $\pdfxyk$.

\framebreak

LDA assumes that each class density is modeled as a \emph{multivariate Gaussian}:

$$
\pdfxyk = \frac{1}{(2\pi)^{\frac{p}{2}}|\Sigma|^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(\xv - \muk)^T \Sigma^{-1} (\xv - \muk)\right)
$$

with equal covariance, i. e. $\Sigma_k = \Sigma \quad \forall k$. \\
  % \item Each class fit as a Gaussian distribution over the feature space
  % \item Different means but same covariance for all classes
  % \item Rather restrictive model assumption
% \end{itemize}

% \framebreak

% Generative approach, each class as \emph{multivariate Gaussian}
% with equal covariance, i. e. $\Sigma_k = \Sigma \quad \forall k$. \\
<<echo = FALSE, warning=FALSE, message=FALSE, fig.height=3.5>>=
library(ggplot2)
library(MASS)
library(car)

# fake data
n = 300
classa = data.frame(mvrnorm(n = n, mu = c(2,2), Sigma = matrix(c(2, 0, 0, 2), ncol = 2, byrow = TRUE)))
classb = data.frame(mvrnorm(n = n, mu = c(10,7), Sigma = matrix(c(8, -6, -6, 8), ncol = 2, byrow = TRUE)))

# Target data frame
df = cbind(classa, rep("a", ncol(classa)))
colnames(df) = c("x1", "x2", "y")
foo = cbind(classb, rep("b", ncol(classb)))
colnames(foo) = c("x1", "x2", "y")
df = rbind(df, foo)

####### LDA
## all from scratch:
mu_a = c(mean(df[which(df$y == "a"), 1]), mean(df[which(df$y == "a"), 2]))
mu_b = c(mean(df[which(df$y == "b"), 1]), mean(df[which(df$y == "b"), 2]))

foo = matrix(c(0, 0, 0, 0), ncol = 2)
for (i in 1:nrow(df)) {
  if (df[i, "y"] == "a") {
    # i = 1
    foo = foo +  as.matrix(t(df[i, c(1, 2)] - mu_a)) %*% as.matrix(df[i, c(1, 2)] - mu_a)
  }
  else {
    foo = foo +  as.matrix(t(df[i, c(1, 2)] - mu_b)) %*% as.matrix(df[i, c(1, 2)] - mu_b)
  }
}
var_lda = 1 / (nrow(df) - 2) * foo

# create ellipsoids
a_ell = as.data.frame(ellipse(center = mu_a, shape = var_lda, radius = 2, draw = FALSE))
colnames(a_ell) = c("x1", "x2")
b_ell = as.data.frame(ellipse(center = mu_b, shape = var_lda, radius = 2, draw = FALSE))
colnames(b_ell) = c("x1", "x2")

dens_a =
  dens_b = expand.grid(
    X1 = seq(min(c(classa$X1, classb$X1)), max(c(classa$X1, classb$X1)), l = 50),
    X2 = seq(min(c(classa$X2, classb$X2)), max(c(classa$X2, classb$X2)), l = 50))
dens_a$dens = sqrt(mvtnorm::dmvnorm(dens_a[,1:2], m = mu_a, sigma = var_lda))
dens_b$dens = sqrt(mvtnorm::dmvnorm(dens_b[,1:2], m = mu_b, sigma = var_lda))

## ggplot
plot1 = ggplot(data = classa, aes(x = X1, y = X2, alpha = 0.2)) +
  geom_contour(data = dens_a, aes(z = dens), col = "blue", alpha = .2, lwd = 1, bins = 5) +
  geom_contour(data = dens_b, aes(z = dens), col = "red", alpha = .2, lwd = 1, bins = 5) +
  geom_point(col = "blue", show.legend = FALSE) +
  geom_point(data = classb, aes(x = X1, y = X2, alpha = 0.2), col = "red", show.legend = FALSE) +
  coord_fixed()
plot1

@


\framebreak
Parameters $\theta$ are estimated in a straight-forward manner by estimating
\begin{eqnarray*}
\hat{\pi}_k &=& n_k / n,\text{ where $n_k$ is the number of class $k$ observations} \\
\hat{\mu}_k &=& \frac{1}{n_k}\sum_{i: \yi = k} \xi \\
\hat{\Sigma} &=& \frac{1}{n - g} \sum_{k=1}^g \sum_{i: \yi = k} (\xi - \hat{\mu}_k) (\xi - \hat{\mu}_k) ^T
\end{eqnarray*}

<<echo = FALSE, warning=FALSE, message=FALSE, fig.height=2.5>>=
plot1
@
\end{vbframe}

\begin{vbframe}{LDA as linear classifier}

  Because of the equal covariance structure of all class-specific Gaussian, the decision boundaries
  of LDA are linear.

<<echo = FALSE, warning=FALSE, message=FALSE, fig.height=5>>=
plotLearnerPrediction("classif.lda",
                      makeClassifTask(data = iris[,-(1:2)], target = "Species"),
                      cv = 0) +
  scale_fill_viridis_d()
@

\framebreak


We can formally show that LDA is a linear classifier, by showing that the posterior probabilities
can be written as linear scoring functions - up to any isotonic / rank-preserving transformation.

$$
  \pikx = \frac{\pi_k \cdot \pdfxyk }{\pdfx} = \frac{\pi_k \cdot \pdfxyk}{\sumjg \pi_j \cdot \pdfxyj}
$$

As the denominator is the same for all classes we only need to consider 
$$\pi_k \cdot \pdfxyk$$ 
and show that this can be written as a linear function of $\xv$.

\framebreak


% For the posterior probability of class $k$ it follows:
\begin{eqnarray*}
& \pi_k \cdot \pdfxyk \\
  \propto& \pi_k \exp\left(- \frac{1}{2} \xv^T\Sigma^{-1}x - \frac{1}{2} \muk^T \Sigma^{-1} \muk + \xv^T \Sigma^{-1} \muk \right) \\
=& \exp\left(\log \pi_k  - \frac{1}{2} \muk^T \Sigma^{-1} \muk + \xv^T \Sigma^{-1} \muk \right) \exp\left(- \frac{1}{2} \xv^T\Sigma^{-1}\xv\right) \\
=& \exp\left(\thetab_{0k} + \xv^T \thetab_k\right) \exp\left(- \frac{1}{2} \xv^T\Sigma^{-1}\xv\right)\\
\propto& \exp\left(\thetab_{0k} + x^T \thetab_k\right) 
\end{eqnarray*}

by defining
$\thetab_{0k} := \log \pi_k  - \frac{1}{2} \muk^T \Sigma^{-1} \muk$ and $\thetab_k := \Sigma^{-1} \muk$.

\lz

We have again left-out all constants which are the same for all classes $k$, 
  so the normalizing constant of our Gaussians and 
  $\exp\left(- \frac{1}{2} \xv^T\Sigma^{-1}\xv\right)$)

\lz

By finally taking the log, we can write our transformed scores as linear:  

$$ f_k(x) =  \thetab_{0k} + x^T \thetab_k $$

\end{vbframe}


\begin{vbframe}{Quadratic discriminant analysis (QDA)}

QDA is a direct generalization of LDA, where the class densities are now Gaussians with unequal covariances $\Sigma_k$.
$$
\pdfxyk = \frac{1}{(2\pi)^{\frac{p}{2}}|\Sigma_k|^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(\xv - \muk)^T \Sigma_k^{-1} (\xv - \muk)\right)
$$

\lz

Parameters are estimated in a straight-forward manner by:\\
\begin{eqnarray*}
\hat{\pi}_k &=& \frac{n_k}{n},\text{ where $n_k$ is the number of class $k$ observations} \\
\hat{\mu}_k &=& \frac{1}{n_k}\sum_{i: \yi = k} \xi \\
\hat{\Sigma_k} &=& \frac{1}{n_k - 1} \sum_{i: \yi = k} (\xi - \hat{\mu}_k) (\xi - \hat{\mu}_k)^T \\
\end{eqnarray*}

\framebreak

\begin{itemize}
  \item Covariance matrices can differ over classes.
  \item Yields better data fit but also requires estimation of more parameters.
\end{itemize}

<<echo = FALSE, warning=FALSE, message=FALSE, fig.height = 4.5>>=
########### QDA
mu_a = c(mean(df[which(df$y == "a"), 1]), mean(df[which(df$y == "a"), 2]))
mu_b = c(mean(df[which(df$y == "b"), 1]), mean(df[which(df$y == "b"), 2]))

foo_a = matrix(c(0, 0, 0, 0), ncol = 2)
for (i in 1:nrow(df[which(df$y == "a"), ])) {
    foo_a = foo_a +  as.matrix(t(df[which(df$y == "a"), ][i, c(1, 2)] - mu_a)) %*% as.matrix(df[which(df$y == "a"), ][i, c(1, 2)] - mu_a)
}
var_qda_a = 1 / (nrow(classa) - 1) * foo_a

foo_b = matrix(c(0, 0, 0, 0), ncol = 2)
for (i in 1:nrow(df[which(df$y == "b"), ])) {
  foo_b = foo_b +  as.matrix(t(df[which(df$y == "b"), ][i, c(1, 2)] - mu_b)) %*% as.matrix(df[which(df$y == "b"), ][i, c(1, 2)] - mu_b)
}
var_qda_b = (1 / (nrow(classb) - 1) ) * foo_b

# create ellipsoids
a_ell = as.data.frame(ellipse(center = mu_a, shape = var_qda_a, radius = 2, draw = FALSE))
colnames(a_ell) = c("x1", "x2")
b_ell = as.data.frame(ellipse(center = mu_b, shape = var_qda_b, radius = 2, draw = FALSE))
colnames(b_ell) = c("x1", "x2")

dens_a =
  dens_b = expand.grid(
    X1 = seq(min(c(classa$X1, classb$X1)), max(c(classa$X1, classb$X1)), l = 50),
    X2 = seq(min(c(classa$X2, classb$X2)), max(c(classa$X2, classb$X2)), l = 50))
dens_a$dens = sqrt(mvtnorm::dmvnorm(dens_a[,1:2], m = mu_a, sigma = var_qda_a))
dens_b$dens = sqrt(mvtnorm::dmvnorm(dens_b[,1:2], m = mu_b, sigma = var_qda_b))

## ggplot
plot1 = ggplot(data = classa, aes(x = X1, y = X2, alpha = 0.2)) +
  geom_contour(data = dens_a, aes(z = dens), col = "blue", alpha = .2, lwd = 1, bins = 5) +
  geom_contour(data = dens_b, aes(z = dens), col = "red", alpha = .2, lwd = 1, bins = 5) +
  geom_point(col = "blue", show.legend = FALSE) +
  geom_point(data = classb, aes(x = X1, y = X2, alpha = 0.2), col = "red", show.legend = FALSE) +
  coord_fixed()
plot1
@

\framebreak



\begin{eqnarray*}
\pikx &\propto& \pi_k \cdot \pdfxyk \\
&\propto& \pi_k |\Sigma_k|^{-\frac{1}{2}}\exp(- \frac{1}{2} \xv^T\Sigma_k^{-1}\xv - \frac{1}{2} \muk^T \Sigma_k^{-1} \muk + \xv^T \Sigma_k^{-1} \muk )
\end{eqnarray*}

Taking the log of the above, we can define a discriminant function that is quadratic in $x$.

$$
\log \pi_k - \frac{1}{2} \log |\Sigma_k| - \frac{1}{2} \muk^T \Sigma_k^{-1} \muk + \xv^T \Sigma_k^{-1} \muk - \frac{1}{2} \xv^T\Sigma_k^{-1}\xv $$


% Let's look at the log-odds now. \\

% \begin{eqnarray*}
% \log \frac{\pikx}{\pi_g(x)}&=& \log \frac{\pi_k}{\pi_g}
% - \frac{1}{2} \log \frac{|\Sigma_k|}{|\Sigma_g|}
% + x^T(\Sigma_k^{-1}\muk - \Sigma_g^{-1}\mu_g) \\
% &-& \frac{1}{2} x^T (\Sigma_k^{-1} - \Sigma_g^{-1})x
% - \frac{1}{2} x^T (\muk^T\Sigma_k^{-1}\muk - \mu_g^T\Sigma_g^{-1}\mu_g)
% \end{eqnarray*}

% We see that this function is quadratic in $x$, hence we obtain a quadratic decision boundary.

% \framebreak


% Finally we will predict again the most probable category given $x$

% $$
% \yh = h(x) = \argmax_{k \in \gset} \pikx.
% $$

\framebreak

<<fig.height=5>>=
plotLearnerPrediction("classif.qda",
                      makeClassifTask(data = iris[,-(1:2)], target = "Species"),
                      cv = 0) +
  scale_fill_viridis_d()

@

\end{vbframe}
\endlecture

