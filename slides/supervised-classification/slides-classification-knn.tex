\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R



\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}
\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
  %Define standard arrow tip
  >=stealth',
  %Define style for boxes
  punkt/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=6.5em,
    minimum height=2em,
    text centered},
  % Define arrow style
  pil/.style={
    ->,
    thick,
    shorten <=2pt,
    shorten >=2pt,}
}
\usepackage{subfig}


% Defines macros and environments
\input{../../style/common.tex}
% \input{common.tex}

%\usetheme{lmu-lecture}
\newcommand{\titlefigure}{figure/reg_class_knn_1}
\newcommand{\learninggoals}{
\item Understand the idea of k-NN classification
\item Know how the hyperparameter k affects the results of k-NN classification}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}



\begin{document}
% Set style/preamble.Rnw as parent.

% Load all R packages and set up knitr

% This file loads R packages, configures knitr options and sets preamble.Rnw as parent file
% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...








% Defines macros and environments
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

%! includes: classification-basicdefs, regression-knn

\lecturechapter{Classification: K-Nearest Neighbors}
\lecture{Introduction to Machine Learning}


\begin{vbframe}{$k$-Nearest Neighbors}

For each point to predict:  
\begin{itemize}
\item Compute $k$-nearest neighbours in training data $N_k(\xv)$
\item Average output $y$ of these $k$ neighbors 
% \end{itemize}

% \begin{itemize}
\item For regression: \\
$$
\fxh = \frac{1}{k} \sum_{i: \xi \in N_k(\xv)} \yi
$$
\item For classification in $g$ groups, a majority vote is used: \\
$$
\hxh = \argmax_{\ell \in \gset} \sum_{i: \xi \in N_k(\xv)} \I(\yi = \ell)
$$
And posterior probabilities can be estimated with:
$$
\hat{\pi}_{\ell}(\xv)= \frac{1}{k} \sum_{i: \xi \in N_k(\xv)} \I(\yi = \ell)
$$
\end{itemize}

\framebreak

Example with subset of iris data ($k = 3$): \\
\begin{columns}[T]
  \begin{column}{0.5\textwidth}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\textwidth]{figure/reg_class_knn_1} 

}



\end{knitrout}
  \end{column}
  \begin{column}{0.5\textwidth}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\begin{tabular}{l|r|r|l|r}
\hline
  & SL & SW & Species & dist\\
\hline
52 & 6.4 & 3.2 & versicolor & 0.200\\
\hline
59 & 6.6 & 2.9 & versicolor & 0.224\\
\hline
\rowcolor[HTML]{DFECFF}  \textcolor{black}{75} & \textcolor{black}{6.4} & \textcolor{black}{2.9} & \textcolor{black}{versicolor} & \textcolor{black}{0.100}\\
\hline
76 & 6.6 & 3.0 & versicolor & 0.200\\
\hline
98 & 6.2 & 2.9 & versicolor & 0.224\\
\hline
104 & 6.3 & 2.9 & virginica & 0.141\\
\hline
\rowcolor[HTML]{DFECFF}  \textcolor{black}{105} & \textcolor{black}{6.5} & \textcolor{black}{3.0} & \textcolor{black}{virginica} & \textcolor{black}{0.100}\\
\hline
111 & 6.5 & 3.2 & virginica & 0.224\\
\hline
116 & 6.4 & 3.2 & virginica & 0.200\\
\hline
\rowcolor[HTML]{DFECFF}  \textcolor{black}{117} & \textcolor{black}{6.5} & \textcolor{black}{3.0} & \textcolor{black}{virginica} & \textcolor{black}{0.100}\\
\hline
138 & 6.4 & 3.1 & virginica & 0.100\\
\hline
148 & 6.5 & 3.0 & virginica & 0.100\\
\hline
\end{tabular}


\end{knitrout}
  \end{column}
\end{columns}
\vspace{0.6cm}
$ \hat{\pi}_{setosa}(\xv_{new}) = \frac{0}{3} = 0\% $ \\
$ \hat{\pi}_{versicolor}(\xv_{new}) = \frac{1}{3} = 33\% $ \\
$ \hat{\pi}_{virginica}(\xv_{new}) = \frac{2}{3} = 67\% $ \\
$ \hh(\xv_{new}) = \textit{virginica}$

\end{vbframe}

\begin{vbframe}{$k$-NN: From small to large $k$}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth,height=7cm]{figure/reg_class_knn_2} 

}



\end{knitrout}
Complex, local model vs smoother, more global model

\end{vbframe}

\begin{vbframe}{$k$-NN as non-parametric model}
\begin{itemize}
\item $k$-NN is a lazy classifier, it has no real training step, it simply stores the complete data 
  - which are needed during prediction 
\item Hence, its parameters are the training data, there is no real compression of information
\item As the number of parameters grows with the number of training points, we call 
  $k$-NN a non-parametric model
\item Hence, $k$-NN is not based on any distributional or strong functional assumption,
  and can, in theory, model data situations of arbitrary complexity
\end{itemize}
\end{vbframe}

% \begin{vbframe}{Summary}
% \begin{itemize}
% \item Accuracy of k-NN can be severely degraded by the presence of noisy or irrelevant features,
  % or if the feature scales are not consistent with their importance.
% \item For $\yh$, we might inversely weigh neighbors with their distance to $x$, e.g., $w_i = 1/d(\xi, x)$
% \end{itemize}
% \end{vbframe}

\endlecture
\end{document}
