% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../../style/preamble.Rnw")
@
% Load all R packages and set up knitr
<<setup-r, child="../../style/setup.Rnw", include = FALSE>>=
@

%! includes: classification-intro

\lecturechapter{ML Basic for Classification - Generative Approach}
\lecture{Introduction to Machine Learning}
\framebreak
\begin{vbframe}{Classification Approaches}

  Two fundamental approaches exist to construct classifiers:\\
  The \textbf{generative approach} and the \textbf{discriminant approach}.

\lz
They tackle the classification problem from different angles:

\begin{itemize}
\item \emph{Generative} classification approaches assume a data generating process in which the distribution of the features $x$ is different for the various classes of the output $y$, and try to learn these conditional distributions:\\ \enquote{Which $y$ tends to have $x$ like these?}
\lz
\item \emph{Discriminant} approaches use \emph{empirical risk minimization} based on a suitable loss function:\\ \enquote{What is the best prediction for $y$ given these $x$?}
\end{itemize}
\end{vbframe}

\begin{vbframe}{Generative approach}

  The \emph{generative approach}
  models $\pdfxyk$, usually by making some assumptions about the structure of these distributions, and employs the Bayes theorem:
  $$\pikx = \postk = \frac{\P(x | y = k) \P(y = k)}{\P(x)} \propto \pdfxyk \pik $$
  to allow the computation of $\pikx$.

  The discriminant functions are then $\pikx$ or $\lpdfxyk + \lpik$.\\

  Prior class probabilities $\pik$ are easy to estimate from the training data.

  \lz

  Examples:
  \begin{itemize}
  \item Naive Bayes classifier
  \item Linear discriminant analysis (generative, linear)
  \item Quadratic discriminant analysis (generative, not linear)
  \end{itemize}

{\small Note: LDA and QDA have 'discriminant' in their name, but are generative models! (\dots sorry.)}

\framebreak


\lz
\lz

\textbf{Representation:} Conditional feature distributions $\pdfxyk$ and prior label probabilities $\pik$. \\
Often restricted to certain kinds of distributions (e.g. $\mathcal N(\mu,\Sigma)$) depending on the specific method, representation then via the distributions' parameters.

\lz

\textbf{Optimization:} Often analytic solutions (LDA, QDA); density estimation (Naive Bayes).

\lz

\textbf{Evaluation:} Classification loss functions. Typically: negative log posterior probability.

\end{vbframe}

\endlecture

