%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure/loss_quadratic_1.png}
\newcommand{\learninggoals}{
  \item Derive the risk minimizer of the L2-loss
  \item Derive the optimal constant model for the L2-loss
}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}

\lecturechapter{Regression Losses: L2-loss}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{L2-Loss}

\vspace*{-0.5cm}

$$
\Lxy = \left(y-\fx\right)^2 \quad \text{or} \quad \Lxy = 0.5 \left(y-\fx\right)^2
$$

\vspace*{-2mm}

\begin{itemize}
\item Tries to reduce large residuals (if residual is twice as large, loss is 4 times as large), hence outliers in $y$ can become problematic
\item Analytic properties: convex, differentiable (gradient no problem in loss minimization)
\item Residuals = Pseudo-residuals: \begin{footnotesize} $\tilde r = - \pd{0.5 (y-\fx)^2}{\fx} = y - \fx = r$\end{footnotesize}
\end{itemize}

% <<loss-quadratic-plot, echo=FALSE, results='hide', fig.height=3>>=
% x = seq(-2, 2, by = 0.01); y = x^2
% qplot(x, y, geom = "line", xlab = expression(y-f(x)), ylab = expression(L(y-f(x))))
% @

% \framebreak




\begin{center}
  \includegraphics[width = 9cm]{figure/loss_quadratic_2.png} \\
\end{center}

\end{vbframe}


\begin{vbframe}{L2-Loss: Risk Minimizer}

Let us consider the (true) risk for  $\Yspace = \R$ and the $L2$-Loss $\Lxy = \left(y-\fx\right)^2$ with unrestricted $\Hspace = \{f: \Xspace \to \R^g\}$. 

\begin{itemize}
\item By the law of total expectation
  \begin{eqnarray*}
    \riskf &=& \E_{xy} \left[\Lxy\right] 
    \\ &=& \E_x \left[\E_{y|x}\left[\Lxy~|~\xv = \xv\right]\right] \\
  &=& \E_x
  \left[\E_{y|x}\left[(y-\fx)^2~|\xv = \xv\right]\right]. 
  \end{eqnarray*}
% \item As our hypothesis space is not restricted at all, we can proceed quite \enquote{arbitrarily} when constructing our model $\hat f$.  

\item Since $\Hspace$ is unrestricted we can choose $f$ as we wish: At any point $\xv = \xv$ we can predict any value $c$ we want. The best point-wise prediction is the conditional mean
$$
  \fxh = \mbox{argmin}_c \E_{y|x}\left[(y - c)^2 ~|~ \xv = \xv \right]\overset{(*)}{=} \E_{y|x} \left[y ~|~ \xv \right]. 
$$

\begin{footnotesize}
$^{(*)}$ follows from:
  \begin{eqnarray*}
  && \mbox{argmin}_c \E\left[(y - c)^2\right] = \mbox{argmin}_c \underbrace{\E\left[(y - c)^2\right] - \left(\E[y] - c\right)^2}_{= \var[y - c] = \var[y]} + \left(\E[y] - c\right)^2 \\ &=& \mbox{argmin}_c \var[y] + \left(\E[y] - c\right)^2 = \E[y]. 
  \end{eqnarray*}
\end{footnotesize}

\end{itemize}


\end{vbframe}

\begin{vbframe}{L2-loss: Optimal Constant Model}

The optimal constant model in terms of the (theoretical) risk for the L2 loss is the expected value over $y$: 
  \begin{eqnarray*}
    \fx &=& \E_{y~|~\xv}\left[y~|~\xv\right] \overset{\text{drop } \xv}{=} \E_y \left[y\right] 
  \end{eqnarray*} 

The optimizer of the empirical risk is $\bar y$ (the empirical mean over $\yi$), which is the empirical estimate for $\E_y \left[y\right]$. 

\begin{center}
\includegraphics[width = 0.5\textwidth ]{figure/L2-loss.png} \\
\end{center}

\framebreak 

\textbf{Proof: }

\vspace{0.2cm}

For the optimal constant model $\fx = \theta$ for the L2-loss $\Lxy = \left(y - \fx\right)^2$ we solve the optimization problem 
$$
\argmin_{f \in \Hspace} \riskef = \argmin_{c \in \R} \sumin (\yi - \theta)^2. 
$$


We calculate the first derivative of $\riske$ w.r.t. $\theta$ and set it to $0$: 


\begin{eqnarray*}
\frac{\partial \risket}{\partial \theta} = 2 \sumin \left(\yi - \theta\right) &\overset{!}{=}& 0 \\
\sumin \yi - n \theta&=& 0 \\
\hat \theta&=& \frac{1}{n} \sumin \yi =: \bar y.
\end{eqnarray*}


\end{vbframe}

\begin{vbframe}{L2 loss means minimizing variance}

Rethinking what we just did:
We optimized for the constant, whose squared distance to all data points is minimal
(in sum, or on average). This turned out to be the mean.

\lz

What happens if we calculcate the incurred loss of $\hat{\theta} = \bar{y}$

\lz

Thats obviously $\riske = \sumin (\yi - \bar{y})^2$. 

\lz

Average this sum by $\frac{1}{n}$ or $\frac{1}{n-1}$, and we get the empirical variance. 

\lz

The same holds true for the pointwise construction / conditional distribution as considered in the slides before.

\end{vbframe}



\endlecture

\end{document}
