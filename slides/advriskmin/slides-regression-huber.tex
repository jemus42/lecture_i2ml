%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure_man/loss_huber_plot2.png}
\newcommand{\learninggoals}{
  \item Know the Huber loss
  \item Understand that there is no closed-form risk minimizer to the Huber loss
  \item Find the optimal constant model via iterative optimization 
}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}


\begin{document}

\lecturechapter{Regression Losses: Huber loss}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{Huber-Loss}

% Description
\vspace*{-0.3cm}

$$
\Lxy = \begin{cases}
  \frac{1}{2}(y - \fx)^2  & \text{ if } |y - \fx| \le \delta \\
  \delta |y - \fx|-\frac{1}{2}\delta^2 \quad & \text{ otherwise }
  \end{cases}, \delta > 0
$$

\begin{itemize}
\item Piece-wise combination of $L1$ and $L2$ loss
\item Analytic properties: Convex, differentiable, robust
\item Combines advantages of $L1$ and $L2$ loss: differentiable + robust
\end{itemize}

\vspace*{-1cm}

\begin{center}
\includegraphics[width = 9cm]{figure_man/loss_huber_plot1.png} \\
\end{center}

\framebreak

The following plot shows the Huber loss for different values of $\delta$.

\begin{center}
\includegraphics[width = 9cm]{figure_man/loss_huber_plot2.png} 
\end{center}

\end{vbframe}

\begin{vbframe}{L2-Loss: Risk Minimizer}

Let us consider the (true) risk for  $\Yspace = \R$ and the Huber loss
$$
\Lxy = \begin{cases}
  \frac{1}{2}(y - \fx)^2  & \text{ if } |y - \fx| \le \delta \\
  \delta |y - \fx|-\frac{1}{2}\delta^2 \quad & \text{ otherwise }
  \end{cases}, \delta > 0
$$

 with unrestricted $\Hspace = \{f: \Xspace \to \R^g\}$. 

\lz 

There is no closed-form solution for the risk minimizer of the Huber loss. 

\lz 

% https://stats.stackexchange.com/questions/298322/what-is-the-population-minimizer-for-huber-loss

However, the risk minimizer for the Huber loss is a \textbf{trimmed mean}: The risk minimizer is the (conditional) mean of values between two (conditional) quantiles. The location of the quantiles depends on the distribution as well as the value of $\delta$. 

\end{vbframe}


\begin{vbframe}{Huber Loss: Optimal constant model}

What is the optimal constant model $\fx = \thetab$ w.r.t. the Huber loss?

\vspace{-0.2cm}
\begin{eqnarray*}
f &=& \argmin_{f \in \Hspace} \riskef\\
\Leftrightarrow
\hat \thetab&=& \argmin_{\thetab\in \R} \sumin L(y, \thetab)
\end{eqnarray*}

with $L(y, \thetab) = \begin{cases}
  \frac{1}{2}(y - \thetab)^2  & \text{ if } |y - \thetab| \le \delta \\
  \delta |y - \thetab|-\frac{1}{2}\delta^2 \quad & \text{ otherwise }
  \end{cases}. $

\begin{itemize}
\item There is no closed-form solution.
\item Numerical optimization methods are necessary.
\item $\to$  the \enquote{optimal} solution can only be approached to a certain degree of accuracy via iterative optimization.
\end{itemize}

% We see that the constant model fitted w.r.t. Huber loss in fact lies between L1- and L2-Loss.

\end{vbframe}

\begin{vbframe}{$L1$- vs. $L2$- vs. Huber Loss}
\begin{itemize}
\item \textbf{Optimization}: $L2$ loss can be differentiated and the empirical risk minimization problem has a closed-form solution; $L1$ is not differentiable and has no closed-form solution.
\item \textbf{Robustness}: $L1$ loss penalizes large residuals less than $L2$ loss, thus, $L1$ loss is more robust to outliers.
\item Huber loss has the robustness of $L1$ loss where residuals are large and flexibility of $L2$ loss where residuals are small.
\end{itemize}

\vspace*{0.1cm}
\begin{center}
\includegraphics[width = 11cm ]{figure_man/different_losses.png}
\end{center}

\end{vbframe}



\endlecture

\end{document}