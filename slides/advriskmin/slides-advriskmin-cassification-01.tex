%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure_man/optimization_steps.jpeg}
\newcommand{\learninggoals}{
  \item Derive the risk minimizer of the 0-1-loss
  \item Derive the optimal constant model for the 0-1-loss
}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}


\begin{document}

\lecturechapter{0-1-Loss}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{0-1-Loss}

\begin{itemize}
  \item Let us first consider a classifier $\hx: \Xspace \to \Yspace$ with $\Yspace = \{1, ..., g\}$ that outputs discrete classes directly. 
  \item The most natural choice for $\Lhxy$ is of course the 0-1-loss that counts the number of misclassifications
  $$
  \Lhxy = \mathds{1}_{\{y \ne \hx\}} =
     \footnotesize \begin{cases} 1 \quad \text{ if } y \ne \hx \\ 0 \quad    \text{ if } y = \hx  \end{cases}.
  $$
  \item For the binary case ($g = 2$) we can express the 0-1-loss for a scoring classifier $\fx$ based on the margin $r := y\fx$

  $$
  \Lr = \mathds{1}_{\{r < 0\}} = \mathds{1}_{\{y\fx < 0\}} = \mathds{1}_{\{y \neq \hx\}}.
  $$

\end{itemize}


\framebreak 

$$
  \Lr = \mathds{1}_{\{r < 0\}} = \mathds{1}_{\{y\fx < 0\}} = \mathds{1}_{\{y \neq \hx\}} 
$$

\begin{itemize}
\item Intuitive, often what we are interested in.
\item Analytic properties:  Not continuous, even for linear $f$ the optimization problem is NP-hard and close to intractable.
\end{itemize}

\begin{center}
\includegraphics[width = 11cm ]{figure_man/0-1-loss.png} \\
\end{center}


\end{vbframe}


\begin{vbframe}{0-1-loss: Risk Minimizer}

By the law of total expection we can in general rewrite the risk as

\vspace*{-0.5cm}

\begin{eqnarray*}
  \riskf  & = & \Exy\left[\Lxy\right] = \E_x \left[ \E_{y|x} [ L(y, \fx) ] \right] \\
          & = & \E_x \left[\sum_{k \in \Yspace} L(k, \fx) \P(y = k~|~ \xv = \xv)\right]\,, 
          % & = & E_x \sum_{k \in \Yspace} L(k, \fx) \pikx,
\end{eqnarray*}

with $\P(y = k| \xv = \xv)$ being the posterior probability for class $k$.

\lz 

The risk minimizer for a general loss function $\Lxy$ is

\begin{eqnarray*}
  \fxh &=& \argmin_{f: \Xspace \to \R^g} \E_x \left[\sum_{k \in \Yspace} L(k, f(\bm{x})) \P(y = k| \xv = \xv)\right]\,.  \\
\end{eqnarray*}


\framebreak 

We compute the point-wise optimizer of the above term for the 0-1-loss (defined on a discrete classifier $\hx$): 

  \begin{eqnarray*}  
  \hxbayes &=& \argmin_{l \in \Yspace} \sum_{k \in \Yspace} L(k, l) \cdot \P(y = k~|~\xv = \xv) \\
  &=& \argmin_{l \in \Yspace} \sum_{k \ne l} \P(y = k~|~\xv = \xv) \\ 
  &=& \argmin_{l \in \Yspace} 1 - \P(y = l~|~\xv = \xv) \\
  &=& \argmax_{l \in \Yspace} \P(y = l~|~ \xv = \xv)
  \end{eqnarray*}

which corresponds to predicting the most probable class. 

\lz 

Note that some literature refers to $\hxbayes$ as the \textbf{Bayes optimal classifier} (without specifying the loss function). 

\lz 

The Bayes risk for the 0-1-loss (also: Bayes error rate) is 

\begin{eqnarray*}  
  \riskbayes &=& \E_x\left[\min_{l \in \Yspace} \left(1 - \P(y = l ~|~ \xv = \xv)\right)\right] \\
             &=& 1 - \E_x \left[\max_{l \in \Yspace} \P(y = l~|~ \xv = \xv)\right]\,.
\end{eqnarray*}

\lz 

In the binary case ($g = 2$), we define $\eta(\xv) := \P(y = 1 ~|~ \xv)$ and write risk minimizer and Bayes risk as follows:  

\begin{eqnarray*}
  \hxbayes &=& \begin{cases} 1 & \eta(\xv) \ge \frac{1}{2} \\ 0 & \eta(\xv) < \frac{1}{2}\end{cases} \\
  \riskbayes &=& \E_x\left[\min(\eta(\xv), 1 - \eta(\xv))\right] = \E_x\left[\max(\eta(\xv), 1 - \eta(\xv))\right] . 
\end{eqnarray*}



% \framebreak 


% If we can estimate $\Pxy$ very well via $\pikx$ through a stochastic model, we can compute the loss-optimal classifications point-wise. 

% \lz

% \textbf{Example}: Assume that our data is generated by a Mixture of Gaussian distributions. 

% \begin{center}
% \includegraphics[width = 9cm ]{figure_man/bayes_error_1.png} \\
% \end{center}

% \framebreak 

% We could try to approximate the $\P(y = k ~|~ \xv = \xv)$ via a stochastic model $\pix$ (shown as contour lines): 

% \begin{center}
% \includegraphics[width = 9cm ]{figure_man/bayes_error_2.png} \\
% \end{center}

% For each new $\xv$, we estimate the class probabilities directly with the stochastic model $\pix$, and our best point-wise prediction is 

% \begin{eqnarray*}
%   \fxh &=& \argmin_{f \in \Hspace} \sum_{k \in \Yspace} L(k, f(\bm{x})) \pix\,.  \\
% \end{eqnarray*}


% \lz 

% But usually we directly adapt to the loss via \textbf{empirical risk minimization}. 

% $$
% \fh = \argmin_{f \in \Hspace} \riske(f) = \argmin_{f \in \Hspace} \sumin \Lxyi.
% $$
  

\framebreak 

\textbf{Example: } Assume that $\P(y = 1) = \frac{1}{2}$ and $
\P(\xv ~|~ y) = \begin{cases}\phi_{\mu_1, \sigma^2}(\xv) \\ \phi_{\mu_2, \sigma^2}(\xv)\end{cases}$.


The decision boundary of the Bayes optimal classifier is shown in orange. 

\begin{center}
\includegraphics[width = 8cm ]{figure_man/bayes_error_4.png} \\
\end{center}

\framebreak 

The Bayes error rate is highlighted as red area. 

\begin{center}
\includegraphics[width = 9cm ]{figure_man/bayes_error_5.png} \\
\end{center}

\end{vbframe}

\endlecture

\end{document}