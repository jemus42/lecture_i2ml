%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@


\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\usepackage{booktabs}

\newcommand{\titlefigure}{figure/robustness.png}
\newcommand{\learninggoals}{
  % \item Understand why you should care about properties of loss functions
  \item Know the concept of robustness 
  \item Learn about analytical and computational properties of loss functions 
}

\title{Introduction to Machine Learning}
% \author{Bernd Bi{}schl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}


\begin{document}

\lecturechapter{Properties of Loss Functions}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{The role of Loss Functions}

Why should we care about how to choose the loss function $\Lxy$?

\begin{itemize}
% \item For regression, the loss usually only depends on residual $\Lxy = L\left(y - \fx\right) = L(\eps)$, this is a \emph{translation invariant} loss
\item \textbf{Statistical} properties: Choice of loss implies statistical assumptions on the distribution of $y ~|~ \xv = \xv$ (see \emph{Maximum Likelihood Estimation vs.
Empirical Risk Minimization})
\item \textbf{Robustness} properties: Some loss functions are more robust towards outliers than others
\item \textbf{Analytical} properties: The computational / optimization complexity of the problem
$$
\argmin_{\thetab \in \Theta} \risket
$$
is influenced by the choice of the loss function. 
\end{itemize}

\end{vbframe}


\begin{vbframe}{Basic Types of Regression Losses}

% https://davidrosenberg.github.io/mlcourse/Archive/2017/Lectures/3b.loss-functions.pdf

\begin{itemize}
  \item Regression losses usually only depend on the \textbf{ residuals}

  \vspace*{-0.5cm}

  \begin{eqnarray*}
    r &:=& y - \fx %\\
    %r^{(i)} &:=& \yi - \fxi .
  \end{eqnarray*}

  \item A loss is called \textbf{distance-based} if
  \begin{itemize}
    \item it can be written in terms of the residual
    $$
      \Lxy = \psi (r) \text{ for some } \psi: \R \to \R
    $$
    \item $\psi(r) = 0 \Leftrightarrow r = 0$ .
  \end{itemize}
  \item A loss is \textbf{translation-invariant}, if $L(y + a, \fx + a) = \Lxy$.
  \item Losses are called \textbf{symmetric} if $\Lxy = L\left(\fx, y\right)$. 
\end{itemize}
  % We will see later that in case of the L2-loss, pseudo-residuals correspond to the residuals - hence the name.

\end{vbframe}

\begin{vbframe}{Robustness}

Outliers (in $y$) have large residuals. For some losses large residuals impact the value of the risk much more than for other losses. 

\begin{table}[]
\begin{tabular}{lllll}
\toprule
$y$  & $\fxh$ & L1 & L2 & Huber ($\eps = 5$) \\ \midrule 
1 & 0 & 1 & 1 & 0.5 \\ 
5 & 0 & 5 & 25 & 12.5 \\ 
10 & 0 & 10 & 100 & 37.5 \\ 
50 & 0 & 50 & 2500  & 237.5 \\ \bottomrule
\end{tabular}
\end{table}

As a consequence, a model is less influenced by outliers than by inliers if ths loss is robust. 

\framebreak 

The L2 loss is an example for a loss function that is not very robust towards outliers. It penalizes large residuals more than the L1 or the Huber loss. The L1 and the Huber loss are thus regarded robust. 

\begin{center}
\includegraphics[width=0.8\textwidth]{figure/robustness.png}
\end{center}

\end{vbframe}

\begin{vbframe}{Analytical Properties}




\begin{itemize}
  \item Smoothness of the loss function \\
  \begin{itemize}
    \item In contrast do derivative-free methods, derivative-based methods require a certain level of smoothness  
    \item For example gradient descent requires differentiability of the problem, Newton-Raphson requires the problem to be twice differentiable
    \item A function is said to be $\mathcal{C}^k$ if it is $k$ times continuously differentiable. We call a function $\mathcal{C}^\infty$ if all higher order derivatives exist and are continuous. 
    \item If the loss function is not smooth, the risk minimization problem is not smooth either. 
    \item The L2 loss is smooth $\mathcal{C}^\infty$, L1 loss is $\mathcal{C}^0$ (continuous, not differentiable). 0-1-loss for classification is not even continuous. 
  \end{itemize}
  \framebreak 
  \item Convexity of loss functions
  \begin{itemize}
    \item Convex loss functions are desirable 
    \item Depending on the choice of the hypothesis space $\Hspace$, convexity of the loss function ensures convexity of the risk minimization problem. 
    \item If the problem is convex, local minima are global ones; we do not need to worry to get stuck in a local minimum
    \item For example, if $\Lxy$ is convex in its second argument, and $\fxt$ is linear in $\thetab$, then $\risket$ is convex; every local minimum of $\risket$ is a global one. If $L$ is not convex, $\risket$ might have multiple local minima (bad!).
  \end{itemize} 
\end{itemize}


\end{vbframe}

\begin{vbframe}{Convergence for separable problems}

The choice of the loss function may have further implications on the optimization of the problem. 


\end{vbframe}



% \begin{vbframe}{Visulizing Losses via Loss Plots}

% We call the plot that shows the point-wise error, i.e. the loss $\Lxy$ vs. the \textbf{residuals} $\eps := y - \fx$ (for regression), \textbf{loss plot}. The pseudo-residual corresponds to the slope of the tangent in $\left(y - \fx, \Lxy \right)$. 

% \vspace*{0.5cm}


% \begin{figure}
% \includegraphics[width = 1\linewidth]{figure_man/loss.png}
% \end{figure}

% %<<echo=FALSE, warning=FALSE, message=FALSE, fig.height = 4>>=
% %   xx = seq(-2, 2, by = 0.01); 
% %   yy = xx^2
% %   plot(xx, yy, type = "l", xlab = "y - f(x)", ylab = "L(y, f(x))")
% %   points(1, 1, col = "red")
% %   lines(x = c(1, 1), y = c(0, 1), col = "red")
% %   points(xx, 2 * xx - 1, type = "l", col = "red")
% %  @

% % We will define a similar plot for classification later on in this chapter.

% \end{vbframe}










% \section{Comparison of Loss Functions}
% 
% 
% \begin{vbframe}{Comparison of Loss Functions}
% 
% 
% \begin{center}
% \includegraphics[width = 10cm]{figure_man/2_7_loss_comparison_plot1.png} \\
% \end{center}
% 
% 
% \framebreak
% 
% 
% This plot shows the optimal constant model $\fx = \thetab$ (also \enquote{featureless predictor}) for the losses that have been discussed.
% 
% \begin{center}
% \includegraphics[width = 10cm, height = 6.5cm]{figure_man/2_8_loss_comparison_constant_model_plot1.png} \\
% \end{center}
% 
% \end{vbframe}




\endlecture

\end{document}