%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@


\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\usepackage{booktabs}

\newcommand{\titlefigure}{figure_man/vgg_example.png}
\newcommand{\learninggoals}{
  % \item Understand why you should care about properties of loss functions
  \item Know the concept of robustness 
  \item Learn about analytical and computational properties of loss functions 
  \item Understand that the loss function may influence convergence of the optimizer
}

\title{Introduction to Machine Learning}
% \author{Bernd Bi{}schl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}


\begin{document}

\lecturechapter{Properties of Loss Functions}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{The role of Loss Functions}

Why should we care about how to choose the loss function $\Lxy$?

\begin{itemize}
% \item For regression, the loss usually only depends on residual $\Lxy = L\left(y - \fx\right) = L(\eps)$, this is a \emph{translation invariant} loss
\item \textbf{Statistical} properties: Choice of loss implies statistical assumptions on the distribution of $y ~|~ \xv = \xv$ (see \emph{Maximum Likelihood Estimation vs.
Empirical Risk Minimization}). 
\item \textbf{Robustness} properties: Some loss functions are more robust towards outliers than others. 
\item \textbf{Analytical} properties: The computational / optimization complexity of the problem. 
$$
\argmin_{\thetab \in \Theta} \risket
$$
is influenced by the choice of the loss function. 
\end{itemize}

\end{vbframe}


\begin{vbframe}{Basic Types of Regression Losses}

% https://davidrosenberg.github.io/mlcourse/Archive/2017/Lectures/3b.loss-functions.pdf

\begin{itemize}
  \item Regression losses usually only depend on the \textbf{ residuals}

  \vspace*{-0.5cm}

  \begin{eqnarray*}
    r &:=& y - \fx %\\
    %r^{(i)} &:=& \yi - \fxi .
  \end{eqnarray*}

  \item A loss is called \textbf{distance-based} if
  \begin{itemize}
    \item it can be written in terms of the residual
    $$
      \Lxy = \psi (r) \text{ for some } \psi: \R \to \R
    $$
    \item $\psi(r) = 0 \Leftrightarrow r = 0$ .
  \end{itemize}
  \item A loss is \textbf{translation-invariant}, if $L(y + a, \fx + a) = \Lxy$.
  \item Losses are called \textbf{symmetric} if $\Lxy = L\left(\fx, y\right)$. 
\end{itemize}
  % We will see later that in case of the L2-loss, pseudo-residuals correspond to the residuals - hence the name.

  \textcolor{blue}{@BB: This slide is a bit without context imo. We are also not saying anything about classification losses - should we? }


\textcolor{blue}{LW: classif losses usually depend on margin; visualizations}

\end{vbframe}

\begin{vbframe}{Robustness}

Outliers (in $y$) have large residuals $r = y - \fx$. For some losses large residuals have a much bigger impact on the risk much more than for other losses. 

\begin{table}[]
\begin{tabular}{lllll}
\toprule
$y$  & $\fxh$ & L1 & L2 & Huber ($\eps = 5$) \\ \midrule 
1 & 0 & 1 & 1 & 0.5 \\ 
5 & 0 & 5 & 25 & 12.5 \\ 
10 & 0 & 10 & 100 & 37.5 \\ 
50 & 0 & 50 & 2500  & 237.5 \\ \bottomrule
\end{tabular}
\end{table}

As a consequence, a model is less influenced by outliers than by inliers if the loss is robust. 

\textcolor{blue}{merge with subsequent slide}

\framebreak 

The L2 loss is an example for a loss function that is not very robust towards outliers. It penalizes large residuals more than the L1 or the Huber loss. The L1 and the Huber loss are thus regarded robust. 

\begin{center}
\includegraphics[width=0.8\textwidth]{figure/robustness.png}
\end{center}

\end{vbframe}

\begin{vbframe}{Analytical Properties: Smoothness}

\textcolor{blue}{LW: points 1-3 together w/ plot on subsequent slide; scnd slide w/ concrete examples (e.g., gd failing w/ lasso}

\begin{itemize}
    \item \textbf{Smoothness} of a function is a property that is measured by the number of continuous derivatives it has. 
    \item A function is said to be $\mathcal{C}^k$ if it is $k$ times continuously differentiable. A function is $\mathcal{C}^\infty$ if it is continuously differently for all orders $k$. 
    \item In contrast do derivative-free methods, derivative-based methods require a certain level of smoothness of the risk function $\risket$. 
    \item Example: Gradient descent requires differentiability of the $\risket$ (existence of $\nabla \risket$), Newton-Raphson requires $\risket$ to be twice differentiable (existence of Hessian $\nabla^2 \risket$). 

    \framebreak 

    \item If the loss function is not smooth, the risk minimization problem is in general not smooth either. 
    \item  Instead, derivative-free optimization need to be used (which might not be desirable)
\end{itemize}

\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure/plot_loss_overview_classif.png} \\
  \begin{footnotesize}
    Squared loss, exponential loss, and squared hinge loss are continuously differentiable. The hinge loss is continuous but not differentiable. The 0-1-loss is not even continuous.   
  \end{footnotesize}
\end{center}


%   \framebreak 
%   \item Convexity of loss functions
% \end{itemize}

\end{vbframe}


\begin{vbframe}{Analytical Properties: Convexity}

\textcolor{blue}{LW: add concrete examples of (non-)convex loss funs}

\begin{itemize}
  % \item A function $\risket$ is convex if 
  % $$
  %   \risk\left(t \thetab^{(1)} + (1 - t) \cdot \thetab^{(2)}\right) \le t \risk\left(\thetab^{(1)}\right) + (1 - t) \cdot \risk\left(\thetab^{(2)}\right) 
  % $$
  % for all $0 \le t \le 1$ and all $\thetab^{(1)}, \thetab^{(2)} \in \Theta$
  % (strictly convex if the above equation is fulfilled with equality). 
  \item In optimization, convex optimization problems are desirable because they have a number of conventient properties. In particular, it holds for convex problems: A local minimum of a convex function is also a global minimum. A strictly convex function has at most \textbf{one} global minimum (uniqueness). 
  \begin{center}
  \includegraphics[width = 0.45\textwidth]{figure_man/convex-vs-nonconfex-landscape.png} \\
  \begin{footnotesize} Li et al., 2018, Visualizing the Loss Landscape of Neural Nets. The problem on the bottom right is convex, the others are not. \end{footnotesize}
  \end{center}
  \item In practical terms complexity means that we do not need to worry to get stuck in a local minimum during risk minimization. 
  \item Note that convexity of $\risket$ does not only depend on convexity of the loss function: Convexity of $\risket$ is also determined by the choice of the hypothesis space!
  \item For example, if $\Lxy$ is convex in its second argument, and $\fxt$ is linear in $\thetab$, then $\risket$ is convex. If $L$ is not convex, $\risket$ might have multiple local minima (bad!).
\end{itemize} 



\end{vbframe}


\begin{vbframe}{Analytical Properties: Convergence}

\textcolor{blue}{LW: not that much detail (belongs to logreg instead, is addressed in demo); story: if loss never reaches 0, w/ linearly separable data, it becomes desirable to have an infinitely steep logistic fun --> theta to infty}

\textcolor{red}{@BB: Do we want to cover this case here in such a detailed manner? }

The choice of the loss function may also imply convergence behavior of the optimization problem. 

\vspace*{0.2cm}

\textbf{Example: } Gradient descent will not converge if we minimize the Bernoulli loss for linearly separable data. 

\vspace*{0.2cm}

First, we take a look at logistic regression for an almost linearly separable dataset consisting of the observations $\xv^{(1)}, \dots, \xv^{(8)}$.
\vfill

\begin{figure}
\includegraphics[width=0.9\textwidth]{figure_man/undet-problem01.png}\\
\end{figure}


Note: WLOG we estimate the
model without intercept, s.t. we can visualize the regression coefficient 
$\bm{\theta}$ in 2D. Also, the symmetry of the data does not influence the generality of our conclusions.

\vspace*{0.2cm}

Because of the symmetry of the data, the direction\footnote[frame]{$\bm{\theta}$ is perpendicular to the decision boundary and points to the "1"-space.} of $\bm{\theta}$ is $\tilde\thetab := (\frac{1}{\sqrt{2}}, -\frac{1}{\sqrt{2}})^\top$.

\medskip

To find $\overline{\theta} := ||\bm{\theta}||_2$, we consider the empirical risk $\riske$ along $\tilde\thetab$:
\begin{align*}
\riske &= \sum^8_{i=1} \log \left[1 + \exp \left(-y^{(i)}\thetab^\top \xv^{(i)}\right)\right] \\
&= \underbrace{\sum^6_{i=1} \log \left[1 + \exp \left(- \overline{\theta} \left|\tilde{\thetab}^\top \xv^{(i)}\right|\right)\right]}_{=: \; f_{\text{cor}}(\overline{\theta}) \;\text{(correctly classified)}} +
\underbrace{\sum^8_{i=7}\log \left[1 + \exp \left( \overline{\theta} \left| \tilde{\thetab}^\top \xv^{(i)}\right|\right)\right]}_{=:\; f_{\text{incor}}(\overline{\theta}) \;\text{(incorrectly classified)}}.
\end{align*}

\end{vbframe}


\begin{vbframe}{Analytical Properties: Convergence}



Clearly, $f_{\text{cor}}$ / $f_{\text{incor}}$ are monotonically decreasing/increasing with rising length of $\thetab$:

\begin{figure}
\includegraphics[width=0.8\textwidth]{figure_man/undet-problem02.png}\\
\end{figure}

\begin{itemize}
\item By removing obs. 7 and 8, we get a linearly separable dataset. \\
\item This also means that we lose our "counterweight", i.e., if a parameter vector $\thetab$ is able to classify the samples perfectly, the vector $2\thetab$ also classifies the samples perfectly, with decreased risk.
\item Therefore, an iterative optimizer such as gradient descent will continually increase $\thetab$ and never halt (in theory).
\item In such cases, regularization can guarantee convergence (see chapter on regularization). 
\end{itemize}

\end{vbframe}




% \begin{vbframe}{Visulizing Losses via Loss Plots}

% We call the plot that shows the point-wise error, i.e. the loss $\Lxy$ vs. the \textbf{residuals} $\eps := y - \fx$ (for regression), \textbf{loss plot}. The pseudo-residual corresponds to the slope of the tangent in $\left(y - \fx, \Lxy \right)$. 

% \vspace*{0.5cm}


% \begin{figure}
% \includegraphics[width = 1\linewidth]{figure_man/loss.png}
% \end{figure}

% %<<echo=FALSE, warning=FALSE, message=FALSE, fig.height = 4>>=
% %   xx = seq(-2, 2, by = 0.01); 
% %   yy = xx^2
% %   plot(xx, yy, type = "l", xlab = "y - f(x)", ylab = "L(y, f(x))")
% %   points(1, 1, col = "red")
% %   lines(x = c(1, 1), y = c(0, 1), col = "red")
% %   points(xx, 2 * xx - 1, type = "l", col = "red")
% %  @

% % We will define a similar plot for classification later on in this chapter.

% \end{vbframe}










% \section{Comparison of Loss Functions}
% 
% 
% \begin{vbframe}{Comparison of Loss Functions}
% 
% 
% \begin{center}
% \includegraphics[width = 10cm]{figure_man/2_7_loss_comparison_plot1.png} \\
% \end{center}
% 
% 
% \framebreak
% 
% 
% This plot shows the optimal constant model $\fx = \thetab$ (also \enquote{featureless predictor}) for the losses that have been discussed.
% 
% \begin{center}
% \includegraphics[width = 10cm, height = 6.5cm]{figure_man/2_8_loss_comparison_constant_model_plot1.png} \\
% \end{center}
% 
% \end{vbframe}




\endlecture

\end{document}