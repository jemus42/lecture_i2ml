%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@


\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\usepackage{booktabs}

\newcommand{\titlefigure}{figure_man/vgg_example.png}
\newcommand{\learninggoals}{
  % \item Understand why you should care about properties of loss functions
  \item Know the concept of robustness 
  \item Learn about analytical and computational properties of loss functions 
  \item Understand that the loss function may influence convergence of the optimizer
}

\title{Introduction to Machine Learning}
% \author{Bernd Bi{}schl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}


\begin{document}

% ------------------------------------------------------------------------------

\lecturechapter{Properties of Loss Functions}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{The role of Loss Functions}

Why should we care about how to choose the loss function $\Lxy$?

\begin{itemize}
% \item For regression, the loss usually only depends on residual $\Lxy = L\left(y - \fx\right) = L(\eps)$, this is a \emph{translation invariant} loss
\item \textbf{Statistical} properties: choice of loss implies statistical assumptions on the distribution of $y ~|~ \xv = \xv$ (see \emph{maximum likelihood estimation vs.
empirical risk minimization}). 
\item \textbf{Robustness} properties: some loss functions are more robust towards outliers than others. 
\item \textbf{Analytical} properties: the computational / optimization complexity of the problem 
$$
\argmin_{\thetab \in \Theta} \risket
$$
is influenced by the choice of the loss function. 
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Basic Types of Regression Losses}

% https://davidrosenberg.github.io/mlcourse/Archive/2017/Lectures/3b.loss-functions.pdf

\begin{itemize}
  \small
  \item Regression losses usually only depend on the \textbf{residuals}
  $r := y - \fx.$
  \item Classification losses are usually expressed in terms of the 
  \textbf{margin} $\nu := y \cdot \fx.$
  \item A loss is called \textbf{distance-based} if
  \begin{itemize}
    \small
    \item it can be written in terms of the residual, i.e., 
    $\Lxy = \psi (r)$ \\for some $\psi: \R \to \R, ~ \text{and}$
    \item $\psi(r) = 0 \Leftrightarrow r = 0$.
  \end{itemize}
  \item A loss is \textbf{translation-invariant} if $L(y + a, \fx + a) = \Lxy$, 
  $a \in \R$.
  \item Losses are called \textbf{symmetric} if $\Lxy = L\left(\fx, y\right)$. 
\end{itemize}

\vfill

\begin{minipage}[b]{0.25\textwidth}
  \includegraphics[width=\textwidth]{figure/plot_loss_overview_classif_2}
  \tiny \centering
  Margin-based losses
\end{minipage}%
\begin{minipage}[b]{0.25\textwidth}
  \includegraphics[width=\textwidth]{figure/loss_dist_based}
  \tiny \centering
  Distance-based: $L1$
\end{minipage}%
\begin{minipage}[b]{0.2\textwidth}
  \includegraphics[width=\textwidth]{figure/loss_transl_inv.png}
  \tiny \centering
  Translation-invariant: $L2$
\end{minipage}%
\begin{minipage}[b]{0.3\textwidth}
  \includegraphics[width=\textwidth]{figure/loss_symmetric}
  \tiny \centering
  Symmetric: Brier score
\end{minipage}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Robustness}

\small

Outliers (in $y$) have large residuals $r = y - \fx$. Some losses are more
strongly affected by large residuals than others. 

\vspace{0.5cm}

\begin{minipage}[c]{0.55\textwidth}
  \footnotesize
  \begin{table}[]
  \begin{tabular}{r|r|r|r}
  \toprule
  $y - \fxh$ & $L1$ & $L2$ & Huber ($\eps = 5$) \\ \hline
  1 & 1 & 1 & 0.5 \\
  5 & 5 & 25 & 12.5 \\
  10 & 10 & 100 & 37.5 \\
  50 & 50 & 2500  & 237.5
\end{tabular}
\end{table}
\end{minipage}%
\begin{minipage}[c]{0.05\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[c]{0.4\textwidth}
  \small
  As a consequence, a model is less influenced by outliers than by inliers if 
  the loss is \textbf{robust}.
\end{minipage}%

\vfill

\begin{minipage}[c]{0.55\textwidth}
  \includegraphics[width=\textwidth]{figure/robustness}
  \footnotesize \centering
\end{minipage}%
\begin{minipage}[c]{0.05\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[c]{0.4\textwidth}
  \small \raggedright
  $L2$ is an example for a loss function that is not very robust towards
  outliers. It penalizes large residuals more than $L1$ or Huber loss, which are
  considered robust.
\end{minipage}%

% \framebreak 
% 
% The L2 loss is an example for a loss function that is not very robust towards outliers. It penalizes large residuals more than the L1 or the Huber loss. The L1 and the Huber loss are thus regarded robust. 
% 
% \begin{center}
% \includegraphics[width=0.8\textwidth]{figure/robustness.png}
% \end{center}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Analytical Properties: Smoothness}

% \textcolor{blue}{LW: points 1-3 together w/ plot on subsequent slide; scnd slide w/ concrete examples (e.g., gd failing w/ lasso}

\begin{itemize}
  \small
  \item \textbf{Smoothness} of a function is a property measured by 
  the number of continuous derivatives. 
  \item A function is said to be $\mathcal{C}^k$ if it is $k$ times 
  continuously differentiable. A function is $\mathcal{C}^\infty$ if it is 
  continuously differently for all orders $k$. 
  \item Derivative-based methods require a certain level of smoothness of the 
  risk function $\risket$. 
  \begin{itemize}
    \small
    \item If the loss function is not smooth, the risk minimization problem will
    generally not be smooth either. 
    \item This may require the use of derivative-free optimization (which 
    might not be desirable).
  \end{itemize}
\end{itemize}

\vfill

\begin{minipage}[c]{0.4\textwidth}
  \includegraphics[width=0.9\textwidth]{figure/plot_loss_overview_classif}
\end{minipage}%
\begin{minipage}[c]{0.05\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[c]{0.55\textwidth}
  \footnotesize \raggedright
  Squared loss, exponential loss and squared hinge loss are continuously 
  differentiable. Hinge loss is continuous but not differentiable. 
  0-1 loss is not even continuous.
\end{minipage}%

\framebreak

\small
\textbf{Example: Lasso regression}

\begin{itemize}
  \small
  \item Problem: Lasso has a non-differentiable 
  objective function $$\riskrt = \| \yv - \Xmat \thetab \|^2_2
  + \lambda \| \thetab \|_1 ~ \in \mathcal{C}^0,$$
  but many optimization methods are derivative-based, e.g.,
  \begin{itemize}
    \small
    \item Gradient descent: requires existence of gradient $\nabla \risket$, 
    \item Newton-Raphson: requires existence of Hessian $\nabla^2 \risket$.
  \end{itemize}
  \item We must therefore resort to alternative optimization 
  techniques -- for instance, coordinate descent with subgradients.
\end{itemize}

\vfill

\begin{minipage}[c]{0.3\textwidth}
  \includegraphics[width=\textwidth]{figure/lasso_unpenalized}
\end{minipage}%
\begin{minipage}[c]{0.05\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[c]{0.3\textwidth}
  \includegraphics[width=0.9\textwidth]{figure/lasso_penalty}
\end{minipage}%
\begin{minipage}[c]{0.05\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[c]{0.3\textwidth}
  \includegraphics[width=0.9\textwidth]{figure/lasso_penalized}
\end{minipage}%

\tiny Example: $y = x_1 + 1.2 x_2 + \epsilon$. \textit{Left:} unpenalized 
objective, \textit{middle:} $L1$ penalty, \textit{right:} penalized objective 
(all as functions of $\thetab$). We see how the $L1$ penalty nudges the optimum 
towards (0, 0) and compromises the original objective's smoothness.

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Analytical Properties: Convexity}

\begin{itemize}
  \footnotesize
  \item A function $\risket$ is convex if
  $$
    \risk\left(t \cdot \thetab + (1 - t) \cdot \tilde \thetab\right) \le t \cdot
    \risk\left(\thetab\right) + (1 - t) \cdot \risk\left(\tilde \thetab \right)
  $$
  $\forall$ $t \in [0, 1], ~\thetab, \tilde \thetab \in \Theta$
  (strictly convex if the above holds with equality).
  \item In optimization, convex optimization problems are desirable because 
  they have a number of conventient properties. 
  \item In particular, it holds for convex problems that local optima are 
  global optima \\$\rightarrow$ a strictly convex function has at most 
  \textbf{one} global minimum (uniqueness). 
  \item Note, however, that convexity of $\risket$ depends both on convexity of 
  \begin{itemize} 
    \footnotesize
    \item $L(\cdot)$ -- given in most cases -- and 
    \item $\fxt$ -- often problematic.
  \end{itemize}  
\end{itemize} 

\vfill

\begin{minipage}[b]{0.5\textwidth}
  \footnotesize \raggedright
  \href{https://arxiv.org/pdf/1712.09913.pdf}{Li et al., 2018: 
  \textit{Visualizing the Loss Landscape of Neural Nets}}. 
  The problem on the bottom right is convex, the others are not (note that 
  very high-dimensional surfaces are coerced into 3D here).  
  \\
  \phantom{foo}
\end{minipage}%
\begin{minipage}[b]{0.05\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[b]{0.45\textwidth}
  \includegraphics[width=0.75\textwidth]{
  figure_man/convex-vs-nonconfex-landscape}
\end{minipage}%

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Analytical Properties: Convergence}

\small
The choice of the loss function may also impact the convergence behavior of the 
optimization problem. 

\vspace{0.2cm}

\begin{minipage}[b]{0.7\textwidth}
  \begin{itemize} 
    \small
    \item Example: gradient descent in logistic regression will not converge for 
    linearly separable data (\textbf{complete separation}). 
    \item This is a direct consequence of the convergence behavior of Bernoulli 
    loss, which reaches 0 only in the infinite limit of the margin.
  \end{itemize}  
\end{minipage}%
\begin{minipage}[b]{0.05\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[b]{0.25\textwidth}
  \includegraphics[width=\textwidth]{figure/plot_bernoulli}
\end{minipage}%

% \vfill
% \includegraphics[width=0.4\textwidth]{figure_man/snap_bernoulli_loss}
% \vfill

\begin{itemize} 
  \item In the case of complete separation, we have
  \footnotesize
  \begin{flalign*}
    \risket &= \sumin \log \left( 1 + \exp \left( - \yi \thetab^T \xi \right)
    \right) \\ &=
    \sumin \log \left( 1 + \exp \left( - | \thetab^T \xi| \right)
    \right),
  \end{flalign*}
  \small
  as every observation is correctly classified (i.e., $\thetab^T \xi < 0$ \\for
  $\yi = -1$ and $\thetab^T \xi > 0$ for $\yi = 1$).
\end{itemize} 

\framebreak

\begin{itemize} 
  \small
  \item $\risket$ thus monotonically decreases in $\thetab$: if a parameter 
  vector $\thetab^\prime$ is able to classify the samples perfectly, then 
  $2\thetab^\prime$ also classifies the samples perfectly, and at lower risk.
  \item Geometrically, this translates to an ever steeper slope of the 
  logistic/softmax function, leading to increasingly sharp discrimination and 
  infinitely running optimization.
  
  \vspace{0.3cm}
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=0.8\textwidth]{figure/softmax_1}
  \end{minipage}%
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=0.8\textwidth]{figure/softmax_2}
  \end{minipage}%
\end{itemize}  

\vfill

In practice, data are seldom linearly separable and misclassified 
examples act as counterweights to increasing parameter values. Besides, we 
can apply \textbf{regularization} to encourage convergence to robust solutions.

\end{vbframe}

% ------------------------------------------------------------------------------

\endlecture

\end{document}