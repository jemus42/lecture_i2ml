%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-trees} % For the comparison of Brier and Gini index


\newcommand{\titlefigure}{figure_man/optimization_steps.jpeg}
\newcommand{\learninggoals}{
  \item Know advanced classification losses
}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}


\begin{document}

\lecturechapter{Advanced Classification Losses}
\lecture{Introduction to Machine Learning}



\begin{vbframe}{Margins} 

Losses can be defined on 

\begin{itemize}
  \item Hard labels $\hx \in \Yspace$
  \item Scores $\fx \in \R$
  \item Probabilities $\pix$
\end{itemize}

\lz 

When considering scoring classifiers $\fx$ we usually define loss functions on the so-called \textbf{margin}

$$
r = y\cdot \fx =  \begin{cases} > 0  \quad &\text{ if } y = \sign(\fx) \text{ (correct classification)}\,, \\
                      < 0 \quad &\text{ if } y \ne \sign(\fx) \text{ (misclassification)}\,, \end{cases}
$$

$|\fx|$ is called \textbf{confidence}.

% \framebreak

% We define the \textbf{loss plot} for binary scoring classifiers as the plot that shows the point-wise loss $\Lxy$, vs. the \textbf{margin} $y \cdot \fx$. Large positive values of $y \cdot \fx$ are good and penalized less.

% \lz

% \textbf{Example:} 0-1-Loss

% <<echo=FALSE, warning=FALSE, message=FALSE, fig.height = 3>>=
% x = seq(-2, 2, by = 0.01); y = as.numeric(x < 0)
% qplot(x, y, geom = "line", xlab = expression(y %.% f(x)), ylab = expression(L(y, f(x))))
%   @

% For probabilistic classifiers, loss plots show the Loss $L(y, \pix)$ versus the class probability $\pix$.


\end{vbframe}



% \begin{vbframe}{Classification Losses: (Naive) L2-Loss}


% $$
% \Lxy = (1 - y\fx)^2,
% $$


% \begin{itemize}
%   \item L2-loss defined on scores
%   \item Predictions with high confidence $|f(x)|$ are penalized  regardless of whether the signs of $y$ and $f(x)$ match.
%   \item Squared loss on the loss functions is thus not the best choice.

%   <<loss-squareclass-plot, echo=FALSE, results='hide', fig.height= 1.8, fig.asp = 0.4>>=
%  x = seq(-2, 5, by = 0.01)
%  plot(x, (1-x)^2, type = "l", xlab = expression(y %.% f(x)), ylab = expression(paste((1-yf(x))^2)), main = "Square loss")
%  box()
% @

%   \item The theoretical risk becomes
%     \begin{eqnarray*}
%     \risk(f) &=& \mathbb{E}_x [(1-\fx)^2 (P(1 | x)) + (1+\fx)^2 (1-P(1 | x))] \\
%     &=& \mathbb{E}_x [1 + 2\fx + \fx^2-4\fx P(1 | x)].
%     \end{eqnarray*}
%   \item By differentiating w.r.t. $f(x)$ we get the minimizer of $\risk(f)$ for the square loss function

%     \begin{eqnarray*}
%     f(\xv) &=& 2\cdot P(1 | \xv) - 1.
%     \end{eqnarray*}
%     \item The empiricla optimzer is then
%     $$
%     \fh(\xv) = \frac{2}{n}\cdot \sumin \I[y^{(i)} = 1] - 1.
%     $$


% \end{itemize}

% \end{vbframe}


\section{0-1-Loss}

\begin{vbframe}{0-1-Loss}

\begin{itemize}
  \item Let us first consider a classifier $\hx$ that outputs discrete classes directly. 
  \item The most natural choice for $\Lhxy$ is of course the 0-1-loss that counts the number of misclassifications
  $$
  \Lhxy = \mathds{1}_{\{y \ne \hx\}} =
     \footnotesize \begin{cases} 1 \quad \text{ if } y \ne \hx \\ 0 \quad    \text{ if } y = \hx  \end{cases}.
  $$
  \item We can express the 0-1-loss also for a scoring classifier $\fx$ based on the margin $r$

  $$
  \Lr = \mathds{1}_{\{r < 0\}} = \mathds{1}_{\{y\fx < 0\}} = \mathds{1}_{\{y \neq \hx\}}.
  $$

\end{itemize}


\framebreak 

$$
  \Lr = \mathds{1}_{\{r < 0\}} = \mathds{1}_{\{y\fx < 0\}} = \mathds{1}_{\{y \neq \hx\}} 
$$

\begin{itemize}
\item Intuitive, often what we are interested in.
\item Analytic properties:  Not continuous, even for linear $f$ the optimization problem is NP-hard and close to intractable.
\end{itemize}

\begin{center}
\includegraphics[width = 11cm ]{figure_man/0-1-loss.png} \\
\end{center}


\end{vbframe}


\section{Brier Score}

\begin{vbframe}{Brier Score}

The binary Brier score is defined on probabilities $\pix \in [0, 1]$ and 0-1-encoded labels $y \in \{0, 1\}$ and measures their squared distance (L2 loss on probabilities).

\begin{eqnarray*}
L\left(y, \pix\right) &=& (\pix - y)^2
\end{eqnarray*}

\vspace{0.2cm}
\begin{center}
\includegraphics[width = 11cm ]{figure_man/Brier-score.png} \\
\end{center}


\end{vbframe}

\begin{vbframe}{Brier Score: Point-wise Optimum}

The minimizer of the (theoretical) risk $\risk(f)$ for the Brier score  

\begin{eqnarray*}
\hat \pi(\xv) &=& \P(y~|~\xv = \xv),
\end{eqnarray*}

which means that the Brier score would reach its minimum if the prediction equals the \enquote{true} probability of the outcome. 

\lz 

\textbf{Proof: }We have seen that the (theoretical) optimal prediction $c$ for an arbitrary loss function at fixed point $\xv$ is

$$
\argmin_{c} \sum_{k \in \Yspace} L(y, c) ~ \P(y = k~|~ \xv = \xv)\,.
$$

We plug in the Brier score

\vspace*{-0.3cm}

\begin{footnotesize}
  \begin{eqnarray*}
    && \argmin_c L(1, c) \underbrace{\P(y = 1| \xv = \xv)}_{:= p} + L(0, c) \underbrace{\P(y = 0| \xv = \xv)}_{= 1 - p} \\ 
    &=&  \argmin_c \quad (c - 1)^2 p + c^2 (1 - p)\\
    &=&  \argmin_c \quad (c - p)^2.
  \end{eqnarray*}
\end{footnotesize}

The expression is minimal if $c = p = \P(y = 1~|~\xv = \xv)$.

\end{vbframe}

\begin{vbframe}{Brier Score: Optimal constant Model}

The optimal constant probability model $\pix = \theta$ w.r.t. the Brier score for labels from $\Yspace = \setzo$ is:

\begin{eqnarray*}
  \min_{\theta} \riske(\theta) &=& \min_{\thetab} \sumin \left(\yi - \theta\right)^2 \\
  \Leftrightarrow \frac{\partial \riske(\theta)}{\partial \theta} &=& - 2 \cdot \sumin (\yi - \theta) = 0 \\
  \hat \theta &=& \frac{1}{n} \sumin \yi.   
\end{eqnarray*}

This is the fraction of class-1 observations in the observed data.\\
(This also directly follows from our $L2$-proof for regression).

\end{vbframe}

\begin{vbframe}{Gini splitting = Brier score minimization}


Interestingly, splitting a classification tree w.r.t. the Gini index is equivalent to minimizing the Brier score in each node. 

\lz 

To prove this we show that 


$$
  \risk(\Np) = n_\Np I(\Np)
$$
  
  where $I$ is the Gini impurity 

$$I(\Np) = \sum_{k\neq k\prime} \pikN \pi^{(\Np)}_{k\prime} = \sum_{k=1}^g \pikN(1-\pikN),
$$



  and $\risk(\Np)$ is calculated w.r.t. the Brier score

$$
  L(y, \pix) = \sum_{k = 1}^g \left([y = k] - \pi_k(\xv)\right)^2.
$$

\framebreak


\begin{eqnarray*}
\risk(\Np) &=& \sum_{\xy \in \Np}  \sum_{k = 1}^g \left([y = k] - \pi_k(\xv)\right)^2 \\
&=& \sum_{k = 1}^g \sum_{\xy \in \Np}   \left([y = k] - \pi_k(\xv)\right)^2 \\
&=& \sum_{k = 1}^g n_{\Np,k}\left(1 - \frac{n_{\Np,k}}{n_{\Np }}\right)^2 + (n_{\Np } - n_{\Np,k})\left(\frac{n_{\Np,k}}{n_{\Np }}\right)^2
\end{eqnarray*}


In the last step, we plugged in the optimal prediction w.r.t. the Brier score (the fraction of class $k$ observations):

$$\hat \pi_k(\xv)= \pikN = \frac{n_{\Np,k}}{n_{\Np }}.$$ 


We further simplify the expression to

% \begin{footnotesize}
\begin{eqnarray*}
\risk(\Np) &=&  \sum_{k = 1}^g n_{\Np,k}\left(\frac{n_{\Np } - n_{\Np,k}}{n_{\Np }}\right)^2 + (n_{\Np } - n_{\Np,k})\left(\frac{n_{\Np,k}}{n_{\Np }}\right)^2 \\
&=& \sum_{k = 1}^g \frac{n_{\Np,k}}{n_{\Np }} \frac{n_{\Np } - n_{\Np,k}}{n_{\Np }} \left(n_{\Np } - n_{\Np,k } + n_{\Np,k}\right) \\
&=& n_{\Np } \sum_{k = 1}^g \pikN \cdot (1 - \pikN) = n_\Np I(\Np).
\end{eqnarray*}
% \end{footnotesize}

\end{vbframe}



\section{Bernoulli Loss}

\begin{vbframe}{Bernoulli Loss}

\vspace*{-0.5cm}
\begin{eqnarray*}
  L_{-1, +1}(y, \fx) &=& \ln(1+\exp(-y\fx)) \\
  L_{0, 1}(y, \fx) &=& - y \cdot \fx + \log(1 + \exp(\fx)). 
\end{eqnarray*}

\begin{itemize}
  \item Two equivalent formulations: Labels $y \in \{-1, 1\}$ or $y \in \{0, 1\}$
  \item Negative log-likelihood of Bernoulli model, e.g., logistic regression
  \item Convex, differentiable
  \item Pseudo-Residuals (0,1 case): $\tilde{r} = y - \frac{1}{1+\exp(-\fx)}$\\   
    Interpretation: $L1$ distance between 0/1-labels and posterior prob!
\end{itemize}

\vspace{0.2cm}
\begin{center}
\includegraphics[width = 9cm ]{figure_man/bernoulli.png} \\
\end{center}

\end{vbframe}




\begin{vbframe}{Bernoulli loss on probabilities}

If scores are transformed into probabilities by the logistic function  $\pix = \left(1 + \exp(- \fx)\right)^{-1}$, we arrive at another equivalent formulation of the loss

  $$
    L_{0,1}(y, \pix) = - y \log \left(\pix\right) - (1 - y) \log \left(1 - \pix\right). 
  $$

\begin{center}
\includegraphics[width = 10cm ]{figure_man/bernoulli-loss.png} \\
\end{center}

Via this form it is easy to show that the point-wise optimum for probability estimates is $\pixh = \P(y~|~\xv = \xv)$.

\end{vbframe}



\begin{vbframe}{Bernoulli loss: Point-wise Optimum}

The theoretical point-wise optimum for scores under Bernoulli loss is actually
the point-wise log-odds:

\begin{eqnarray*}
\fh(\xv) &=&  \ln \biggl(\frac{\P(y~|~\xv = \xv)}{1-\P(y~|~\xv = \xv)}\biggr).
\end{eqnarray*}

The function is undefined when $P(y~|~\xv = \xv) = 1$ or $P(y~|~\xv = \xv) = 0$, but predicts a smooth curve which grows when $P(y~|~\xv = \xv)$ increases and equals $0$ when $P(y~|~\xv = \xv) = 0.5$.

\lz 

\textbf{Proof: } We consider the case $\Yspace = \{-1, 1\}$. We have seen that the (theoretical) optimal prediction $c$ for an arbitrary loss function at fixed point $\xv$ is

$$
\argmin_{c} \sum_{k \in \Yspace} L(y, c) \P(y = k| \xv = \xv)\,.
$$

\framebreak 

We plug in the Bernoulli loss

\vspace*{-0.3cm}

\begin{footnotesize}
  \begin{eqnarray*}
    && \argmin_c L(1, c) \underbrace{\P(y = 1| \xv = \xv)}_p + L(-1, c) \underbrace{\P(y = -1| \xv = \xv)}_{1 - p} \\ 
    &=&  \argmin_c \ln(1 + \exp(-c)) p + \ln(1 + \exp(c)) (1 - p).
  \end{eqnarray*}
\end{footnotesize}

\vspace*{-0.3cm}

Setting the derivative w.r.t. $c$ to zero yields

\vspace*{-0.3cm}

\begin{footnotesize}
  \begin{eqnarray*}
  0 &=& - \frac{\exp(-c)}{1 + \exp(-c)} p + \frac{\exp(c)}{1 + \exp(c)} (1 - p) \\ 
   &=& - \frac{\exp(-c)}{1 + \exp(-c)} p + \frac{1}{1 + \exp(- c)} (1 - p) \\ 
  % &=& -  \frac{\exp(-c)}{1 + \exp(-c)} p + \frac{1}{1 + \exp(-c)} - \frac{1}{1 + \exp(-c)} p \\
  &=& - p + \frac{1}{1 + \exp(-c)} \\
  \Leftrightarrow p &=& \frac{1}{1 + \exp(-c)} \\
  \Leftrightarrow c &=& \ln\left(\frac{p}{1 - p}\right)
  \end{eqnarray*}
\end{footnotesize}


% \framebreak 

% Another common equivalent formulation of this loss function is defined on labels $y \in \{0, 1\}$ instead of $y \in \{-1, 1\}$

% $$
%   L_{0, 1}(y, \fx) = - y \cdot \fx + \log(1 + \exp(\fx)). 
% $$

% We indicate this by subscripts of the loss function. 

\end{vbframe}



\begin{vbframe}{Bernoulli: Optimal constant Model}

The optimal constant probability model $\pix = \theta$ w.r.t. the Bernoulli loss for labels from $\Yspace = \{0, 1\}$) is:

\begin{eqnarray*}
  \thetah = \argmin_{\theta} \risket = \frac{1}{n} \sumin \yi
\end{eqnarray*}

Again, this is the fraction of class-1 observations in the observed data.
We can simply prove this again by setting the derivative of the risk to 0 and solving for $\theta$.

\framebreak

The optimal constant score model $\fx = \theta$ w.r.t. the Bernoulli loss labels from $\Yspace = \setmp$ or $\Yspace = \setzo$ is:

\begin{eqnarray*}
  \thetah = \argmin_{\theta} \risket = \ln \frac{n_{+1}}{n_{-1}} = \ln \frac{n_{+1}/n}{n_{-1}/n} 
\end{eqnarray*}

where $n_{-1}$ and $n_{+1}$ are the numbers of negative and positive observations, respectively.

\lz

This again shows a tight (and unsurprising) connection of this loss to log-odds.

\lz

Proving this is also a (quite simple) exercise.

\end{vbframe}

\begin{vbframe}{Bernoulli-Loss: Naming Convention}

We have seen three loss functions that are closely related. In the literature, there are different names for the losses: 

\begin{eqnarray*}
  L_{-1+1}(y, \fx) &=& \ln(1+\exp(-y\fx)) \\
  L_{0, 1}(y, \fx) &=& - y \cdot \fx + \log(1 + \exp(\fx)). 
\end{eqnarray*}

are referred to as Bernoulli, Binomial or logistic loss. 

  $$
    L_{0,1}(y, \pix) = - y \log \left(\pix\right) - (1 - y) \log \left(1 - \pix\right). 
  $$

is referred to as cross-entropy or log-loss. 

\lz 

For simplicity, we will call all of them \textbf{Bernoulli loss}, and rather make clear whether they are defined on labels $y \in \{0, 1\}$ or $y \in \{-1, 1\}$ and on scores $\fx$ or probabilities $\pix$. 

\end{vbframe}



\begin{vbframe}{Entropy splitting = log loss minimization}

The logarithmic loss for multiple classes $y \in \{1, 2, ..., g\}$ is defined as
$$ L(y, \pikx) = \sum_{k = 1}^g [y = k] \cdot \log \left(\pi_k(\xv)\right). $$
\vspace{-0.4cm}
\begin{eqnarray*}
\risk(\Np) &=& \sum_{\xy \in \Np}  \sum_{k = 1}^g [y = k] \log \pi_k(\xv) \\
&=& \sum_{k = 1}^g \sum_{\xy \in \Np} [y = k] \log \pikN \\
&=& \sum_{k = 1}^g n_{\Np k} \log \pikN = n_{\Np} \sum_{k = 1}^g \pikN \log \pikN = n_\Np I(\Np)
\end{eqnarray*} 
Plugging in the optimal constant $\pi_k(\xv) = \pikN$.


\framebreak

\textbf{Conclusion}: 
  
  Stumps/trees with entropy splitting use the same loss function as logistic regression (binary) / softmax regression (multiclass). While logistic regression is based on the hypothesis space of \textbf{linear functions}, stumps/trees use \textbf{step functions} as hypothesis spaces. 

\end{vbframe}







% \vspace*{-0.2cm}

% \begin{eqnarray*}
%   \Lxy = \log \left[1 + \exp \left(-y\fx\right)\right].
% \end{eqnarray*}

% We transform scores into probabilities by

% $$
% \pix = \P(y = 1 ~|~\xv) = s(\fx) = \frac{1}{1 + \exp(- \fx)},
% $$

% with $s(.)$ being the logistic sigmoid function as introduced in chapter 2.

% \framebreak

% As already shown before, an equivalent approach that directly outputs probabilities $\pix$ is minimizing the \textbf{Bernoulli loss}

% \begin{eqnarray*}
% \Lxy = -y \log \left(\pix\right) - (1 - y) \log \left(1 - \pix\right)
% \end{eqnarray*}

% for $\pix$ in the hypothesis space

% \begin{eqnarray*}
%   \Hspace = \left\{\pi: \Xspace \to [0, 1] ~|~\pix = s(\thetab^\top \xv)\right\}
% \end{eqnarray*}

% with $s(.)$ again being the logistic function.

% \framebreak


% Logistic Regression with one feature $\xv \in \R$. The figure shows how $\xv \mapsto \pix$.

% <<fig.height=4>>=
% set.seed(1234)
% n = 20
% x = runif(n, min = 0, max = 7)
% y = x + rnorm(n) > 3.5
% df = data.frame(x = x, y = y)

% model = glm(y ~ x,family = binomial(link = 'logit'), data = df)
% df$score = predict(model)
% df$prob = predict(model, type = "response")
% x = seq(0, 7, by = 0.01)
% dfn = data.frame(x = x)
% dfn$prob = predict(model, newdata = dfn, type = "response")
% dfn$score = predict(model, newdata = dfn)

% p2 = ggplot() + geom_line(data = dfn, aes(x = x, y = prob))
% p2 = p2 + geom_point(data = df, aes(x = x, y = prob, colour = y), size = 2)
% p2 = p2 + xlab("x") + ylab(expression(pi(x)))
% p2 = p2 + theme(legend.position = "none")
% p2
% @

% \framebreak

% Logistic regression with two features:

\section{Exponential Loss}

\begin{vbframe}{Classification Losses: Exponential Loss}

Another possible choice for a (binary) loss function that is a smooth approximation to the 0-1-loss:
\begin{itemize}
\item $\Lxy = \exp(-y\fx)$, used in AdaBoost
\item Convex, differentiable (thus easier to optimize than 0-1-loss)
\item The loss increases exponentially for wrong predictions with high confidence; if the prediction is right with a small confidence only, there, loss is still positive
\item No closed-form analytic solution to empirical risk minimization
\end{itemize}


\begin{figure}
\includegraphics[width = 0.8\textwidth]{figure_man/exponential-loss.png}
\end{figure}


\end{vbframe}


\section{AUC Loss}


\begin{vbframe}{Classification Losses: AUC-loss}

\begin{itemize}
\item Often AUC is used as an evaluation criterion for binary classifiers
\item Let $Y \in \{-1, 1\}$ with observations $n_{-1}$ number of negative and $n_{1}$ of positive samples %$y_i, i = 1, \ldots, n_{-1} + n_1$.
\item The AUC can then be defined as
$$AUC = n_{-1}^{-1} n_1^{-1} \sum_{i: y_i = 1} \sum_{j: y_j = -1} I(f_i > f_j)$$
\item This is not differentiable wrt $f$ due to $I(f_i > f_j)$
\item But the indicator function can be approximated by the distribution function of the triangular distribution on $[-1, 1]$ with mean $0$
\item However, direct optimization of the AUC is usually not as good as optimization wrt a common loss and tuning via AUC in practice 

\end{itemize}
\end{vbframe}




\section{Summary}

\begin{vbframe}{Summary of Loss Functions}

\vspace*{-0.5cm}

\begin{table}[] 
\footnotesize
\renewcommand{\arraystretch}{1.5} %<- modify value to suit your needs
\begin{tabular}{c|lll}
Name & Formula & Differentiable \\ \hline
0-1 & $L(y, \hx) = [y \neq \hx]$  & \xmark \\
Brier & $L(y, \pix) = \left(\pix - y\right)^2$ & \checkmark \\
Bernoulli & $L_{-1+1}(y, \fx)= \ln[1 + \exp(-y\fx)]$ & \checkmark \\
Bernoulli & $L_{0,1}(y, \fx) = - y \fx + \log(1 + \exp(\fx))$ & \checkmark \\
Bernoulli & $L_{0,1}(y, \pix) = - y \log\pix - (1 - y) \log (1 - \pix)$ & \checkmark \\
\end{tabular}
\end{table}

\begin{table}[] 
\footnotesize
\renewcommand{\arraystretch}{1.5} %<- modify value to suit your needs
  \begin{tabular}{c|lll}
  Name & Point-wise Opt. & Optimal Constant\\ \hline
  0-1 & $\hxh = \argmax_{l \in \Yspace} \P(y = l~|~ \xv)$  & $\hx = \text{mode} \left\{\yi\right\}$ \\
  Brier & $\pixh = \P(y = 1~|~\xv = \xv)$ & $\hat{\pi} = \frac{1}{n} \sumin \yi$\\
  Bernoulli & $\pixh = \P(y = 1~|~\xv = \xv)$ & $\hat{\pi} = \frac{1}{n} \sumin \yi$\\
  Bernoulli & $\fxh = \log\left(\frac{\P(y = 1 ~|~\xv)}{1 - \P(y = 1 ~|~\xv)}\right)$ & $\fh = \ln \frac{n_{+1}}{n_{-1}}$  
  \end{tabular}
\end{table}


\framebreak 

There are other loss functions for classification tasks, for example:

\begin{itemize}
  \item Hinge-Loss 
  \item Exponential-Loss
\end{itemize}

As for regression, loss functions might also be customized to an objective that is defined by an application. 

\end{vbframe}

\begin{vbframe}{Risk minimizing functions}

Overview of binary classification losses and the corresponding risk minimizing functions:

\lz

\begin{tabular}{p{2.5cm}|p{3.5cm}|p{4.5cm}}
  loss name & loss formula  & minimizing function \\
  \hline
  0-1 & $[y \neq \hx]$ & $\hxh = \footnotesize \begin{cases} 1 \quad \text{ if } \pix > 1/2 \\ -1 \quad \pix < 1/2  \end{cases}$ \\
  Hinge & $\max\{0, 1 - \yf\}$ & $\fh(x) =  \footnotesize \begin{cases} 1 \quad \text{ if } \pix > 1/2 \\ -1 \quad \pix < 1/2  \end{cases}$ \\
  Logistic & $\ln(1+\exp(-y\fx))$ & $\fh(x) =  \ln \biggl(\frac{\pix}{1-\pix}\biggr)$ \\
  Cross entropy & $-y\ln(\pix) \newline -(1-y)\ln(1-\pix)$ & \\
  & & \\
  Exponential & $\exp(-y\fx)$ &

\end{tabular}

\end{vbframe}




% \begin{vbframe}{Classification Losses: Hinge Loss}
% \begin{itemize}
% \item $\Lxy = \max\{0, 1 - \yf\}$, used in SVMs
% \item Convex
% \item No derivatives for $\yf = 1$, optimization becomes harder
% \item No closed-form analytical solution to empirical risk minimization
% \item More robust, outliers in $y$ are less problematic
% \item Correctly classified samples with confidence $> 1$ are not penalized
% \end{itemize}
% 
% <<loss-hinge-plot, echo=FALSE, results='hide', fig.height=3, fig.align='center'>>=
% x = seq(-2, 2, by = 0.01); y = pmax(0, 1 - x)
% qplot(x, y, geom = "line", xlab = expression(y %.% f(x)), ylab = expression(L(yf(x))))
% @
% 
% \end{vbframe}




\endlecture

\end{document}