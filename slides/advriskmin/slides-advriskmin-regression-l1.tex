%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure_man/loss_absolute_plot2.png}
\newcommand{\learninggoals}{
  \item Derive the risk minimizer of the L1-loss
  \item Derive the optimal constant model for the L1-loss
}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}


\begin{document}

\lecturechapter{Regression Losses: L1-loss}
\lecture{Introduction to Machine Learning}


\begin{vbframe}{L1-Loss}

% Description
\vspace*{-0.3cm}

$$
\Lxy = \left|y-\fx\right|
$$

\begin{itemize}
\item More robust than $L2$, outliers in $y$ are less problematic.
\item Analytical properties: convex, not differentiable for $y = f(\bm{x})$ (optimization becomes harder).
\end{itemize}

\vspace*{-6mm}

\begin{center}
  \includegraphics[width = 9cm]{figure_man/loss_absolute_plot1.png} \\
\end{center}


\end{vbframe}


\begin{vbframe}{L1-Loss: Risk Minimizer}

We calculate the (true) risk for the $L1$-Loss $\Lxy = \left|y-\fx\right|$ with unrestricted $\Hspace = \{f: \Xspace \to \Yspace\}$. 

\begin{itemize}
  \item We use the law of total expectation 
  $$
    \riskf = \E_x \left[\E_{y|x}\left[|y-\fx|~|\xv = \xv\right]\right]. 
  $$
  \item As the functional form of $f$ is not restricted, we can just optimize point-wise at any point $\xv = \xv$. The best prediction at $\xv = \xv$ is then 

  $$
    \fxh = \mbox{argmin}_c \E_{y|x}\left[|y - c|\right]= \text{med}_{y|x} \left[y ~|~ \xv \right]. 
  $$

\end{itemize}

  \framebreak 

\begin{footnotesize}
\textbf{Proof:} Let $p(y)$ be the density function of $y$. Then: 
  \begin{eqnarray*}
  && \mbox{argmin}_c \E\left[|y - c|\right] = \mbox{argmin}_c \int_{-\infty}^\infty |y - c| ~ p(y) \text{d}y \\
  &=& \mbox{argmin}_c \int_{-\infty}^c -(y - c)~p(y)~\text{d}y + \int_c^\infty (y - c)~p(y)~\text{d}y 
  \end{eqnarray*}

We now compute the derivative of the above term and set it to $0$

\begin{eqnarray*}
0 &=& \frac{\partial}{\partial c} \left(\int_{-\infty}^c -(y - c)~p(y)~\text{d}y + \int_c^\infty (y - c)~p(y)~\text{d}y\right) \\ &\overset{^\ast\text{Leibniz}}{=}& \int_{-\infty}^c  ~p(y)~\text{d}y - \int_c^\infty ~p(y)~\text{d}y =   \P_y (y \le c) - \left(1 - \P_y (y \le c)\right) \\
&=& 2 \cdot \P_y (y \le c) - 1 \\
\Leftrightarrow 0.5 &=& \P_y (y \le c),
\end{eqnarray*}

which yields $c = \text{med}_y(y)$. 

\framebreak 

$^\ast$ \textbf{Note} that since we are computing the derivative w.r.t. the integration boundaries, we need to use Leibniz integration rule 

\begin{eqnarray*}
  \frac{\partial}{\partial c} \left(\int_a^c g(c, y) ~\text{d}y\right) &=& g(c, c) + \int_a^c \frac{\partial}{\partial c} g(c, y) ~\text{d}y \\
  \frac{\partial}{\partial c} \left(\int_c^a g(c, y) ~\text{d}y\right) &=& - g(c, c) + \int_c^a \frac{\partial}{\partial c} g(c, y) ~\text{d}y    
\end{eqnarray*}

We get 

\vspace*{-0.3cm}

\begin{eqnarray*}
&&\frac{\partial}{\partial c} \left(\int_{-\infty}^c -(y - c)~p(y)~\text{d}y + \int_c^\infty (y - c)~p(y)~\text{d}y \right) \\
&=& \frac{\partial}{\partial c} \left(\int_{-\infty}^c \underbrace{-(y - c)~p(y)}_{g_1(c, y)}~\text{d}y\right) + \frac{\partial}{\partial c} \left(\int_c^\infty \underbrace{(y - c)~p(y)}_{g_2(c, y)}~\text{d}y \right) \\
&=& \underbrace{g_1(c, c)}_{=0} + \int_{-\infty}^c \frac{\partial}{\partial c}  \left(-(y - c)\right)~p(y)~\text{d}y - \underbrace{g_2(c, c)}_{= 0} + \int_c^\infty \frac{\partial}{\partial c}  (y - c)~p(y)~\text{d}y \\
&=& \int_{-\infty}^c  ~p(y)~\text{d}y + \int_c^\infty -~p(y)~\text{d}y. 
\end{eqnarray*}

\end{footnotesize}

\end{vbframe}


\begin{vbframe}{L1-Loss: Optimal constant model}

The optimal constant model in terms of the theoretical risk for the L1 loss is the median over $y$:

\begin{eqnarray*}
  \fx &=& \text{med}_{y|x} \left[y ~|~ \xv \right] \overset{\text{drop } \xv}{=}  \text{med}_{y} \left[y \right]
  \end{eqnarray*} 

The optimizer of the empirical risk is $\text{med}(\yi)$ over $\yi$, which is the empirical estimate for $\text{med}_{y} \left[y \right]$. 

\vspace*{-0.3cm}

\begin{center}
\includegraphics[width = 0.5\textwidth ]{figure_man/l1_vs_l2.png} \\
\end{center}


\framebreak 

\textbf{Proof}: 

\begin{itemize}
  \item Firstly note that for $n = 1$ the median $\thetah = \text{med}(\yi) = y^{(1)}$ obviously minimizes the empirical risk $\riske$ associated to the $L1$ loss $L$. 


% Take proof from this page? 
% https://math.stackexchange.com/questions/113270/the-median-minimizes-the-sum-of-absolute-deviations-the-l-1-norm/1024462#1024462

  \item Hence let $n > 1$ in the following: Let 

  $$
    S_{a,b}:\mathbb{R} \rightarrow \mathbb{R}^+_0, \theta \mapsto |a- \theta| + |b-\theta|
  $$

  for $a, b \in \mathbb{R}$. It holds that
  \begin{eqnarray*}
  S_{a,b}(\theta) &=& \begin{cases}|a-b| ,& \text{ for } \theta \in [a,b]\\ |a-b| + 2\cdot\min\{|a-\theta|,|b-\theta|\}
  ,& \text{ otherwise. }\end{cases}
  \end{eqnarray*}
  Thus, any $\thetah \in [a,b]$ minimizes $S_{a,b}$. \\


  \framebreak

  W.l.o.g. assume now that all $\yi$ are sorted in increasing order.

  Let us define $i_{\max} = n / 2$ for $n$ even and $i_{\max} = (n - 1) / 2$ for $n$ odd and consider the intervals 
  $$
    \mathcal{I}_i := [y^{(i)},y^{(n+1-i)}], i \in \{1, ..., i_{\max}\}. 
  $$

  By construction $\mathcal{I}_{j+1} \subseteq \mathcal{I}_j$ for $j \in \{1,\dots,i_{\max}-1\}$ and $\mathcal{I}_{i_{\max}} \subseteq \mathcal{I}_i$. With this, $\riske$ can be expressed as
  \begin{footnotesize}
  \begin{eqnarray*}
  \riske(\theta) &=& \sumin L(\yi,\theta) = \sumin|\yi - \theta| \\ 
  &=& \underbrace{|\yi[1] - \theta| + |\yi[n] - \theta|}_{= S_{\yi[1], \yi[n]}(\theta)} + \underbrace{|\yi[2] - \theta| + |\yi[n - 1] - \theta|}_{= S_{\yi[2], \yi[n - 1]}(\theta)} + ...  \\
  &=& \begin{cases} \sum\limits_{i = 1}^{i_{\max}} S_{\yi, \yi[n + 1 - i]}(\theta) & \text{ for } n \text{ is even} \\
  \sum\limits_{i = 1}^{i_{\max}} \left(S_{\yi, \yi[n + 1 - i]}(\theta)\right) + |\yi[(n + 1)/2] - \theta| & \text{ for } n \text{ is odd}. \end{cases}
  \end{eqnarray*}
  \end{footnotesize}

  % \begin{eqnarray*}
  % \begin{cases}
  % \sum^{\overbrace{n/2}^{=:i_{\max}}}_{i=1} \overbrace{S_{y^{(i)},y^{(n+1-i)}}(c)}^{=:S_i(c)},&  \text{ for } n \text{ is even}\\
  % (\sum^{\overbrace{(n-1)/2}^{=:i_{\max}}}_{i=1} \underbrace{S_{y^{(i)},y^{(n+1-i)}}(c)}_{=:S_i(c)}) + \underbrace{|y^{(n+1)/2}-c|}_{=:S_0(c)},& \text{ for } n \text{ is odd}. 
  % \end{cases}
  % \end{eqnarray*}
  % \end{footnotesize}
  % Now we define for $i \in \{1,\dots,i_{\max}\} \; \mathcal{I}_i := [y^{(i)},y^{(n+1-i)}].$ \\
  % From construction it follows that for  $j \in \{1,\dots,i_{\max}-1\}$
  % $$\mathcal{I}_{j+1} \subseteq \mathcal{I}_j \Rightarrow \forall i \in \{1,\dots,i_{\max}\}:  \mathcal{I}_{i_{\max}} \subseteq \mathcal{I}_i.$$
  \framebreak 


  From this follows that
  \begin{itemize}
  \item for \enquote{$n$ is even}: $\thetah \in  \mathcal{I}_{i_{\max}} = [y^{(n/2)},y^{(n/2+1)}]$ minimizes $S_i$ for all $i \in \{1,\dots, i_{\max}\} \; \Rightarrow$  it minimizes $\riske$,
  \item for \enquote{$n$ is odd}: $\thetah = y^{(n+1)/2} \in \mathcal{I}_{i_{\max}}$ minimizes $S_i$ for all $i \in \{1,\dots, i_{\max}\}$ and its minimal for
  $|\yi[(n + 1)/2] - \theta| \; \Rightarrow$ it minimizes $\riske$,
  \end{itemize}

  Since the median fulfills these conditions, we can conclude that it minimizes 
  the $L1$ loss.
\end{itemize}

% \framebreak


% \begin{center}
% \includegraphics[width = 11cm ]{figure_man/L1-loss.png} \\
% \end{center}

% \framebreak 

% We see that the $L1$-Loss is more robust w.r.t. outliers than the $L2$-Loss. 
% \vspace{0.3cm}


% \begin{center}
% \includegraphics[width = 9cm ]{figure_man/L1andL2-loss.png} \\
% \end{center}

\framebreak 

\end{vbframe}


% \begin{vbframe}{Robustness}

% We see that the $L1$-Loss is more robust w.r.t. outliers than the $L2$-Loss. 
% \vspace{0.3cm}

% \begin{center}  
% \includegraphics[width = 9cm ]{figure_man/L1andL2-loss.png} \\
% \end{center}

% \end{vbframe}

\endlecture

\end{document}
