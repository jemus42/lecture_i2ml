
\input{../../2021/style/preamble4tex}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\begin{document}

\lecturechapter{12}{Hypothesis Testing}
\lecture{Fortgeschrittene Computerintensive Methoden}

\begin{vbframe}{Benchmark Experiments}

\begin{itemize}
\item In benchmark experiments, different learning algorithms are applied to one or more datasets with the aim of comparing and ranking their performances.
\item To ensure comparability, synchronized train and test sets, i.e., the same resampling method with the same train-test splits, should be used to calculate and compare the performances.
\item Results of benchmark experiments produce a dataset which can be further analyzed and visualized. 
\newline
\textbf{Example}: Benchmark results (per CV-fold) of CART and random forest using 2-fold CV with MSE as performance measure:

\vspace{0.2cm}
\footnotesize

\begin{center}
\begin{tabular}{c|c|c|c}
\hline
Dataset & k-th fold & MSE (rpart) & MSE (randomForest)\\
\hline
BostonHousing & 1 & 17.13 & 29.4\\
\hline
BostonHousing & 2 & 8.90 & 20.5\\
\hline
mtcars & 1 & 7.53 & 35.0\\
\hline
mtcars & 2 & 6.73 & 38.9
\end{tabular}
\end{center}
\end{itemize}
\framebreak

\end{vbframe}

% Inspiration:
% https://marthawhite.github.io/mlcourse/lectures/2017/Lec20-MeasuringPerformance.pdf bzw. https://iu.instructure.com/courses/1600120
% https://slideplayer.com/slide/3430766 bzw. https://faculty.tarleton.edu/crawford/math-5364-data-mining-i.html
% http://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_resampled/
\normalsize

\begin{vbframe}{Hypothesis Testing in Benchmarking}
We want to know if the difference in performance between models (or algorithms) is significant or if it only occurred by chance.

\lz
\textbf{Null Hypothesis Statistical Testing (NHST)} in benchmarking:

\begin{itemize}
\item Formulate a null hypothesis $H_0$ (e.g., the expected generalization error of two algorithms is equivalent).
\item Use a hypothesis test to reject $H_0$ (not rejecting $H_0$ does not mean accepting it).
\item Rejecting $H_0$ gives us some confidence in the belief that the observed results may be not merely random.
\end{itemize}

% \begin{multicols}{2}
% p-value: the probability that, when the null hypothesis is true, the statistical summary would be greater than or equal to the actual observed results
%
% Reject $H_0$ if $p<\alpha$
%
% \begin{flushright}\includegraphics[width=0.5\textwidth]{figure_man/table-of-errors.png} \end{flushright}
% \end{multicols}
% Typical example in medicine:
%
% \begin{itemize}
% \item $H_0$: on average, the new drug is not better than the current drug
% \item $H_a$: on average, the new drug is better
% \end{itemize}

\lz Typical example in machine learning:

\begin{itemize}
\item $H_0$: on average, model 1 does not perform better than model 2.
\item $H_1$: on average, model 1 outperforms model 2.
\item Aim: Reject $H_0$ with confidence level of $1-\alpha$.
\end{itemize}

\framebreak

%It is important to clearly define what should be tested with $H_0$ and $H_1$, i.e., in the previous example, $H_0$ does not take into account \textit{how much} learner A outperforms learner B. With enough data even very small differences can be significant.
Selection of an appropriate hypothesis test is at least based on the type of problem, i.e., whether the aim is to compare
\begin{itemize}
\item 2 models / algorithms on a single domain (i.e., on a single dataset),
\item 2 algorithms across different domains (i.e., on multiple datasets),
\item multiple algorithms across different domains / datasets.
\end{itemize}

% \textbf{Limitations of NHST:}
% \begin{itemize}
% \item We wish to estimate the probability of our hypothesis (new learner is better than current learner) given the data $p(H|D)$. However, the p-value estimates the probability that the evidence from our experiments is correct
% given our hypothesis $p(D|H)$.
% \item The approach does not take into account \textit{by how much} the new learner outperforms the current one. With enough data even very small differences can be significant.
% \end{itemize}
%
% hypothesis testing can be useful if applied properly and understood correctly. However, the underlying assumptions of the tests need to be examined and the result of a test should not be overvalued.
% At the very least, the impossibility of rejecting $H_0$ using a reasonable effect size helps guard against claiming the new algorithm outperforms the current one.
% \framebreak

Overview of different hypothesis tests for matched samples:

\begin{center}\includegraphics[width=\textwidth]{figure_man/tests_overview.png} \end{center}

\footnotesize
{\tiny{Source: \code{\url{https://marthawhite.github.io/mlcourse/lectures/2017/Lec20-MeasuringPerformance.pdf}}, p. 142.}\par}

\end{vbframe}

%\section{Compare 2 Models on a Single Domain}


% \begin{vbframe}{Weaknesses of parametric tests}
%
% \begin{itemize}
% \item \textbf{Commensurability:} One has to be able to compare the differences over the data
% sets, thus if the used performance measures have different properties for different
% tasks, this assumption might be violated.
% \item \textbf{Normal distribution:} Unless the number of data sets is greater then 30, asymptotically
% normal distributed differences can not be assumed. Due to the nature
% of the underlying results, even at more than 30 data sets, such tests might prove
% unreliable.
% \item \textbf{Outliers:} Extreme values might skew the statistics and thus decrease a tests
% power.
% \end{itemize}
%
% \end{vbframe}

\begin{vbframe}{McNemar test}
% https://slideplayer.com/slide/9574350/
\begin{itemize}
\item The \textbf{McNemar test} is a non-parametric test used on paired nominal data that does not make any distributional assumptions.
\item It can be applied to compare the performance of two \textbf{models} when the considered performance measure is based on an outer loss with a nominal or binary output, e.g., accuracy is based on a binary outer loss.
\item Both models are trained on a training set and evaluated on a test set. Based on the test set, a \textbf{contingency table} that compares the two models (model 1 and model 2) is calculated:
\end{itemize}

\begin{minipage}{0.34\textwidth}
\includegraphics[width=\textwidth]{figure_man/mcnemar_1.png} \end{minipage}
\begin{minipage}{0.64\textwidth}
\begin{itemize}
\item A: $\#$ obs. correctly classified by both.
\item B: $\#$ obs. misclassified by model 2 but not by model 1.
\item C: $\#$ obs. misclassified by model 1 but not by model 2.
\item D: $\#$ obs. misclassified by both.
\end{itemize}
\end{minipage}
%
% \begin{minipage}[c]{0.4\linewidth}
%   \begin{center}
%   \renewcommand{\arraystretch}{1.5}
%   \begin{tabular}{cc|cc}
%       & & \multicolumn{2}{c}{$f_2$} \\
%       & & $0$ & $1$ \\
%       \hline
%       \multirow{2}{*}{ $f_1$} & $0$ & $c_{00}^{Mc}$ & $c_{01}^{Mc}$ \\
%       & $1$ & $c_{10}^{Mc}$ & $c_{11}^{Mc}$ \\
%   \end{tabular}
%   \end{center}
% \end{minipage} % no space if you would like to put them side by side
% \begin{minipage}[c]{0.59\linewidth}
% \vspace{0.15cm}
%   $c_{00}^{Mc} = \sum_{i=1}^{|S_{test}|} \left[ I(f_1(\mathbf{x_i}) \neq y_i) \wedge I(f_2(\mathbf{x_i}) \neq y_i) \right]$ \vspace{0.05cm}
%
%   $c_{01}^{Mc} = \sum_{i=1}^{|S_{test}|} \left[ I(f_1(\mathbf{x_i}) \neq y_i) \wedge I(f_2(\mathbf{x_i}) = y_i)\right]$ \vspace{0.05cm}
%
%   $c_{10}^{Mc} = \sum_{i=1}^{|S_{test}|} \left[ I(f_1(\mathbf{x_i}) = y_i) \wedge I(f_2(\mathbf{x_i}) \neq y_i)\right]$ \vspace{0.05cm}
%
%   $c_{11}^{Mc} = \sum_{i=1}^{|S_{test}|} \left[ I(f_1(\mathbf{x_i}) = y_i) \wedge I(f_2(\mathbf{x_i}) = y_i)\right]$
%
% \end{minipage}
% \vspace{0.5cm}
%
% Testing: $H_0$: $c_{01}^{Mc} = c_{01}^{Mc} = c_{null}^{Mc}$
%
% $$\chi^2_{Mc} =  \frac{(|c_{01}^{Mc} - c_{01}^{Mc}| - 1)^2}{c_{01}^{Mc} + c_{10}^{Mc}} \overset{approx}{\sim} \chi^2_{1,1-\alpha}$$
%
% If $c_{01}^{Mc} + c_{01}^{Mc} < 20$, approximation doesn't work, use binomial instead

\framebreak

\begin{minipage}[c]{0.625\linewidth}
Given such a contingency table, the accuracy of each model can be computed as follows:
\begin{itemize}
  \item Model 1: (A+B)/(A+B+C+D)
  \item Model 2: (A+C)/(A+B+C+D)
\end{itemize}

Even if the models have \textbf{equal} accuracy (indicating equal performance), cells B and C may differ because the models may misclassify different instances.
\end{minipage}
\begin{minipage}[c]{0.365\linewidth}
  \includegraphics{figure_man/mcnemar_1.png}
\end{minipage}

\lz McNemar tests the following hypotheses:
\begin{itemize}
\item $H_0:$ Both models have the same performance (we expect B = C).
\item $H_1:$ Performances of the two models are not equal.
\end{itemize}
%\lz
The test statistic is computed by
$$\chi^2_{Mc} =  \tfrac{(|B-C| - 1)^2}{B + C} \sim \chi^2_{1}.$$

\textbf{Note}: The McNemar test should only be used if $B + C > 20$.

\framebreak

\textbf{Example}:

\begin{center}
  %\renewcommand{\arraystretch}{1.5}
%\hspace*{\fill}
  \begin{tabular}{cc|cc}
      & & \multicolumn{2}{c}{Random Forest} \\
      & & correct & wrong \\
      \hline
      \multirow{2}{*}{Tree} & correct & 30 & 5 \\
      & wrong & 17 & 42 \\
  \end{tabular}
\end{center}

Calculate the test statistic:

$$\chi^2_{Mc} =  \frac{(|5-17| - 1)^2}{5 + 17} = 5.5 > 3.841 = \chi^2_{1,0.95}.$$

We can reject $H_0$ at a significance level of 0.05, i.e., we reject the hypothesis that the tree and the random forest have the same performance.
% and assume that our tree and our random forest classify the data differently

% \framebreak
%
% \begin{itemize}
% \item McNemar test is a non-parametric test that compares the errors of two classifiers.
% \item classifiers have to be trained and tested on the same dataset.
% % \item $H_0$: the two classifiers have the same error rates, \\ $H_1$: the error rates differ
% \item Goodness-of-fit measure: Compares observed counts with expected distribution under $H_0$.
% \item Relies on contingency matrix and is very similar to $\chi^2$-Test.
% \item McNemar test considers if observations are classified correctly or not and does not provide any quantification of these measures.
% \item According Dietterich(1998), the McNemar test should only be used if $B + C > 20$.
% \end{itemize}
%
% \end{vbframe}

% \begin{vbframe}{T-test vs. McNemar Test}
%
% Apart from t-test being parametric and the McNemar test being non-parametric, two tests have other significant differences. Because of its nature, it is possible to construct intervals for t-test however it is not possible to work with intervals for McNemar test which only works with nominal performance measures. McNemar test takes into account that whether observations are classified correctly or not and it does not provide any quantification of these measures.
%
% \lz
%
% According to the Dietterich(1998), one can proceed with McNemar test when the number of disagreement between two classifiers $c_{01}^{Mc} + c_{01}^{Mc} $ is greater than 20.
%
\end{vbframe}


\begin{vbframe}{Two-matched-samples t-test}

A two-matched-samples t-test (i.e., a paired t-test) is the simplest hypothesis test if the aim is to compare two \textbf{models} on a single test set based on arbitrary performance measures.

However, it is a parametric test and distributional assumptions must be made (which are often problematic).

\lz The t-test relies on several assumptions:

\begin{itemize}
\item \textbf{(Pseudo-)normality}, usually met when sample size > 30.
\item \textbf{I.i.d. samples}, usually met if the loss of individual observations from a single test set is considered. % (this assumption is violated in case of resampling as ). %difficult as data is often limited (assumption is violated in the case of resampling).
\item \textbf{Equal variances of populations}, can be investigated by plots.
\end{itemize}

\framebreak

A paired t-test to compare two different models $\fh_1$ and $\fh_2$ w.r.t. a performance measure calculated on a test set of size $n_{\text{test}}$:

\begin{itemize}
\item $H_0$: $GE(\fh_{1}) = GE(\fh_{2})$ vs. $H_1$: $GE(\fh_{1}) \neq GE(\fh_{2})$
\item Test statistic $T = \sqrt{n_{\text{test}}} \frac{\bar{d}}{\sigma_{d}}$, with
%Teststatistic $t=\frac{d\left(\fh_A,\fh_B\right)}{\sqrt{\sigma_{A}^2 + \sigma_{B}^2}}$, with
\begin{itemize}
\item mean performance difference of both models
$\bar{d} = \GEh{\Dtest}(\fh_{1}) - \GEh{\Dtest}(\fh_{2})$, and
\item standard deviation of this mean difference
$$\sigma_{d} = \sqrt{\frac{1}{n_{\text{test}} - 1}\sum_{i=1}^{n_{\text{test}}} \left(d_i - \bar{d} \right)^2},$$
where $d_i = L(\yi, \fh_1 (\xi)) - L(\yi, \fh_2 (\xi))$ and $\bar{d} = \sum\limits_{i=1}^{n_{\text{test}}} d_i$.
%the variance of binomial distribution: \newline
\end{itemize}
\end{itemize}

\textbf{Note}: Here, $d_i$ is the difference of the outer loss of individual observations from the test set between the two models to be compared.
%\textbf{Issue}: $\GEh{n} \left(\fh_A\right)$ and $ \GEh{n} \left(\fh_B\right)$ are not independent as they are calculated on the same test set.

\begin{itemize}
\item We could also use a \textbf{$k$-fold CV paired t-test} to compare two \textbf{algorithms} (instead of two models) on a single dataset.
\item Instead of comparing the outer loss of individual observations we would then compare the individual generalization errors per CV fold (i.e., the generalization error of the $k$ prediction models induced by the learning algorithm in each CV fold).
%\item Instad of using the difference of the outer loss of individual observations
\item Although the test sets do not overlap, the performance differences are not independent across CV folds due to overlapping training sets (which violates the assumption of i.i.d. samples).
%However, the paired t-test is a parametric test and is not recommended in practice as the assumptions of the student's t-test are often violated.
%\item A probably better alternative is, therefore, the Friedman test.
\item To partly overcome the issue of overlapping training sets across folds, Dietterich\footnote{Dietterich (1998). Approximate statistical tests for comparing supervised classification learning algorithms.} suggests using 5 times 2-fold CV so that at least within each repetition neither the training nor the test sets overlap.
\end{itemize}
\end{vbframe}

%\section{Compare Multiple Models Multiple Domains}

\begin{vbframe}{Friedman test}
So far we have only compared 2 models / algorithms on one dataset. The \textbf{Friedman test} can be used to compare multiple classifiers on multiple datasets. The hypotheses to be tested are:
\begin{itemize}
\item $H_0:$ All algorithms are equivalent in their performance and hence their average ranks should be equal.
\item $H_1:$ The average rank for at least one algorithm is different.
\end{itemize}

\lz
Suppose we want to evaluate $m$ data sets and $k$ algorithms. A \textbf{Friedman test} is constructed as follows:

\begin{itemize}
  \item Separately rank each algorithm for each dataset from best-performing (rank 1) to worst-performing, using any performance measure of interest.
  \item If there is a $d$-way tie after rank $r$, assign a rank of $ \left[(r+1) + (r+2) + ... + (r+d)\right] /d $ to all tied classifiers.
  \item $R_{ij}$ is the rank of algorithm $j$ on data set $i$.
\end{itemize}

\end{vbframe}

\begin{vbframe}{Friedman test}
After obtaining the rank for each algorithm $j$ on different datasets $i$, calculate the following quantities:

\begin{itemize}
  \item The overall mean rank:
  $ \bar{R} = \frac{1}{mk} \sum_{i=1}^{m} \sum_{j=1}^{k} R_{ij}. $
  \item The total sum of squares:
  $ SS_{Total} = m \sum_{j=1}^{k} (\bar{R}_{.j} - \bar{R})^2 $, where $\bar{R}_{.j} =  \frac{1}{m} \sum_{i=1}^{m} R_{ij}$.
  \item The error sum of squares:
  $ SS_{Error} = \frac{1}{m(k-1)} \sum_{i=1}^{m} \sum_{j=1}^{k} (R_{ij} - \bar{R})^2. $
\end{itemize}

\lz

The Friedman test statistic is calculated as:

$${\chi_F}^2 = \frac{SS_{Total}}{SS_{Error}} \sim \chi_{k-1}^2 \text{ for large \textit{m} ($>15$) and \textit{k} ($>5$).}$$

In the case of smaller $m$ and $k$, the $\chi^2$ approximation is imprecise and a look-up of $\chi_F^2$ values that were approximated specifically for the Friedman test is suggested.
\end{vbframe}


\begin{vbframe}{Post-hoc tests}
A Friedman test checks if all algorithms are ranked equally or not. However, it does not provide information w.r.t. the best-performing algorithm.
To address this issue, post-hoc tests can be used.

\lz \textbf{Post-hoc Nemenyi test}:
\begin{itemize}
\item Compares all algorithms pairwise to find the best-performing algorithm after $H_0$ of the Friedman test was rejected.
\item For $m$ data sets and $k$ algorithms, $\frac{k(k-1)}{2}$ comparisons are made.
\item Calculates the average rank of algorithm $j$ on all $m$ data sets: $\bar{R}_{.j} =\frac{1}{m} \sum_{i=1}^m R_{ij}.$
\end{itemize}

\lz %The performance of two algorithms are significantly different when their average ranks differ by at least the significance level $q_{\alpha}$.
For any two algorithms $j_1$ and $j_2$, we compute the test statistic as:
$$q = \frac{\bar{R}_{.j_1} - \bar{R}_{.j_2}}{\sqrt{\frac{k(k+1)}{6m}}}.$$


\framebreak

\textbf{Post-hoc Bonferroni-Dunn test}:

\begin{itemize}
\item Compares all algorithms with a baseline (i.e., $k-1$ comparisons).
\item It is used after a Friedman test to find which algorithms differ from the baseline significantly.
\item It uses the Bonferroni correction to prevent randomly accepting one of the algorithms as significant due to multiple testing.
\end{itemize}
The test statistic is the same as before:
$$q = \frac{\bar{R}_{.j_1} - \bar{R}_{.j_{baseline}}}{\sqrt{\frac{k(k+1)}{6m}}}.$$

The performances of $j_1$ and $j_2$ differ significantly if $|q| > q_{\alpha}$, where the critical value $q_{\alpha}$ is obtained from a table of the studentized range statistic, scaled through division by $\sqrt{2}$.
%Another difference between Bonferroni-Dunn Test with Nemenyi test is that total number of comparisons are 1/k of Nemenyi test.
\end{vbframe}

% \begin{vbframe}{ROC Analysis}
%
% Any binary classifier $\hx$ can be characterized by its
% \begin{itemize}
%   \item true positive rate: $\text{TPR} = \cfrac{\text{\#TP}}{\text{\#TP} + \text{\#FN}}$ (Sensitivity, Recall)
%   \item false positive rate: $\text{FPR} = 1-\text{specificity} = \cfrac{\text{\#FP}}{\text{\#FP} + \text{\#TN}}$
% \end{itemize}
% \lz
%
% For a given scoring classifier $\fx$, define the associated binary classifier $h(x, \theta) = \I(f(x) \geq \theta)$ with decision threshold $\theta$.
% \lz
%
% The \textbf{R}eceiver \textbf{O}perating \textbf{C}haracteristic (ROC) curve of a scoring classifier is created by plotting \textit{tpr} vs. \textit{fpr} for all possible decision thresholds $\theta$.\\
% (Any given binary classifier occupies a single point (tpr, fpr)  in \enquote{ROC-Space}.)
%
% \framebreak
%
% ROC curves are insensitive to the class distribution in the sense that they are not affected by changes in the ratio $\np/\nn$:
%
% \begin{table}[]
% \centering
% \begin{tabular}{|l|c|c|}
%                 \hline
%    $\np/\nn = 1$            & True Positive & True Negative \\ \hline
% Pred. Positive & 40            & 25            \\ \hline
% Pred. Negative & 10            & 25           \\ \hline
% \end{tabular}
% \end{table}
% \begin{table}[]
% \centering
% \begin{tabular}{|l|c|c|}
%                 \hline
%      $\np/\nn = 2$        & True Positive & True Negative \\ \hline
% Pred. Positive & 80            & 25            \\ \hline
% Pred. Negative & 20            & 25           \\ \hline
% \end{tabular}
% \end{table}
%
% True positive rates ($\text{TPR} = 0.8$) and false positive rates ($\text{FPR} = 0.5$) do not change.


% \begin{itemize}
%   \item Note that the class distribution is
% the relationship of the left column to the right column.
%   \item Any performance metric that uses values from both
% columns will be inherently sensitive to class skews (such as accuracy, precision, F measure)
%   \item ROC curves are based upon tpr and fpr.
%   \item Each measure uses only values in their respective columns.
%   \item Changing the class distribution will therefore not change the ROC curve.
% \end{itemize}

%\end{vbframe}

\endlecture
\end{document}
