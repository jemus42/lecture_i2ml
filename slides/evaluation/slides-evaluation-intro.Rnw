<<set-parent, include = FALSE>>=
knitr::set_parent("../../style/preamble.Rnw")
@
<<setup, child="../../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{Evaluation: Introduction and Remarks}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{Goals of performance evaluation}


\textbf{Model evaluation}\\
<<out.width = "0.2\\textwidth", fig.height=5>>=
set.seed(31415)

x = 1:5
y = 2 + 0.5 * x + rnorm(length(x), 0, 1.5)
dat = data.frame(x = x, y = y)
model = lm(y ~ x)
dat$y_hat <- predict(model)

ggplot(data = dat, aes(x = x, y = y)) +
    geom_abline(intercept = coef(model)[1], slope = coef(model)[2]) +
    geom_segment(aes(xend = x, yend = y_hat), color = "red") +
    geom_point() +
    theme_void()
@
     
     

\textbf{Algorithm evaluation}\\
\begin{center}
    { \Huge \checkmark }
\end{center}


\textbf{Model selection}
\begin{center}
    \begin{tikzpicture}
    \draw [thick] (0,0) rectangle (.5,.5);
    \draw [fill=green, thick] (0.5,0) rectangle (1,1);
    \draw [thick] (1,0) rectangle (1.5,.75);
    \end{tikzpicture}
\end{center}

\textbf{Learning curve estimation}
<<fig.height=2, fig.width=5, out.width="0.2\\textwidth">>=
# rsrc data from rsrc/learning_curve.R
load("rsrc/learning_curve.RData")

opt = res.rpart
opt$mmce = opt$mmce - mean(opt$mmce) + 0.08

ggplot(data = opt, aes(x = percentage, y = mmce)) + 
    geom_line(aes(colour = measure), size = 3) + 
    theme_void() + theme(legend.position = "none")
@
    
\end{vbframe}



\begin{vbframe}{Goals of performance evaluation}

\begin{itemize}
  \item \textbf{Model evaluation:}
    Estimate \emph{generalization} error of a model on new (unseen) data, drawn from the same data generating process that training data came from.
  \item \textbf{Algorithm evaluation:}
    Estimate \emph{generalization} error of a learning algorithm, trained on a data set
    of a certain size, on new (unseen) data, all drawn from the same data generating process.
  \item \textbf{Model selection:}
    Select the best model from a set of potential candidate models (e.g., different model classes, different
    hyperparameter settings, different features)
  \item \textbf{Learning curve estimation:}
    How does the generalization error scale when an algorithm is trained on training sets of different sizes?
\end{itemize}

All goals are related, i.e., reliable estimation of (predictive) performance is the foundation for all of them.

\end{vbframe}

\begin{vbframe}{Performance Estimation}
\begin{itemize}
\item Goal: Estimate performance on new data 
\item For now, we start by assuming to have a fixed model, already fitted on some data.
\item We also assume to have some reasonable data for testing available. 
 \item ML performance evaluation provides clear and simple protocols for reliable model validation. 
   These protocols are often much simpler than classical statistical model diagnosis and rely only on few assumptions
 \item Most important assumption: Data we use is realistic and i.i.d.  
\item ML evaluation is still hard enough and offers LOTS of options to cheat yourself and especially your clients, mistakes can happen on many levels. 
\end{itemize}
\end{vbframe}

\begin{vbframe}{Performance Estimation}
 Clearly, we are looking for a statistical estimator - for the generalization error! 
 Different levels of randomness are involved: 
\begin{itemize}
 \item Even if we are evaluating on a fixed test data set, this is only a sample and not full reality. 
   The sample can be too small, then our estimator will be of high variance; 
   the sample could not be from the distribution of interest, then our estimator will be biased.
 \item The same holds true for our model - it was only fitted on a sample. This creates another source of randomness, the training
   data sample. This is true, even if the fitting algorithm is deterministic. 
\item In ML, many learning algorithms are stochastic: Think random forest, or stochastic gradient descent. This is a third source of randomness.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Metrics: Inner vs. Outer Loss}

\begin{itemize}
\item To judge the performance of a model on a given data set, we might want to produce a quantitative 
measure of the performance on that set. 
\item Usually we define a function that measures the quality of a prediction per observation.
\item We then aggregate over the complete set - often by some form of averaging.
\end{itemize}

\lz

Don't we already know this? Sounds like loss functions and risk estimation? 
It nearly is the same. Nearly! 
\end{vbframe}

\begin{vbframe}{Metrics: Inner Loss}
\begin{itemize}
\item We already covered this, its associated empirical risk is optimized during model fitting
\item The keyword above is \textbf{optimization}, some functions are much tamer to handle than others, 
  smoothness and differentiability are often required for efficient optimization
\item For this reason, we often choose something that's easier to handle numerically
\item Another pretty practical reason might be: Our toolkit of choice only implements the optimization of a certain inner loss; 
  changing the outer loss to something custom is simple, changing the inner often is not
\end{itemize}
\end{vbframe}

\begin{vbframe}{Metrics: Outer Loss}
\begin{itemize}
\item Performance metric to assess the model
\item Should be carefully considered and selected
\item There are no objectively better or worse metrics - YOU have to select depending on your application, domain
  and what you want
\item Except for \enquote{should be reasonable and reflect what I want} there are no huge requirements for an outer metric, it is a function which takes a vector of prediction and a vector of labels and evaluates them
\item Think about what will happen after a (wrong) prediction of your model, that should help to design an outer loss
\item Yes, in model selection (later), we optimize it, but we use special techniques there anyway that can deal with arbitrary metrics 
\end{itemize}
\end{vbframe}

\begin{vbframe}{Metrics: Inner vs. Outer Loss}

Usually, it is desired that inner and outer loss match, however, this is not always possible,
as the outer loss is often numerically hard(er) to be handled during optimization and we might
opt to approximate it.

Examples:\\
\begin{itemize}
  \item In logistic regression we minimize the binomial loss 
  \item In kNN there is no explicit loss minimization
  \item But when evaluating the models we might be more interested in (cost-weighted) classification error 
  \item Or some of the more advanced metrics from ROC analysis like AUC
\end{itemize}

Nowadays, one can also optimize many of the harder losses directly, but this is less standard, less
often implemented and we will not cover this here.

\end{vbframe}

\endlecture

