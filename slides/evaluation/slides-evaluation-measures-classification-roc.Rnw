<<set-parent, include = FALSE>>=
knitr::set_parent("../../style/preamble.Rnw")
@
<<setup, child="../../style/setup.Rnw", include = FALSE>>=
requireNamespace("ROCR")
@
\lecturechapter{Evaluation: Measures for Binary Classification: ROC Measures}
\lecture{Introduction to Machine Learning}

% \begin{vbframe}{Imbalanced Binary Labels}
% \begin{itemize}
%   \item Consider a binary classifier for diagnosing a serious medical condition
%   \item Here, label distribution is often \textbf{imbalanced}, i.e, not many people have the disease
%   \item Evaluating with error rate for imbalanced labels is often inappropriate
%   \item Assume that only 0.5\,\% of 1000 patients have the disease
%   \item Always returning "no disease" has an error rate of 0.5\,\%, which sounds good
%   \item However, this sends all sick patients home, which is the worst possible system, even classifying everyone as "sick" might be better, depending on what happens next
%   \item This problem is sometimes known as the \textbf{accuracy paradox}
% \end{itemize}
% \end{vbframe}
% 
% \begin{vbframe}{Binary Classifiers and Costs}
% \begin{itemize}
%   \item Another point of view is imbalanced costs
%   \item In our example, classifying a sick patient as healthy, should incur a much higher loss then classifying a healthy patient as sick
%   \item The costs depend a lot on what happens next: We can likely assume that our system is some type of screening filter,
%     often the next step after labeling someone as "sick" might be a more invasive, expensive, but more reliable test for the disease
%   \item Erroneously subjecting someone to that second step is not good (psychologically, economically, or because the second test might introduce
%     medical risks), but sending someone home to get worse or die seems much worse
%   \item Such a situation not only arises under label imbalance, but also when labels are maybe balanced but costs differ
%   \item We could see this as imbalanced costs of misclassification, rather than imbalanced labels; both situations are tightly connected
% \end{itemize}
% \end{vbframe}
% 
% \begin{vbframe}{Binary Classifiers and Costs}
% \begin{itemize}
%   \item Problem is: If we could specify costs precisely, we could evaluate against them, we might even optimize our model for them
%   \item This important subfield of ML is called \textbf{cost-sensitive learning}, which we will not cover in this lecture unit
%   \item Unfortunately, users often have a notoriously hard time to come up with precise cost numbers in imbalanced scenarios
%   \item Evaluating "from different perspectives", with multiple metrics, often helps, especially to get a first impression
%     of the quality of the system
% \end{itemize}
% \end{vbframe}
% 

% 
% 
% \begin{vbframe}{Confusion Matrix and ROC Metrics}
% \begin{itemize}
%   \item From now on, we call one class "positive", one "negative" and their respective sizes
%     $n_+$ and $n_-$
%   \item The positive class is the more important, often smaller one
%   \item We represent all predictions in a confusion matrix and count correct and incorrect class assignments
%   \item False Positive means: We assigned "positive", but were wrong
% \end{itemize}
% % FIGURE SOURCE: No source
% \includegraphics[width=0.7\textwidth]{figure_man/roc-confmatrix1.png}
% \end{vbframe}


\begin{vbframe}{Labels: ROC Metrics}
From the confusion matrix (binary case), we can calculate "ROC" metrics.

% % FIGURE SOURCE: No source
% \includegraphics[width=0.7\textwidth]{figure_man/roc-confmatrix2.png}

% \begin{center}
% \small
% \begin{tabular}{cc|>{\centering\arraybackslash}p{7em}>{\centering\arraybackslash}p{8em}|>{\centering\arraybackslash}p{8em}}
%     & & \multicolumn{2}{c}{\bfseries True Class $y$} & \\
%     & & $+$ & $-$ & \\
%     \hline
%     \bfseries Pred.     & $+$ & True Positive (TP)  & False Positive (FP) & Positive Predictive Value (PPV) = $\frac{\text{TP}}{\text{TP} + \text{FP}}$\\
%               $\hat{y}$ & $-$ & False Negative (FN) & True Negative (TN) & Negative Predictive Value (NPV) = $\frac{\text{TN}}{\text{FN} + \text{TN}}$\\
%     \hline
%     & & TPR = $\frac{\text{TP}}{\text{TP} + \text{FN}}$ & TNR = $\frac{\text{TN}}{\text{FP} + \text{TN}}$ & Accuracy = $\frac{\text{TP}+ \text{TN}}{\text{TOTAL}}$
% \end{tabular}
% \end{center}

\begin{center}
\small
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{cc||cc|c}
    & & \multicolumn{2}{c|}{\bfseries True Class $y$} & \\
    & & $+$ & $-$ & \\ 
    \hline \hline
    \bfseries Pred.     & $+$ & TP & FP & PPV = $\frac{\text{TP}}{\text{TP} + \text{FP}}$\\
              $\hat{y}$ & $-$ & FN & TN & NPV = $\frac{\text{TN}}{\text{FN} + \text{TN}}$\\
    \hline
    & & TPR = $\frac{\text{TP}}{\text{TP} + \text{FN}}$ & TNR = $\frac{\text{TN}}{\text{FP} + \text{TN}}$ & Accuracy = $\frac{\text{TP}+ \text{TN}}{\text{TOTAL}}$
\end{tabular}
\renewcommand{\arraystretch}{1}
\end{center}

\begin{itemize}
  \item True Positive Rate: How many of the true 1s did we predict as 1?
  \item True Negative Rate: How many of the true 0s did we predict as 0?
  \item Positive Predictive Value: If we predict 1 how likely is it a true 1?
  \item Negative Predictive Value: If we predict 0 how likely is it a true 0?
\end{itemize}
\end{vbframe}


\begin{frame}{History ROC}
ROC = receiver operating characteristics

\lz

Initially developed by electrical engineers and radar engineers during World War II for detecting enemy objects in battlefields. 

\begin{center}
\includegraphics[width=.4\textwidth]{figure_man/receiver_operator.jpg}
{\tiny \url{http://media.iwm.org.uk/iwm/mediaLib//39/media-39665/large.jpg}}
\end{center}

Still has the funny name.
\end{frame}


\begin{vbframe}{Labels: ROC}
Example
\begin{center}
  % FIGURE SOURCE: No source
  \includegraphics[width=\textwidth]{figure_man/roc-confmatrix-example.png}
\end{center}

\end{vbframe}

\begin{vbframe}{More metrics and alternative terminology}

Unfortunately, for many concepts in ROC, 2-3 different terms exist.

\begin{center}
% FIGURE SOURCE: https://en.wikipedia.org/wiki/F1_score#Diagnostic_testing
\includegraphics[width=0.95\textwidth]{figure_man/roc-confmatrix-allterms.png}
\end{center}
\href{https://en.wikipedia.org/wiki/F1_score#Diagnostic_testing}{\beamergotobutton{Clickable version/picture source}} $\phantom{blablabla}$
\href{https://upload.wikimedia.org/wikipedia/commons/0/0e/DiagnosticTesting_Diagram.svg}{\beamergotobutton{Interactive diagram}}
\end{vbframe}


\begin{vbframe}{Labels: $F_1$-Measure}

% \begin{itemize}
%   \item It is difficult to achieve a high \textit{Positive Predictive Value} and high \textit{True Positive Rate} simultaneously.
%   \item A classifier that predicts more "positives" will tend to be more sensitive (higher TPR), but it will also tend to give more false positives (lower TNR, lower PPV).
%   \item A classifier that predicts more "negatives" will tend to be more precise (higher PPV), but it will also produce more false negatives (lower TPR).
% \end{itemize}

\lz
A measure that balances two conflicting goals\\[.5em]
\begin{enumerate}
 \item Maximising Positive Predictive Value
 \item Maximising True Positive Rate\\[.5em]
\end{enumerate}
is the harmonic mean of PPV and TPR:
$$F_1 = 2 \cfrac{PPV \cdot TPR}{PPV + TPR}$$

\lz \lz
Note: still doesnâ€™t account for the number of true negatives.
\end{vbframe}

\begin{vbframe}{Labels: $F_1$-Measure}
\begin{itemize}
  \item A model with $TPR = 0$ (no one predicted as positive from the positive class) or $PPV=0$ (no real positives among the predicted) has an $F_1$ of 0
  \item Predicting always "neg": $F_1 = 0$
  \item Predicting always "pos": $F_1 = 2 PPV / (PPV + 1) = 2 n_+ / (n_+ + n)$,\\ 
  which will be rather small, if the size of the positive class $n_+$ is small.
\end{itemize}

Tabulated $F_1$-Score for different TPR (rows) and PPV (cols) combinations. 
<<echo=FALSE, fig.width=6, fig.height=3, out.width="0.8\\textwidth", comment=NA>>=
oo = options()
options(digits = 2)
tpr = ppv = seq(0, 1, by = 0.2)
f1 = function(tpr, ppv) 2*tpr*ppv / (tpr + ppv)
g = expand.grid(tpr, ppv)
f1_tab = outer(tpr, ppv, f1)
f1_tab[1,1] = 0
colnames(f1_tab) = sprintf("%.1f", tpr)
rownames(f1_tab) = sprintf("%.1f", ppv)
print(f1_tab)
options(oo)
@
$\rightarrow$ Tends more towards the lower of the 2 combined values.
\end{vbframe}

\endlecture

