<<set-parent, include = FALSE>>=
knitr::set_parent("../../style/preamble.Rnw")
@

<<setup, child="../../style/setup.Rnw", include = FALSE>>=
@

%! includes: regression-losses, evaluation-intro

\lecturechapter{Evaluation: Measures for Regression}
\lecture{Introduction to Machine Learning}





\begin{vbframe}{Mean Squared Error}

% The \textbf{Mean Squared Error} compares the mean of the squared distances between the target variable $y$ and the predicted target $\yh$.

\begin{center}
$
MSE = \frac{1}{n} \sumin (\yi - \yih)^2 \in [0;\infty]
$
\hspace{4em} $\rightarrow$ L2 loss.
\end{center}

\lz

Single observations with a large prediction error heavily influence the \textbf{MSE}, as they enter quadratically.

<<echo=FALSE, out.width="0.7\\textwidth", fig.width = 7, fig.height = 2.5>>=
source("rsrc/plot_loss.R")

set.seed(31415)

x = 1:5
y = 2 + 0.5 * x + rnorm(length(x), 0, 1.5)
data = data.frame(x = x, y = y)
model = lm(y ~ x)

plotModQuadraticLoss(data = data, model = model, pt_idx = c(1,4))
@

Similar measures: sum of squared errors (SSE), root mean squared error (RMSE, brings measurement back to the original scale of the outcome).

\end{vbframe}

\begin{vbframe}{Mean Absolute Error}

% A more robust (but not neccessarily better) way to compute a performance measure is the \textbf{Mean Absolute Error}:
\begin{center}
$
MAE = \frac{1}{n} \sumin |\yi - \yih| \in [0;\infty]
$
\hspace{4em} $\rightarrow$ L1 loss.
\end{center}

\lz
Less influenced by large errors and maybe more intuitive than the MSE.

<<echo=FALSE, out.width="0.7\\textwidth", fig.width = 7, fig.height = 3>>=
plotModAbsoluteLoss(data, model = model, pt_idx = c(1,4))
@

Similar measures: median absolute error (for even more robustness).

\end{vbframe}

\begin{vbframe}{$R^2$}

Well known measure from statistics.

\[
R^2 = 1 - \frac{\sumin (\yi - \yih)^2}{\sumin (\yi - \bar{y})^2} = 1 - \frac{SSE_{LinMod}}{SSE_{Intercept}}
\]

\begin{itemize}
\item Usually introduced as \textit{fraction of variance explained} by the model
\item Simpler: Compares SSE of constant model (baseline) with complex model (LM)
\item $R^2=1$: all residuals are 0, we predict perfectly, \\
$R^2=0$: we predict as badly as the constant model
\item If measured on the training data, $R^2 \in [0;1]$ (LM must be at least as good as the constant)
\item On other data $R^2$ can even be negative, as there is no guarantee that the LM generalizes better than a constant (overfitting)
\end{itemize}
\end{vbframe}

\begin{vbframe}{Generalized $R^2$ for ML}
A simple generalization of $R^2$ for ML seems to be:

\[
1 - \frac{Loss_{ComplexModel}}{Loss_{SimplerModel}}
\]

\begin{itemize}
% \item This introduces a general measure of comparison between a simpler baseline, and a more complex model considered as an alternative
\item Works for arbitrary measures (not only SSE), for arbitrary models, on any data set of interest
\item E.g. model vs constant, LM vs. non-linear model, tree vs. forest, model without some features vs. model with them included
% \item In ML we would rather use that metric on a holdout-test set, there is no reason not to do that
\item Fairly unknown; our terminology (generalized $R^2$) is non-standard
\end{itemize}
\end{vbframe}


\endlecture
