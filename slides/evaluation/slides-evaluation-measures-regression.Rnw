<<set-parent, include = FALSE>>=
knitr::set_parent("../../style/preamble.Rnw")
@

<<setup, child="../../style/setup.Rnw", include = FALSE>>=
@

%! includes: regression-losses evaluation-intro

\lecturechapter{Evaluation: Simple Metrics for Regression and Classification}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{Regression: MSE}

The \textbf{Mean Squared Error} compares the mean of the squared distances between the target variable $y$ and the predicted target $\yh$.

\[
MSE = \frac{1}{n} \sumin (\yi - \yih)^2 \in [0;\infty]
\]

Single observations with a large prediction error heavily influence the \textbf{MSE}, as they enter quadratically.

<<echo=FALSE, out.width="0.7\\textwidth", fig.width = 7, fig.height = 2.5>>=
source("rsrc/plot_loss.R")

set.seed(31415)

x = 1:5
y = 2 + 0.5 * x + rnorm(length(x), 0, 1.5)
data = data.frame(x = x, y = y)
model = lm(y ~ x)

plotModQuadraticLoss(data = data, model = model, pt_idx = c(1,4))
@

We could also sum the errors up (SSE), or take the root (RMSE) to bring the measurement back to the original scale of the outcome.

\end{vbframe}

\begin{vbframe}{Regression: MAE}

A more robust (but not neccessarily better) way to compute a performance measure is the \textbf{Mean Absolute Error}:

\[
MAE = \frac{1}{n} \sumin |\yi - \yih| \in [0;\infty]
\]

Less influenced by large errors and maybe more intuitive than the MSE.

<<echo=FALSE, out.width="0.7\\textwidth", fig.width = 7, fig.height = 3>>=
plotModAbsoluteLoss(data, model = model, pt_idx = c(1,4))
@

Instead of averaging we might also consider the median for even more robustness.

\end{vbframe}

\begin{vbframe}{Regression: $R^2$}

Another well known measure from statistics is $R^2$.

\[
R^2 = 1 - \frac{\sumin (\yi - \yih)^2}{\sumin (\yi - \bar{y})^2} = 1 - \frac{SSE_{LinMod}}{SSE_{Intercept}}
\]

\begin{itemize}
\item Usually introduced as \textit{fraction of variance explained} by the model
\item Much simpler explanation: It compares the SSE of a constant model (baseline) with a more complex model (LM), on some data, usually the same as used for model fitting
\item $R^2=1$ implies: all residuals are 0, we predict perfectly, $R^2=0$ implies we predict as badly as a naked constant
\item If measured on the training data, $R^2 \in [0;1]$, as the LM must be at least as good as the constant, and both SSEs are non-negative
\item On other data it could even be negative, as there is no guarantee that the LM generalizes better than a constant (overfitting possible)
\end{itemize}
\end{vbframe}

\begin{vbframe}{Generalized $R^2$ for ML}
A simple generalization of $R^2$ for ML seems to be:

\[
1 - \frac{Loss_{ComplexModel}}{Loss_{SimplerModel}}
\]

\begin{itemize}
\item This introduces a general measure of comparison between a simpler baseline, and a more complex model considered as an alternative
\item This works for arbitrary measures (not only SSE), for arbitrary models, on any data set of interest
\item E.g. model vs constant, LM vs. non-linear model, tree vs. forest, model without some features vs. model with them included
\item In ML we would rather use that metric on a holdout-test set, there is no reason not to do that
\item I do not see this being used or known very often, and my terminology (generalized $R^2$) is non-standard
\end{itemize}
\end{vbframe}


\endlecture
