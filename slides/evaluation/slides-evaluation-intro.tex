\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R



\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}
\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
  %Define standard arrow tip
  >=stealth',
  %Define style for boxes
  punkt/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=6.5em,
    minimum height=2em,
    text centered},
  % Define arrow style
  pil/.style={
    ->,
    thick,
    shorten <=2pt,
    shorten >=2pt,}
}
\usepackage{subfig}


% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}



\begin{document}


% This file loads R packages, configures knitr options and sets preamble.Rnw as parent file
% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...








% Defines macros and environments
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
%! includes: basics-learners 

\lecturechapter{Evaluation: Introduction and Remarks}
\lecture{Introduction to Machine Learning}

% \begin{vbframe}{Goals of performance evaluation}
% 
% 
% \textbf{Model evaluation}\\
% <<out.width = "0.2\\textwidth", fig.height=5>>=
% set.seed(31415)
% 
% x = 1:5
% y = 2 + 0.5 * x + rnorm(length(x), 0, 1.5)
% dat = data.frame(x = x, y = y)
% model = lm(y ~ x)
% dat$y_hat <- predict(model)
% 
% ggplot(data = dat, aes(x = x, y = y)) +
%     geom_abline(intercept = coef(model)[1], slope = coef(model)[2]) +
%     geom_segment(aes(xend = x, yend = y_hat), color = "red") +
%     geom_point() +
%     theme_void()
% @
%      
%      
% 
% \textbf{Algorithm evaluation}\\
% \begin{center}
%     { \Huge \checkmark }
% \end{center}
% 
% 
% \textbf{Model selection}
% \begin{center}
%     \begin{tikzpicture}
%     \draw [thick] (0,0) rectangle (.5,.5);
%     \draw [fill=green, thick] (0.5,0) rectangle (1,1);
%     \draw [thick] (1,0) rectangle (1.5,.75);
%     \end{tikzpicture}
% \end{center}
% 
% \textbf{Learning curve estimation}
% <<fig.height=2, fig.width=5, out.width="0.2\\textwidth">>=
% # rsrc data from rsrc/learning_curve.R
% load("rsrc/learning_curve.RData")
% 
% opt = res.rpart
% opt$mmce = opt$mmce - mean(opt$mmce) + 0.08
% 
% ggplot(data = opt, aes(x = percentage, y = mmce)) + 
%     geom_line(aes(colour = measure), size = 3) + 
%     theme_void() + theme(legend.position = "none")
% @
%     
% \end{vbframe}
% 
% 
% 
% \begin{vbframe}{Goals of performance evaluation}
% 
% \begin{itemize}
%   \item \textbf{Model evaluation:}
%     Estimate \emph{generalization} error of a model on new (unseen) data, drawn from the same data generating process that training data came from.
%   \item \textbf{Algorithm evaluation:}
%     Estimate \emph{generalization} error of a learning algorithm, trained on a data set
%     of a certain size, on new (unseen) data, all drawn from the same data generating process.
%   \item \textbf{Model selection:}
%     Select the best model from a set of potential candidate models (e.g., different model classes, different
%     hyperparameter settings, different features)
%   \item \textbf{Learning curve estimation:}
%     How does the generalization error scale when an algorithm is trained on training sets of different sizes?
% \end{itemize}
% 
% All goals are related, i.e., reliable estimation of (predictive) performance is the foundation for all of them.
% 
% \end{vbframe}

% \begin{vbframe}{Performance Estimation}
% \begin{itemize}
% \item Goal: Estimate performance on new data 
% \item For now, we start by assuming to have a fixed model, already fitted on some data.
% \item We also assume to have some reasonable data for testing available. 
%  \item ML performance evaluation provides clear and simple protocols for reliable model validation. 
%    These protocols are often much simpler than classical statistical model diagnosis and rely only on few assumptions
%  \item Most important assumption: Data we use is realistic and i.i.d.  
% \item ML evaluation is still hard enough and offers LOTS of options to cheat yourself and especially your clients, mistakes can happen on many levels. 
% \end{itemize}
% \end{vbframe}


\begin{vbframe}{Performance Evaluation}
\begin{center}
How well does my model perform...\\

\lz

\begin{tikzpicture}
    \draw [fill=purple, thick] (0,0) rectangle (.5,.5);
    \draw [fill=green, thick] (0.5,0) rectangle (1,1);
    \draw [fill=orange, thick] (1,0) rectangle (1.5,.75);
\end{tikzpicture}

\lz
... on data from the same data-generating process?\\
\lz
In practice:\\
\lz
... on current data (training data)?\\
... on new data (test data)?\\
... based on a certain measure/metric?\\
...

\end{center}
\end{vbframe}


% \begin{vbframe}{Performance Evaluation: Assumption}
% (for now)
% \begin{itemize}
% \item We have a fixed model, already fitted on some data.
% \item We have appropriate test data. 
% \item Data is realistic and i.i.d.
% \end{itemize}
% \end{vbframe}


\begin{vbframe}{Performance Evaluation}
ML performance evaluation provides clear and simple protocols for reliable model
validation. 

\begin{itemize}
\item Often simpler than classical statistical model diagnosis 
\item Relies only on few assumptions
\item Still hard enough and offers \textbf{lots} of options to cheat / make mistakes 
\end{itemize}
\end{vbframe}

\begin{vbframe}{Performance Measures}

We measure performance using a statistical estimator for the 
\textbf{generalization error} (GE).

\lz
$\text{GE} = $ expected loss of a fixed model

\lz
$\hat{\text{GE}} = $ average loss 

\lz \lz

Example: Mean squared error (L2 loss)
\[
\hat{\text{GE}} = MSE = \frac{1}{n} \sumin (\yi - \yih)^2
\]

\end{vbframe}

% \begin{vbframe}{Performance Estimation}
%  Clearly, we are looking for a statistical estimator - for the generalization error! 
%  Different levels of randomness are involved: 
% \begin{itemize}
%  \item Even if we are evaluating on a fixed test data set, this is only a sample and not full reality. 
%    The sample can be too small, then our estimator will be of high variance; 
%    the sample could not be from the distribution of interest, then our estimator will be biased.
%  \item The same holds true for our model - it was only fitted on a sample. This creates another source of randomness, the training
%    data sample. This is true, even if the fitting algorithm is deterministic. 
% \item In ML, many learning algorithms are stochastic: Think random forest, or stochastic gradient descent. This is a third source of randomness.
% \end{itemize}
% \end{vbframe}

\begin{vbframe}{Measures: Inner vs. Outer Loss}

\begin{center}
\lz
Inner loss = loss used in learning\\
\lz
Outer loss = loss used in evaluation\\
= evaluation measure
\lz
%% CC-0 https://publicdomainvectors.org/en/free-clipart/Grumpy-student-vector-image/76859.html

\includegraphics[width=0.3\textwidth]{figure_man/student.pdf}\\

\end{center}

\end{vbframe}



% \begin{vbframe}{Measures: Inner vs. Outer Loss}
% 
% \begin{itemize}
% \item To judge the performance of a model on a given data set, we might want to produce a quantitative 
% measure of the performance on that set. 
% \item Usually we define a function that measures the quality of a prediction per observation.
% \item We then aggregate over the complete set - often by some form of averaging.
% \end{itemize}
% 
% \lz
% 
% Don't we already know this? Sounds like loss functions and risk estimation? 
% It nearly is the same.
% \end{vbframe}

% \begin{vbframe}{Measures: Inner Loss}
% \begin{itemize}
% \item We already covered this, its associated empirical risk is optimized during model fitting
% \item The keyword above is \textbf{optimization}, some functions are much tamer to handle than others, 
%   smoothness and differentiability are often required for efficient optimization
% \item For this reason, we often choose something that's easier to handle numerically
% \item Another pretty practical reason might be: Our toolkit of choice only implements the optimization of a certain inner loss; 
%   changing the outer loss to something custom is simple, changing the inner often is not
% \end{itemize}
% \end{vbframe}
% 
% \begin{vbframe}{Measures: Outer Loss}
% \begin{itemize}
% \item Performance measure to assess the model
% \item Should be carefully considered and selected
% \item There are no objectively better or worse measures - YOU have to select depending on your application, domain
%   and what you want
% \item Except for \enquote{should be reasonable and reflect what I want} there are no huge requirements for an outer measure, it is a function which takes a vector of prediction and a vector of labels and evaluates them
% \item Think about what will happen after a (wrong) prediction of your model, that should help to design an outer loss
% \item Yes, in model selection (later), we optimize it, but we use special techniques there anyway that can deal with arbitrary measures 
% \end{itemize}
% \end{vbframe}


\begin{vbframe}{Measures: Inner vs. Outer Loss}
\lz
Optimally: inner loss = outer loss\\[.5em]
Not always possible:\\ some losses are hard to optimize / no loss is specified directly\\

\lz

\underline{Example}:\\[.5em]
\begin{tabular}{ll}
Logistic Regression & $\rightarrow$ minimize binomial loss \\
kNN & $\rightarrow$ no explicit loss minimization\\
\end{tabular}
\begin{itemize}
  \item When evaluating the models we might be interested in (cost-weighted) classification error 
  \item Or some of the more advanced measures from ROC analysis like AUC
\end{itemize}

% Nowadays, one can also optimize many of the harder losses directly, but this is less standard, less
% often implemented and we will not cover this here.

\end{vbframe}

\endlecture

\end{document}
