\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{figure/eval_mclass_roc_sp_12_1}
\newcommand{\learninggoals}{
\item Understand the ROC curve
\item Be able to compute a ROC curve manually
\item Understand the definition of AUC and what a certain value of AUC means (and what not!)}


\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}


\begin{document}

\lecturechapter{Evaluation: Measures for Binary Classification: ROC Visualization}
\lecture{Introduction to Machine Learning}

% \begin{vbframe}{ROC Space}
% \end{itemize}
% \begin{center}
% \includegraphics[width=0.7\textwidth, height=5.0cm]{figure_man/roc-space1.png}
% \end{center}
% \end{vbframe}


\begin{vbframe}{Labels: ROC Space}
% \begin{itemize}
% \item We characterize a classifier by its TPR and FPR values and plot them in a coordinate
%       system
% \item We could also use 2 different ROC metrics which define a trade-off, like TPR and PPV!
% \end{itemize}


Plot True Positive Rate and False Positive Rate:
\begin{columns}
\begin{column}{0.5\textwidth}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\textwidth]{figure/eval_mclass_roc_sp_1} 

}



\end{knitrout}

\end{column}
\begin{column}{0.5\textwidth} 

% \includegraphics[width=\textwidth]{figure_man/roc-confmatrix2.png}
\begin{center}
\small
\begin{tabular}{cc|cc}
    & & \multicolumn{2}{c}{\bfseries True Class $y$} \\
    & & $+$ & $-$ \\
    \hline
    \bfseries Pred.     & $+$ & TP & FP \\
              $\yh$ & $-$ & FN & TN \\
\end{tabular}

\lz\lz

TPR = $\frac{\text{TP}}{\text{TP} + \text{FN}}$

\lz

FPR = $\frac{\text{FP}}{\text{FP} + \text{TN}}$
\end{center}


\end{column}
\end{columns}

\end{vbframe}


\begin{vbframe}{Labels: ROC Space}
  \begin{itemize}
  \item The best classifier lies on the top-left corner
  \item The diagonal $\approx$ random labels (with different proportions).\\ 
  Assign positive $x$ as "pos" with 25\% probability $\rightarrow$ $TPR = 0.25$.\\ 
  Assign negative $x$ as "pos" with 25\% probability $\rightarrow$ $FPR = 0.25$.
\end{itemize}

\lz
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.4\textwidth]{figure/eval_mclass_roc_sp_2} 

}



\end{knitrout}
\end{vbframe}

\begin{vbframe}{Labels: ROC Space}

In practice, we should never obtain a classifier below the diagonal.\\

\lz

Inverting the predicted labels ($0 \rightarrow 1$ and $1 \rightarrow 0$) will result in a reflection at the diagonal. 

\lz

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.4\textwidth]{figure/eval_mclass_roc_sp_3} 

}



\end{knitrout}
\end{vbframe}



\begin{vbframe}{Label Distribution in TPR and FPR}
TPR and FPR are insensitive to the class distribution:\\
Not affected by changes in the ratio $\np/\nn$ (at prediction).\\

\begin{columns}
\begin{column}{0.45\textwidth}

\underline{Example 1}:\\
Proportion $\np/\nn = 1$\\

\lz

{
\tiny
\centering
\tiny
\begin{tabular}{|l|c|c|}
                \hline
               & Actual Positive & Actual Negative \\ \hline
Pred. Positive & 40            & 25            \\ \hline
Pred. Negative & 10            & 25           \\ \hline
\end{tabular}
}
 
\lz 

MCE = 35/100\\
$TPR = 0.8$\\ 
$FPR = 0.5$ 

\end{column}
\begin{column}{0.45\textwidth} 

\underline{Example 2}:\\
Proportion $\np/\nn = 2$\\

\lz

{
\tiny
\begin{tabular}{|l|c|c|}
                \hline
               & Actual Positive & Actual Negative \\ \hline
Pred. Positive & 80            & 25            \\ \hline
Pred. Negative & 20            & 25           \\ \hline
\end{tabular}
}
 
\lz 
 
MCE = 45/150 = 30/100\\
$TPR = 0.8$\\ 
$FPR = 0.5$ 
\end{column}
\end{columns}



\lz

Note: If class proportions differ during training, the above is not true. Estimated posterior probabilities can change!

\end{vbframe}



% \begin{vbframe}{Scoring Classifiers}
% \begin{itemize}
% \item A scoring classifier is a model which outputs scores or probabilities, instead of discrete labels, and nearly all modern classifiers can do that.
% \item Thresholding flexibly converts measured probabilities to labels.
%   Predict $1$ (positive class) if $\fxh > c$ else predict $0$.
% \item Normally we could use $c = 0.5$ to convert, but for imbalanced or cost-sensitive situations another threshold could be much better.
% \item After thresholding, any metric defined on labels can be used.
% \end{itemize}
% \begin{center}
% % FIGURE SOURCE: https://docs.google.com/presentation/d/1GmlgtjSCTHgSAveVGf-x1ojAjGP2llPhFKjn_6M4Sig/edit?usp=sharing
% \includegraphics[width=0.5\textwidth]{figure_man/confusion_matrix_measures}
% \end{center}
% \end{vbframe}

\begin{vbframe}{From Probabilities to Labels: ROC Curve}

Remember: Both probabilistic and scoring classifiers can output classes by thresholding.\\
$$\hx:= [\pix) \ge c] \quad \text{ or } \quad \hx = [\fx \ge c]$$
% \begin{center}
%   \includegraphics{../supervised-classification/figure_man/classifiers.png}
% \end{center}

\begin{columns}
\begin{column}{0.45\textwidth}

\textbf{To draw a ROC curve}:\\

Iterate through all possible thresholds $c$

\lz 


$\rightarrow$ Visual inspection of all possible thresholds / results

\end{column}
\begin{column}{0.45\textwidth} 

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\textwidth]{figure/eval_mclass_roc_sp_4}

}



\end{knitrout}


\end{column}
\end{columns}

\end{vbframe}



% \begin{vbframe}{ROC Curve}
% \begin{center}
% \includegraphics[width=\textwidth]{figure_man/roc-curves2.png}
% \end{center}
% \end{vbframe}

% \begin{vbframe}{ROC Curve}
% \begin{itemize}
%   \item Rank test observations on decreasing score
%   \item Set $\alpha=1$, so we start in $(0, 0)$; we predict everything as "neg"
%   \item For each observation $x$ (in the decreasing order).
%   \begin{itemize}
%     \item Reduce threshold, so prediction for next observation changes
%     \item If $x$ is "pos", move TPR $1/n_+$ up, as we have one TP more
%     \item If $x$ is "neg", move FPR $1/n_-$ right, as we have one FP more
%   \end{itemize}
% \end{itemize}
% \end{vbframe}


\begin{vbframe}{ROC Curve}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.8\textwidth]{figure/eval_mclass_roc_sp_5} 

}



\end{knitrout}

$c =$ 0.9\\ 
$\rightarrow$ TPR = 0.167 \\
$\rightarrow$ FPR = 0


\framebreak

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.8\textwidth]{figure/eval_mclass_roc_sp_6} 

}



\end{knitrout}

$c =$ 0.85\\ 
$\rightarrow$ TPR = 0.333 \\
$\rightarrow$ FPR = 0

\framebreak

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.8\textwidth]{figure/eval_mclass_roc_sp_7} 

}



\end{knitrout}

$c =$ 0.66\\ 
$\rightarrow$ TPR = 0.5 \\
$\rightarrow$ FPR = 0

\framebreak

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.8\textwidth]{figure/eval_mclass_roc_sp_8}

}



\end{knitrout}

$c =$ 0.6\\ 
$\rightarrow$ TPR = 0.5 \\
$\rightarrow$ FPR = 0.167


\framebreak

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.8\textwidth]{figure/eval_mclass_roc_sp_9} 

}



\end{knitrout}

$c =$ 0.55\\ 
$\rightarrow$ TPR = 0.667 \\
$\rightarrow$ FPR = 0.167

\framebreak

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.8\textwidth]{figure/eval_mclass_roc_sp_10} 

}



\end{knitrout}

$c =$ 0.3\\ 
$\rightarrow$ TPR = 0.833 \\
$\rightarrow$ FPR = 0.5

\framebreak

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.8\textwidth]{figure/eval_mclass_roc_sp_11} 

}



\end{knitrout}

\end{vbframe}

\begin{vbframe}{ROC Curve}
\begin{itemize}
  \item The closer the curve to the top-left corner, the better
  \item If ROC curves cross, a different model can be better in different parts of the ROC space
\end{itemize}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=.65\textwidth]{figure/eval_mclass_roc_sp_12} 

}



\end{knitrout}
\end{vbframe}


\begin{vbframe}{AUC: Area Under ROC Curve}

\begin{itemize}
  \item The AUC (in [0,1]) is a single metric to evaluate scoring classifiers
  \item AUC = 1: Perfect classifier
  \item AUC = 0.5: Randomly ordered
  % \item AUC = 0: Perfect, with inverted labels
\end{itemize}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.8\textwidth]{figure/eval_mclass_roc_sp_12_1} 

}



\end{knitrout}
\end{vbframe}


\begin{vbframe}{AUC: Area Under ROC Curve}
Interpretation: Probability that classifier ranks a random positive higher than a random negative observation

\begin{center}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1xj9_84181bqFpr0EMqdGHE6dUf_vAf1qcs9z-siUsCw/edit?usp=sharing
\includegraphics[width=0.8\textwidth,page=1]{figure_man/auc_interpretation.pdf}
\end{center}

\end{vbframe}


\begin{vbframe}{Partial AUC}
\begin{itemize}
  \item Sometimes it can be useful to look at a \href{http://journals.sagepub.com/doi/pdf/10.1177/0272989X8900900307}{specific region under the ROC curve}  $\Rightarrow$ partial AUC (pAUC).
  \item Examples: focus on a region with low FPR or a region with high TPR:
\end{itemize}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.9\textwidth]{figure/eval_mclass_roc_sp_13} 

}



\end{knitrout}

\end{vbframe}




\endlecture
\end{document}
