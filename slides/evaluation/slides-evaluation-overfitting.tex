\input{../../style/preamble}

\newcommand{\titlefigure}{figure/eval_ofit_1}
\newcommand{\learninggoals}{
\item Understand what overfitting is and why it is a problem
\item Understand how to avoid overfitting}


\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}




\begin{document}

\lecturechapter{Evaluation: Overfitting}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{Overfitting}

\begin{columns}[T,onlytextwidth]
\column{0.5\textwidth}
Overfitting learner \\
\vspace{0.5cm}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\textwidth]{figure/eval_ofit_1} 

}



\end{knitrout}
Better training set performance (seen examples)

\column{0.5\textwidth}
Non-overfitting learner \\
\vspace{0.5cm}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\textwidth]{figure/eval_ofit_2} 

}



\end{knitrout}
Better test set performance (unseen examples)
\end{columns}


\framebreak


\begin{itemize}
  \item Happens when algorithm models patterns beyond the data- generating process,
    e.g., noise or artefacts in the training data
  \item Reason: too many hypotheses and not enough data to tell them apart
  \item Less in bigger data sets
  \item If hypothesis space is not constrained, there may never be enough data
  \item Many learners have a parameter that allows constraining (\textit{regularization})
  % \item In this unit we will only give a very basic definition, and not really talk
  %   about measures against overfitting (see regularization!)
  \item Check for overfitting by validating on a new unseen test data set
\end{itemize}

\end{vbframe}


% \begin{vbframe}{Overfitting and Noise}
% \begin{itemize}
%   \item Overfitting is seriously exacerbated by \textit{noise} (errors in the training data)
%   \item An unconstrained learner will start to model that noise
%   \item It can also arise when relevant features are missing in the data
%   \item In general it's better to make some mistakes on training data ("ignore some observations") than trying to get all correct
% \end{itemize}
% \end{vbframe}

% \begin{vbframe}{Avoiding Overfitting}
% \begin{itemize}
%   \item You should never believe your model until you've \textit{verified it on data that the learner didn't see}
%   \item Scientific method applied to machine learning: model must make new predictions that can be experimentally verified
%   \item Use less complex models 
%   \item Get more, or better data
%   \item Some learner can do "early stopping" before perfectly fitting (i.e., overfitting) the training data
%   \item Use regularization
% \end{itemize}
% \end{vbframe}

% \begin{vbframe}{Triple Trade-Off}

% In all learning algorithms that are trained from data, there is a trade-off between three factors:
% \begin{itemize}
  % \item The complexity of the hypothesis we fit to the training data
  % \item The amount of training data (in terms of both instances and informative features)
  % \item The generalization error on new examples
% \end{itemize}
% If the capacity of the learning algorithm is large enough to a) approximate the data generating process and b) exploit the information contained in the data, the generalization error will decrease as the amount of training data increases.

% For a fixed size of training data, the generalization error decreases first and then starts to increase (overfitting) as the complexity of the hypothesis space $H$ increases.
% \end{vbframe}

\begin{vbframe}{Trade-Off Between Generalization Error and Complexity}

% Apparent error (on the training data) and real error (prediction error on new data) evolve in the opposite direction with increasing complexity:

\lz
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/eval_ofit_3} 

}



\end{knitrout}

\lz
$\Rightarrow$ Optimization regarding model complexity is desirable:\\ 
Find the right amount of complexity for the given amount of data where generalization error becomes minimal.

\end{vbframe}

\endlecture
\end{document}
