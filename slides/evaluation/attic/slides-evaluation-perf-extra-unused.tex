\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R



\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}
\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
  %Define standard arrow tip
  >=stealth',
  %Define style for boxes
  punkt/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=6.5em,
    minimum height=2em,
    text centered},
  % Define arrow style
  pil/.style={
    ->,
    thick,
    shorten <=2pt,
    shorten >=2pt,}
}
\usepackage{subfig}


% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
\newcommand{\titlefigure}{figure_man/intro-titlefig.jpg}
\newcommand{\learninggoals}{
\item Understand the goal of performance estimation
\item Know the definition of generalization error
\item Understand the difference between outer and inner loss}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}


\begin{document}


% This file loads R packages, configures knitr options and sets preamble.Rnw as parent file
% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...


% Defines macros and environments
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-automl.tex}
%! includes: basics-learners 

\lecturechapter{Evaluation: Introduction and Remarks}
\lecture{Introduction to Machine Learning}

% ------------------------------------------------------------------------------

\begin{vbframe}{Regression: Defining a custom loss}

Assume a use case, where the target variable can have a wide range of values across different orders of magnitude.
A possible solution would be to use a loss functions that allows for better model evaluation and comparison.
The \textbf{Mean Squared Logarithmic Absolute Error} is not strongly influenced by large values due to the logarithm.

\[
\frac{1}{n} \sumin (\log(|\yi - \yih| + 1))^2
\]

% which data/model???

% <<echo=FALSE, out.width="0.7\\textwidth", fig.width = 7, fig.height = 3>>=
% plotModAbsLogQuadLoss(data, model = model, pt_idx = c(1,4))
% @
\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{List of Classification Performance Measures}

\small

\begin{center}
\begin{tabular}{c l}
\hline

 \textbf{Classification}     & \textbf{Explanation}                                               \\
\hline
 \textit{Accuracy}           &  Fraction of correct classifications                               \\
 \textit{Balanced Accuracy}  &  Fraction of correct classifications in each class                 \\
 \textit{Recall}             &  Fraction of positives a classifier captures                       \\
 \textit{Precision}          &  Fraction of positives in instances predicted as positive          \\
 \textit{F1-Score}           &  Tradeoff between precision and recall                             \\
 \textit{AUC}                &  Measures calibration of predicted probabilities                   \\
 \textit{Brier Score}        &  Squared difference between predicted probability and  \\
  & true label   \\
 \textit{LogLoss}            &  Emphasizes errors for predicted probabilities close to \\
 & 0 and 1    \\
\hline
\end{tabular}
\end{center}

\normalsize

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Bias-Variance Tradeoff}

We can decompose the generalization error for $L_2$-loss as follows:
\begin{eqnarray*}
  \GED &=& \E( L(y, \fhD(x)) | \D) \\
  &=& \E((y-\hat{f}(x))^2) \\
  &\overset{Var(y)=\E(y^2)-\E(y)^2}{=}& Var(y) + \E(y)^2 + Var(\hat f(x)) + \\
  &&\E(\hat f(x))^2 - 2 \E(y) \E(\hat f(x)) \\
  &\overset{\E(y) = f(x)}{=}& Var(y) + Var(\hat f(x)) + (f(x) - \E(\hat f(x)))^2\\
  &=& \sigma^2 + Var(\hat{f}(x)) + Bias(\hat{f}(x))^2
\end{eqnarray*}

where

\begin{itemize}
  \item $\sigma^2$: intrinsic variability of the data, cannot be avoided
  \item $Var(\hat f(x))$: variance of the model, the learners's tendency to learn random things irrespective of the signal (\textit{overfitting})
  \item $Bias(\hat f(x))^2$: systematic bias of the model (\textit{underfitting})
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Bias-Variance Tradeoff}

  \begin{itemize}
    \item We can reduce the model's variance on the cost of its bias and vice versa by controlling the model complexity.
    \item We search for the perfect Bias-Variance-Tradeoff that minimizes our expected prediction error.
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=7cm]{figure_man/bias_variance.png}
\end{figure}

\end{vbframe}

% ------------------------------------------------------------------------------

\endlecture
\end{document}
