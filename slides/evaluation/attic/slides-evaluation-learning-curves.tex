\input{../../style/preamble}


%\newcommand{\titlefigure}{} %MISSING!
\newcommand{\learninggoals}{
  \item XXXXXXXXXXXXXX
  \item XXXXXXXXX
}




\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}




\begin{document}

\lecturechapter{Evaluation: Learning Curves}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{Learning Curves}

\begin{itemize}
\item The \textit{Learning Curve} compares the performance of a model on training and test data over a varying number of training instances.\\
$\rightarrow$ How fast can learner learn the given relationship in the data?
\item Learning usually fast in the beginning %and saturate after data becomes larger an larger
% \item We should generally see performance improve as the number of training points increases When we separate training and testing sets and graph them individually
% \item We can get an idea of how well the model can generalize to new data
\item Visualizes when a learner has learned as much as it can:
\begin{itemize}
\item when performance on training and test set reach a plateau.
\item when gap between training and test error remains the same.
\end{itemize}
\end{itemize}





\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.9\textwidth]{figure/eval_learn_curves_1} 

}



\end{knitrout}

\framebreak

An ideal learning curve looks like:

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/eval_learn_curves_2} 

}



\end{knitrout}


\framebreak

In general, there are two reasons for a bad looking learning curve:

\begin{enumerate}
\item High bias in model / underfitting
\begin{itemize}
\item training and test errors converge at a high value.
\item model can't learn underlying relationship and has high systematic errors, no matter how big the training set.
\item poor fit, which also translates into high test error.
\end{itemize}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/eval_learn_curves_3} 

}



\end{knitrout}

\framebreak

\item High variance in model / Overfitting
\begin{itemize}
\item large gap between training and test errors.
\item model requires more training data to improve.
\item model has a poor fit and does not generalize well.
%\item Can simplify the model with fewer or less complex features
\end{itemize}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/eval_learn_curves_4} 

}



\end{knitrout}
\end{enumerate}
\end{vbframe}

\endlecture

\end{document}
