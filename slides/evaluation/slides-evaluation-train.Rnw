<<set-parent, include = FALSE>>=
knitr::set_parent("../../style/preamble.Rnw")
@
<<setup, child="../../style/setup.Rnw", include = FALSE>>=
@

%! includes: evaluation-intro 


\lecturechapter{Evaluation: Training Error}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{Training Error}

The \emph{training error} (also called apparent error or resubstitution error)
is estimated by the averaging error over the same data set we fitted on:

\lz

\begin{center}
% FIGURE SOURCE: https://docs.google.com/drawings/d/1m0Uwf5bvWuP1agyZ0TOd0qaBakJVtfe_PspxqPL3mxU/edit?usp=sharing
\includegraphics[width=.55\textwidth]{figure_man/train_error.pdf}
\end{center}

\vspace{-0.8cm}

\end{vbframe}

\begin{vbframe}{Example: Polynomial Regression}

Sample data from sinusoidal function
$0.5 + 0.4 \cdot \sin (2 \pi x) + \epsilon$\\
with measurement error $\epsilon$.

<<echo=FALSE, out.width="0.85\\textwidth", fig.width = 5.5, fig.height = 2, fig.align="center">>=
source("rsrc/plot_train_test.R")

ggTrainTestPlot(data = mydf, truth.fun = .h, truth.min = 0, truth.max = 1, 
    test.plot = FALSE, test.ind = ind)[["plot"]] + ylim(0, 1)
@

Assume data generating process unknown.\\
Try to approximate with a $d$th-degree polynomial:
\[ \fxt = \theta_0 + \theta_1 x + \cdots + \theta_d x^d = \sum_{j = 0}^{d} \theta_j x^j\text{.} \]

\framebreak

Models of different \textit{complexity}, i.e., of different orders of the polynomial
are fitted. How should we choose $d$?

<<echo=FALSE, out.width="0.7\\textwidth", fig.width = 5, fig.height = 3, fig.align="center">>=
out[["plot"]] + ylim(0, 1) + theme(legend.position = "top")
@

\begin{itemize}
\item d=1: MSE = \Sexpr{sprintf("%.03f", out$train.test$degree1[1])}: Clear underfitting
\item d=3: MSE = \Sexpr{sprintf("%.03f", out$train.test$degree3[1])}: Pretty OK?
\item d=9: MSE = \Sexpr{sprintf("%.03f", out$train.test$degree9[1])}: Clear overfitting
\end{itemize}

Simply using the training error seems to be a bad idea.

\end{vbframe}

\begin{vbframe}{Training Error Problems}
\begin{itemize}
  \item Unreliable and overly optimistic estimator of future performance.
    E.g. training error of 1-NN is always zero, as each observation is its own NN during test time
    (assuming we do not have repeated measurements with conflicting labels).
  \item Goodness-of-fit measures like (classical) $R^2$, likelihood, AIC, BIC, deviance are all based on the training error.
  \item For models of restricted capacity, and given enough data, the training error may provide reliable information.\\ 
  E.g. LM with $p = 5$ features, $10^6$ training points.\\ 
  But: impossible to determine when training error becomes unreliable.
\end{itemize}
\end{vbframe}

\endlecture
