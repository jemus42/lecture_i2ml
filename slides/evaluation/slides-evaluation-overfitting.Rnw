% Introduction to Machine Learning
% Day 3

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../../style/preamble.Rnw")
@

% Load all R packages and set up knitr
<<setup, child="../../style/setup.Rnw", include = FALSE>>=
@

%! includes: evaluation-intro

\lecturechapter{Overfitting}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{Overfitting}

\begin{columns}[T,onlytextwidth]
\column{0.5\textwidth}
Overfitting learner \\
\vspace{0.5cm}
<<echo=FALSE, results='hide', out.width='\\textwidth'>>=
library(mlr)
library(mlbench)
library(plyr)

data = as.data.frame(mlbench.2dnormals(n = 200, cl = 3))
data$classes = mapvalues(data$classes, "3", "1")
task = makeClassifTask(data = data, target = "classes")
learner = makeLearner("classif.ksvm")
plotLearnerPrediction(learner, task, kernel = "rbfdot", C = 1, sigma = 100, pointsize = 4)
@
Better training set performance (seen examples)

\column{0.5\textwidth}
Non-overfitting learner \\
\vspace{0.5cm}
<<echo=FALSE, results='hide', out.width='\\textwidth'>>=
plotLearnerPrediction(learner, task, kernel = "rbfdot", C = 1, sigma = 1, pointsize = 4)
@
Better test set performance (unseen examples)
\end{columns}


\framebreak


\begin{itemize}
  \item Overfitting is a well-known problem in ML for non-linear, powerful learning algorithms
  \item It happens when your algorithm starts modelling patterns in the data that are not actually true in the real world,
    e.g., noise or artefacts in the training data
  \item Happens when you have too many hypotheses and not enough data to tell them apart
  \item The more data, the more "bad" hypotheses are eliminated
  \item If the hypothesis space is not constrained, there may never be enough data
  \item There is often a parameter that allows you to constrain (\textit{regularize}) the learner
  \item In this unit we will only give a very basic definition, and not really talk
    about measures against overfitting (see regularization!)
\end{itemize}

\end{vbframe}


% \begin{vbframe}{Overfitting and Noise}
% \begin{itemize}
%   \item Overfitting is seriously exacerbated by \textit{noise} (errors in the training data)
%   \item An unconstrained learner will start to model that noise
%   \item It can also arise when relevant features are missing in the data
%   \item In general it's better to make some mistakes on training data ("ignore some observations") than trying to get all correct
% \end{itemize}
% \end{vbframe}

\begin{vbframe}{Avoiding Overfitting}
\begin{itemize}
  \item You should never believe your model until you've \textit{verified it on data that the learner didn't see}
  \item Scientific method applied to machine learning: model must make new predictions that can be experimentally verified
  \item Use less complex models 
  \item Get more, or better data
  \item Some learner can do "early stopping" before perfectly fitting (i.e., overfitting) the training data
  \item Use regularization
\end{itemize}
\end{vbframe}

% \begin{vbframe}{Triple Trade-Off}

% In all learning algorithms that are trained from data, there is a trade-off between three factors:
% \begin{itemize}
  % \item The complexity of the hypothesis we fit to the training data
  % \item The amount of training data (in terms of both instances and informative features)
  % \item The generalization error on new examples
% \end{itemize}
% If the capacity of the learning algorithm is large enough to a) approximate the data generating process and b) exploit the information contained in the data, the generalization error will decrease as the amount of training data increases.

% For a fixed size of training data, the generalization error decreases first and then starts to increase (overfitting) as the complexity of the hypothesis space $H$ increases.
% \end{vbframe}

\begin{vbframe}{Trade-Off Between Generalization Error and Complexity}

% Apparent error (on the training data) and real error (prediction error on new data) evolve in the opposite direction with increasing complexity:

\lz
<<fig.height=3, fig.width=8, echo=FALSE>>=
par(mar = c(2.1, 2.1, 0, 0))
x <- seq(0, 1, length.out = 20)
y1 <- c(1 - x[1:4], 1.5 - 4 * x[5:6], 0.5 - 0.8 * x[7:10], 0.15 - 0.1 * x[11:20])
X <- seq(0, 1, length.out = 1000)
Y1 <- predict(smooth.spline(x, y1, df = 10), X)$y
plot(X, Y1, type = "l", axes = FALSE, xlab = "Complexity", ylab = "Error")
mtext("Complexity", side = 1, line = 1)
mtext("Error", side = 2, line = 1)
Y2 <- 0.5 * X
lines(X, Y1 + Y2)
abline(v = 0.42, lty = 2)
text(0.4, 0.93, "Underfitting", pos = 2)
text(0.44, 0.93, "Overfitting", pos = 4)
arrows(0.4, 0.98, 0.2, 0.98, length = 0.1)
arrows(0.44, 0.98, 0.64, 0.98, length = 0.1)
box()
text(0.85, 0.13, "Apparent error")
text(0.85, 0.55, "Actual error")
@

\lz
$\Rightarrow$ Optimization regarding the model complexity is desirable:\\ 
Find the right amount of complexity for the given amount of data where generalization error becomes minimal.

\end{vbframe}

\endlecture
