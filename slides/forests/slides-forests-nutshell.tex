
\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

%\newcommand{\titlefigure}{figure/reg_l2_residual.pdf}
\newcommand{\learninggoals}{
\item 
\item 
\item 
}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}
\lecturechapter{Random Forest: In a Nutshell}
\lecture{Introduction to Machine Learning}

% ------------------------------------------------------------------------------

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% L2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Learning and Prediction with RF}
\begin{itemize}
\item \small Stabilizes tree learner by bagging (bootstrap aggregation)
\item \small Randomizes tree learner and combines models into one meta model
\item \small Can be adapted to learning task, i.e., classification or regression
\end{itemize}

\begin{columns}  
\begin{column}{0.1\textwidth} 
\begin{center}
\vspace{2.2cm}
Training
\end{center}
\end{column}
\begin{column}{0.9\textwidth} 
\begin{center}
%https://docs.google.com/presentation/d/1y5E3s9xQLjcJWPL3zllrldjHtuonyhEP/edit#slide=id.p1
  \includegraphics[width = 1\textwidth]{slides/forests/figure_man/nutshell-randomforest-learning.png}
\end{center}
\end{column}
\end{columns}
\end{vbframe} 

\begin{vbframe}
\begin{columns}
\begin{column}{0.05\textwidth} 
\begin{center}
\vspace{0.7cm}
Prediction
\end{center}
\end{column}
\begin{column}{0.95\textwidth} 
\begin{center}
%https://docs.google.com/presentation/d/13rYrVvmecOc8pePPsOcQtAzy9cJZk3m4/edit#slide=id.p1
  \includegraphics[width = 0.6\textwidth]{slides/forests/figure_man/nutshell-randomforest-prediction.png} 
\end{center}
\end{column}
\end{columns}

\end{vbframe} 

% ------------------------------------------------------------------------------

\begin{vbframe}{Aggregation rules for different tasks}
\begin{columns}  
\begin{column}{0.5\textwidth} 
\begin{center}
%https://docs.google.com/presentation/d/1XrGkql56NxQsAI5XNNonmzfsTWlKWAok/edit#slide=id.p1
 \includegraphics[width = 1\textwidth]{slides/forests/figure_man/nutshell-randomforest-aggregation-classif.png}
 \end{center}
\end{column}
\begin{column}{0.5\textwidth} 
\begin{center}
%https://docs.google.com/presentation/d/1Ta22VthQmGVshGoC3JAGEfKYPK46RHcB/edit#slide=id.p1
  \includegraphics[width = 1\textwidth]{slides/forests/figure_man/nutshell-randomforest-aggregation-regression.png}
\end{center}
\end{column}
\end{columns}



\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Performance of RF}
\begin{itemize}
\item \small In general: Increasing the ensemble size stabilizes the predictions
\item \small RF is often sub-obtimal for regression tasks. Without further regularization it tends to overfit 
\end{itemize}
\begin{columns}  
\begin{column}{0.5\textwidth} 
\begin{center}
 \includegraphics[width = 1\textwidth]{slides/forests/figure/nutshell_forest_ensemblesize_1.pdf}
 \end{center}
\end{column}
\begin{column}{0.5\textwidth} 
\begin{center}
  \includegraphics[width = 1\textwidth]{slides/forests/figure/nutshell_forest_ensemblesize_3.pdf}
\end{center}
\end{column}
\end{columns}

\end{vbframe}

\begin{vbframe}{Performance of RF}
\begin{itemize}
\item \small RF performs well for classification tasks
\begin{center}
 \includegraphics[width = 0.7\textwidth]{slides/forests/figure/nutshell_classif_combined_1.pdf}
\end{center}
\end{itemize}



\end{vbframe}

\begin{vbframe}{Performance of RF}
\begin{itemize} 
\item \small Trees should be decorrelated, i.e., make mistakes in different directions
\item \small Avoid correlation by
    \begin{itemize}
        \item \small Bootstrap sampling 
        \item \small Randomized splits. In each node of each tree, consider different features for splitting:
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\begin{center}
%https://docs.google.com/presentation/d/1rJZbyGoX4I9RFKO9VbetJxZX4o9dy_aZ/edit#slide=id.p1
  \includegraphics[width = 0.6\textwidth]{slides/forests/figure_man/nutshell-randomforest-random-split.png}
\end{center}




\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{feature importance}
% Two ways for measuring feature importance
\begin{itemize}
\item \small Measure contributions of different features to the model:
    \begin{itemize}
        \item \small Measure based on improvement in split criterion
        \item \small E.g. Feature importance of 'Health':
    \end{itemize}
\end{itemize}    
%https://docs.google.com/presentation/d/1ApCeEQsttK47pUyTkeJMhaxbBxyUX9YD/edit#slide=id.p1
\includegraphics[width = \textwidth]{slides/forests/figure_man/nutshell-random-forest-feature-importance-split.png}

\vspace{1cm}
\begin{itemize}
    \item \small Measure based on permutations of OOB obs.
    \item \small Difference between loss of permuted-model prediction and original model predicition
\end{itemize}
%https://docs.google.com/presentation/d/1hCLliyZ3EyZbZr6iTdtzXYUKC3JMzPn5/edit#slide=id.p1
\includegraphics[width = \textwidth]{slides/forests/figure_man/nutshell-randomforest-OOB_1.png}

%https://docs.google.com/presentation/d/1Id4MrrZ8s_W69SA_dIADLmNFoHV58tVo/edit#slide=id.p1
\includegraphics[width = \textwidth]{slides/forests/figure_man/nutshell-randomforest-OOB_2.png}

\end{vbframe}

% ------------------------------------------------------------------------------


\endlecture
\end{document}