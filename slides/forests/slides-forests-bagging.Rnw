% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../../style/preamble.Rnw")
library(ggplot2)
@

% Load all R packages and set up knitr
<<setup, child="../../style/setup.Rnw", include = FALSE>>=
@

%! includes: basics-riskminimization, basics-supervised,  performance-resampling

\lecturechapter{Random Forests: Bagging Ensembles}
\lecture{Introduction to Machine Learning}
\sloppy



\begin{vbframe}{Bagging}

\begin{itemize}
  \item Bagging is short for \textbf{B}ootstrap \textbf{Agg}regation.
  \item It's an \textbf{ensemble method}, i.e., it combines many models into one 
        big \enquote{meta-model}
  \item Such model ensembles often work much better than their members alone would.
  \item for reasons, the constituent models of an ensemble are called \textbf{base learners} 
      % that improves instable / high variance learners by variance smoothing
\end{itemize}

\framebreak 
In a \textbf{bagging} ensemble, all base learners are of the same type. The only difference between the models is the data they are trained on.\\
Specifically, we train base learners $\blm, m = 1, \dots, M$ on $M$ \textbf{bootstrap} samples of training data $\D$:
\begin{itemize}
  \item Draw $n$ observations from $\D$ with replacement
  \item Fit the base learner on each of the $B$ bootstrap samples to get models $\hat f(x) = \blmh, m = 1, \dots, M$
\end{itemize}

\begin{center}
% FIGURE SOURCE: No source
\includegraphics[width=0.55\textwidth]{figure_man/bootstrapping.png}
\end{center}

\framebreak

\textbf{Aggregate} the predictions of the $M$ fitted base learners to get the
\textbf{ensemble model} $\fMh$:
  \begin{itemize}
    \item Aggregate via averaging (regression) or majority voting (classification)
    \item Posterior class probabilities $\pikxh$ can be estimated by calculating predicted class frequencies over the ensemble
  \end{itemize}

\begin{center}
% FIGURE SOURCE: No source
\includegraphics[width=0.6\textwidth]{figure_man/rf_majvot_averaging.png}
\end{center}
\end{vbframe}

% \begin{algorithm}[H]
%   \small
%   \setstretch{1.15}
%   \caption*{Bagging algorithm}
%   \begin{algorithmic}[1]
%     \State {\bf Input: } Dataset $\D$, base learner, number of bootstraps $M$
%     \For {$m = 1 \to M$}
%       \State Draw a bootstrap sample $\D^{[m]}$ from $\D$.
%       \State Train base learner on $\D^{[m]}$ to obtain model $\blm$
%     \EndFor
%     \State Aggregate the predictions of the $M$ estimators (via averaging or majority voting), to determine the bagging estimator:
%     \begin{align*}
%     \fM &= \frac{1}{M} \sum_{m=1}^M \blm \\
%     \text{or}\quad \fM &= \argmax_{k \in \Yspace} \sum_{m=1}^M \I\left(\blm = k\right)
%     \end{align*}
%   \end{algorithmic}
% \end{algorithm}

\begin{vbframe}{Why/when does Bagging help?}

In one sentence:\\
\lz

Because the variability of the average of the predictions of many base learner models is smaller than the variability of the predictions from one such base learner model.\\

If the error of a base learner model is mostly due to (random) variability and not due to structural reasons, combining many such base learners by bagging helps by reducing this variability.


\framebreak
\begin{scriptsize}
 Assume we use quadratic loss and measure instability of the ensemble with $\ambifM = \tfrac{1}{M}\sum^M_{m} \left(\blm- \fM \right)^2$:
 \vskip -2em
 \begin{align*}
 \ambifM &= \tfrac{1}{M}\sum^M_{m} \left(\blm- \fM\right)^2 \\
         &= \tfrac{1}{M}\sum^M_{m} \left(\left(\blm - y\right)  + \left(y - \fM\right)\right)^2\\
         &= \tfrac{1}{M}\sum^M_{m} L(y, \blm) + L(y, \fM) \underbrace{- 2 \left(y - \tfrac{1}{M}\sum^M_{m}\blm\right)\left(y - \fM\right)}_{- 2 L\left(y, \fM\right)} \\[-.5em]
  \intertext{so, if we take the expected value over the data's distribution:}
  \E_{xy}\left[L\left(y, \fM\right)\right] &= \tfrac{1}{M}\sum^M_{m} \E_{xy}\left[L\left(y, \blm \right)\right] - \E_{xy}\left[\ambifM\right]
  \end{align*}
\end{scriptsize}
$\Rightarrow$ The expected loss of the ensemble is lower than the average loss of the single base learner by the amount of instability in the ensemble's base learners.\\ The more accurate and diverse the base learners, the better.
\framebreak
\end{vbframe}

\begin{vbframe}{Improving Bagging}
\begin{scriptsize}
\begin{align*}
\shortintertext{How to make $\E_{xy}\left[\ambifM\right]$ as large as possible?}
\E_{xy}\left[L\left(y, \fM \right)\right] &= \tfrac{1}{M}\sum^M_{m} \E_{xy}\left[L\left(y, \blm \right)\right] - \E_{xy}\left[\ambifM\right] \\
\shortintertext{Assume $\E_{xy}\left[\blm\right] = 0$ for simplicity, $\var_{xy}\left[\blm\right] = \E_{xy}\left[(\blm)^2\right] = \sigma^2$, $\corr_{xy}\left[\blm, \bl{m'}\right] = \rho$ for all $m, m'$.}
\implies 
\var_{xy}\left[\fM\right] &= \tfrac{1}{M} \sigma^2 +  \tfrac{M-1}{M} \rho \sigma^2 \qquad\left(... = \E_{xy}\left[(\fM)^2\right]\right)\\
 \E_{xy}\left[\ambifM\right] &= \tfrac{1}{M}\sum^M_{m} \E_{xy}\left[\left(\blm- \fM\right)^2\right]\\
 & = \tfrac{1}{M}\left(M \E_{xy}\left[(\blm)^2\right] + M \E_{xy}\left[(\fM)^2\right] - 
     2 M \E_{xy}\left[\blm\fM\right]\right) \\
  & = \sigma^2  + \E_{xy}\left[(\fM)^2\right] - 2 \tfrac{1}{M}\sum^M_{m'} \underbrace{\E_{xy}\left[\blm \bl{m'} \right]}_{\mathclap{\qquad\qquad\qquad\qquad= \cov_{xy}\left[\blm \bl{m'} \right] + \E_{xy}\left[\blm\right]\E_{xy}\left[\bl{m'}\right]}} \\
  &=  \sigma^2  + \left(\tfrac{1}{M} \sigma^2 +   \tfrac{M-1}{M} \rho \sigma^2\right) - 2\left(\tfrac{M-1}{M} \rho\sigma^2 + \tfrac{1}{M}\sigma^2 + 0 \cdot 0 \right)\\
  &= \tfrac{M-1}{M} \sigma^2 (1-\rho)
\end{align*}
\end{scriptsize}

\begin{small}
\begin{align*}
\E_{xy}\left[L\left(y, \fM\right)\right] &= \textcolor{blue}{\tfrac{1}{M}\sum^M_{m} \E_{xy}\left[L\left(y, \blm \right)\right]} - \E_{xy}\left[\ambifM\right]\\
\E_{xy}\left[\ambifM\right] &\cong 
\textcolor{purple}{\frac{M-1}{M}} \textcolor{cyan}{\var_{xy}\left[\blm\right]} \textcolor{violet}{\left(1 - \corr_{xy}\left[\blm, \bl{m'}\right]\right)}
\end{align*}
\end{small}
\begin{itemize}
\item[$\Rightarrow$] \textcolor{blue}{\textbf{better base learners}} are better {\small (... duh)}
\item[$\Rightarrow$] \textcolor{purple}{\textbf{more base learners}} are better {\small (theoretically, at least...)}\\
\item[$\Rightarrow$] \textcolor{cyan}{\textbf{more variable base learners}} are better {\small(as long as their risk stays the same, of course!)}
\item[$\Rightarrow$] \textcolor{violet}{\textbf{less correlation between base learners}} is better:\\ bagging helps more if baselearners are wrong in different ways so that their errors \enquote{cancel} each other out.\\
\end{itemize}

<<eval=FALSE, echo=FALSE, fig.height= 2.5, fig.align="center">>=
# artificial graphic, numbers are fictive
rho = seq(0, 1, by = 0.001)
B = c(5, 50)
sigma = 20

grid = expand.grid(rho = rho, B = B)

grid$var = grid$rho * sigma + (1 - grid$rho) / grid$B * sigma
grid = grid[order(grid$B), ]
grid$B = as.factor(grid$B)

horizontal = data.frame(
  B = as.factor(B),
  intercept = sigma / B,
  intercept.label = sigma / B + c(0, 0.7))

p1 = ggplot(data = grid, aes(x = rho, y = var)) +
  geom_line(aes(group = B, colour = B)) +
  geom_hline(aes(yintercept = 20), colour = "black", lty = 2) +
  geom_hline(data = horizontal, aes(yintercept = intercept, colour = B), lty = 2) +
  xlab(expression(paste("Correlation of Trees ", rho))) +
  ylab("Variance") +
  labs(colour = "Number of Trees") +
  annotate("text", x = 1.1, y = sigma, label = "sigma^2", parse = TRUE) +
  geom_text(data = horizontal, aes(x = rep(1.08, 2), y = intercept.label, color = B,
    label = paste0("sigma^2 / ", B)), parse = TRUE, show.legend = FALSE, hjust = 0) +
  coord_cartesian(xlim = c(0, 1), clip = "off") +
  ylim(c(0, 20)) + 
  theme_minimal()

p1
@
\end{vbframe}

\begin{vbframe}{Bagging: Synopsis}

  \begin{itemize}
    \item Basic idea: fit the same model repeatedly on many \textbf{bootstrap} replications of the training dataset and \textbf{aggregate} the results
    \item Gains performance by reducing the variance of predictions, but (slightly) increases the bias: it reuses training data many times, so small mistakes can get amplified. 
    \item Works best for unstable/high variance base learners where small changes in the training set can cause large changes in the prediction:\\
    e.g. CART, Neural networks, Step-wise/forward/backward variable selection for regression\\
     \item Works best if base learners' predictions are only weakly correlated: they don't all make the same mistakes.
         \item Can degrade performance for stable methods like $k$-NN, LDA, Naive Bayes, linear regression
  \end{itemize}
\end{vbframe}

\endlecture
