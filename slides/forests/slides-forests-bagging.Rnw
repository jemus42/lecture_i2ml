% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../../style/preamble.Rnw")
library(ggplot2)
@

% Load all R packages and set up knitr
<<setup, child="../../style/setup.Rnw", include = FALSE>>=
@


\lecturechapter{Random Forests: Bagging Ensembles}
\lecture{Introduction to Machine Learning}
\sloppy



\begin{vbframe}{Bagging}

\begin{itemize}
  \item Bagging is short for \textbf{B}ootstrap \textbf{Agg}regation.
  \item It's an \textbf{ensemble method}, i.e., it combines many models into one 
        big \enquote{meta-model}
  \item Such model ensembles often work much better than their members alone would.
  \item for reasons, the constituent models of an ensemble are called \textbf{base learners} 
      % that improves instable / high variance learners by variance smoothing
\end{itemize}

\framebreak 
In a \textbf{bagging} ensemble, all base learners are of the same type. The only difference between the models is the data they are trained on.\\

\lz

Specifically, we train them on $M$ \textbf{bootstrap} samples of training data $\D$:
\begin{itemize}
  \item Draw $n$ observations from $\D$ with replacement
  \item Fit the base learner on each of the $M$ bootstrap samples
\end{itemize}

\begin{center}
% FIGURE SOURCE: No source
\includegraphics[width=0.45\textwidth]{figure_man/bootstrapping.png}
\end{center}

\framebreak

\textbf{Aggregate} the predictions of the $M$ fitted base learners:
  \begin{itemize}
    \item Aggregate via averaging (regression) or majority voting (classification)
    \item Posterior class probabilities $\pikxh$ can be estimated by calculating predicted class frequencies over the ensemble
  \end{itemize}

\begin{center}
% FIGURE SOURCE: No source
\includegraphics[width=0.7\textwidth]{figure_man/rf_majvot_averaging.png}
\end{center}

% \begin{algorithm}[H]
%   \small
%   \setstretch{1.15}
%   \caption*{Bagging algorithm}
%   \begin{algorithmic}[1]
%     \State {\bf Input: } Dataset $\D$, base learner, number of bootstraps $M$
%     \For {$m = 1 \to M$}
%       \State Draw a bootstrap sample $\D^{[m]}$ from $\D$.
%       \State Train base learner on $\D^{[m]}$ to obtain model $\blm$
%     \EndFor
%     \State Aggregate the predictions of the $M$ estimators (via averaging or majority voting), to determine the bagging estimator:
%     \begin{align*}
%     \fM &= \frac{1}{M} \sum_{m=1}^M \blm \\
%     \text{or}\quad \fM &= \argmax_{k \in \Yspace} \sum_{m=1}^M \I\left(\blm = k\right)
%     \end{align*}
%   \end{algorithmic}
% \end{algorithm}

\framebreak

why/when help?
let's say our risk is based on squared errors.
think of model predictions $\hat f(x)$ as random variables with expected value & variance with which we try to approximate the unknown "true" model $f(x) = y$ .
we can decompose the total error we make into 

a) bias: the difference between f(x) and E(hat f) (systematic error, for example because the true f has a shape that the learner that gives us our model hat f isn't flexible enough to cover). 
b) Var(hat f) gives us the size of its random fluctuations (unsystematic error): different random selections of training data give us a different fitted model that makes different predictions. 
taken together, the variance and the bias constitute the size of the prediction error we expect to make. 

let's ignore the bias for now.
say we have M models.  then the 


  \begin{itemize}
    \item Reduces variance of the predictor, but (slightly) increases its bias
    \item Bagging works best for unstable/high variance learners (learners where small perturbations of the training set can cause large changes in the prediction)

    \begin{itemize}
      \item Classification and regression trees
      \item Neural networks
      \item Step-wise/forward/backward variable selection for regression
    \end{itemize}

    \item For stable estimation methods bagging might degrade performance
    \begin{itemize}
      \item k-nearest neighbor
      \item discriminant analysis
      \item naive Bayes
      \item linear regression
    \end{itemize}

  \end{itemize}
\end{vbframe}

% \begin{vbframe}{Why does bagging work?}
% \begin{itemize}
%   \item Suppose we have a numerical target variable and are looking at quadratic loss.
%   \item The training datasets are given by $\D$, and the base learner estimators derived from it are $f(x)$. $f(x)$ is a random variable whose realized value depends on the values drawn from $\D$.
%   \item The datasets are sampled independently from distribution $\P_{xy}$ (data generating process).
%   \item The {\em theoretical} aggregated estimator is given by
%     \begin{align*}
%       f_{\text{A}} (x) &= \E_\D[f(x)].
%     \end{align*}

%   \framebreak


%  \item So: The more unstable or diverse $f(x)$ is, the more error reduction we can obtain by bagging.
%  \item But the bagging estimator only approximates the theoretical $f_A$ (bootstrap), we therefore suffer from approximation error (bias) by using the empirical distribution function instead of the true data generating process and only performing $M$ bootstrap iterations instead of all possible bootstrap samples.
% \item Bagging does not necessarily lead to an improved classifier -- (pathological) example:
% \begin{itemize}
% \item Binary outcome, $y = 1$ for all values of $x$
% \item Consider random classifier $f$ with $\text{P}(\fx = 1) = 0.4$
% (independent of $x$)
% \item Expected misclassification rate for $f$ is 0.6
% \item Expected misclassification rate for a majority-vote bagging estimator is
% $\P(z \leq \tfrac{M}{2}) \stackrel{M \to \infty}{\longrightarrow} 1$ for $z \sim B(M, p = 0.4)$.
% \end{itemize}

%\framebreak
%\newcommand{\ambiblm}{\text{ambi}\left(\blm\right)}
%\newcommand{\ambifM}{\text{ambi}\left(\fM\right)}
% \item Bagging improves predictions if the baselearners in the ensemble are diverse:
% \begin{itemize}
% \item Measure diversity as \enquote{ambiguity} of baselearners and ensemble with $\ambiblm = \left(\blm- \fM \right)^2$,
%  $\ambifM = \tfrac{1}{M}\sum^M_{m} \ambiblm$
%  \item for quadratic loss, we can write:
%  \begin{scriptsize}
%  \begin{align*}
%  \ambifM &= \tfrac{1}{M}\sum^M_{m} \left(\blm- \fM\right)^2 \\
%          &= \tfrac{1}{M}\sum^M_{m} \left(\left(\blm - y\right)  + \left(y - \fM\right)\right)^2\\
%          &= \tfrac{1}{M}\sum^M_{m} L(y, \blm) + L(y, \fm) -\\
%          & \qquad\qquad\underbrace{- 2 \left(y - \tfrac{1}{M}\sum^M_{m}\blm\right)\left(y - \fM\right)}_{- 2 L(y, \fm)} \\
%   \text{so } \E_{xy}\left[L(y, \fm)\right] = \tfrac{1}{M}\sum^M_{m} \E_{xy}\left[L(y, \blm)\right] - \E_{xy}\left[\ambifM\right]
%   \end{align*}
%   \end{scriptsize}
%   \end{itemize}
%   \item The expected loss of the ensemble is always below the average loss of the single base learners, by the amount of ambiguity in the base learners.
%   \item The more accurate and diverse the base learners, the better the ensemble.
% \end{itemize}
% \end{vbframe}

\begin{vbframe}{Variance of Bagging}


  \[ \rho \sigma^2 + \frac{1-\rho}{B} \sigma^2 = \left( \rho + (1 - \rho) \frac{1}{B} \right) \sigma^2 \]
  $\sigma^2$ is variance of a tree and $\rho$ the correlation between trees

\begin{itemize}
  \item If trees are highly correlated ($\rho \approx 1$), variance $\rightarrow \sigma^2$
  \item If trees are uncorrelated ($\rho \approx 0$), variance $\rightarrow \frac{\sigma^2}{B}$
  \item Variance can be reduced by increasing the number of trees $B$
\end{itemize}

<<eval=TRUE, echo=FALSE, fig.height= 2.5, fig.align="center">>=
# artificial graphic, numbers are fictive
rho = seq(0, 1, by = 0.001)
B = c(5, 50)
sigma = 20

grid = expand.grid(rho = rho, B = B)

grid$var = grid$rho * sigma + (1 - grid$rho) / grid$B * sigma
grid = grid[order(grid$B), ]
grid$B = as.factor(grid$B)

horizontal = data.frame(
  B = as.factor(B),
  intercept = sigma / B,
  intercept.label = sigma / B + c(0, 0.7))

p1 = ggplot(data = grid, aes(x = rho, y = var)) +
  geom_line(aes(group = B, colour = B)) +
  geom_hline(aes(yintercept = 20), colour = "black", lty = 2) +
  geom_hline(data = horizontal, aes(yintercept = intercept, colour = B), lty = 2) +
  xlab(expression(paste("Correlation of Trees ", rho))) +
  ylab("Variance") +
  labs(colour = "Number of Trees") +
  annotate("text", x = 1.1, y = sigma, label = "sigma^2", parse = TRUE) +
  geom_text(data = horizontal, aes(x = rep(1.08, 2), y = intercept.label, color = B,
    label = paste0("sigma^2 / ", B)), parse = TRUE, show.legend = FALSE, hjust = 0) +
  coord_cartesian(xlim = c(0, 1), clip = "off") +
  ylim(c(0, 20))

p1
@
\end{vbframe}

\endlecture
