% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../../style/preamble.Rnw")
library(rpart)
library(rpart.plot)
library(randomForest)
library(rattle)
library(smoof)
@

% Load all R packages and set up knitr
<<setup, child="../../style/setup.Rnw", include = FALSE>>=
@


\lecturechapter{Random Forest: Bagging Ensembles}
\lecture{Introduction to Machine Learning}
\sloppy



\begin{vbframe}{Bagging}

\begin{itemize}
  \item Bagging is based on \textbf{B}ootstrap \textbf{Agg}regation.
  \item Ensemble that improves instable / high variance learners by variance smoothing
\end{itemize}

Train on $B$ \textbf{bootstrap} samples of data $D$:
\begin{itemize}
  \item Draw $n$ observations with replacement
  \item Fit the base learner on each of the $B$ bootstrap samples
\end{itemize}

\begin{center}
% FIGURE SOURCE: No source
\includegraphics[width=0.45\textwidth]{figure_man/bootstrapping.png}
\end{center}



\framebreak

\textbf{Aggregate} the predictions of the $B$ estimators:
  \begin{itemize}
    \item Aggregate via averaging (regression) or majority voting (classification)
    \item Posterior probabilities for $x$ in classification can be estimated by calculating class frequencies over the ensemble
  \end{itemize}

\begin{center}
% FIGURE SOURCE: No source
\includegraphics[width=0.7\textwidth]{figure_man/rf_majvot_averaging.png}
\end{center}

% \begin{algorithm}[H]
%   \small
%   \setstretch{1.15}
%   \caption*{Bagging algorithm}
%   \begin{algorithmic}[1]
%     \State {\bf Input: } Dataset $\D$, base learner, number of bootstraps $M$
%     \For {$m = 1 \to M$}
%       \State Draw a bootstrap sample $\D^{[m]}$ from $\D$.
%       \State Train base learner on $\D^{[m]}$ to obtain model $\blm$
%     \EndFor
%     \State Aggregate the predictions of the $M$ estimators (via averaging or majority voting), to determine the bagging estimator:
%     \begin{align*}
%     \fM &= \frac{1}{M} \sum_{m=1}^M \blm \\
%     \text{or}\quad \fM &= \argmax_{k \in \Yspace} \sum_{m=1}^M \I\left(\blm = k\right)
%     \end{align*}
%   \end{algorithmic}
% \end{algorithm}

\framebreak

  \begin{itemize}
    \item Reduces variance of the predictor, but (slightly) increases its bias
    \item Bagging works best for unstable/high variance learners (learners where small perturbations of the training set can cause large changes in the prediction)

    \begin{itemize}
      \item Classification and regression trees
      \item Neural networks
      \item Step-wise/forward/backward variable selection for regression
    \end{itemize}

    \item For stable estimation methods bagging might degrade performance
    \begin{itemize}
      \item k-nearest neighbor
      \item discriminant analysis
      \item naive Bayes
      \item linear regression
    \end{itemize}

  \end{itemize}
\end{vbframe}

% \begin{vbframe}{Why does bagging work?}
% \begin{itemize}
%   \item Suppose we have a numerical target variable and are looking at quadratic loss.
%   \item The training datasets are given by $\D$, and the base learner estimators derived from it are $f(x)$. $f(x)$ is a random variable whose realized value depends on the values drawn from $\D$.
%   \item The datasets are sampled independently from distribution $\P_{xy}$ (data generating process).
%   \item The {\em theoretical} aggregated estimator is given by
%     \begin{align*}
%       f_{\text{A}} (x) &= \E_\D[f(x)].
%     \end{align*}

%   \framebreak


%  \item So: The more unstable or diverse $f(x)$ is, the more error reduction we can obtain by bagging.
%  \item But the bagging estimator only approximates the theoretical $f_A$ (bootstrap), we therefore suffer from approximation error (bias) by using the empirical distribution function instead of the true data generating process and only performing $M$ bootstrap iterations instead of all possible bootstrap samples.
% \item Bagging does not necessarily lead to an improved classifier -- (pathological) example:
% \begin{itemize}
% \item Binary outcome, $y = 1$ for all values of $x$
% \item Consider random classifier $f$ with $\text{P}(\fx = 1) = 0.4$
% (independent of $x$)
% \item Expected misclassification rate for $f$ is 0.6
% \item Expected misclassification rate for a majority-vote bagging estimator is
% $\P(z \leq \tfrac{M}{2}) \stackrel{M \to \infty}{\longrightarrow} 1$ for $z \sim B(M, p = 0.4)$.
% \end{itemize}

%\framebreak
%\newcommand{\ambiblm}{\text{ambi}\left(\blm\right)}
%\newcommand{\ambifM}{\text{ambi}\left(\fM\right)}
% \item Bagging improves predictions if the baselearners in the ensemble are diverse:
% \begin{itemize}
% \item Measure diversity as \enquote{ambiguity} of baselearners and ensemble with $\ambiblm = \left(\blm- \fM \right)^2$,
%  $\ambifM = \tfrac{1}{M}\sum^M_{m} \ambiblm$
%  \item for quadratic loss, we can write:
%  \begin{scriptsize}
%  \begin{align*}
%  \ambifM &= \tfrac{1}{M}\sum^M_{m} \left(\blm- \fM\right)^2 \\
%          &= \tfrac{1}{M}\sum^M_{m} \left(\left(\blm - y\right)  + \left(y - \fM\right)\right)^2\\
%          &= \tfrac{1}{M}\sum^M_{m} L(y, \blm) + L(y, \fm) -\\
%          & \qquad\qquad\underbrace{- 2 \left(y - \tfrac{1}{M}\sum^M_{m}\blm\right)\left(y - \fM\right)}_{- 2 L(y, \fm)} \\
%   \text{so } \E_{xy}\left[L(y, \fm)\right] = \tfrac{1}{M}\sum^M_{m} \E_{xy}\left[L(y, \blm)\right] - \E_{xy}\left[\ambifM\right]
%   \end{align*}
%   \end{scriptsize}
%   \end{itemize}
%   \item The expected loss of the ensemble is always below the average loss of the single base learners, by the amount of ambiguity in the base learners.
%   \item The more accurate and diverse the base learners, the better the ensemble.
% \end{itemize}
% \end{vbframe}

\begin{vbframe}{Variance of Bagging}


  \[ \rho \sigma^2 + \frac{1-\rho}{B} \sigma^2 = \left( \rho + (1 - \rho) \frac{1}{B} \right) \sigma^2 \]
  $\sigma^2$ is variance of a tree and $\rho$ the correlation between trees

\begin{itemize}
  \item If trees are highly correlated ($\rho \approx 1$), variance $\rightarrow \sigma^2$
  \item If trees are uncorrelated ($\rho \approx 0$), variance $\rightarrow \frac{\sigma^2}{B}$
  \item Variance can be reduced by increasing the number of trees $B$
\end{itemize}

<<eval=TRUE, echo=FALSE, fig.height= 2.5, fig.align="center">>=
# artificial graphic, numbers are fictive
rho = seq(0, 1, by = 0.001)
B = c(5, 50)
sigma = 20

grid = expand.grid(rho = rho, B = B)

grid$var = grid$rho * sigma + (1 - grid$rho) / grid$B * sigma
grid = grid[order(grid$B), ]
grid$B = as.factor(grid$B)

horizontal = data.frame(
  B = as.factor(B),
  intercept = sigma / B,
  intercept.label = sigma / B + c(0, 0.7))

p1 = ggplot(data = grid, aes(x = rho, y = var)) +
  geom_line(aes(group = B, colour = B)) +
  geom_hline(aes(yintercept = 20), colour = "black", lty = 2) +
  geom_hline(data = horizontal, aes(yintercept = intercept, colour = B), lty = 2) +
  xlab(expression(paste("Correlation of Trees ", rho))) +
  ylab("Variance") +
  labs(colour = "Number of Trees") +
  annotate("text", x = 1.1, y = sigma, label = "sigma^2", parse = TRUE) +
  geom_text(data = horizontal, aes(x = rep(1.08, 2), y = intercept.label, color = B,
    label = paste0("sigma^2 / ", B)), parse = TRUE, show.legend = FALSE, hjust = 0) +
  coord_cartesian(xlim = c(0, 1), clip = "off") +
  ylim(c(0, 20))

p1
@
\end{vbframe}

\endlecture
