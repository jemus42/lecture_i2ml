\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R



\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}
\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
  %Define standard arrow tip
  >=stealth',
  %Define style for boxes
  punkt/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=6.5em,
    minimum height=2em,
    text centered},
  % Define arrow style
  pil/.style={
    ->,
    thick,
    shorten <=2pt,
    shorten >=2pt,}
}
\usepackage{subfig}


% Defines macros and environments
\input{../../style/common.tex}
% \input{common.tex}

%\usetheme{lmu-lecture}
\newcommand{\titlefigure}{figure/cart_forest_fimp_1a}
\newcommand{\learninggoals}{
\item Understand that the goal of defining variable importance is to enhance interpretability of the random forest
\item Know definition of variable importance based on improvement in split criterion
\item Know definition of variable importance based on permutations of OOB observations}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}



\begin{document}
% Introduction to Machine Learning
% Day 4

% Set style/preamble.Rnw as parent.


% Load all R packages and set up knitr

% This file loads R packages, configures knitr options and sets preamble.Rnw as parent file
% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...








% Defines macros and environments
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-trees.tex}
\input{../../latex-math/ml-bagging.tex}

%! includes: forests-intro


\lecturechapter{Random Forests: Feature Importance}
\lecture{Introduction to Machine Learning}

\sloppy


\begin{vbframe}{Variable Importance}

\begin{itemize}
  \item Single trees are highly interpretable
  \item Random forests as ensembles of trees lose this feature
  \item Contributions of the different features to the model are difficult to evaluate
  \item Way out: variable importance measures
  \item Basic idea: by how much would the performance of the random forest decrease if a specific feature were removed or rendered useless?
\end{itemize}


\framebreak

\begin{algorithm}[H]
  \small
  \caption*{Measure based on improvement in split criterion}
  \begin{algorithmic}[0]
    \For{features $x_j$, $j = 1$ to $p$}
    \For{tree base learners $\blmh$, $m = 1$ to $M$}
    \State {Find all nodes $\Np$ in $\blmh$ that use $x_j$.} 
    \State {Compute improvement in splitting criterion achieved by them.}
    \State {Add up these improvements.}
    \EndFor
    \State {Add up improvements over all trees to get feature importance of $x_j$.}
    %\State {This is the feature importance for $x_j$.}
    \EndFor
  \end{algorithmic}
\end{algorithm}
\vskip -2em
\begin{figure}
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/cart_forest_fimp_1} 

}



\end{knitrout}
% \caption{Two importance measures on the iris dataset.}
\end{figure}

\framebreak

\begin{minipage}[b]{0.35\textwidth}
  % FIGURE SOURCE: https://docs.google.com/drawings/d/1A0hdwZrwHI8zwMdchY-rn0bpGcNYO-Y4te20DMSwQ84/edit
  \includegraphics[width = 0.9\textwidth]{figure_man/forests-featimp-new}
\end{minipage}%
\begin{minipage}[b]{0.65\textwidth}
  \begin{algorithm}[H]
  \scriptsize
  \caption*{\small{Measure based on permutations of OOB \\observations}}
  \begin{algorithmic}[0]
    \State Estimate OOB error $\widehat{\text{err}}_{\text{OOB}}$.
    \For{features $x_j$, $j = 1$ to $p$}
      \State {Perform permutation $\psi_j$ on $x_j$ to distort \\ 
      \phantom{\textbf{for}} feature-target 
      relation for $x_j$.}
    \For{distorted observations $(\mathbf{x}^{(i)}_{\psi_j}, \yi)$, 
    $i = 1$ to $n$}
      % \State {Find trees for which $\xyi$ is OOB.}
      \State {Compute OOB prediction $\yih_{\text{OOB}, \psi_j}$.}
      \State {Compute corresponding loss 
      $L(\yi, \yih_{\text{OOB}, \psi_j})$.}
    \EndFor
    \State {Estimate importance of $j$-th variable}
    \State {$\widehat{\text{VI}_j} = \widehat{\text{err}}_{\text{OOB}, \psi_j} - 
    \widehat{\text{err}}_{\text{OOB}} $}
    \State {\phantom{$\widehat{\text{VI}_j}$} 
    $= \meanin L(\yi, \yih_{\text{OOB}, \psi_j}) - 
    \widehat{\text{err}}_{\text{OOB}}$.}
    \EndFor
  \end{algorithmic}
\end{algorithm}

\end{minipage}

% \framebreak

% <<>>=
% lrn = makeLearner("classif.randomForest", importance = TRUE)
% mod = train(lrn, iris.task)
% mlr::getFeatureImportance(mod)
% @

% \framebreak

% <<echo=TRUE, size="footnotesize", fig.height=4>>=
% rf = getLearnerModel(mod)
% randomForest::varImpPlot(rf,
%   main = "Variable Importance")
% @

% \framebreak

% <<echo=TRUE, size="footnotesize", fig.height=4>>=
% v = generateFilterValuesData(iris.task,
%   method = c("rf.importance", "cforest.importance"))
% plotFilterValues(v)
% @


\end{vbframe}


\endlecture
\end{document}
