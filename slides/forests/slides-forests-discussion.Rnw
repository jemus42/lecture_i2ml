% Introduction to Machine Learning
% Day 4

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../../style/preamble.Rnw")
library(methods)
library(rpart)
library(rpart.plot)
library(randomForest)
library(rattle)
library(smoof)
@

% Load all R packages and set up knitr
<<setup, child="../../style/setup.Rnw", include = FALSE>>=
@

%! includes: forests-intro, forests-featureimportance

\lecturechapter{Random Forests: Advantages and Disadvantages}
\lecture{Introduction to Machine Learning}

\sloppy
\begin{vbframe}{Random Forest: Advantages}

\begin{itemize}
  \item All advantages of trees also apply to RF: not much preprocessing required, missing value handling, etc.
  \item Easy to parallelize
  \item Often works well (enough)
  \item Integrated variable importance
  \item Integrated estimation of generalization performance via OOB error
  \item Works well on high-dimensional data 
  \item Works well on data with irrelevant \enquote{noise} variables
  \item Often not much tuning necessary
\end{itemize}

\end{vbframe}

\begin{vbframe}{Random Forest: Disadvantages}

\begin{itemize}
  \item Often suboptimal for regression
  \item Same extrapolation problem as for trees
  \item Harder to interpret than trees (but many extra tools are nowadays
    available for interpreting RFs)
  % \item Less aggressive risk minimization compared to boosting
  \item Implementation can be memory-hungry
  \item Prediction is computationally demanding for large ensembles
\end{itemize}

\end{vbframe}

\begin{vbframe}{Random Forest: Synopsis}
\textbf{Hypothesis Space:}\\
Random forest models are (sums of) step functions over rectangular partitions of (subspaces of) $\Xspace$.\\
Their maximal complexity is controlled by the number of trees in the random forest ensemble and the stopping criteria for the constituent trees.

\lz

\textbf{Risk:}\\
Like trees, random forests can use any kind of loss function for regression or classification.

\lz

\textbf{Optimization:}\\
Exhaustive search over all (randomly selected!) candidate splits in each node of each tree to minimize the empirical risk in the child nodes.\\ 

{\small
Like all bagging methods, optimization can be done in parallel over the ensemble members.}

\end{vbframe}

\endlecture
