% Introduction to Machine Learning
% Day 4

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../../style/preamble.Rnw")
library(methods)
library(rpart)
library(rpart.plot)
library(randomForest)
library(rattle)
library(smoof)
@

% Load all R packages and set up knitr
<<setup, child="../../style/setup.Rnw", include = FALSE>>=
@

%! includes: forests-intro, forests-featureimportance

%! includes: forests-intro, forests-featureimportance

\lecturechapter{Random Forest: Advantage and Disadvantage}
\lecture{Introduction to Machine Learning}

\sloppy
\begin{vbframe}{Random Forest: Advantages}

\begin{itemize}
\item Bagging is easy to implement
  \item Can be applied to basically any model
  \item All advantages of trees propagate to the RF, especially w.r.t. preprocessing
  \item Easy to parallelize
  \item Often works well (enough)
  \item Integrated variable importance
  \item Integrated estimation of OOB error
  \item Can work on high-dimensional data
  \item Often not much tuning necessary
\end{itemize}

\end{vbframe}

\begin{vbframe}{Random Forest: Disadvantages}

\begin{itemize}
  \item Often suboptimal for regression
  \item Same extrapolation problem as for trees
  \item Harder to interpret than trees, but many extra tools are nowadays
    available for interpreting RFs
  \item Less aggressive risk minimization compared to boosting
  \item Implementation can be memory-hungry
  \item Prediction is computationally demanding for large ensembles
\end{itemize}

\end{vbframe}

\begin{vbframe}{Random Forest: Synopsis}
\textbf{Hypothesis Space:}\\
Random forest models are (sums of) step functions over rectangular partitions of (subspaces of) $\Xspace$.\\
Their maximal complexity is controlled by the number of trees in the random forest ensemble and the stopping criteria for the constituent trees.

\lz

\textbf{Risk:}\\
Like trees, random forests can use any kind of loss function for regression or classification.

\lz

\textbf{Optimization:}\\
Exhaustive search over all candidate splits in each node of each tree to minimize the empirical risk in the child nodes.\\ 

{\small
Like all bagging methods, optimization can be done in parallel over the ensemble members.}

\end{vbframe}

\endlecture
