% Introduction to Machine Learning
% Day 4

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../../style/preamble.Rnw")
library(methods)
library(rpart)
library(rpart.plot)
library(randomForest)
library(rattle)
library(smoof)
@

% Load all R packages and set up knitr
<<setup, child="../../style/setup.Rnw", include = FALSE>>=
@

%! includes: forests-intro, forests-featureimportance


\lecturechapter{Random Forest: Advantage and Disadvantage}
\lecture{Introduction to Machine Learning}

\sloppy
\begin{vbframe}{Random Forest: Advantages}

\begin{itemize}
\item Bagging is easy to implement
  \item Can be applied to basically any model
  \item All advantages of trees propagate to the RF, especially w.r.t. preprocessing
  \item Easy to parallelize
  \item Often works well (enough)
  \item Integrated variable importance
  \item Integrated estimation of OOB error
  \item Can work on high-dimensional data
  \item Often not much tuning necessary
\end{itemize}

\end{vbframe}

\begin{vbframe}{Random Forest: Disadvantages}

\begin{itemize}
  \item Often suboptimal for regression
  \item Same extrapolation problem as for trees
  \item Harder to interpret than trees, but many extra tools are nowadays
    available for interpreting RFs
  \item Does not really optimize loss aggressively in comparison to boosting
  \item Implementations sometimes memory-hungry
  \item Prediction can be (slightly) slower, as it is an ensemble
\end{itemize}

\end{vbframe}

\endlecture
