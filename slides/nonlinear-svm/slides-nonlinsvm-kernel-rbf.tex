\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-svm}

\newcommand{\titlefigure}{figure_man/non-lin-svm-rbf-as-basis.png}
\newcommand{\learninggoals}{
  \item \textcolor{blue}{XXX}
  \item \textcolor{blue}{XXX}
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{The Gaussian RBF Kernel}
\lecture{Introduction to Machine Learning}



\begin{vbframe}{RBF Kernel}

The \enquote{radial} \textbf{Gaussian kernel} is defined as
$$k(\xv, \tilde \xv) = \exp(-\frac{\|\xv - \tilde \xv\|^2}{2\sigma^2}) \quad \text{ or } \quad k(\xv, \tilde \xv) = \exp(-\gamma \|\xv - \tilde \xv\|^2)$$

\begin{center}
\includegraphics[width = 11cm ]{figure_man/rbf_kernel.png}
\end{center}

A straightforward extension is
$$k(\xv,  \tilde \xv) = \exp\big(- (\xv -  \tilde \xv)^T C (\xv -  \tilde \xv)\big)$$
for a symmetric, positive definite matrix $C$.

\framebreak

  \begin{itemize}

    \item With a Gaussian kernel, all RKHS basis functions 
      $\phix = k(\xv, \cdot)$ are linearly independent - which we will not prove here.
    \item This means that all (finite) data sets are linearly separable!
    \item Do we then need soft-margin machines? 
      The answer is \enquote{yes}. The roles of the nonlinear feature map and the
    soft-margin constraints are very different:
    \begin{itemize}
      \item The purpose of the kernel (and its feature map) is to make learning
      \enquote{easy}.
\item Even in an infinite-dimensional feature space we may want some
      margin violators because we should not trust noisy data. A hard-margin
      SVM with Gaussian kernels may be able to separate any dataset but will usually overfit.
    \end{itemize}
  \end{itemize}
\end{vbframe}

\begin{frame}{Weighted mixture of Gaussians}

 Via the RKHS / basis function intuition we can understand the effect of the RBF kernel much better as a local model.
$$ \fx = \sumin \alpha_i \yi k(\xi, \xv)  + \theta_0 $$
\vspace{-1cm}
\begin{columns}
\begin{column}{0.7\textwidth}

\begin{center}
\includegraphics<1>[width=0.9\textwidth,trim={2.5cm 5cm 2.5cm 5cm},clip]{figure_man/non-lin-svm-rbf-as-basis-1.png}%
\includegraphics<2>[width=0.9\textwidth,trim={2.5cm 5cm 2.5cm 5cm},clip]{figure_man/non-lin-svm-rbf-as-basis-2.png}%
\includegraphics<3>[width=0.9\textwidth,trim={2.5cm 5cm 2.5cm 5cm},clip]{figure_man/non-lin-svm-rbf-as-basis-3.png}%
\includegraphics<4>[width=0.9\textwidth,trim={2.5cm 5cm 2.5cm 5cm},clip]{figure_man/non-lin-svm-rbf-as-basis-4.png}%
\includegraphics<5>[width=0.9\textwidth,trim={2.5cm 5cm 2.5cm 5cm},clip]{figure_man/non-lin-svm-rbf-as-basis-5.png}%
\includegraphics<6>[width=0.9\textwidth,trim={2.5cm 5cm 2.5cm 5cm},clip]{figure_man/non-lin-svm-rbf-as-basis-6.png}%
\includegraphics<7>[width=0.9\textwidth,trim={2.5cm 5cm 2.5cm 5cm},clip]{figure_man/non-lin-svm-rbf-as-basis.png}%
\end{center}
\end{column}
\begin{column}{0.4\textwidth}

\footnotesize

\lz

All support vectors are assigned RBF "bumps", these are weighted with the dual variables / Lagrange multipliers $\alpha_i$ and labels $\yi$.
We then "mix" these bumps together to form the decision score function.
Which becomes a bumpy surface.


\end{column}
\end{columns}

\end{frame}

\begin{vbframe}{RBF Kernel Width}

A large $\sigma$ (or a small $\gamma$) will make the decision boundary very smooth and in the limit almost linear. 

\begin{center}
\includegraphics[width = 11cm ]{figure_man/kernel_width1.png}
\end{center}

\framebreak

A small $\sigma$ parameter makes the function more \enquote{wiggly}, in the limit we totally over fit the data by basically modelling each training data point - and maximal uncertainty at all other test points.

\begin{center}
\includegraphics[width = 11cm ]{figure_man/kernel_width2.png}
\end{center}

\end{vbframe}
\endlecture
\end{document}
